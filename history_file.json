 2/1:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    chanel_list = []
    for f in fiel_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                chanel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arrange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_directory

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
 2/2:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import fiona as fio
import matplotlib as plt
 2/3:
# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.
# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.
%matplotlib notebook
 2/4:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    chanel_list = []
    for f in fiel_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                chanel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arrange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_directory

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
 2/5:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import fiona as fio
import matplotlib as plt
 2/6:
# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.
# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.
%matplotlib notebook
 3/1:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import fiona as fio
import matplotlib as plt
 3/2:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    chanel_list = []
    for f in fiel_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                chanel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arrange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_directory

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
 3/3:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    chanel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                chanel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arrange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_directory

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
 3/4:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                chanel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arrange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_directory

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
 3/5:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                chanel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arrange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_directory

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
 3/6:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                chanel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arrange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_directory

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
 3/7:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arrange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_directory

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
 3/8:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_directory

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
 3/9:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_dictionary

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
3/10:
# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.
# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.
%matplotlib notebook
3/11:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
    test = np.random.randint(low =0, high = 255, size =(200,200))
    show_band(test, color_map = 'winter')
3/12:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
    test = np.random.randint(low =0, high = 255, size =(200,200))
    show_band(test, color_map = 'winter')
3/13:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
3/14:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import fiona as fio
import matplotlib.pyplot as plt
3/15:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
    test = np.random.randint(low =0, high = 255, size =(200,200))
    show_band(test, color_map = 'winter')
3/16:
# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.
# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.
%matplotlib notebook
3/17:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import fiona as fio
import matplotlib.pyplot as plt
3/18:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_dictionary

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
3/19:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
    test = np.random.randint(low =0, high = 255, size =(200,200))
    show_band(test, color_map = 'winter')
3/20:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
3/21:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import fiona as fio
import matplotlib.pyplot as plt
 4/1:
# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.
# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.
%matplotlib notebook
 4/2:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import fiona as fio
import matplotlib.pyplot as plt
 4/3:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import fiona as fio
import matplotlib.pyplot as plt
 4/4:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_dictionary

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
 4/5:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
    test = np.random.randint(low =0, high = 255, size =(200,200))
    show_band(test, color_map = 'winter')
 4/6:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
 4/7:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona as fio
 4/8:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_dictionary

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
 4/9:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
    test = np.random.randint(low =0, high = 255, size =(200,200))
    show_band(test, color_map = 'winter')
4/10:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
 5/1:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona as fio
 6/1:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona as fio
 6/2:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
    test = np.random.randint(low =0, high = 255, size =(200,200))
    show_band(test, color_map = 'winter')
 6/3:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
 6/4:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_dictionary

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
 6/5:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
    test = np.random.randint(low =0, high = 255, size =(200,200))
    show_band(test, color_map = 'winter')
 6/6:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
 6/7:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
test = np.random.randint(low =0, high = 255, size =(200,200))
show_band(test, color_map = 'winter')
 6/8:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
 6/9:
def show_band(band, color_map = 'gray', remove_negative =True):
    matrix = band.astype(float)
    if remove_negative:
        matrix[matrix<=0] = np.nan
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(matrix)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
6/10:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
6/11:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
 7/1:
def clip_area(vactor_file, raster_file, save_image_to):
    with fio.open(vector_file, 'r') as clipper:
        geometry = [feature["geometry"] for feature in clipper]
        
    with rio.open(raster_file, 'r') as raster_source
        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)
        metadata = raster._source.meta.copy()
        
    metadata.update({"driver": "GTiff",
                     "height": clipped_image.shape[1],
                     "width": clipped_image.shape[2],
                     "transform":transform})
    with rio.open(save_image_to, "w", **metadata) as g_tiff:
        g_tiff.write(clipped_image)
 7/2:
def clip_area(vactor_file, raster_file, save_image_to):
    with fio.open(vector_file, 'r') as clipper:
        geometry = [feature["geometry"] for feature in clipper]
        
    with rio.open(raster_file, 'r') as raster_source:
        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)
        metadata = raster._source.meta.copy()
        
    metadata.update({"driver": "GTiff",
                     "height": clipped_image.shape[1],
                     "width": clipped_image.shape[2],
                     "transform":transform})
    with rio.open(save_image_to, "w", **metadata) as g_tiff:
        g_tiff.write(clipped_image)
 7/3:
with rio.open(clipped_bands[5], 'r') as src:
    band_matrix = src.read(1)
show_band(band_matrix)
 7/4:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona as fio
 7/5:
with rio.open(clipped_bands[5], 'r') as src:
    band_matrix = src.read(1)
show_band(band_matrix)
 7/6: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
 7/7:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_dictionary

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
 7/8: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
 7/9:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
7/10:
clipped_bands = read_landsat_images('clipped/')
for band in clipped_bands:
    print(clipped_bands[band])
7/11:
with rio.open(clipped_bands[5], 'r') as src:
    band_matrix = src.read(1)
show_band(band_matrix)
7/12:
def clip_area(vactor_file, raster_file, save_image_to):
    with fio.open(vector_file, 'r') as clipper:
        geometry = [feature["geometry"] for feature in clipper]
        
    with rio.open(raster_file, 'r') as raster_source:
        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)
        metadata = raster._source.meta.copy()
        
    metadata.update({"driver": "GTiff",
                     "height": clipped_image.shape[1],
                     "width": clipped_image.shape[2],
                     "transform":transform})
    with rio.open(save_image_to, "w", **metadata) as g_tiff:
        g_tiff.write(clipped_image)
7/13: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
7/14:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
7/15:
clipped_bands = read_landsat_images('clipped/')
for band in clipped_bands:
    print(clipped_bands[band])
7/16:
with rio.open(clipped_bands[5], 'r') as src:
    band_matrix = src.read(1)
show_band(band_matrix)
7/17:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
7/18:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
test = np.random.randint(low =0, high = 255, size =(200,200))
show_band(test, color_map = 'winter')
7/19:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
7/20:
def show_band(band, color_map = 'gray', remove_negative =True):
    matrix = band.astype(float)
    if remove_negative:
        matrix[matrix<=0] = np.nan
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(matrix)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
7/21:
def clip_area(vactor_file, raster_file, save_image_to):
    with fio.open(vector_file, 'r') as clipper:
        geometry = [feature["geometry"] for feature in clipper]
        
    with rio.open(raster_file, 'r') as raster_source:
        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)
        metadata = raster._source.meta.copy()
        
    metadata.update({"driver": "GTiff",
                     "height": clipped_image.shape[1],
                     "width": clipped_image.shape[2],
                     "transform":transform})
    with rio.open(save_image_to, "w", **metadata) as g_tiff:
        g_tiff.write(clipped_image)
7/22: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
7/23:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
7/24:
clipped_bands = read_landsat_images('clipped/')
for band in clipped_bands:
    print(clipped_bands[band])
7/25:
with rio.open(clipped_bands[5], 'r') as src:
    band_matrix = src.read(1)
show_band(band_matrix)
7/26:
# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.
# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.
%matplotlib notebook
 8/1:
# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.
# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.
%matplotlib notebook
 8/2:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona as fio
 8/3:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_dictionary

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
 8/4:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
test = np.random.randint(low =0, high = 255, size =(200,200))
show_band(test, color_map = 'winter')
 8/5:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
 8/6:
def show_band(band, color_map = 'gray', remove_negative =True):
    matrix = band.astype(float)
    if remove_negative:
        matrix[matrix<=0] = np.nan
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(matrix)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
 8/7:
def clip_area(vactor_file, raster_file, save_image_to):
    with fio.open(vector_file, 'r') as clipper:
        geometry = [feature["geometry"] for feature in clipper]
        
    with rio.open(raster_file, 'r') as raster_source:
        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)
        metadata = raster._source.meta.copy()
        
    metadata.update({"driver": "GTiff",
                     "height": clipped_image.shape[1],
                     "width": clipped_image.shape[2],
                     "transform":transform})
    with rio.open(save_image_to, "w", **metadata) as g_tiff:
        g_tiff.write(clipped_image)
 8/8: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
 8/9:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
8/10:
clipped_bands = read_landsat_images('clipped/')
for band in clipped_bands:
    print(clipped_bands[band])
8/11:
with rio.open(clipped_bands[5], 'r') as src:
    band_matrix = src.read(1)
show_band(band_matrix)
8/12:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona as fio
8/13:
def clip_area(vactor_file, raster_file, save_image_to):
    with fio.open(vector_file, 'r') as clipper:
        geometry = [feature["geometry"] for feature in clipper]
        
    with rio.open(raster_file, 'r') as raster_source:
        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)
        metadata = raster._source.meta.copy()
        
    metadata.update({"driver": "GTiff",
                     "height": clipped_image.shape[1],
                     "width": clipped_image.shape[2],
                     "transform":transform})
    with rio.open(save_image_to, "w", **metadata) as g_tiff:
        g_tiff.write(clipped_image)
8/14:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
8/15:
with rio.open(clipped_bands[5], 'r') as src:
    band_matrix = src.read(1)
show_band(band_matrix)
8/16:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
8/17:
clipped_bands = read_landsat_images('clipped/')
for band in clipped_bands:
    print(clipped_bands[band])
8/18:
with rio.open(clipped_bands[5], 'r') as src:
    band_matrix = src.read(1)
show_band(band_matrix)
8/19:
def clip_area(vactor_file, raster_file, save_image_to):
    with fiona.open(vector_file, 'r') as clipper:
        geometry = [feature["geometry"] for feature in clipper]
        
    with rio.open(raster_file, 'r') as raster_source:
        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)
        metadata = raster._source.meta.copy()
        
    metadata.update({"driver": "GTiff",
                     "height": clipped_image.shape[1],
                     "width": clipped_image.shape[2],
                     "transform":transform})
    with rio.open(save_image_to, "w", **metadata) as g_tiff:
        g_tiff.write(clipped_image)
8/20: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
8/21:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
8/22:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
8/23:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_dictionary

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
8/24:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
test = np.random.randint(low =0, high = 255, size =(200,200))
show_band(test, color_map = 'winter')
8/25:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
8/26:
def show_band(band, color_map = 'gray', remove_negative =True):
    matrix = band.astype(float)
    if remove_negative:
        matrix[matrix<=0] = np.nan
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(matrix)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
8/27:
def clip_area(vactor_file, raster_file, save_image_to):
    with fiona.open(vector_file, 'r') as clipper:
        geometry = [feature["geometry"] for feature in clipper]
        
    with rio.open(raster_file, 'r') as raster_source:
        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)
        metadata = raster._source.meta.copy()
        
    metadata.update({"driver": "GTiff",
                     "height": clipped_image.shape[1],
                     "width": clipped_image.shape[2],
                     "transform":transform})
    with rio.open(save_image_to, "w", **metadata) as g_tiff:
        g_tiff.write(clipped_image)
8/28: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
8/29:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
8/30:
clipped_bands = read_landsat_images('clipped/')
for band in clipped_bands:
    print(clipped_bands[band])
8/31:
with rio.open(clipped_bands[5], 'r') as src:
    band_matrix = src.read(1)
show_band(band_matrix)
8/32:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
8/33:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
8/34:
clipped_bands = read_landsat_images('clipped/')
for band in clipped_bands:
    print(clipped_bands[band])
8/35:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
8/36:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
8/37:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
8/38:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
8/39:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
8/40:
clipped_bands = read_landsat_images('clipped/')
for band in clipped_bands:
    print(clipped_bands[band])
8/41:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
8/42:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
8/43:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
8/44:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
8/45:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
8/46:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
8/47:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
8/48:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
8/49:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
8/50:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_dictionary

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
8/51:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
test = np.random.randint(low =0, high = 255, size =(200,200))
show_band(test, color_map = 'winter')
8/52:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
8/53:
def show_band(band, color_map = 'gray', remove_negative =True):
    matrix = band.astype(float)
    if remove_negative:
        matrix[matrix<=0] = np.nan
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(matrix)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
8/54:
def clip_area(vactor_file, raster_file, save_image_to):
    with fiona.open(vector_file, 'r') as clipper:
        geometry = [feature["geometry"] for feature in clipper]
        
    with rio.open(raster_file, 'r') as raster_source:
        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)
        metadata = raster._source.meta.copy()
        
    metadata.update({"driver": "GTiff",
                     "height": clipped_image.shape[1],
                     "width": clipped_image.shape[2],
                     "transform":transform})
    with rio.open(save_image_to, "w", **metadata) as g_tiff:
        g_tiff.write(clipped_image)
8/55: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
8/56:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
8/57:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
8/58:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
 9/1:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
 9/2:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
 9/3:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_dictionary

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
 9/4:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
 9/5:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
10/1:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
10/2:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
11/1:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
11/2:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
11/3:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
11/4:
# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.
# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.
%matplotlib notebook
12/1:
# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.
# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.
%matplotlib notebook
12/2:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
12/3:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
12/4:
# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.
# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.
%matplotlib notebook
12/5:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
12/6:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = np.arange(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_dictionary

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
12/7:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
test = np.random.randint(low =0, high = 255, size =(200,200))
show_band(test, color_map = 'winter')
12/8:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
12/9:
def show_band(band, color_map = 'gray', remove_negative =True):
    matrix = band.astype(float)
    if remove_negative:
        matrix[matrix<=0] = np.nan
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(matrix)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
12/10:
def clip_area(vactor_file, raster_file, save_image_to):
    with fiona.open(vector_file, 'r') as clipper:
        geometry = [feature["geometry"] for feature in clipper]
        
    with rio.open(raster_file, 'r') as raster_source:
        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)
        metadata = raster._source.meta.copy()
        
    metadata.update({"driver": "GTiff",
                     "height": clipped_image.shape[1],
                     "width": clipped_image.shape[2],
                     "transform":transform})
    with rio.open(save_image_to, "w", **metadata) as g_tiff:
        g_tiff.write(clipped_image)
12/11: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
12/12:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
12/13:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
12/14:
def clip_area(vector_file, raster_file, save_image_to):
    with fiona.open(vector_file, 'r') as clipper:
        geometry = [feature["geometry"] for feature in clipper]
        
    with rio.open(raster_file, 'r') as raster_source:
        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)
        metadata = raster._source.meta.copy()
        
    metadata.update({"driver": "GTiff",
                     "height": clipped_image.shape[1],
                     "width": clipped_image.shape[2],
                     "transform":transform})
    with rio.open(save_image_to, "w", **metadata) as g_tiff:
        g_tiff.write(clipped_image)
12/15: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
12/16:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
12/17:
def clip_area(vector_file, raster_file, save_image_to):
    with fiona.open(vector_file, 'r') as clipper:
        geometry = [feature["geometry"] for feature in clipper]
        
    with rio.open(raster_file, 'r') as raster_source:
        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)
        metadata = raster_source.meta.copy()
        
    metadata.update({"driver": "GTiff",
                     "height": clipped_image.shape[1],
                     "width": clipped_image.shape[2],
                     "transform":transform})
    with rio.open(save_image_to, "w", **metadata) as g_tiff:
        g_tiff.write(clipped_image)
12/18: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
12/19:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
12/20:
clipped_bands = read_landsat_images('clipped/')
for band in clipped_bands:
    print(clipped_bands[band])
12/21:
with rio.open(clipped_bands[5], 'r') as src:
    band_matrix = src.read(1)
show_band(band_matrix)
12/22:
def calculate_index(index_name, landsat_8_bands):
    indexes = {
        'ndvi': (5,4),
        'ndbi': (6,5),
        'ndwi': (3,6)
    }
    
    if index_name in indexes:
        bands = indexes[index_name]
        
        with rio.open(landsat_8_bands[bands[0]]) as a:
            band_a = (a.read()[0]/1000)astype(np.float)
        with rio.open(landsat_8_bands[bands[1]]) ab b:
            band_b = (b.read()[1]/1000)astype(np.float)
            
        numerator = band_a - band_b
        denominator = band_a + band_b
        
        idx = numerator/denominator
        idx[idx >1] = 1
        idx[idx <-1] = -1
        return idx
    else:
        raise ValueError('Brak wskaźnika do wyboru, dostępne wskaźniki to: ndbi, ndvi i ndwi')
12/23:
def calculate_index(index_name, landsat_8_bands):
    indexes = {
        'ndvi': (5,4),
        'ndbi': (6,5),
        'ndwi': (3,6)
    }
    
    if index_name in indexes:
        bands = indexes[index_name]
        
        with rio.open(landsat_8_bands[bands[0]]) as a:
            band_a = (a.read()[0]/1000).astype(np.float)
        with rio.open(landsat_8_bands[bands[1]]) ab b:
            band_b = (b.read()[1]/1000).astype(np.float)
            
        numerator = band_a - band_b
        denominator = band_a + band_b
        
        idx = numerator/denominator
        idx[idx >1] = 1
        idx[idx <-1] = -1
        return idx
    else:
        raise ValueError('Brak wskaźnika do wyboru, dostępne wskaźniki to: ndbi, ndvi i ndwi')
12/24:
def calculate_index(index_name, landsat_8_bands):
    indexes = {
        'ndvi': (5,4),
        'ndbi': (6,5),
        'ndwi': (3,6)
    }
    
    if index_name in indexes:
        bands = indexes[index_name]
        
        with rio.open(landsat_8_bands[bands[0]]) as a:
            band_a = (a.read()[0]/1000).astype(np.float)
        with rio.open(landsat_8_bands[bands[1]]) as b:
            band_b = (b.read()[1]/1000).astype(np.float)
            
        numerator = band_a - band_b
        denominator = band_a + band_b
        
        idx = numerator/denominator
        idx[idx >1] = 1
        idx[idx <-1] = -1
        return idx
    else:
        raise ValueError('Brak wskaźnika do wyboru, dostępne wskaźniki to: ndbi, ndvi i ndwi')
12/25:
ndvi = calculate_index('ndvi', clipped_bands)
ndvi [ndvi ==0] =-1
show_band(ndvi, color_map ='virdis', remove_negative =True)
12/26:
ndwi = calculate_index('ndwi', clipped_bands)
ndwi [ndwi ==0] =np.nan
shwow_band(ndwi, color_map = 'virdis', remove_negative =False)
12/27:
def calculate_index(index_name, landsat_8_bands):
    indexes = {
        'ndvi': (5,4),
        'ndbi': (6,5),
        'ndwi': (3,6)
    }
    
    if index_name in indexes:
        bands = indexes[index_name]
        
        with rio.open(landsat_8_bands[bands[0]]) as a:
            band_a = (a.read()[0]/1000).astype(np.float)
        with rio.open(landsat_8_bands[bands[1]]) as b:
            band_b = (b.read()[0]/1000).astype(np.float)
            
        numerator = band_a - band_b
        denominator = band_a + band_b
        
        idx = numerator/denominator
        idx[idx >1] = 1
        idx[idx <-1] = -1
        return idx
    else:
        raise ValueError('Brak wskaźnika do wyboru, dostępne wskaźniki to: ndbi, ndvi i ndwi')
12/28:
ndvi = calculate_index('ndvi', clipped_bands)
ndvi [ndvi ==0] =-1
show_band(ndvi, color_map ='virdis', remove_negative =True)
12/29:
ndvi = calculate_index('ndvi', clipped_bands)
ndvi [ndvi ==0] =-1
show_band(ndvi, color_map ='viridis', remove_negative =True)
12/30:
ndwi = calculate_index('ndwi', clipped_bands)
ndwi [ndwi ==0] =np.nan
shwow_band(ndwi, color_map = 'virdis', remove_negative =False)
12/31:
ndwi = calculate_index('ndwi', clipped_bands)
ndwi [ndwi ==0] =np.nan
show_band(ndwi, color_map = 'viridis', remove_negative =False)
12/32:
ndbi = calculate_index('ndbi', clipped_bands)
ndbi [ndbi ==0] =np.nan
shwow_band(ndbi, color_map = 'virdis', remove_negative =False)
12/33:
ndbi = calculate_index('ndbi', clipped_bands)
ndbi [ndbi ==0] =np.nan
shwow_band(ndbi, color_map = 'viridis', remove_negative =False)
12/34:
ndbi = calculate_index('ndbi', clipped_bands)
ndbi [ndbi ==0] =np.nan
show_band(ndbi, color_map = 'viridis', remove_negative =False)
12/35: show_band(ndbi -ndvi, color_map ='virdis', remove_negative = False)
12/36: show_band(ndbi -ndvi, color_map ='viridis', remove_negative = False)
13/1:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = range(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_dictionary

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
13/2:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
13/3:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = range(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_dictionary

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
13/4:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
test = np.random.randint(low =0, high = 255, size =(200,200))
show_band(test, color_map = 'winter')
13/5:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
13/6:
def show_band(band, color_map = 'gray', remove_negative =True):
    matrix = band.astype(float)
    if remove_negative:
        matrix[matrix<=0] = np.nan
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(matrix)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
13/7:
def clip_area(vector_file, raster_file, save_image_to):
    with fiona.open(vector_file, 'r') as clipper:
        geometry = [feature["geometry"] for feature in clipper]
        
    with rio.open(raster_file, 'r') as raster_source:
        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)
        metadata = raster_source.meta.copy()
        
    metadata.update({"driver": "GTiff",
                     "height": clipped_image.shape[1],
                     "width": clipped_image.shape[2],
                     "transform":transform})
    with rio.open(save_image_to, "w", **metadata) as g_tiff:
        g_tiff.write(clipped_image)
13/8: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
13/9:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
13/10:
clipped_bands = read_landsat_images('clipped/')
for band in clipped_bands:
    print(clipped_bands[band])
13/11:
with rio.open(clipped_bands[5], 'r') as src:
    band_matrix = src.read(1)
show_band(band_matrix)
13/12:
def calculate_index(index_name, landsat_8_bands):
    indexes = {
        'ndvi': (5,4),
        'ndbi': (6,5),
        'ndwi': (3,6)
    }
    
    if index_name in indexes:
        bands = indexes[index_name]
        
        with rio.open(landsat_8_bands[bands[0]]) as a:
            band_a = (a.read()[0]/1000).astype(np.float)
        with rio.open(landsat_8_bands[bands[1]]) as b:
            band_b = (b.read()[0]/1000).astype(np.float)
            
        numerator = band_a - band_b
        denominator = band_a + band_b
        
        idx = numerator/denominator
        idx[idx >1] = 1
        idx[idx <-1] = -1
        return idx
    else:
        raise ValueError('Brak wskaźnika do wyboru, dostępne wskaźniki to: ndbi, ndvi i ndwi')
13/13:
ndvi = calculate_index('ndvi', clipped_bands)
ndvi [ndvi ==0] =-1
show_band(ndvi, color_map ='viridis', remove_negative =True)
13/14:
ndwi = calculate_index('ndwi', clipped_bands)
ndwi [ndwi ==0] =np.nan
show_band(ndwi, color_map = 'viridis', remove_negative =False)
13/15:
ndbi = calculate_index('ndbi', clipped_bands)
ndbi [ndbi ==0] =np.nan
show_band(ndbi, color_map = 'viridis', remove_negative =False)
13/16: show_band(ndbi -ndvi, color_map ='viridis', remove_negative = False)
13/17:
# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.
# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.
%matplotlib notebook
13/18:
import os
import numpy as np
import rasterio as rio
import rasterio.mask as rmask
import matplotlib.pyplot as plt
import fiona
13/19:
def read_landsat_images (folder_name):
    file_list = os.listdir(folder_name)
    channel_list = []
    for f in file_list:
        if (f.startswith('LC') and f.endswith('.tif')):
            if  'band' in f:
                channel_list.append(folder_name +f)
    channel_list.sort()
    channel_numbers = range(1,8)
    bands_dictionary = dict(zip(channel_numbers, channel_list))
    return bands_dictionary

#test
satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')
for band in satelite_images:
    print(band, satelite_images[band])
13/20:
def show_band(band, color_map = 'gray'):
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(band)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
    
    #test
test = np.random.randint(low =0, high = 255, size =(200,200))
show_band(test, color_map = 'winter')
13/21:
band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
with rio.open(band_list[5], 'r') as src:
    band_matrix = src.read(1)

show_band(band_matrix)
13/22:
def show_band(band, color_map = 'gray', remove_negative =True):
    matrix = band.astype(float)
    if remove_negative:
        matrix[matrix<=0] = np.nan
    fig = plt.figure(figsize=(8,8))
    image_layer = plt.imshow(matrix)
    image_layer.set_cmap(color_map)
    plt.colorbar()
    plt.show()
13/23:
def clip_area(vector_file, raster_file, save_image_to):
    with fiona.open(vector_file, 'r') as clipper:
        geometry = [feature["geometry"] for feature in clipper]
        
    with rio.open(raster_file, 'r') as raster_source:
        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)
        metadata = raster_source.meta.copy()
        
    metadata.update({"driver": "GTiff",
                     "height": clipped_image.shape[1],
                     "width": clipped_image.shape[2],
                     "transform":transform})
    with rio.open(save_image_to, "w", **metadata) as g_tiff:
        g_tiff.write(clipped_image)
13/24: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')
13/25:
vector = 'vector/znin_powiat.shp'
clipped_folder = 'clipped/'
for band in bands:
    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'
    clip_area(vector, bands[band], destination)
13/26:
clipped_bands = read_landsat_images('clipped/')
for band in clipped_bands:
    print(clipped_bands[band])
13/27:
with rio.open(clipped_bands[5], 'r') as src:
    band_matrix = src.read(1)
show_band(band_matrix)
13/28:
def calculate_index(index_name, landsat_8_bands):
    indexes = {
        'ndvi': (5,4),
        'ndbi': (6,5),
        'ndwi': (3,6)
    }
    
    if index_name in indexes:
        bands = indexes[index_name]
        
        with rio.open(landsat_8_bands[bands[0]]) as a:
            band_a = (a.read()[0]/1000).astype(np.float)
        with rio.open(landsat_8_bands[bands[1]]) as b:
            band_b = (b.read()[0]/1000).astype(np.float)
            
        numerator = band_a - band_b
        denominator = band_a + band_b
        
        idx = numerator/denominator
        idx[idx >1] = 1
        idx[idx <-1] = -1
        return idx
    else:
        raise ValueError('Brak wskaźnika do wyboru, dostępne wskaźniki to: ndbi, ndvi i ndwi')
13/29:
ndvi = calculate_index('ndvi', clipped_bands)
ndvi [ndvi ==0] =-1
show_band(ndvi, color_map ='viridis', remove_negative =True)
13/30:
ndwi = calculate_index('ndwi', clipped_bands)
ndwi [ndwi ==0] =np.nan
show_band(ndwi, color_map = 'viridis', remove_negative =False)
13/31:
ndbi = calculate_index('ndbi', clipped_bands)
ndbi [ndbi ==0] =np.nan
show_band(ndbi, color_map = 'viridis', remove_negative =False)
13/32: show_band(ndbi -ndvi, color_map ='viridis', remove_negative = False)
14/1: import numpy as np
14/2:
a = np.array([1,2,3])
a+a
14/3:
def print_array_details(a):
    print('Dimensions: %d, shape: %s, dtype: %s' % (a.ndim, a.shape, a.dtype))
14/4:
def print_array_details(a):
    print('Dimensions: %d, shape: %s, dtype: %s' % (a.ndim, a.shape, a.dtype))
14/5: print_array_details(a)
14/6: a = np.array([1,2,3,4,5,6,7,8])
14/7: a = np.array([1,2,3,4,5,6,7,8])
14/8: a
14/9: print_array_details(a)
14/10: print_array_details(a)
14/11: print_array_details(a)
14/12: a = a.reshape([2,4])
14/13: a
14/14: print_array_details(a)
14/15: a=a.reshape([2,2,2])
14/16: a
14/17: print_array_details(a)
14/18: x = np.array([[1,2,3] , [4,5,6]], np.int32)
14/19: x.shape
14/20: x.shape=(6,)
14/21: x
14/22: x = x.astype('int64')
14/23: x
14/24: x.dtype
14/25: a = np.zeros([2,3])
14/26: a
14/27: a.dtype
14/28: np.ones([2,3])
14/29: empty_array = np.empty((2,3))
14/30: empty_array
14/31: np.random.random((2,3))
14/32: np.linspace(2, 10, 5)
14/33: np.arange(2,10,2)
14/34: a = np.array([1,2,3,4,5,6])
14/35: a[2]
14/36: a[3:5]
14/37: a[:4:2]
14/38: a[:4:2] = 0
14/39: a
14/40: a[::-1]
14/41: a = np.arange(8)
14/42: a
14/43: a.shape = (2,2,2)
14/44: a
14/45: a[1]
14/46: a*2
14/47: a-2
14/48: a/2.0
14/49: a = np.array([45, 65, 76, 32, 99, 22])
14/50: a<50
14/51: a = np.arange(8).reshape((2,4))
14/52: a
14/53: a.min(axis=1)
14/54: a.min(axis=2)
14/55: a.min
14/56: a.min()
14/57: a.min(axis=0)
14/58: a.sum(axis=0)
14/59: a.sum(axis=1)
14/60: a.mean(axis=1)
14/61: a.mean(axis=0)
14/62: a.std(axis=1)
15/1: ufo_path='/users/wioletanytko/documents/worskpace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'
15/2: ufo_path='/users/wioletanytko/documents/worskpace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'
15/3: ufo_df = pd.read_csv(ufo_path, delimiter='\t')
15/4: import pandas an pd
15/5: import pandas as pd
15/6: ufo_df = pd.read_csv(ufo_path, delimiter='\t')
15/7: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'
15/8: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'
15/9: ufo_df = pd.read_csv(ufo_path, delimiter='\t')
18/1: ufo_df = pd.read_csv(ufo_path, delimiter='\t', nrows=5)
18/2: import pandas as pd
18/3: ufo_df = pd.read_csv(ufo_path, delimiter='\t', nrows=5)
18/4: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'
18/5: ufo_df = pd.read_csv(ufo_path, delimiter='\t', nrows=5)
18/6: ufo_df
18/7: ufo_df = pd.read_csv(ufo_path, delimiter='\t', nrows=5, header=None)
18/8: ufo_df
18/9: ufo_df = pd.read_csv(ufo_path, delimiter='\t', nrows=5, header=None, names=['date1', 'date2','2','3','4','5'])
18/10: ufo_df
18/11: ufo_df = pd.read_csv(ufo_path, delimiter='\t', nrows=5, header=None, names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'])
18/12: ufo_df
18/13: ufo_df = pd.read_csv(ufo_path, delimiter='\t', nrows=10, header=None, names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'])
18/14: ufo_df
18/15:
ufo_df = pd.read_csv(
    ufo_path, delimiter='\t',
    nrows=100,
    header=None,
    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],
    error_bad_lines=False)
18/16: ufo_df
18/17: head(ufo_df)
18/18: pd.head(ufo_df)
18/19: ufo_df.head
18/20: ufo_df.head()
18/21: ufo_df.head(10)
18/22:
ufo_df = pd.read_csv(
    ufo_path, delimiter='\t',
    nrows=1000,
    header=None,
    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],
    error_bad_lines=False)
18/23: ufo_df
18/24: ufo_df[900]
18/25: ufo_df.columns
18/26: ufo_df.iloc(2)
18/27: ufo_df.index
18/28: ufo_df.loc(1)
18/29: ufo_df.loc[1]
18/30: ufo_df.loc[0]
18/31: ufo_df.set_index('Date_occured')
18/32: df.loc['19951009']
18/33: ufo_df.loc['19951009']
18/34: ufo_df.loc[19951009]
18/35: ufo_df.loc['19951009']
18/36: ufo_df.iloc['19951009']
18/37: ufo_df.loc['19951010']
18/38: ufo_df.loc[19951010]
18/39: ufo_df.loc[19930701]
18/40: ufo_df.loc['19930701']
18/41: ufo_df.columns
18/42: ufo_df.loc(19951009)
18/43: ufo_df.loc('19951009')
18/44: ufo_df.loc[19951009]
18/45: ufo_df.loc['19951009']
18/46: ufo_df.loc[19951009]
18/47: ufo_df.loc['19951009']
18/48: ufo_df.set_index('Date_occured')
18/49: ufo_df.loc['19951009']
18/50: ufo_df.loc[19951009]
18/51: ufo_df.reset_index()
18/52: ufo_df.loc[999]
18/53: ufo_df.set_index('Location')
18/54: ufo_df.loc['Iowa City, IA']
18/55: ufo_df.iloc['Iowa City, IA']
18/56: ufo_df.reset_index
18/57: ufo_df.set_index('Short_description')
18/58: ufo_df.iloc['triangle']
18/59: ufo_df.dtypes()
18/60: ufo_df.dtypes
18/61: ufo_df.loc['light']
18/62: ufo_df.reset_index()
18/63: ufo_df.set_index('Date_occured')
18/64: ufo_df.loc[19960305]
18/65: ufo_df.reset_index()
18/66: ufo_df.loc[0]
18/67: ufo_df.loc[0]['Long_description']
18/68: ufo_df.loc[0]['Date_occured']
18/69: ufo_df.set_index('Date_occured')
18/70: ufo_df.loc[19951009]
18/71: ufo_df.loc["19951009"]
18/72: ufo_df.loc["19951009"]['Short_description']
18/73: ufo_df.loc['19951009']['Short_description']
18/74: ufo_df.loc[19951009]['Short_description']
18/75: ufo_df.index
18/76: ufo_df.set_index('Date_occured')
18/77: ufo_df.index
18/78: ufo_df.loc[0]
18/79: ufo_df.set_index('Date_occured', inplace=True)
18/80: ufo_df.loc[19951009]
18/81: ufo_df.loc[19951009:19951010]
18/82: ufo_df.loc[19951009:19951011]
18/83: ufo_df.index
18/84: ufo_df[19951009:19951010]
18/85: ufo_df.loc[19951009:19951010]
18/86: ufo_df.loc[19951010]
18/87: ufo_df.reset_index()
18/88: ufo_df.loc[0]
18/89: ufo_df.groupby('Date_occured')
18/90: df.groups.keys()
18/91: ufo_df.groups.keys()
18/92: ufo_df.group.keys()
18/93: ufo_df.groups.keys()
18/94: ufo_df = ufo_df.groupby('Date_occured')
18/95: ufo_df.groups.keys()
18/96: ufo_df.groups.keys().len()
18/97: len(ufo_df.groups.keys())
18/98: ufo_df.groups.keys()
18/99: 19970404_group = ufo_df.get_group(19970404)
18/100: april_group = ufo_df.get_group(19970404)
18/101: april_group.head()
18/102: ufo_df
18/103: ufo_df.head()
18/104: ufo_df.reset_index()
17/1: import pandas as pd
17/2:
ufo_df = pd.read_csv(
    ufo_path, delimiter='\t',
    header=None,
    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],
    error_bad_lines=False)
17/3: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'
17/4:
ufo_df = pd.read_csv(
    ufo_path, delimiter='\t',
    header=None,
    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],
    error_bad_lines=False)
17/5: ufo_df.head()
19/1: ufo_df.reset_index()
20/1: import pandas as pd
20/2: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'
20/3:
ufo_df = pd.read_csv(
    ufo_path, delimiter='\t',
    nrows=5,
    header=None,
    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],
    error_bad_lines=False)
20/4: ufo_df.head()
20/5: ufo_df.loc[0]['Date_occured']
20/6: pd.to_datetime(ufo_df.loc[0]['Date_occured'])
20/7: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')
20/8: ufo_df['Date_occured']
20/9: ufo_df.head()
20/10: ufo_df['Date_occured']=pd.ufo_df['Date_occured', format='%Y%m%d']
20/11: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')
20/12: ufo_df.head()
20/13: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'],ufo_df['Date_reported'], format='%Y%m%d')
20/14: ufo_df.head()
20/15: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')
20/16: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')
20/17: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')
20/18: ufo_df.head()
20/19: ufo_df['Date_reported']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')
20/20: import pandas as pd
20/21: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'
20/22:
ufo_df = pd.read_csv(
    ufo_path, delimiter='\t',
    nrows=5,
    header=None,
    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],
    error_bad_lines=False)
20/23: ufo_df.head()
20/24: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')
20/25: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')
20/26: ufo_df['Date_reported']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')
20/27: ufo_df.head()
21/1: import pandas as pd
21/2: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'
21/3:
ufo_df = pd.read_csv(
    ufo_path, delimiter='\t',
    nrows=5,
    header=None,
    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],
    error_bad_lines=False)
21/4: ufo_df.head()
21/5: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')
21/6: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')
21/7: ufo_df['Date_reported']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')
21/8: ufo_df.head()
21/9: import pandas as pd
21/10: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'
21/11:
ufo_df = pd.read_csv(
    ufo_path, delimiter='\t',
    header=None,
    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],
    error_bad_lines=False)
21/12: ufo_df.head()
21/13: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')
21/14: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')
21/15: ufo_df.loc[pd.str.len(ufo_df['Date_occured'])>8]
21/16: ufo_df.loc[str.len(ufo_df['Date_occured'])>8]
21/17: ufo_df.loc[ufo_df['Date_occured'].str.len()>8]
21/18: ufo_df.loc[ufo_df['Date_occured'].str.len()>8]
21/19: import pandas as pd
21/20: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'
21/21:
ufo_df = pd.read_csv(
    ufo_path, delimiter='\t',
    header=None,
    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],
    error_bad_lines=False)
21/22: ufo_df.head()
21/23: ufo_df.loc[ufo_df['Date_occured'].str.len()>8]
21/24: ufo_df.index()
21/25: ufo_df.columns
21/26: ufo_df.dtypes
21/27: ufo_df.loc[ufo_df['Date_occured']>99999999]
21/28: bad_date = ufo_df.loc[ufo_df['Date_occured']>99999999]
21/29: bad_date.head()
21/30: bad_date = ufo_df.loc[ufo_df['Date_occured']>100]
21/31: bad_date = ufo_df.loc[ufo_df['Date_occured']>100]
21/32: bad_date.head()
21/33: bad_date = ufo_df.loc[ufo_df['Date_occured']>19951009]
21/34: bad_date.head()
21/35: ufo_df['Date_occured_len']=ufo_df.loc['Date_occured'].astype(str).len()
21/36: ufo_df['Date_occured_len']=0#ufo_df.loc['Date_occured'].astype(str).len()
21/37: ufo_df.head
21/38: ufo_df.head()
21/39: ufo_df['Date_occured_len']=ufo_df.loc['Date_occured'].astype(str).map(len)
21/40: ufo_df['Date_occured_len']=ufo_df['Date_occured'].astype(str).map(len)
21/41: ufo_df.head()
21/42: ufo_df['Date_occured_len']=ufo_df['Date_occured'].astype(str).len()
21/43: ufo_df['Date_occured_len']=ufo_df['Date_occured'].astype(str).str.len()
21/44: ufo_df.head()
21/45: ufo_df.head()
21/46: bad_date_df = ufo_df.loc[ufo_df['Date_occured_len'] != 8]
21/47: bad_date.head()
21/48: bad_date_df.head()
21/49: bad_date_df
21/50: bad_date_df.info
21/51: bad_date_df.info()
21/52: (ufo_df['Date_occured_len'] != 8).count()
21/53: (ufo_df['Date_occured_len'] == 8).count()
21/54: [ufo_df['Date_occured_len'] == 8].count()
21/55: ufo_df[ufo_df['Date_occured_len'] == 8].count()
21/56: ufo_df[ufo_df['Date_occured_len'] == 8].count()['Date_occured_len']
21/57: ufo_df[ufo_df['Date_occured_len'] != 8].count()['Date_occured_len']
21/58: ufo_df['Date_reported_len']=ufo_df['Date_reported'].astype(str).str.len()
21/59: bad_date_df = ufo_df.loc[(ufo_df['Date_occured_len'] != 8) | (udo_df['Date_reported_len'] != 8)]
21/60: bad_date_df = ufo_df.loc[(ufo_df['Date_occured_len'] != 8) | (ufo_df['Date_reported_len'] != 8)]
21/61: bad_date_df
21/62: bad_date_df.info()
21/63: ufo_df[ufo_df['Date_occured_len'] != 8].count()['Date_occured_len']
21/64: ufo_df[(ufo_df['Date_reported_len'] != 8) | (ufo_df[''])].count()['Date_reported_len']
21/65: ufo_df[ufo_df['Date_reported_len'] != 8].count()['Date_reported_len']
21/66: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')
21/67: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')
21/68: ufo_df.drop(ufo_df[ufo_df.Date_occured_len !=8].index, inplace=True)
21/69: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')
21/70: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')
21/71: ufo_df['Date_occured'].astype(str).str.contains('00')
21/72: zerodate_df = ufo_df['Date_occured'].astype(str).str.contains('00')
21/73: zerodate
21/74: zerodate_df
21/75: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.contains('00')
21/76: ufo_df
21/77: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.contains('0000')
21/78: ufo_df
21/79: ufo_df.loc[ufo_df['zerodate'] == True]
21/80: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.contains(r'....00..')
21/81: ufo_df.loc[ufo_df['zerodate'] == True]
21/82: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.contains(r'....00..')
21/83: ufo_df.loc[ufo_df['zerodate'] == True]
21/84: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.contains(r'.00..')
21/85: ufo_df.loc[ufo_df['zerodate'] == True]
21/86: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'00..')
21/87: ufo_df.loc[ufo_df['zerodate'] == True]
21/88: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00..')
21/89: ufo_df.loc[ufo_df['zerodate'] == True]
21/90: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00.')
21/91: ufo_df.loc[ufo_df['zerodate'] == True]
21/92: ufo_df.loc[ufo_df['zerodate'] == True]
21/93: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'00')
21/94: ufo_df.loc[ufo_df['zerodate'] == True]
21/95: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'00..')
21/96: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'..00..')
21/97: ufo_df.loc[ufo_df['zerodate'] == True]
21/98: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00')
21/99: ufo_df.loc[ufo_df['zerodate'] == True]
21/100: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'..00')
21/101: ufo_df.loc[ufo_df['zerodate'] == True]
21/102: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00..')
21/103: ufo_df.loc[ufo_df['zerodate'] == True]
21/104: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00')
21/105: ufo_df.loc[ufo_df['zerodate'] == True]
21/106: ufo_df.loc[ufo_df['zerodate'] == True]
21/107: ufo_df.loc[ufo_df['zerodate'] == True]
21/108: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'...00')
21/109: ufo_df.loc[ufo_df['zerodate'] == True]
21/110: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00')
21/111: ufo_df.loc[ufo_df['zerodate'] == True]
21/112: ufo_df.drop(ufo_df.loc['zerodate'] == True)
21/113: ufo_df.drop(ufo_df[ufo_df.loc['zerodate'] == True])
21/114: ufo_df.drop(ufo_df.loc[ufo_df['zerodate'] == True])
21/115: ufo_df.drop(ufo_df.loc[ufo_df['zerodate'].index == True])
21/116: ufo_df.drop(ufo_df.loc[ufo_df['zerodate'] == True].index, inplace=True)
21/117: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')
21/118: ufo_df.Date_occured
21/119: ufo_df.sort_values(by=['Date_occured'])
21/120: ufo_df.sort_values(by=['Date_occured']).head()
21/121: ufo_df.drop(40901, inplace=True)
21/122: ufo_df.sort_values(by=['Date_occured']).head()
21/123: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')
21/124: ufo_df['Date_reported']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')
21/125: ufo_df.head()
21/126: bad_date_df.head()
21/127: mask = ufo_df.Date_occured>1999-01-01
21/128: mask = ufo_df.Date_occured>(1999-01-01)
21/129: mask = ufo_df.Date_occured>('1999-01-01')
21/130: mask
21/131: since_1999 = ufo_df[mask]
21/132: since_1999.head()
21/133: ufo_df.describe()
21/134: ufo_df.describe(include=['object'])
21/135: ufo_df.colums
21/136: ufo_df.columns
21/137: ufo_df.dtype
21/138: ufo_df.dtype()
21/139: ufo_df.dtypes()
21/140: ufo_df.dtypes
21/141: ufo_df.describe(include=['datetime64'])
21/142: import pandas as pd
21/143: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'
21/144:
ufo_df = pd.read_csv(
    ufo_path, delimiter='\t',
    header=None,
    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],
    error_bad_lines=False)
21/145: ufo_df.head()
21/146: ufo_df.dtypes
21/147: bad_date = ufo_df.loc[ufo_df['Date_occured']>19951009]
21/148: bad_date.head()
21/149: ufo_df['Date_occured_len']=ufo_df['Date_occured'].astype(str).str.len()
21/150: ufo_df['Date_reported_len']=ufo_df['Date_reported'].astype(str).str.len()
21/151: ufo_df.head()
21/152: bad_date_df.head()
21/153: bad_date_df = ufo_df.loc[(ufo_df['Date_occured_len'] != 8) | (ufo_df['Date_reported_len'] != 8)]
21/154: bad_date_df.head()
21/155: bad_date_df.tail()
21/156: bad_date_df.sort_values(by=['Date_occured_len'])
21/157: bad_date_df.info()
21/158: ufo_df[ufo_df['Date_occured_len'] != 8].count()['Date_occured_len']
21/159: ufo_df[ufo_df['Date_reported_len'] != 8].count()['Date_reported_len']
21/160: ufo_df.drop(ufo_df[ufo_df.Date_occured_len !=8].index, inplace=True)
21/161: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')
21/162: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00')
21/163: ufo_df.loc[ufo_df['zerodate'] == True]
21/164: ufo_df.drop(ufo_df.loc[ufo_df['zerodate'] == True].index, inplace=True)
21/165: ufo_df.sort_values(by=['Date_occured']).head()
21/166: ufo_df.drop(40901, inplace=True)
21/167: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')
21/168: ufo_df['Date_reported']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')
21/169: ufo_df.head()
21/170: mask = ufo_df.Date_occured>('1999-01-01')
21/171: since_1999 = ufo_df[mask]
21/172: since_1999.head()
21/173: ufo_df.describe(include=['datetime64'])
21/174: ufo_df.dtypes
21/175: new=ufo_df['Location'].str.split(',')
21/176: new.head()
21/177: new.head(20)
21/178: new['len'] = new.str.len()
21/179: new.head()
21/180: newest = new.str.len()
21/181: newest.head()
21/182: newest.describe
21/183: newest.describe()
21/184: new=ufo_df['Location'].str.split(',', n=1)
21/185: newest = new.str.len()
21/186: newest.describe()
21/187: newest.head()
21/188: new.head()
21/189: new.sort_values()
21/190: new.loc[3394]
21/191: newest.sort_values()
21/192: newest.loc(19201)
21/193: new.loc(19201)
21/194: new.loc(61392)
21/195: new.head()
21/196: new.loc(20583)
21/197: new(20583)
21/198: new.loc(2)
21/199: new.head(2)
21/200: new.loc[2]
21/201: new.loc[19201]
21/202: new.loc[6295]
21/203: ufo_df.info
21/204: ufo_df.info()
21/205: new.head()
21/206: new.loc[0][0]
21/207: new.loc[0][1]
21/208: new.loc[0][0][1]
21/209: new.loc[0][0],new.loc[0][1]
21/210: ufo_df['City'] = new[0]
21/211: new.info()
21/212: new.len()
21/213: len(new)
21/214: ufo_df['City'] = new[0]
21/215: new[0]
21/216: ufo_df['City'][0] = 'Test'
21/217: ufo_df['City'] = new
21/218: ufo_df[['City', 'State']] = new([0,1])
21/219: ufo_df[['City', 'State']] = new[0,1]
21/220: ufo_df['City', 'State'] = new[0,1]
21/221: new.columns
21/222: new.head()
21/223: ufo_df['City'] = new[0]
21/224: ufo_df['City'] = new.str[0]
21/225: ufo_df.head()
21/226: ufo_df['State'] = new.str[1]
21/227: ufo_df.head()
21/228: ufo_df.drop(['Date_occured_len', 'Date_reported_len', 'zerodate'])
21/229: ufo_df.drop(['Date_occured_len', 'Date_reported_len', 'zerodate'], axis=1)
21/230: states_abbreviations = [ 'AL', 'AK', 'AS', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FM', 'FL', 'GA', 'GU', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MH', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'MP', 'OH', 'OK', 'OR', 'PW', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VI', 'VA', 'WA', 'WV', 'WI', 'WY' ]
21/231: states_mask = df.State in states_abbreaviations
21/232: states_mask = ufo_df.State in states_abbreaviations
21/233: states_mask = ufo_df.State in states_abbreviations
21/234: states_mask = ufo_df.State.isin(states_abbreviations)
21/235: states_mask.head()
21/236: ufo_df.States[0]
21/237: ufo_df.States[0, axis=1]
21/238: ufo_df.State[0]
21/239: ufo_df.str.strip()
21/240: ufo_df.State.str.strip()
21/241: states_mask = ufo_df.State.isin(states_abbreviations)
21/242: states_mask.head()
21/243: ufo_df.State[0]
21/244: ufo_df.State.str.strip()
21/245: states_abbreviations = [ 'AL', 'AK', 'AS', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FM', 'FL', 'GA', 'GU', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MH', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'MP', 'OH', 'OK', 'OR', 'PW', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VI', 'VA', 'WA', 'WV', 'WI', 'WY' ]
21/246: states_mask = ufo_df.State.isin(states_abbreviations)
21/247: states_mask.head()
21/248: ufo_df.State[0]
21/249: ufo_df.State.str.strip(inplace=True)
21/250: ufo_df.dtypes
21/251: ufo_df.State = ufo_df.State.astype(str)
21/252: ufo_df.dtypes
21/253: ufo_df.State.str.strip()
21/254: states_mask = ufo_df.State.isin(states_abbreviations)
21/255: states_mask.head()
21/256: ufo_df.State[0]
21/257: ufo_df.State = ufo_df.State.str.strip()
21/258: states_abbreviations = [ 'AL', 'AK', 'AS', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FM', 'FL', 'GA', 'GU', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MH', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'MP', 'OH', 'OK', 'OR', 'PW', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VI', 'VA', 'WA', 'WV', 'WI', 'WY' ]
21/259: states_mask = ufo_df.State.isin(states_abbreviations)
21/260: states_mask.head()
21/261: ufo_df.State[0]
21/262: ufo_us = ufo_df[states_mask]
21/263: ufo_us.head()
21/264: ufo_df.drop(['Date_occured_len', 'Date_reported_len', 'zerodate'], axis=1, inplace=True)
21/265: ufo_us = ufo_df[states_mask]
21/266: ufo_us.head()
21/267: ufo_us.info
21/268: ufo_us.info()
21/269: ufo_us.describe(inclue=['datetime64'])
21/270: ufo_us.describe(include=['datetime64'])
21/271: ufo_us.quantile(columns=['Date_occured'])
21/272: ufo_us.quantile()
21/273: ufo_us.quantile(axis=1)
21/274: ufo_us.quantile(axis=1, numeric_only = False)
21/275: ufo_us.quantile(axis=0, numeric_only = False)
21/276: ufo_us.quantile(axis='columns', numeric_only = False)
21/277: ufo_us.Date_occured.quantile(numeric_only = False)
21/278: ufo_us.Date_occured.quantile()
21/279: ufo_us.Date_occured.quantile([.25, .5, .75])
21/280: ufo_us.describe[include='datetime64']
21/281: ufo_us.describe(include=['datetime64'])
21/282: ufo_us.Date_occured.max()
21/283: ufo_us.Date_occured.min()
21/284: hist = ufo_us.hist(column='Date_occured')
21/285: hist = ufo_us.hist(column='Date_occured', bins=20)
21/286: time_mask = (ufo_us.Date_occured > '1990-01-01')
21/287: ufo_us = ufo_us[time_mask]
21/288: ufo_us.info()
21/289: hist = ufo_us.hist(column='Date_occured', bins=20)
21/290: ufo_us.['Date_occured'].plot.hist()
21/291: ufo_us['Date_occured'].plot.hist()
21/292: ufo_us['Duration'].plot.hist()
21/293: ufo_us.dtypes()
21/294: ufo_us.dtypes
21/295: ufo_us['Duration'].hist()
19/2: z=2
22/1: import pandas as pd
22/2: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'
22/3:
ufo_df = pd.read_csv(
    ufo_path, delimiter='\t',
    header=None,
    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],
    error_bad_lines=False)
22/4: ufo_df.head()
22/5: ufo_df.dtypes
22/6: bad_date = ufo_df.loc[ufo_df['Date_occured']>19951009]
22/7: bad_date.head()
22/8: ufo_df['Date_occured_len']=ufo_df['Date_occured'].astype(str).str.len()
22/9: ufo_df['Date_reported_len']=ufo_df['Date_reported'].astype(str).str.len()
22/10: ufo_df.head()
22/11: bad_date_df = ufo_df.loc[(ufo_df['Date_occured_len'] != 8) | (ufo_df['Date_reported_len'] != 8)]
22/12: bad_date_df.sort_values(by=['Date_occured_len'])
22/13: bad_date_df.info()
22/14: ufo_df[ufo_df['Date_occured_len'] != 8].count()['Date_occured_len']
22/15: ufo_df[ufo_df['Date_reported_len'] != 8].count()['Date_reported_len']
22/16: ufo_df.drop(ufo_df[ufo_df.Date_occured_len !=8].index, inplace=True)
22/17: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')
22/18: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00')
22/19: ufo_df.loc[ufo_df['zerodate'] == True]
22/20: ufo_df.drop(ufo_df.loc[ufo_df['zerodate'] == True].index, inplace=True)
22/21: ufo_df.sort_values(by=['Date_occured']).head()
22/22: ufo_df.drop(40901, inplace=True)
22/23: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')
22/24: ufo_df['Date_reported']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')
22/25: ufo_df.head()
22/26: mask = ufo_df.Date_occured>('1999-01-01')
22/27: since_1999 = ufo_df[mask]
22/28: since_1999.head()
22/29: ufo_df.describe(include=['datetime64'])
22/30: ufo_df.info()
22/31: new=ufo_df['Location'].str.split(',', n=1)
22/32: len(new)
22/33: newest = new.str.len()
22/34: new.loc[0][0],new.loc[0][1]
22/35: new.head()
22/36: ufo_df['City'] = new.str[0]
22/37: ufo_df['State'] = new.str[1]
22/38: ufo_df.head()
22/39: ufo_df.drop(['Date_occured_len', 'Date_reported_len', 'zerodate'], axis=1, inplace=True)
22/40: ufo_df.State = ufo_df.State.str.strip()
22/41: ufo_df.dtypes
22/42: states_abbreviations = [ 'AL', 'AK', 'AS', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FM', 'FL', 'GA', 'GU', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MH', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'MP', 'OH', 'OK', 'OR', 'PW', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VI', 'VA', 'WA', 'WV', 'WI', 'WY' ]
22/43: states_mask = ufo_df.State.isin(states_abbreviations)
22/44: states_mask.head()
22/45: ufo_us = ufo_df[states_mask]
22/46: ufo_us.head()
22/47: ufo_us.Date_occured.quantile([.25, .5, .75])
22/48: ufo_us.Date_occured.max()
22/49: ufo_us.Date_occured.min()
22/50: time_mask = (ufo_us.Date_occured > '1990-01-01')
22/51: ufo_us = ufo_us[time_mask]
22/52: import matplotlib.pyplot as pp
22/53: ufo_us.dtypes
22/54: z=2
22/55: z
22/56: pp.hist(ufo_us['Date_occured'])
22/57: pp.hist(ufo_us['Date_occured'], bins=20)
22/58: pp.hist(ufo_us['Date_occured'], bins=20, density = True)
22/59: import matplotlib.pyplot as plt
22/60: plt.hist(ufo_us['Date_occured'], bins=20, density = True)
22/61: plt.hist(ufo_us['Date_occured'], bins=20)
22/62: histogram = ufo_us['Date_occured'].hist()
22/63: histogram = ufo_us['Date_occured'].hist(bins=20)
22/64: histogram.set_label('ufo')
22/65: histogram.plot()
22/66: histogram.showt()
22/67: histogram.show()
22/68: histogram = ufo_us['Date_occured'].hist(bins=20, labels=('year', 'number'))
22/69: histogram = ufo_us['Date_occured'].hist(bins=20, label=('year', 'number'))
22/70: histogram = ufo_us['Date_occured'].hist(bins=20, label=['year', 'number'])
22/71: %matplotlib inline
22/72: histogram = ufo_us['Date_occured'].hist(bins=20, label=['year', 'number'])
19/3: x = pd.period_range(pd.datetime.now(), periods=200, freq='d')
19/4: import pandas as pd
19/5: x = pd.period_range(pd.datetime.now(), periods=200, freq='d')
19/6: import matplotlib.pyplot as plt
19/7: x = x.to_timestamp().to_pydatetime()
19/8: plt.plot(x,y)
19/9: y = np.random.randn(200, 3).cumsum(0)
19/10: import numpy as np
19/11: y = np.random.randn(200, 3).cumsum(0)
19/12: plt.plot(x,y)
19/13: import matplotlib as mpl
19/14: plt.rcParams['figure.figsize'] = (8,4)
19/15: plt.gcf().set_size_inches(8,4)
19/16:
plt.plot(x,y)
plt.gcf().set_size_inches(8,4)
19/17: y = np.random.randn(200)
19/18:
plt.plot(x,y)
plt.gcf().set_size_inches(8,4)
19/19: y = np.random.randn(200, 3)
19/20:
plt.plot(x,y)
plt.gcf().set_size_inches(8,4)
19/21: y = np.random.randn(200, 3).cumsum(0)
19/22:
plt.plot(x,y)
plt.gcf().set_size_inches(8,4)
19/23:
plots = plt.plot(x,y)
plt.gcf().set_size_inches(8,4)
19/24: plots
19/25: plt.legend(plots, ('foo', 'bar', 'baz'), loc='best')
19/26:
plt.legend(plots, ('foo', 'bar', 'baz'), loc='best')
framealpha=0.5
prop={'size': 'small', 'familu': 'monospace'}
19/27:
plots = plt.plot(x,y)
plt.gcf().set_size_inches(8,4)
19/28:
plots = plt.plot(x,y)
plots
19/29:
plots = plt.plot(x,y)
plt.legend(plots, ('foo', 'bar', 'baz'), loc='best')
19/30:
plots = plt.plot(x,y)
plt.legend(plots, ('foo', 'bar', 'baz'), loc='best')
framealpha=0.5
prop={'size': 'small', 'familu': 'monospace'}
22/73:
histogram = ufo_us['Date_occured'].hist(bins=20, label=['year', 'number'])
plt.title('Ufo observation')
22/74:
histogram = ufo_us['Date_occured'].hist(bins=20, label=['year', 'number'])
plt.title('Ufo observation')
plt.xlabel('Year')
plt.ylabel('Observations')
22/75: ufo_us['year'] = ufo_us['Date_occured'].year
22/76: ufo_us['Date_occured']
22/77: ufo_us['Date_occured'][0]
22/78: ufo_us['Date_occured'][0].year
22/79: ufo_us = ufo_us['Date_occured'].year
22/80: ufo_us['Year'] = ufo_us['Date_occured'].year
22/81: ufo_us['Year'] = ufo_us['Date_occured'].dt.year
22/82: ufo_us['Year'] = ufo_us['Date_occured'].dt.year
22/83: ufo_us.head()
22/84: ufo_us['Month'] = ufo_us['Date_occured'].dt.month
22/85: ufo_us.head()
22/86: ufo_grouped = ufo_us.groupby('State').count()
22/87: ufo_grouped.head()
22/88: ufo_grouped = ufo_us.groupby('State', 'Year', 'Month').count()
22/89: ufo_grouped = ufo_us.groupby(['State', 'Year', 'Month']).count()
22/90: ufo_grouped.head()
22/91: ufo_grouped = ufo_us.groupby(['State', 'Year', 'Month']).['Date_occured']count()
22/92: ufo_grouped = ufo_us.groupby(['State', 'Year', 'Month'])['Date_occured']count()
22/93: ufo_grouped = ufo_us.groupby(['State', 'Year', 'Month'])['Date_occured'].count()
22/94: ufo_grouped.head()
22/95: ufo_grouped.info()
22/96: ufo_grouped.info
22/97: ufo_grouped
22/98: ufo_grouped.describe()
22/99: ufo_grouped.head()
22/100: ufo_grouped.columns
22/101: ufo_grouped.sort_values(by=['Year'])
22/102: ufo_us.sort_values(by=['Year'])
22/103: ufo_us.sort_values(by=['Year', 'Month'])
22/104: period = pd.df({'Year': range(1990, 2010)})
22/105: period = pd.DataFrame({'Year': range(1990, 2010)})
22/106: period.head()
22/107: period_years = pd.DataFrame({'Year': range(1990, 2010)})
22/108: period
22/109: period_months = pd.DataFrame({'Month': range(1, 13)})
22/110: period_months
22/111: period = pd.merge(period_years, period_months, how='outer')
22/112: period = period_years.assign(foo=1).merge(period_months.assign(foo=1))
22/113: period
22/114: period = period_years.assign(foo=1).merge(period_months.assign(foo=1)).drop('foo',1)
22/115: period
22/116: period_years = pd.DataFrame({'Year': range(1990, 2011)})
22/117: period_months = pd.DataFrame({'Month': range(1, 13)})
22/118: period = period_years.assign(foo=1).merge(period_months.assign(foo=1)).drop('foo',1)
22/119: period
22/120: ufo_grouped_2 = ufo_us.groupby(ufo_us.Date_occured.dy.year)
22/121: ufo_grouped_2 = ufo_us.groupby(ufo_us.Date_occured.dt.year)
22/122: ufo_grouped_2.head()
22/123: ufo_grouped_2 = ufo_us.groupby(ufo_us.Date_occured.dt.year)['Date_occured'].count()
22/124: ufo_grouped_2.head()
22/125: ufo_grouped_2[0]
22/126: ufo_grouped_2
22/127: ufo_grouped_2.columns()
22/128: ufo_grouped_2.columns
22/129: ufo_grouped_2.index
22/130:
ufo_grouped_2 = ufo_us.groupby([
    ufo_us.Date_occured.dt.year,
    ufo_us.Date_occured.dt.month,
    ufo_us.State])
    ['Date_occured'].count()
22/131:
ufo_grouped_2 = ufo_us.groupby([
    ufo_us.Date_occured.dt.year,
    ufo_us.Date_occured.dt.month,
    ufo_us.State])['Date_occured'].count()
22/132: ufo_grouped_2.
22/133: ufo_grouped_2
22/134:
ufo_grouped_2 = ufo_us.groupby([
    ufo_us.State,
    ufo_us.Date_occured.dt.year,
    ufo_us.Date_occured.dt.month,
    ])['Date_occured'].count()
22/135: ufo_grouped_2
22/136: ufo_grouped_2.columns
22/137: ufo_grouped_2.index
22/138: ufo_grouped_2
22/139: ufo_grouped_2.head()
22/140: ufo_grouped_2.index[0]
22/141: state, year, month = ufo_grouped_2.index[0]
22/142: state
22/143: ufo_grouped_2.index
22/144: ufo_grouped_2.index.levels
22/145: ufo_grouped_2.index.levels[0]
22/146: ufo_grouped_2[0]
22/147: ufo_grouped_2[1]
22/148: x=2
22/149:
for x in range(2):
    print(x)
22/150:
for month in range(1, 13):
    for year in range(10):
        print(month, year)
22/151:
for month in range(1, 13):
    for year in range(10):
        print(year, month)
22/152:
for month in range(1, 13):
    for year in range(1990, 2011):
        print(year, month)
22/153:
for year in range(1990, 2011):
    for month in range(1, 13):
        print(year, month)
22/154: ufo_grouped_2.index.levels[1]
22/155: state, year, month = ufo_grouped_2.index
22/156: ufo_grouped_2.index
22/157: state, year, month = ufo_grouped_2.index.levels
22/158: state
22/159: state[0]
22/160: ufo_grouped_2
22/161: ufo_grouped_2.index[0]
22/162: ufo_grouped_2.loc['Ak', 1990, 1]
22/163: ufo_grouped_2.loc['AK', 1990, 1]
22/164: state
22/165: year
22/166: observations = pd.DataFrame(columns=['State', 'Year', 'Month'])
22/167: observations.loc[0]='a','b', 'c'
22/168: observations
22/169: states, years, months = ufo_grouped_2.index.levels
22/170: i=0
22/171:
for state in states:
    for year in years:
        for month in months:
            observations.loc[i]=state, year, month
            i+=1
22/172: observations
22/173: observations.loc['AK', 1990, 1]
22/174: observations['counted'] = 0
22/175: observations
22/176: observations[0]
22/177: observations.loc[0]
22/178: observations.loc[0]['State']
22/179: observations.index
22/180:
for x in observations.index:
    print(x)
22/181:
for x in observations.index:
    print(observations.loc[x]['State'], observations.loc[x]['Year'])
22/182: my_indexes = (observations.loc[x]['State'], observations.loc[x]['Year'], observations.loc[x]['Month'] for x in observations.index)
22/183: my_indexes = ((observations.loc[x]['State'], observations.loc[x]['Year'], observations.loc[x]['Month']) for x in observations.index)
22/184: my_indexes.head()
22/185: ufo_grouped_2.loc[('AK', 1990, 1)]
22/186: all_observations = [ufo_grouped_2.loc[element] for element in my_indexes]
22/187:
for element in my_indexes:
    print element
22/188:
for element in my_indexes:
    print(element)
22/189:
for element in my_indexes:
    print(observations.loc[element])
22/190:
for element in my_indexes:
    print(observations.loc[element])
22/191:
for element in my_indexes:
    print(ufo_grouped_2.loc[element])
22/192:
for element in my_indexes:
    print(ufo_grouped_2.loc[element])
22/193: my_indexes = ((observations.loc[x]['State'], observations.loc[x]['Year'], observations.loc[x]['Month']) for x in observations.index)
22/194:
for element in my_indexes:
    print(ufo_grouped_2.loc[element])
22/195:
for element in my_indexes:
    print(ufo_grouped_2.loc[*element])
22/196:
for element in my_indexes:
    print(ufo_grouped_2.loc[element])
22/197:
for element in my_indexes:
    print(element)
22/198: my_indexes = [(observations.loc[x]['State'], observations.loc[x]['Year'], observations.loc[x]['Month']) for x in observations.index]
22/199: my_indexes[0]
22/200: ufo_grouped_2.loc[my_indexes[0]]
22/201:
for single_index in observations.index:
    observations[single_index]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)
22/202: observations[0]['counted'] = ufo_grouped_2.get(my_indexes[0], default=0)
22/203: observations.loc[0]['counted'] = ufo_grouped_2.get(my_indexes[0], default=0)
22/204:
for single_index in observations.index
    observations.loc[0]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)
22/205:
for single_index in observations.index:
    observations.loc[0]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)
22/206: observations.loc[100]
22/207: observations.head(100)
22/208: observations.loc[300]
22/209: observations.loc[400]
22/210: observations.info()
22/211: observations.describe()
22/212: observations.loc[900]
22/213: observations.loc[800]
22/214:
for single_index in observations.index:
    observations.loc[0]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)
22/215:
for single_index in observations.index:
    observations.loc[single_index]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)
22/216: ufo_grouped_2.get(my_indexes[0])
22/217: ufo_grouped_2.get(my_indexes[0], default=0)
22/218: ufo_grouped_2.get(my_indexes[900], default=0)
22/219: observations.index
22/220:
for single_index in observations.index:
    observations.loc[single_index]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)
22/221:
for single_index in range(100):
    observations.loc[single_index]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)
22/222:
for single_index in range(100):
    observations.loc[single_index]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)
22/223:
for single_index in range(100):
    observations.loc[single_index]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)
22/224:
for single_index in range(100):
    observations.loc[single_index]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)
22/225: observations.head(100)
22/226: observations.loc[0]['counted'] = 1
22/227: observations.loc[0]['counted'] = 1
22/228: observations.head()
22/229: observations[0]['counted'] = 1
22/230: observations[0, 'counted'] = 1
22/231: observations.head()
22/232: observations.loc[0, 'counted'] = 2
22/233: observations.head()
22/234: observations.loc[0, 'counted'] = 2
22/235: observations.head()
22/236: observations.drop(0, 'counted')
22/237: observations.columns
22/238: observations.drop((0, 'counted'))
22/239: observations.drop((0, 'counted'), axis=1)
22/240: observations.head()
22/241:
for single_index in range(100):
    observations.loc[single_index, 'counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)
22/242: observations.head()
22/243: observations.drop((0, 'counted'), axis=1)
22/244: observations.columns
22/245: observations.drop((0, 'counted'), axis=1, inplace=True)
22/246: observations.head()
22/247: observations.columns
22/248: observations.head(100)
22/249: observations.describe()
22/250: observations.head(300)
22/251:
for single_index in observations.index:
    observations.loc[single_index, 'counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)
22/252: observations.head(300)
22/253: observations.describe()
22/254: states_years_months = ((state, year, month) for state in states for year in years for month in months)
22/255: ex = dict(state='AK', year= 2000, month= 1)
22/256: observations.append(ex)
22/257: observations.append(pd.DataFrame(ex))
19/31: df = pd.DateFrame(columns=['Year', 'Month'])
19/32: df = pd.DataFrame(columns=['Year', 'Month'])
19/33: d1 = dict(year=1990, month=10)
19/34: df2 = pd.DataFrame(d1)
19/35: l1=[d1]
19/36: df2 = pd.DataFrame(l1)
22/258: states_years_months = [(state, year, month) for state in states for year in years for month in months]
22/259: states_years_months
22/260:
for element in states_years_months:
    state, year, month = element
    d1=dict(state=state, year = year, month = month)
22/261: list_of_dics = []
22/262:
for element in states_years_months:
    state, year, month = element
    d1=dict(state=state, year = year, month = month)
22/263:
for element in states_years_months:
    state, year, month = element
    d1=dict(state=state, year = year, month = month)
    list_of_dics.append(d1)
22/264: list_of_dics
22/265: df2 = pd.DataFrame(list_of_dics)
22/266: df2
22/267:
for element in states_years_months:
    state, year, month = element
    d1=dict(State=state, Year = year, Month = month)
    list_of_dics.append(d1)
22/268: df2 = pd.DataFrame(list_of_dics)
22/269: df2
22/270: observations = pd.DataFrame(columns=['State', 'Year', 'Month'])
22/271: states_years_months = [(state, year, month) for state in states for year in years for month in months]
22/272: list_of_dics = []
22/273:
for element in states_years_months:
    state, year, month = element
    d1=dict(State=state, Year = year, Month = month)
    list_of_dics.append(d1)
22/274: df2 = pd.DataFrame(list_of_dics)
22/275: df2
22/276: states_years_months[0]
22/277:
for single_index in df2:
    df2.loc[single_index, 'counted'] = ufo_grouped_2.get(states_years_months[single_index], default=0)
22/278: df2['Observations']=0
22/279:
for single_index in df2:
    df2.loc[single_index, 'Observations'] = ufo_grouped_2.get(states_years_months[single_index], default=0)
22/280:
for single_index in df2.index:
    df2.loc[single_index, 'Observations'] = ufo_grouped_2.get(states_years_months[single_index], default=0)
22/281: df2.head(50)
22/282: df2.head(150)
22/283: df2.describe()
22/284: x=[1,2,3]
22/285: y= x**2
22/286: y= [1,4,9]
22/287: plt.plot(x,y)
22/288: x=['a', 'b','c']
22/289: y= [1,4,9]
22/290: plt.plot(x,y)
22/291: plt.plot(df2.State, df2.Observations)
22/292:
plt.plot(df2.State, df2.Observations)
plt.rcParams['figure.figsize']=(8,4)
22/293:
plt.plot(df2.State, df2.Observations)
plt.rcParams['figure.figsize']=(12,6)
22/294:
plt.plot(df2.State, df2.Observations)
plt.rcParams['figure.figsize']=(16,8)
22/295:
plt.plot(df2.State, df2.Observations)
plt.rcParams['figure.figsize']=(32,16)
22/296: z=[1,2,3]
22/297: plt.plot(x,y,z)
22/298: plt.plot((x,y),z)
22/299: df2.YearMonth = 'x'
22/300: df2.YearMonth = df2.Year.as_type(str)
22/301: df2.YearMonth = df2.Year.astype(str)
22/302: df2.head()
22/303: df2.head(50)
22/304: df2.YearMonth = df2.Year.astype(str)
22/305: df2.YearMonth = 'x' #df2.Year.astype(str)
22/306: df2.head(50)
22/307: df2['YearMonth'] = 'x' #df2.Year.astype(str)
22/308: df2.head(50)
22/309: df2['YearMonth'] = df2.Year.astype(str)
22/310: df2.head(50)
22/311: df2['YearMonth'] = df2.Year.astype(str) + df2.Month.astype(str)
22/312: df2.head(50)
22/313: df2.head(20)
22/314:
for state in pd2.State:
    plt.plot(pd2.YearMonth, pd2.Observations)
22/315:
for state in df2.State:
    plt.plot(df2.YearMonth, df2.Observations)
22/316: df2.head()
22/317: akdf = df2[df2.State == 'AK']
22/318: akdf.head()
22/319: plt.plot(akdf.YearMonth, akdf.Observations)
22/320:
plt.plot(df2.State, df2.Observations)
plt.rcParams['figure.figsize']=(32,16)
plt.xlabel('YearMonth')
22/321:
plt.plot(df2.State, df2.Observations)
plt.rcParams['figure.figsize']=(32,16)
plt.xlabel('YearMonth')
22/322:
plt.plot(df2.State, df2.Observations)
#plt.rcParams['figure.figsize']=(32,16)
plt.xlabel('YearMonth')
22/323:
plt.plot(df2.State, df2.Observations)
#plt.rcParams['figure.figsize']=(32,16)
plt.ylabel('YearMonth')
22/324:
plt.plot(df2.State, df2.Observations)
#plt.rcParams['figure.figsize']=(32,16)
plt.xlabel('YearMonth')
22/325:
plt.plot(df2.State, df2.Observations)
plt.rcParams['figure.figsize']=(16,8)
plt.xlabel('YearMonth')
22/326:
plt.plot(df2.State, df2.Observations)
plt.rcParams['figure.figsize']=(16,8)
plt.xlabel('YearMonth')
plt.ylabel('Ufo observations')
22/327:
plt.plot(akdf.YearMonth, akdf.Observations)
plt.rcParams['figure.figsize']=(16,8)
plt.xlabel('YearMonth')
plt.ylabel('Ufo observations')
22/328:
plt.plot(akdf.YearMonth, akdf.Observations)
plt.rcParams['figure.figsize']=(16,8)
plt.xlabel('YearMonth')
plt.ylabel('Ufo observations')
plt.grid(True)
22/329: akdf.head(20)
22/330: akdf.describe()
22/331:
all_dfs=[]
for state in states_abbreviations:
    df = df2[df2.State==state]
    all_dfs.append(df)
22/332: all_dfs
22/333: all_dfs[0]
22/334: all_dfs[49]
22/335: len(all_dfs)
22/336: len(states_abbreviations)
22/337: len(states_abbreviations)
22/338: len(states_abbreviations)
22/339: all_dfs_2 = [df2.State==state for state in states_abbreviations]
22/340: all_dfs_2[0]
22/341: all_dfs_2 = [df2[df2.State==state] for state in states_abbreviations]
22/342: all_dfs_2[0]
22/343: all_dfs = [df2[df2.State==state] for state in states_abbreviations]
22/344:
for df in all_dfs:
    plt.plot(df.YearMonth, df.Observations)
22/345:
for df in all_dfs:
    plt.figure()
    plt.plot(df.YearMonth, df.Observations)
23/1:
for df in all_dfs[:2]:
    plt.figure()
    plt.plot(df.YearMonth, df.Observations)
23/2: import pandas as pd
23/3: import pandas as pd
23/4: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'
23/5:
ufo_df = pd.read_csv(
    ufo_path, delimiter='\t',
    header=None,
    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],
    error_bad_lines=False)
23/6: ufo_df.head()
23/7: ufo_df.dtypes
23/8: bad_date = ufo_df.loc[ufo_df['Date_occured']>19951009]
23/9: bad_date.head()
23/10: ufo_df['Date_occured_len']=ufo_df['Date_occured'].astype(str).str.len()
23/11: ufo_df['Date_reported_len']=ufo_df['Date_reported'].astype(str).str.len()
23/12: ufo_df.head()
23/13: bad_date_df = ufo_df.loc[(ufo_df['Date_occured_len'] != 8) | (ufo_df['Date_reported_len'] != 8)]
23/14: bad_date_df.sort_values(by=['Date_occured_len'])
23/15: bad_date_df.info()
23/16: ufo_df[ufo_df['Date_occured_len'] != 8].count()['Date_occured_len']
23/17: ufo_df[ufo_df['Date_reported_len'] != 8].count()['Date_reported_len']
23/18: ufo_df.drop(ufo_df[ufo_df.Date_occured_len !=8].index, inplace=True)
23/19: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')
23/20: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00')
23/21: ufo_df.loc[ufo_df['zerodate'] == True]
23/22: ufo_df.drop(ufo_df.loc[ufo_df['zerodate'] == True].index, inplace=True)
23/23: ufo_df.sort_values(by=['Date_occured']).head()
23/24: ufo_df.drop(40901, inplace=True)
23/25: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')
23/26: ufo_df['Date_reported']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')
23/27: ufo_df.head()
23/28: mask = ufo_df.Date_occured>('1999-01-01')
23/29: since_1999 = ufo_df[mask]
23/30: since_1999.head()
23/31: ufo_df.describe(include=['datetime64'])
23/32: ufo_df.info()
23/33: new=ufo_df['Location'].str.split(',', n=1)
23/34: len(new)
23/35: newest = new.str.len()
23/36: new.loc[0][0],new.loc[0][1]
23/37: new.head()
23/38: ufo_df['City'] = new.str[0]
23/39: ufo_df['State'] = new.str[1]
23/40: ufo_df.head()
23/41: ufo_df.drop(['Date_occured_len', 'Date_reported_len', 'zerodate'], axis=1, inplace=True)
23/42: ufo_df.State = ufo_df.State.str.strip()
23/43: ufo_df.dtypes
23/44: states_abbreviations = [ 'AL', 'AK', 'AS', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FM', 'FL', 'GA', 'GU', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MH', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'MP', 'OH', 'OK', 'OR', 'PW', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VI', 'VA', 'WA', 'WV', 'WI', 'WY' ]
23/45: states_mask = ufo_df.State.isin(states_abbreviations)
23/46: states_mask.head()
23/47: ufo_us = ufo_df[states_mask]
23/48: ufo_us.head()
23/49: ufo_us.Date_occured.quantile([.25, .5, .75])
23/50: ufo_us.Date_occured.max()
23/51: ufo_us.Date_occured.min()
23/52: time_mask = (ufo_us.Date_occured > '1990-01-01')
23/53: ufo_us = ufo_us[time_mask]
23/54: import matplotlib.pyplot as plt
23/55: %matplotlib inline
23/56:
histogram = ufo_us['Date_occured'].hist(bins=20, label=['year', 'number'])
plt.title('Ufo observation')
plt.xlabel('Year')
plt.ylabel('Observations')
23/57: ufo_us['Year'] = ufo_us['Date_occured'].dt.year
23/58: ufo_us['Month'] = ufo_us['Date_occured'].dt.month
23/59: ufo_us.head()
23/60:
ufo_grouped_2 = ufo_us.groupby([
    ufo_us.State,
    ufo_us.Date_occured.dt.year,
    ufo_us.Date_occured.dt.month,
    ])['Date_occured'].count()
23/61: states, years, months = ufo_grouped_2.index.levels
23/62: ufo_grouped_2.index[0]
23/63: observations = pd.DataFrame(columns=['State', 'Year', 'Month'])
23/64: states_years_months = [(state, year, month) for state in states for year in years for month in months]
23/65: list_of_dics = []
23/66:
for element in states_years_months:
    state, year, month = element
    d1=dict(State=state, Year = year, Month = month)
    list_of_dics.append(d1)
23/67: df2 = pd.DataFrame(list_of_dics)
23/68: df2['Observations']=0
23/69: states_years_months[0]
23/70: my_indexes = [(observations.loc[x]['State'], observations.loc[x]['Year'], observations.loc[x]['Month']) for x in observations.index]
23/71:
for single_index in df2.index:
    df2.loc[single_index, 'Observations'] = ufo_grouped_2.get(states_years_months[single_index], default=0)
23/72: df2.head(150)
23/73: df2.describe()
23/74: x=['a', 'b','c']
23/75: y= [1,4,9]
23/76: z=[1,2,3]
23/77: df2['YearMonth'] = df2.Year.astype(str) + df2.Month.astype(str)
23/78: df2.head()
23/79: akdf = df2[df2.State == 'AK']
23/80: akdf.describe()
23/81:
plt.plot(akdf.YearMonth, akdf.Observations)
plt.rcParams['figure.figsize']=(16,8)
plt.xlabel('YearMonth')
plt.ylabel('Ufo observations')
plt.grid(True)
23/82: all_dfs = [df2[df2.State==state] for state in states_abbreviations]
23/83:
for df in all_dfs[:2]:
    plt.figure()
    plt.plot(df.YearMonth, df.Observations)
23/84:
for df in all_dfs[:4]:
    plt.figure()
    plt.plot(df.YearMonth, df.Observations)
23/85:
plt.figure
for df in all_dfs[:4]:
    plt.plot(df.YearMonth, df.Observations)
23/86:
plt.figure
for df in all_dfs[:4]:
    plt.subplot(2,2)
    plt.plot(df.YearMonth, df.Observations)
23/87:
plt.figure
plt.subplot(2,2)
for df in all_dfs[:4]:
    plt.plot(df.YearMonth, df.Observations)
23/88:
plt.figure
plt.subplots(2,2)
for df in all_dfs[:4]:
    plt.plot(df.YearMonth, df.Observations)
23/89:
plt.figure
plt.subplots(4)
for df in all_dfs[:4]:
    plt.plot(df.YearMonth, df.Observations)
23/90:
plt.figure
plt.subplots(4)
for x in range(4):
    plt.plot(all_dfs[x].YearMonth, all_dfs[x].Observations)
23/91:
plt.figure
fig, axes = plt.subplots(4)
for x in range(4):
    axes[x].plot(all_dfs[x].YearMonth, all_dfs[x].Observations)
23/92:
plt.figure
fig, axes = plt.subplots(10)
for x in range(10):
    axes[x].plot(all_dfs[x].YearMonth, all_dfs[x].Observations)
24/1: import pandas as pd
24/2: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'
24/3:
hwg_df = pd.read_csv(
    heights_weights_gender_data)
24/4: hwg_df.head()
24/5: hwg_df.describe()
24/6: hwg_df.Height =hwg_df.Height * 2.54
24/7: hwg_df.describe()
24/8: hwg_df.Weight = hwg_df.Weight * 0.454
24/9: hwg_df.describe()
26/1: hwg_df.guantile(q=range(0,100,10))
26/2: import pandas as pd
26/3: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'
26/4:
hwg_df = pd.read_csv(
    heights_weights_gender_data)
26/5: hwg_df.head()
26/6: hwg_df.describe()
26/7: hwg_df.Height =hwg_df.Height * 2.54
26/8: hwg_df.Weight = hwg_df.Weight * 0.454
26/9: hwg_df.describe()
26/10: hwg_df.guantile(q=range(0,100,10))
26/11: hwg_df.quantile(q=range(0,100,10))
26/12: hwg_df.quantile(q=range(0,1,0.01))
26/13: hwg_df.quantile(q=0.1)
26/14: hwg_df.quantile(q=[0.1, 0.2])
26/15: hwg_df.quantile(q=range(0,1,0.1))
26/16: hwg_df.quantile(q=[x*0.1 for x in range(0,10)])
26/17: help(quantile)
26/18: help(str)
26/19: help(pd.quantile)
26/20: help(pd.DataFrame.quantile)
26/21: pd.__dic__
26/22: pd.__dic__()
26/23: dic(pd)
26/24: pd.__dict__
26/25: DataFrame in pd.__dict__
26/26: 'DataFrame' in pd.__dict__
26/27: pd.__dict__('DataFrame')
26/28: pd.__dict__['DataFrame']
26/29: pd.__dict__['DataFrame'].__dict__
26/30: dir(pd.DataFrame)
26/31: hwg_df.quantile(q=[x*0.2 for x in range(0,11)])
26/32: import pandas as pd
26/33: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'
26/34:
hwg_df = pd.read_csv(
    heights_weights_gender_data)
26/35: hwg_df.head()
26/36: hwg_df.Height =hwg_df.Height * 2.54
26/37: hwg_df.Weight = hwg_df.Weight * 0.454
26/38: hwg_df.describe()
26/39: hwg_df.quantile(q=[x*0.2 for x in range(0,11)])
26/40: 'DataFrame' in pd.__dict__
26/41: hwg_df.describe()
26/42: hwg_df.quantile(q=[x*0.2 for x in range(0,11)])
26/43: hwg_df.quantile(q=[x*0.2 for x in range(0,6)])
26/44: hwg_df.quantile(q=0.25), hwg_df.quantile(q=0.75)
26/45: hwg_df.Height.quantile(q=0.25), hwg_df.Height.quantile(q=0.75)
26/46: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)
26/47: help(avg)
26/48:
def my_var(x):
    m = sum(x)/len(x)
    return sum((x-m)**2)/len(x)
26/49: my_var([1,2,3,4,5])
26/50:
def my_var(x):
    m = sum(x)/len(x)
    return sum([(element-m)**2 for element in x])/len(x)
26/51: my_var([1,2,3,4,5])
26/52:
def my_var(x):
    m = sum(x)/len(x)
    return sum([(element-m)**2 for element in x])/(len(x)-1)
26/53: my_var([1,2,3,4,5])
26/54: my_var(hwg_df.Height)
26/55: hwg_df.Height.mean() -hwg_df.Height.variance()
26/56: hwg_df.Height.mean() -hwg_df.Height.var()
26/57: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() -hwg_df.Height.var()
26/58: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()
26/59:
def my_sd(x):
    return sqrt(my_var(x))
26/60: my_std(hwg_df.Height)
26/61: my_sd(hwg_df.Height)
26/62:
def my_sd(x):
    return my_var(x)**(0.5)
26/63: my_sd(hwg_df.Height)
26/64: my_sd(hwg_df.Height) - hwg_df.Height.std()
26/65: df = hwg_df
26/66: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std
26/67: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()
26/68: from scipy import stats
26/69: stats.percentileofscore(df.Height, 158.8)
26/70: stats.percentileofscore(df.Height, 178.34)
26/71: 82.5-17.31
26/72: histogram = df.Height.hist(bins=100)
26/73: import pandas as pd
26/74: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'
26/75:
hwg_df = pd.read_csv(
    heights_weights_gender_data)
26/76: hwg_df.head()
26/77: hwg_df.Height =hwg_df.Height * 2.54
26/78: hwg_df.Weight = hwg_df.Weight * 0.454
26/79: hwg_df.describe()
26/80: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)
26/81:
def my_var(x):
    m = sum(x)/len(x)
    return sum([(element-m)**2 for element in x])/(len(x)-1)
26/82:
def my_sd(x):
    return my_var(x)**(0.5)
26/83: my_var([1,2,3,4,5])
26/84: my_var(hwg_df.Height)
26/85: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()
26/86: my_sd(hwg_df.Height) - hwg_df.Height.std()
26/87: df = hwg_df
26/88: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()
26/89: from scipy import stats
26/90: stats.percentileofscore(df.Height, 158.8)
26/91: stats.percentileofscore(df.Height, 178.34)
26/92: 82.5-17.31
26/93: histogram = df.Height.hist(bins=100)
26/94: df.describe()
26/95: histogram = df.Height.hist(bins=64)
26/96:
histogram = df.Height.hist(bins=64)
plt.tilte('Height')
plt.xlabel('Height')
plt.ylabel('Number')
26/97:
histogram = df.Height.hist(bins=64)
plt.title('Height')
plt.xlabel('Height')
plt.ylabel('Number')
26/98:
import pandas as pd
import matplotlib.pyplot as plt
26/99:
histogram = df.Height.hist(bins=64)
plt.title('Height')
plt.xlabel('Height')
plt.ylabel('Number')
26/100:
histogram = df.Height.hist(bins=64)
plt.title('Height distribution')
plt.xlabel('Height')
plt.ylabel('Number')
26/101:
histogram = df.Height.hist(bins=64, density=True)
plt.title('Height distribution')
plt.xlabel('Height')
plt.ylabel('Number')
26/102:
histogram = df.Height.hist(bins=13, density=True)
plt.title('Height distribution')
plt.xlabel('Height')
plt.ylabel('Number')
26/103:
histogram = df.Height.hist(bins=1000, density=True)
plt.title('Height distribution')
plt.xlabel('Height')
plt.ylabel('Number')
26/104: df.Height.plot.kde()
26/105:
df.Height.plot.kde()
plt.xlabel('Height')
26/106:
df.Height.plot.kde()
plt.xlabel('Height')
26/107:
histogram = df.Height.hist(bins=1000, density=True)
plt.title('Height distribution')
plt.xlabel('Height')
plt.ylabel('Number')
26/108:
kde = df.Height.plot.kde()
plt.xlabel('Height')
26/109: kde = df.Height.plot.kde()
26/110:
kde = df.Height.plot.kde()
plt.xlabel='Height'
26/111:
histogram = df.Height.hist(bins=1000, density=True)
plt.title('Height distribution')
plt.xlabel('Height')
plt.ylabel('Number')
26/112:
kde = df.Height.plot.kde()
plt.xlabel='Height'
26/113:
kde = df.Height.plot.kde()
plt.xlabel = 'Height'
26/114:
kde = df.Height.plot.kde()
plt.ylabel = 'Height'
26/115:
kde = df.Height.plot.kde()
plt.xlabel = 'Height'
26/116:
histogram = df.Height.hist(bins=1000, density=True)
plt.title('Height distribution')
plt.xlabel('Height')
plt.ylabel('Number')
26/117:
import pandas as pd
import matplotlib.pyplot as plt
26/118:
histogram = df.Height.hist(bins=64, density=True)
plt.title('Height distribution')
plt.xlabel('Height')
plt.ylabel('Number')
26/119:
histogram = df.Height.hist(bins=64, density=True)
plt.title('Height distribution')
26/120:
histogram = df.Height.hist(bins=64)
plt.title('Height distribution')
26/121:
kde = df.Height.plot.kde()
plt.xlabel = 'Height'
26/122:
histogram = df.Height.hist(bins=64)
plt.title('Height distribution')
plt.xlabel('Height')
26/123:
import pandas as pd
import matplotlib.pyplot as plt
26/124:
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
26/125:
histogram = df.Height.hist(bins=64)
plt.title('Height distribution')
plt.xlabel('Height')
26/126:
histogram = df.Height.hist(bins=64)
plt.title('Height distribution')
plt.xlabel('x')
26/127:
histogram = df.Height.hist(bins=64)
plt.title('Height distribution')
plt.ylabel('x')
26/128:
histogram = df.Height.hist(bins=64)
plt.ylabel('x')
26/129: histogram = df.Height.hist(bins=64)
26/130:
histogram = df.Height.hist(bins=64)
histogram.xlabel = ('x')
26/131:
histogram = df.Height.hist(bins=64)
plt.xlabel = ('x')
26/132:
histogram = df.Height.hist(bins=64)
plt.xlabel = ('Height')
26/133:
kde = df.Height.plot.kde()
plt.xlabel = 'Height'
26/134:
kde = df.Height.plot.kde()
plt.xlabel('Height')
26/135:
histogram = df.Height.hist(bins=64)
plt.xlabel = ('Height')
26/136:
histogram = df.Height.hist(bins=64)
plt.xlabel('Height')
26/137:
histogram = df.Height.hist(bins=64)
plt.title('Height')
26/138:
histogram = df.Height.hist(bins=64, label=['Height', 'Number'])
plt.title('Height')
26/139: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])
26/140: kde = df.Height.plot.kde(label=['Height', 'Probability'])
26/141: kde = df.Height.plot.kde()
26/142:
kde = df.Height.plot.kde()
plt.title('kde')
26/143:
kde = df.Height.plot.kde()
plt.title('kde!!!')
26/144:
kde = df.Height.plot.kde()
plt.title('kde!!!')
plt.xlabel('x')
28/1:
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
28/2: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'
28/3:
hwg_df = pd.read_csv(
    heights_weights_gender_data)
28/4: hwg_df.head()
28/5: hwg_df.Height =hwg_df.Height * 2.54
28/6: hwg_df.Weight = hwg_df.Weight * 0.454
28/7: hwg_df.describe()
28/8: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)
28/9:
def my_var(x):
    m = sum(x)/len(x)
    return sum([(element-m)**2 for element in x])/(len(x)-1)
28/10:
def my_sd(x):
    return my_var(x)**(0.5)
28/11: my_var([1,2,3,4,5])
28/12: my_var(hwg_df.Height)
28/13: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()
28/14: my_sd(hwg_df.Height) - hwg_df.Height.std()
28/15: df = hwg_df
28/16: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()
28/17: from scipy import stats
28/18: stats.percentileofscore(df.Height, 158.8)
28/19: stats.percentileofscore(df.Height, 178.34)
28/20: 82.5-17.31
28/21: df.describe()
28/22: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])
28/23:
kde = df.Height.plot.kde()
plt.title('kde!!!')
plt.xlabel('x')
28/24: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])
28/25:
kde = df.Height.plot.kde()
plt.title('kde!!!')
plt.xlabel('Height')
plt.ylabel('Probability')
28/26:
kde = df.Height.plot.kde()
plt.title('KDE)
plt.xlabel('Height')
plt.ylabel('Probability')
28/27:
kde = df.Height.plot.kde()
plt.title('KDE')
plt.xlabel('Height')
plt.ylabel('Probability')
28/28: df.head()
28/29: df.Height[df.Gender='Male']
28/30: df.Height[df.Gender=='Male']
28/31: df.Height[df.Gender=='Male'].plot.kde()
28/32:
df.Height[df.Gender=='Male'].plot.kde()
df.Height[df.Gender=='Female'].plot.kde()
29/1:
f1, male = df.Height[df.Gender=='Male'].plot.kde()
df.Height[df.Gender=='Female'].plot.kde()
29/2:
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
29/3: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'
29/4:
hwg_df = pd.read_csv(
    heights_weights_gender_data)
29/5: hwg_df.head()
29/6: hwg_df.Height =hwg_df.Height * 2.54
29/7: hwg_df.Weight = hwg_df.Weight * 0.454
29/8: hwg_df.describe()
29/9: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)
29/10:
def my_var(x):
    m = sum(x)/len(x)
    return sum([(element-m)**2 for element in x])/(len(x)-1)
29/11:
def my_sd(x):
    return my_var(x)**(0.5)
29/12: my_var([1,2,3,4,5])
29/13: my_var(hwg_df.Height)
29/14: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()
29/15: my_sd(hwg_df.Height) - hwg_df.Height.std()
29/16: df = hwg_df
29/17: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()
29/18: from scipy import stats
29/19: stats.percentileofscore(df.Height, 158.8)
29/20: stats.percentileofscore(df.Height, 178.34)
29/21: 82.5-17.31
29/22: df.describe()
29/23: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])
29/24:
kde = df.Height.plot.kde()
plt.title('KDE')
plt.xlabel('Height')
plt.ylabel('Probability')
29/25: df.head()
29/26:
f1, male = df.Height[df.Gender=='Male'].plot.kde()
df.Height[df.Gender=='Female'].plot.kde()
29/27:
f1 = df.Height[df.Gender=='Male'].plot.kde()
df.Height[df.Gender=='Female'].plot.kde()
29/28:
f1 = df.Height[df.Gender=='Male'].plot.kde()
f1.set_title('male')
df.Height[df.Gender=='Female'].plot.kde()
29/29:
f1 = df.Height[df.Gender=='Male', df.Gender=='Female'].plot.kde()
f1.set_title('male')
#df.Height[df.Gender=='Female'].plot.kde()
29/30:
f1 = df.Height[df.Gender=='Male'].plot.kde()
f1.set_title('male')
#df.Height[df.Gender=='Female'].plot.kde()
29/31:
f1 = df.Height[df.Gender=='Male'].plot.kde()
plt.legend(f1, ('Male'))
#df.Height[df.Gender=='Female'].plot.kde()
29/32:
f1 = df.Height[df.Gender=='Male'].plot.kde()
plt.legend(f1, ('Male',))
#df.Height[df.Gender=='Female'].plot.kde()
29/33:
f1 = df.Height[df.Gender=='Male'].plot.kde()
plt.legend(f1, ['Male'])
#df.Height[df.Gender=='Female'].plot.kde()
29/34:
f1 = df.Height[df.Gender=='Male'].plot.kde(label='Male')
#plt.legend(f1, ['Male'])
#df.Height[df.Gender=='Female'].plot.kde()
29/35:
f1 = df.Height[df.Gender=='Male'].plot.kde(label='Male!!!')
#plt.legend(f1, ['Male'])
#df.Height[df.Gender=='Female'].plot.kde()
29/36:
f1 = df.Height[df.Gender=='Male'].plot.kde(label='Male!!!')
df.legend()
#plt.legend(f1, ['Male'])
#df.Height[df.Gender=='Female'].plot.kde()
29/37:
f1, = df.Height[df.Gender=='Male'].plot.kde(label='Male!!!')
df.legend()
#plt.legend(f1, ['Male'])
#df.Height[df.Gender=='Female'].plot.kde()
29/38:
f1, = df.Height[df.Gender=='Male'].plot.kde()
f1.set_label('Male!!!')
#plt.legend(f1, ['Male'])
#df.Height[df.Gender=='Female'].plot.kde()
29/39:
f1, = df.Height[df.Gender=='Male'].plot.kde()
f1.set_label('Male!!!')
plt.legend()
#plt.legend(f1, ['Male'])
#df.Height[df.Gender=='Female'].plot.kde()
29/40:
f1, = df.Height[df.Gender=='Male'].plot.kde()
#df.Height[df.Gender=='Female'].plot.kde()
29/41:
f1 = df.Height[df.Gender=='Male'].plot.kde(label='male!')
#df.Height[df.Gender=='Female'].plot.kde()
29/42:
f1 = df.Height[df.Gender=='Male'].plot.kde(label='male!')
plt.legend()
#df.Height[df.Gender=='Female'].plot.kde()
29/43:
f1 = df.Height[df.Gender=='Male'].plot.kde(label='Male')
df.Height[df.Gender=='Female'].plot.kde(label='Female')
plt.legend()
29/44:
df.Height[df.Gender=='Male'].plot.kde(label='Male')
df.Height[df.Gender=='Female'].plot.kde(label='Female')
plt.legend()
29/45:
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.legend()
29/46:
df.Height[df.Gender=='Male'].plot.kde(label='Male')
df.Height[df.Gender=='Female'].plot.kde(label='Female')
plt.legend()
plt.xlabel('Height')
29/47:
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.legend()
plt.xlabel('Weight')
29/48:
plt.figure
fig, axes = plt.subplots(2)
axes[1].df.Weight[df.Gender=='Male'].plot.kde(label='Male')
29/49:
plt.figure
fig, axes = plt.subplots(2)
axes[1].plot(Weight[df.Gender=='Male'].plot.kde(label='Male'))
29/50:
plt.figure
fig, axes = plt.subplots(2)
axes[1].plot(df.Weight[df.Gender=='Male'].plot.kde(label='Male'))
29/51:
plt.figure
fig, axes = plt.subplots(2)
axes[1].plot(df.Weight[df.Gender=='Male'].kde(label='Male'))
29/52:
plt.figure
fig, axes = plt.subplots(2)
axes[1].plot(df.Weight[df.Gender=='Male'].plot.kde(label='Male'))
29/53:
plt.figure
fig, axes = plt.subplots(2)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
29/54:
plt.figure
fig, axes = plt.subplots(2)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/55:
plt.figure
fig, axes = plt.subplot()
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/56:
plt.figure
plt.subplot()
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/57:
plt.figure
plt.subplot()
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.subplot()
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/58:
plt.figure
plt.subplot()
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.subplot()
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/59:
plt.figure
plt.subplot(1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.subplot(1)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/60:
plt.figure
plt.subplot(1,1,0)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.subplot(1,1,0)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/61:
plt.figure
plt.subplot(1,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.subplot(1,1,1)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/62:
plt.figure
plt.subplot(1,1,1)
plt.subplot(1,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/63:
plt.figure
plt.subplot()
plt.subplot()
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/64:
plt.figure
plt.subplot()
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.subplot()
29/65:
plt.figure
plt.subplot(2,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.subplot(2,1,2)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/66:
plt.figure
plt.subplot(2,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.subplot(2,1,2)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/67:
plt.figure
plt.subplot(2,1,1, sharex=True)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.subplot(2,1,2, sharex=True)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/68:
plt.figure
ax1 = plt.subplot(2,1,1, sharex=True)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.subplot(2,1,2)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/69:
plt.figure
ax1 = plt.subplot(2,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.subplot(2,1,2)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/70:
plt.figure
ax1 = plt.subplot(2,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
ax2 = plt.subplot(2,1,2, sharex = ax1)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
29/71:
plt.figure
ax1 = plt.subplot(2,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
ax2 = plt.subplot(2,1,2, sharex = ax1)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.grid(True)
29/72:
plt.figure
ax1 = plt.subplot(2,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.grid(True)
ax2 = plt.subplot(2,1,2, sharex = ax1)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.grid(True)
29/73:
plt.figure
ax1 = plt.subplot(2,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.grid(True)
ax2 = plt.subplot(2,1,2, sharex = ax1)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.grid(True)
plt.xlabel('Weight')
29/74:
plt.figure
ax1 = plt.subplot(2,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.grid(True)
plt.legend
ax2 = plt.subplot(2,1,2, sharex = ax1)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.grid(True)
plt.xlabel('Weight')
plt.legend()
29/75:
plt.figure
ax1 = plt.subplot(2,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.grid(True)
plt.legend()
ax2 = plt.subplot(2,1,2, sharex = ax1)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.grid(True)
plt.xlabel('Weight')
plt.legend()
29/76:
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
%matplotlib inline
29/77:
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
import numpy as np
%matplotlib inline
29/78:
mu = 0
variance = 5
sigma = variance**0.5
x = np.linspace(mu-3*sigma, mu+3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma))
29/79:
mu = 0
variance = 1
sigma = variance**0.5
x = np.linspace(mu-3*sigma, mu+3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma))
29/80:
mu = 0
variance = 10
sigma = variance**0.5
x = np.linspace(mu-3*sigma, mu+3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma))
29/81: sigma
29/82:
mu = 0
variance = 1
sigma = variance**0.5
x = np.linspace(mu-3*sigma, mu+3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma))
29/83: sigma
29/84:
mu = 0
variance = 5
sigma = variance**0.5
x = np.linspace(mu-3*sigma, mu+3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma))
29/85: sigma
29/86:
mu = 10
variance = 5
sigma = variance**0.5
x = np.linspace(mu-3*sigma, mu+3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma))
29/87: sigma
29/88:
mu = 10
variance = 5
sigma = variance**0.5
x = np.linspace(mu-3*sigma, mu+3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma))
plt.plot(x, stats.cauchy.pdf(x, mu, sigma))
29/89:
mu = 10
variance = 5
sigma = variance**0.5
x = np.linspace(mu-3*sigma, mu+3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Gauss')
plt.plot(x, stats.cauchy.pdf(x, mu, sigma))
29/90:
mu = 10
variance = 5
sigma = variance**0.5
x = np.linspace(mu-3*sigma, mu+3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Gauss')
plt.plot(x, stats.cauchy.pdf(x, mu, sigma), label='Cauchy')
plt.legend()
29/91: plt.scatter(df.Height, df.Weight)
29/92:
plt.figure(figsize=(10,10))
ax1 = plt.subplot(2,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.grid(True)
plt.legend()
ax2 = plt.subplot(2,1,2, sharex = ax1)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.grid(True)
plt.xlabel('Weight')
plt.legend()
29/93:
plt.scatter(df.Height, df.Weight)
plt.geom_smooth()
29/94:
plt.scatter(df.Height, df.Weight)
plt.plot(regression_line)
29/95:
def f1(x,a,b):
    return a*x + b

a,b = scipy.optimize.curve_fit(f1, df.Height, df.Weight )
29/96:
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.optimize import curve_fit
import numpy as np
%matplotlib inline
29/97:
def f1(x,a,b):
    return a*x + b

a,b = curve_fit(f1, df.Height, df.Weight )
29/98: plt.scatter(df.Height, df.Weight)
29/99:
plt.scatter(df.Height, df.Weight)
def f1(x,a,b):
    return a*x + b

popt,pcov = curve_fit(f1, df.Height, df.Weight )
29/100:
plt.scatter(df.Height, df.Weight)
def f1(x,a,b):
    return a*x + b

popt,pcov = curve_fit(f1, df.Height, df.Weight )
plt.plot(df.Height, f1(df.Height, *popt))
29/101:
plt.scatter(df.Height, df.Weight)
def f1(x,a,b):
    return a*x + b

popt,pcov = curve_fit(f1, df.Height, df.Weight )
plt.plot(df.Height, f1(df.Height, *popt), color = 'red')
29/102:
plt.scatter(df.Height, df.Weight)
def f1(x,a,b):
    return a*x**2 + c*x + b

popt,pcov = curve_fit(f1, df.Height, df.Weight )
plt.plot(df.Height, f1(df.Height, *popt), color = 'red')
29/103:
plt.scatter(df.Height, df.Weight)
def f1(x,a,c,b):
    return a*x**2 + c*x + b

popt,pcov = curve_fit(f1, df.Height, df.Weight )
plt.plot(df.Height, f1(df.Height, *popt), color = 'red')
29/104:
plt.scatter(df.Height, df.Weight)
def f1(x,a,b):
    return a*x + b

popt,pcov = curve_fit(f1, df.Height, df.Weight )
plt.plot(df.Height, f1(df.Height, *popt), color = 'red')
29/105: popt
29/106:
plt.scatter(df.Height, df.Weight)
def f1(x,a,b):
    return a*x + b

popt,pcov = curve_fit(f1, df.Height, df.Weight )
plt.plot(df.Height, f1(df.Height, *popt), color = 'red')
xlabel('Height')
ylabel('Weight')
29/107:
plt.scatter(df.Height, df.Weight)
def f1(x,a,b):
    return a*x + b

popt,pcov = curve_fit(f1, df.Height, df.Weight )
plt.plot(df.Height, f1(df.Height, *popt), color = 'red')
plt.xlabel('Height')
plt.ylabel('Weight')
29/108:
plt.scatter(df.Height, df.Weight)
def f1(x,a,b):
    return a*x + b

popt,pcov = curve_fit(f1, df.Height, df.Weight )
plt.plot(df.Height, f1(df.Height, *popt), color = 'red')
plt.xlabel('Height [cm]')
plt.ylabel('Weight [kg]')
29/109: 1.38*160-159
29/110: colors = dict(Male='red', Woman='blue')
29/111: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x])
29/112: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x]))
29/113: colors = dict(Male='red', Female='blue')
29/114: colors = dict(Male='red', Female='blue')
29/115: colors
29/116: df.head(15)
29/117: df.head(100)
29/118: df.head()
29/119: df.columns
29/120: df.dtypes()
29/121: df.dtypes
29/122: df.Gender.loc[0]
29/123: df.Gender.loc[0] == 'Male'
29/124: df.Gender
29/125: df.Gender.loc[9970]
29/126: plt.scatter(df.Height, df.Weight, c=df['Gender'].apply(lambda x: colors[x]))
29/127: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x]))
29/128: plt.scatter(df[df.Gender='Male']['Height'], df.Weight)
29/129: plt.scatter(df[df.Gender=='Male']['Height'], df.Weight)
29/130: plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'])
29/131:
plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'])
plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'])
29/132:
plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')
plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')
29/133:
plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')
plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')
plt.legend()
29/134:
plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')
plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')
plt.legend()
plt.xlabel('Height')
plt.ylabel('Weight')
30/1:
import pandas as pd
import matplotlib.pyplot as plt
30/2:
spam_path = 'data/spam'
spam2_path = 'data/spam2'
easyham_path = 'data/easy_ham'
easyham2_path = 'data/easy_ham2'
hardham_path = 'data/hard_ham'
hardham2_path = 'data/har_ham_2'
30/3:
import pandas as pd
import matplotlib.pyplot as plt
import os
30/4: os.listdir(spam_path)
30/5:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
30/6: os.listdir(spam_path)
32/1: all_emails = os.listdir(spam_path)
32/2:
import pandas as pd
import matplotlib.pyplot as plt
import os
32/3:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
32/4:
def get_messages(path_to_file):
    for email in os.listdir(path_to_file):
32/5: email =all_emails[0]
32/6: all_emails = os.listdir(spam_path)
32/7: email =all_emails[0]
32/8: print(email.open())
32/9: email =spam_path + all_emails[0]
32/10: print(email.open())
32/11: print(open(email))
32/12: email =open(spam_path + all_emails[0])
32/13: print(email.read())
32/14: all_spam_emails = [spam_path + email_name for email in os.listdir(spam_path)]
32/15: all_spam_emails = [spam_path + email_name for email_name in os.listdir(spam_path)]
32/16: email =open(all_spam_emails[0])
32/17: print(email.read())
32/18: email =open(all_spam_emails[0]).read()
32/19: print(email)
32/20: email
32/21: email.split('\n')
32/22: splited_email = email.split('\n')
32/23: print(splited_email[0])
32/24: print(email)
32/25: splited_email = email.split('\n\n')
32/26: print(splited_email[0])
32/27: len(splitted_email)
32/28: len(splited_email)
32/29: test = 'aaabbbcccnnaabbcc'
32/30: test.find('nn')
32/31: pos = test.find('nn')
32/32: test2 = test[pos:]
32/33: print(test2)
32/34: test = 'aaabbbccc\n\naabbcc'
32/35: pos = test.find('nn')
32/36: test2 = test[pos:]
32/37: print(test2)
32/38: pos = test.find('\n\n')
32/39: test2 = test[pos:]
32/40: print(test2)
32/41: print(test2.rstrip())
32/42: print(test2.rstrip())
32/43: print(test2.rstrip())
32/44: print(test2.rstrip('\n'))
32/45: print(test2.strip('\n'))
32/46: test = 'aaabbbccc\n\naa\n\nbbcc'
32/47: pos = test.find('\n\n')
32/48: test2 = test[pos:]
32/49: print(test2.strip('\n'))
32/50: test = 'aaabbbccc\n\naa\n\nbbcc\n\n'
32/51: pos = test.find('\n\n')
32/52: test2 = test[pos:]
32/53: print(test2.strip('\n'))
32/54:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names]
    all_email_messages = []
    for file in path_to_all_files:
        email = open(file).read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        email.close()
32/55: all_spam_emails = [spam_path + email_name for email_name in os.listdir(spam_path)]
32/56: email =open(all_spam_emails[0]).read()
32/57: print(email)
32/58: splited_email = email.split('\n\n')
32/59: print(splited_email[0])
32/60: all_spam = get_messages(spam_path)
32/61:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names]
    all_email_messages = []
    for file in path_to_all_files:
        email = open(file).read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
32/62: all_spam = get_messages(spam_path)
32/63:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names]
    all_email_messages = []
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
32/64: all_spam = get_messages(spam_path)
32/65: all_spam[0]
32/66: all_spam
32/67:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names]
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
32/68: all_spam = get_messages(spam_path)
32/69: all_spam
32/70: all_spam[0]
32/71: print(all_spam[0])
32/72: print(all_spam[99])
32/73: print(all_spam[199])
33/1:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
33/2:
import pandas as pd
import matplotlib.pyplot as plt
import os
33/3:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
33/4:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
33/5: all_spam = get_messages(spam_path)
33/6: import re
33/7: test = ['Testing for emails \n searching']
33/8: test = 'Testing for emails \n searching'
33/9: result = re.findall(r'\w+', test)
33/10: result
33/11:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
33/12: test = 'Testing for emails \n searching for'
33/13: result = re.findall(r'\w+', test)
33/14: result
33/15: set(result)
33/16: result_set = set(result)
33/17: result_set + set('lol')
33/18: result_set = result_set | set('lol')
33/19: result_set
33/20: result_set = set(result)
33/21: result_set = result_set | set(['lol'])
33/22: result_set
33/23:
def build_corpus(messages_vector):
    corpus = set()
    for message in messages vector:
        message_words = re.findall(r'\w+', message)
        corpus = corpus | set(message_words)
    return corpus
33/24:
def build_corpus(messages_vector):
    corpus = set()
    for message in messages_vector:
        message_words = re.findall(r'\w+', message)
        corpus = corpus | set(message_words)
    return corpus
33/25: test_corpus = build_corpus(all_spam[0])
33/26: print(test_corpus)
33/27:
def build_corpus(messages_vector):
    corpus = set()
    for message in messages_vector:
        message_words = re.findall(r'\w+', message)
        print(message_words)
        corpus = corpus | set(message_words)
    return corpus
33/28: test_corpus = build_corpus(all_spam[0])
33/29: test_corpus = build_corpus([all_spam[0]])
33/30: print(test_corpus)
33/31:
def build_corpus(messages_vector):
    corpus = set()
    for message in messages_vector:
        message_words = re.findall(r'\w+', message)
        #print(message_words)
        corpus = corpus | set(message_words)
    return corpus
33/32: test_corpus = build_corpus([all_spam[0]])
33/33: print(test_corpus)
33/34: test.count('for')
33/35:
def build_corpus(messages_vector):
    corpus = set()
    for message in messages_vector:
        message_words = re.findall(r'\w+', message)
        #print(message_words)
        corpus = corpus | set(message_words)
    return list(corpus)
33/36: test_corpus = build_corpus([all_spam[0]])
33/37: print(test_corpus)
33/38: occurances = {word: all_spam[0].count(word) for word in test_corpus}
33/39: print(occurances)
33/40: test_corpus = build_corpus(all_spam)
33/41: print(test_corpus)
33/42: occurances = {word: all_spam[0].count(word) for word in test_corpus}
33/43: print(occurances)
33/44:
def count_words(message, corpus):
    occurances = {word: message.count(word) for word in corpus}
33/45:
def count_words(message, corpus):
    occurances = {word: message.count(word) for word in corpus}
    return occurances
33/46:
def count_words(message, corpus):
    occurances = {word: message.count(word) for word in corpus}
    return occurances
34/1:
def count_words(message, corpus):
    occurances = {word: message.count(word) for word in corpus if message.count(word)>1}
    return occurances
34/2: occurances = {word: all_spam[0].count(word) for word in test_corpus}
34/3:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
34/4:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
34/5:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
34/6: all_spam = get_messages(spam_path)
34/7:
def build_corpus(messages_vector):
    corpus = set()
    for message in messages_vector:
        message_words = re.findall(r'\w+', message)
        #print(message_words)
        corpus = corpus | set(message_words)
    return list(corpus)
34/8: test_corpus = build_corpus(all_spam)
34/9: print(test_corpus)
34/10:
def count_words(message, corpus):
    occurances = {word: message.count(word) for word in corpus if message.count(word)>1}
    return occurances
34/11: occurances = {word: all_spam[0].count(word) for word in test_corpus}
34/12: test = count_words(all_spam[0], corpus)
34/13: test = count_words(all_spam[0], test_corpus)
34/14: test
34/15: test = count_words([all_spam[0]], test_corpus)
34/16: test
34/17: test = count_words(all_spam[0], test_corpus)
34/18: test
34/19: all_spam[0]
34/20:
def count_words(message, corpus):
    occurances = {word: message.count(word) for word in corpus}
    return occurances
34/21: test = count_words(all_spam[0], test_corpus)
34/22: test
34/23: test.sorted(key=get())
34/24: test.sort(key=get())
34/25: sorted(test, key=test.get())
34/26: sorted(test, key=test.getitem())
34/27: sorted(test, key=test.get())
34/28: sorted(test, key=test.__getitem__())
34/29: sorted(test, key=test.__getitem__
34/30: sorted(test, key=test.__getitem__)
34/31: sorted(test, key=test.get)
34/32: sorted(test, key=test.get, reverse=True)
34/33: [(key, test[key]) for key in sorted(test, key=test.get) ]
34/34: [(key, test[key]) for key in sorted(test, key=test.get, reverse=True) ]
34/35: all_spam[0].count('e')
34/36: words_in_message =  re.findall(r'\w+', all_spam[0])
34/37: print(words_in_message)
34/38: from collections import Counter
34/39: print(Counter(words_in_message))
34/40: Counter(words_in_message)
34/41:
def count_words(message, corpus):
    words_in_message =  re.findall(r'\w+', message)
    occurances = Counter(words_in_message)
    return occurances
34/42: words_in_message =  re.findall(r'\w+', all_spam[0])
34/43: print(words_in_message)
34/44: from collections import Counter
34/45: Counter(words_in_message)
34/46: test = count_words(all_spam[0], test_corpus)
34/47: all_spam[0].count('e')
34/48:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
34/49:
def count_words(message, corpus):
    words_in_message =  re.findall(r'\w+', message)
    occurances = Counter(words_in_message)
    return occurances
34/50: matrix = pd.DataFrame()
34/51: test_occurance = count_words(all_spam[0])
34/52: test_occurance = count_words(all_spam[0], test_corpus)
34/53: test_occurance
34/54: matrix = pd.DataFrame(columns=[test_corpus])
34/55: matrix.head()
34/56: matrix = pd.DataFrame(data=test_corpus)
34/57: matrix.head()
34/58: matrix = pd.DataFrame(index=test_corpus)
34/59: matrix.head()
34/60: matrix['test'] = 0
34/61: matrix.head()
34/62: matrix[1] = 0
34/63: matrix.head()
34/64: matrix.drop(['test', 1], axis = 1)
34/65: matrix.head()
34/66: matrix.head()
34/67: matrix.drop(['test', 1], axis = 1, inplace=True)
34/68: matrix.head()
34/69: matrix.at['Minister', 'test'] = 10
34/70: matrix.head()
34/71: matrix.drop(['test'], axis = 1, inplace=True)
34/72: matrix.head()
34/73:
for key in test_occurance.keys():
    matrix.at[key, 1] = test_occurance[key]
34/74: matrix.head()
34/75: matrix.head(50)
34/76: matrix.loc['help']
34/77: matrix.head()
34/78: matrix.loc['home']
34/79:
for i, message in enumerate(all_spam):
    occurances = count_words(message, test_corpus)
    for key in occurances.keys():
        matrix[key,i] = occurances[key]
34/80:
for i, message in enumerate(all_spam[:10]):
    occurances = count_words(message, test_corpus)
    for key in occurances.keys():
        matrix[key,i] = occurances[key]
34/81: matrix.head()
34/82: matrix = pd.DataFrame(index=test_corpus)
34/83: matrix.head()
34/84:
for i, message in enumerate(all_spam[:10]):
    occurances = count_words(message, test_corpus)
    for key in occurances.keys():
        matrix.at[key,i] = occurances[key]
34/85: matrix.head()
34/86: matrix.head(25)
34/87: matrix.head(50)
34/88: len(all_spam)
34/89: matrix = pd.DataFrame(index=test_corpus)
34/90: matrix.head(50)
34/91:
for i, message in enumerate(all_spam[400:]):
    occurances = count_words(message, test_corpus)
    for key in occurances.keys():
        matrix.at[key,i] = occurances[key]
34/92: matrix.head(50)
34/93: sorted(all_spam, key=str.len())
34/94: sorted(all_spam, key=len(str))
34/95: sorted(all_spam, key=lambda x: len(x))
34/96: sorted(test_corpus, key=lambda x: len(x))
34/97: sorted(test_corpus, key=lambda x: len(x), reverse=True)
34/98: [x in test_corpus if len(x)>20]
34/99: [x for x in test_corpus if len(x)>20]
34/100: len([x for x in test_corpus if len(x)>20])
34/101: len(test_corpus)
34/102:
def build_corpus(messages_vector):
    corpus = set()
    for message in messages_vector:
        message_words = [x for x in re.findall(r'\w+', message) if len(x)<20 and not any(i.isdigit() for i in x)] 
        #print(message_words)
        corpus = corpus | set(message_words)
    return list(corpus)
34/103: test_corpus = build_corpus(all_spam)
34/104: print(test_corpus)
34/105: len(test_corpus)
34/106:
for i, message in enumerate(all_spam):
    occurances = count_words(message, test_corpus)
    for key in occurances.keys():
        matrix.at[key,i] = occurances[key]
34/107: matrix.head(50)
34/108: matrix = pd.DataFrame(index=test_corpus)
34/109:
for i, message in enumerate(all_spam):
    occurances = count_words(message, test_corpus)
    for key in occurances.keys():
        matrix.at[key,i] = occurances[key]
34/110: matrix.head(50)
34/111:
def build_corpus(messages_vector):
    corpus = set()
    for message in messages_vector:
        message_words = [x for x in re.findall(r'\w+', message) if len(x)<20 and x.isalpha()] 
        #print(message_words)
        corpus = corpus | set(message_words)
    return list(corpus)
34/112: test_corpus = build_corpus(all_spam)
34/113: print(test_corpus)
34/114:
def count_words(message, corpus):
    words_in_message =  re.findall(r'\w+', message)
    occurances = Counter(words_in_message)
    return occurances
34/115: test_occurance = count_words(all_spam[0], test_corpus)
34/116: matrix = pd.DataFrame(index=test_corpus)
34/117: matrix.head(50)
34/118:
for i, message in enumerate(all_spam):
    occurances = count_words(message, test_corpus)
    for key in occurances.keys():
        matrix.at[key,i] = occurances[key]
34/119:
for i, message in enumerate(all_spam[:100]):
    occurances = count_words(message, test_corpus)
    for key in occurances.keys():
        matrix.at[key,i] = occurances[key]
34/120:
for i, message in enumerate(all_spam[:200]):
    occurances = count_words(message, test_corpus)
    for key in occurances.keys():
        matrix.at[key,i] = occurances[key]
34/121:
for i, message in enumerate(all_spam[300]):
    occurances = count_words(message, test_corpus)
    for key in occurances.keys():
        matrix.at[key,i] = occurances[key]
34/122: matrix.head(50)
34/123: matrix = pd.DataFrame(index=test_corpus)
34/124: matrix.head(50)
34/125:
for i, message in enumerate(all_spam[:300]):
    occurances = count_words(message, test_corpus)
    for key in occurances.keys():
        matrix.at[key,i] = occurances[key]
36/1:
a=dict(a=1)
b=dict(a=2)
a.update(b)
print(a)
36/2:
def build_corpus(messages_vector):
    corpus = []
    for message in messages_vector:
        message_words = [x for x in re.findall(r'\w+', message) if len(x)<20 and x.isalpha()] 
        #print(message_words)
        corpus.append(message_words)
    return [x for x in corpus if Counter(corpus)[x] > 1]
36/3: test_corpus = build_corpus(all_spam)
36/4:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
36/5:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
36/6:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
36/7: all_spam = get_messages(spam_path)
36/8:
def build_corpus(messages_vector):
    corpus = []
    for message in messages_vector:
        message_words = [x for x in re.findall(r'\w+', message) if len(x)<20 and x.isalpha()] 
        #print(message_words)
        corpus.append(message_words)
    return [x for x in corpus if Counter(corpus)[x] > 1]
36/9: test_corpus = build_corpus(all_spam)
37/1:
a=dict(a=1)
b=dict(a=2)
37/2: l=[a, b]
37/3:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
37/4: pd.DataFrame(l)
37/5:
a=dict(a=1, b=2)
b=dict(a=2, c=3)
37/6: l=[a, b]
37/7: pd.DataFrame(l)
37/8: pd.DataFrame(l, orientation=index)
37/9: pd.DataFrame(l, orient='index')
37/10: pd.DataFrame.from_dict(l, orient='index')
37/11: pd.DataFrame.from_dict(l)
37/12: pd.DataFrame(l)
37/13: pd.DataFrame.from_dict(l, orient='index')
37/14: pd.DataFrame.from_dict(l, orient='columns')
37/15: pd.DataFrame.from_dict(l, orient='index')
37/16: z=pd.DataFrame(index=['a', 'b', 'c'])
37/17: zpd.DataFrame.from_dict(l, orient='index')
37/18: pd.DataFrame.from_dict(l, orient='index')
37/19: z.DataFrame.from_dict(l, orient='index')
37/20: z.from_dict(l, orient='index')
37/21: z=pd.DataFrame(l)
37/22: z
37/23: z = pd.DataFrame(l)
37/24: z
37/25: z.transpose()
37/26: z
37/27: z.transpose(inplace=True)
37/28: z=z.transpose(inplace=True)
37/29: z=z.transpose()
37/30: z
37/31: matrix.head()
37/32:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
37/33:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
37/34:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
37/35: all_spam = get_messages(spam_path)
37/36: spam_dicts = [Counter(message) for message in all_spam]
37/37: matrix = pd.DataFrame(spam_dicts)
37/38: matrix
37/39: all_spam
37/40: spam_dicts[0]
37/41:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    occurances = Counter(words_in_message)
    return occurances
37/42: spam_dicts = [count_words(message) for message in all_spam]
37/43: matrix = pd.DataFrame(spam_dicts)
37/44: spam_dicts[0]
37/45: matrix
37/46:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    occurances = Counter(words_in_message)
    return occurances
37/47: matrix = matrix.transpose()
37/48: matrix
37/49:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word for word in words_in_message if word.isalpha() and len(word)<20]
    occurances = Counter(words_in_message)
    return occurances
37/50:
def build_corpus(messages_vector):
    corpus = []
    for message in messages_vector:
        message_words = [x for x in re.findall(r'\w+', message) if len(x)<20 and x.isalpha()] 
        #print(message_words)
        corpus.append(message_words)
    return [x for x in corpus if Counter(corpus)[x] > 1]
37/51: spam_dicts = [count_words(message) for message in all_spam]
37/52: matrix = pd.DataFrame(spam_dicts)
37/53: matrix = matrix.transpose()
37/54: matrix
37/55: matrix.loc('html')
37/56: matrix.loc('dear')
37/57: matrix.iloc('dear')
37/58: matrix
37/59: matrix.loc('A')
37/60: matrix
37/61: matrix.index
37/62: matrix.loc('AAD')
37/63: matrix.loc['A']
37/64: matrix.loc['html']
37/65: matrix['sum']=matrix.sum(axis=1)
37/66: matrix
37/67: matrix=matrix.loc[matrix.sum>1]
37/68: matrix=matrix.loc[matrix['sum']>1]
37/69: matrix
37/70:
def build_corpus(messages_vector):
    corpus = []
    for message in messages_vector:
        message_words = [x for x in re.findall(r'\w+', message) if <1len(x)<20 and x.isalpha()] 
        #print(message_words)
        corpus.append(message_words)
    return [x for x in corpus if Counter(corpus)[x] > 1]
37/71:
def build_corpus(messages_vector):
    corpus = []
    for message in messages_vector:
        message_words = [x for x in re.findall(r'\w+', message) if 1<len(x)<20 and x.isalpha()] 
        #print(message_words)
        corpus.append(message_words)
    return [x for x in corpus if Counter(corpus)[x] > 1]
37/72:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word for word in words_in_message if word.isalpha() and 1<len(word)<20]
    occurances = Counter(words_in_message)
    return occurances
37/73: spam_dicts = [count_words(message) for message in all_spam]
37/74: matrix = pd.DataFrame(spam_dicts)
37/75: matrix = matrix.transpose()
37/76: matrix['sum']=matrix.sum(axis=1)
37/77: matrix=matrix.loc[matrix['sum']>1 ]
37/78: matrix
37/79: matrix.loc['html']
37/80: matrix
37/81: matrix['number_of_documents']=matrix.count(axis=1)
37/82: matrix
37/83: matrix.loc['AAD']
37/84:  z = [matrix.loc['AAD']>0]
37/85: z
37/86: matrix.loc['AAD'][z]
37/87:  z = [x for x in matrix.loc['AAD'] if x>0]
37/88: z
37/89: matrix['number_of_documents']=matrix.count(axis=1)-1
37/90: matrix
37/91: matrix['number_of_documents']=matrix.count(axis=1)-1
37/92:  z = [x for x in matrix.loc['AAD'] if x>0]
37/93: z
37/94: matrix['number_of_documents']=(matrix.count(axis=1)-1)
37/95:  z = [x for x in matrix.loc['AAD'] if x>0]
37/96: z
37/97: matrix
37/98: matrix.drop(columns=['number_of_documents'])
37/99: matrix
37/100: matrix
37/101: matrix=matrix.drop(columns=['number_of_documents'])
37/102: matrix
37/103: matrix['number_of_documents']=(matrix.count(axis=1)-1)
37/104: matrix
37/105: matrix.loc['kiedy']
37/106: matrix.loc['ów']
37/107: all_spam[6]
37/108: print(all_spam[6])
37/109: matrix.loc['jak']
37/110: matrix.loc['gdy']
37/111: matrix.loc['do']
37/112: matrix.loc['pl']
37/113: print(all_spam[8])
37/114: print(all_spam[99])
37/115: matrix.loc['/div']
37/116: matrix.loc['div']
37/117: matrix.loc['<']
37/118: matrix
37/119: matrix.loc['porn']
37/120: matrix.loc['www']
37/121: matrix.loc['viagra']
37/122: matrix.sort_values(by=['sum'])
37/123: matrix.sort_values(by=['sum'],ascending=False)
37/124: matrix.sort_values(by=['number_of_documents'],ascending=False)
38/1: matrix=matrix.drop(columns=['number_of_documents'])/len(all_spam)
38/2:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
38/3:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
38/4:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
38/5: all_spam = get_messages(spam_path)
38/6:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word for word in words_in_message if word.isalpha() and 1<len(word)<20]
    occurances = Counter(words_in_message)
    return occurances
38/7: spam_dicts = [count_words(message) for message in all_spam]
38/8: matrix = pd.DataFrame(spam_dicts)
38/9: matrix = matrix.transpose()
38/10: matrix['sum']=matrix.sum(axis=1)
38/11: matrix=matrix.loc[matrix['sum']>1 ]
38/12: matrix['number_of_documents']=(matrix.count(axis=1)-1)
38/13: matrix=matrix.drop(columns=['number_of_documents'])/len(all_spam)
38/14: matrix['documents_percentage']=matrix['number_of_documents']/
38/15:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
38/16:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
38/17:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
38/18: all_spam = get_messages(spam_path)
38/19:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word for word in words_in_message if word.isalpha() and 1<len(word)<20]
    occurances = Counter(words_in_message)
    return occurances
38/20: spam_dicts = [count_words(message) for message in all_spam]
38/21: matrix = pd.DataFrame(spam_dicts)
38/22: matrix = matrix.transpose()
38/23: matrix['sum']=matrix.sum(axis=1)
38/24: matrix=matrix.loc[matrix['sum']>1 ]
38/25: matrix['number_of_documents']=(matrix.count(axis=1)-1)
38/26: matrix['documents_percentage']=matrix['number_of_documents']/len(all_spam)
38/27: matrix.sort_values(by=['number_of_documents'],ascending=False)
38/28: stopwords=["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
38/29:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word for word in words_in_message if word.isalpha() and 1<len(word)<20 and word is not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
38/30:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word for word in words_in_message if word.isalpha() and 1<len(word)<20 and word not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
38/31: spam_dicts = [count_words(message) for message in all_spam]
38/32: matrix = pd.DataFrame(spam_dicts)
38/33: matrix = matrix.transpose()
38/34: matrix['sum']=matrix.sum(axis=1)
38/35: matrix=matrix.loc[matrix['sum']>1 ]
38/36: matrix['number_of_documents']=(matrix.count(axis=1)-1)
38/37: matrix['documents_percentage']=matrix['number_of_documents']/len(all_spam)
38/38: matrix.sort_values(by=['number_of_documents'],ascending=False)
38/39:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if word.isalpha() and 1<len(word)<20 and word not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
38/40: spam_dicts = [count_words(message) for message in all_spam]
38/41: matrix = pd.DataFrame(spam_dicts)
38/42: matrix = matrix.transpose()
38/43: matrix['sum']=matrix.sum(axis=1)
38/44: matrix=matrix.loc[matrix['sum']>1 ]
38/45: matrix['number_of_documents']=(matrix.count(axis=1)-1)
38/46: matrix['documents_percentage']=matrix['number_of_documents']/len(all_spam)
38/47: matrix.sort_values(by=['number_of_documents'],ascending=False)
38/48:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
38/49:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
38/50: stopwords=["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
38/51:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
38/52: all_spam = get_messages(spam_path)
38/53:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if word.isalpha() and 1<len(word)<20 and word not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
38/54: spam_dicts = [count_words(message) for message in all_spam]
38/55: matrix = pd.DataFrame(spam_dicts)
38/56: matrix = matrix.transpose()
38/57: matrix['sum']=matrix.sum(axis=1)
38/58: matrix=matrix.loc[matrix['sum']>1 ]
38/59: matrix['number_of_documents']=(matrix.count(axis=1)-1)
38/60: matrix['documents_percentage']=matrix['number_of_documents']/len(all_spam)
38/61: matrix.sort_values(by=['number_of_documents'],ascending=False)
38/62:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if word.isalpha() and 1<len(word)<20 and word.lower() not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
38/63: spam_dicts = [count_words(message) for message in all_spam]
38/64: matrix = pd.DataFrame(spam_dicts)
38/65: matrix = matrix.transpose()
38/66: matrix['sum']=matrix.sum(axis=1)
38/67: matrix=matrix.loc[matrix['sum']>1 ]
38/68: matrix['number_of_documents']=(matrix.count(axis=1)-1)
38/69: matrix['documents_percentage']=matrix['number_of_documents']/len(all_spam)
38/70: matrix.sort_values(by=['number_of_documents'],ascending=False)
38/71:
i = 0
for message in all_spam:
    if 'html' in message:
        i+=1
        
i/500
38/72: len(all_spam)
38/73:
i = 0
for message in all_spam:
    if 'html' in message:
        i+=1

print('i: ', i)
i/500
38/74:
x = 0
for message in all_dicts:
    if 'html' in message.keys():
        x+=1

print('x: ', x)
x/500
38/75:
x = 0
for message in spam_dicts:
    if 'html' in message.keys():
        x+=1

print('x: ', x)
x/500
38/76: len(spam_dicts)
39/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
39/2:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
39/3: stopwords=["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
39/4:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
39/5: all_spam = get_messages(spam_path)
39/6:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
39/7: spam_dicts = [count_words(message) for message in all_spam]
39/8: matrix = pd.DataFrame(spam_dicts)
39/9: matrix = matrix.transpose()
39/10: matrix['sum']=matrix.sum(axis=1)
39/11: matrix=matrix.loc[matrix['sum']>1 ]
39/12: matrix['number_of_documents']=(matrix.count(axis=1)-1)
39/13: matrix['documents_percentage']=matrix['number_of_documents']/len(all_spam)
39/14: matrix.sort_values(by=['number_of_documents'],ascending=False)
39/15:
i = 0
for message in all_spam:
    if 'html' in message:
        i+=1

print('i: ', i)
i/500
39/16: len(all_spam)
39/17:
x = 0
for message in spam_dicts:
    if 'html' in message.keys():
        x+=1

print('x: ', x)
x/500
39/18: len(spam_dicts)
40/1: matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
40/2:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
40/3:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
40/4: stopwords=["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
40/5:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
40/6: all_spam = get_messages(spam_path)
40/7:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
40/8: spam_dicts = [count_words(message) for message in all_spam]
40/9: matrix = pd.DataFrame(spam_dicts)
40/10: matrix = matrix.transpose()
40/11: matrix['sum']=matrix.sum(axis=1)
40/12: matrix=matrix.loc[matrix['sum']>1 ]
40/13: matrix['number_of_documents']=(matrix.count(axis=1)-1)
40/14: matrix['documents_percentage']=matrix['number_of_documents']/len(all_spam)
40/15: matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
40/16: matrix.sort_values(by=['number_of_documents'],ascending=False)
40/17:
i = 0
for message in all_spam:
    if 'html' in message:
        i+=1

print('i: ', i)
i/500
40/18: len(all_spam)
40/19:
x = 0
for message in spam_dicts:
    if 'html' in message.keys():
        x+=1

print('x: ', x)
x/500
40/20: len(spam_dicts)
40/21: matrix.sort_values(by=['sum'],ascending=False)
40/22: all_spam[496]
40/23:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
40/24:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
40/25: stopwords=["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
40/26:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
40/27: all_spam = get_messages(spam_path)
40/28:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
40/29: spam_dicts = [count_words(message) for message in all_spam]
40/30: matrix = pd.DataFrame(spam_dicts)
40/31: matrix = matrix.transpose()
40/32: matrix['sum']=matrix.sum(axis=1)
40/33: matrix=matrix.loc[matrix['sum']>1 ]
40/34: matrix['number_of_documents']=(matrix.count(axis=1)-1)
40/35: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)
40/36:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
40/37:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
40/38: stopwords=["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
40/39:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
40/40: all_spam = get_messages(spam_path)
40/41:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
40/42: spam_dicts = [count_words(message) for message in all_spam]
40/43: matrix_spam = pd.DataFrame(spam_dicts)
40/44: matrix_spam = matrix_spam.transpose()
40/45: matrix_spam['sum']=matrix_spam.sum(axis=1)
40/46: matrix_spam=matrix_spam.loc[matrix_spam['sum']>1 ]
40/47: matrix_spam['number_of_documents']=(matrix_spam.count(axis=1)-1)
40/48: matrix_spam_spam['documents_percentage']=matrix_spam_spam['number_of_documents']/len(all_spam)
40/49: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)
40/50: matrix_spam['word_density'] = matrix_spam['sum']/matrix_spam['sum'].sum()
40/51: matrix_spam.sort_values(by=['sum'],ascending=False)
40/52:
i = 0
for message in all_spam:
    if 'html' in message:
        i+=1

print('i: ', i)
i/500
40/53: len(all_spam)
40/54:
x = 0
for message in spam_dicts:
    if 'html' in message.keys():
        x+=1

print('x: ', x)
x/500
40/55: len(spam_dicts)
40/56: all_spam[496]
40/57:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
40/58:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
40/59: stopwords=["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
40/60:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
40/61: all_spam = get_messages(spam_path)
40/62:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
40/63: spam_dicts = [count_words(message) for message in all_spam]
40/64: matrix_spam = pd.DataFrame(spam_dicts)
40/65: matrix_spam = matrix_spam.transpose()
40/66: matrix_spam['sum']=matrix_spam.sum(axis=1)
40/67: matrix_spam=matrix_spam.loc[matrix_spam['sum']>1 ]
40/68: matrix_spam['number_of_documents']=(matrix_spam.count(axis=1)-1)
40/69: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)
40/70: matrix_spam['word_density'] = matrix_spam['sum']/matrix_spam['sum'].sum()
40/71: matrix_spam.sort_values(by=['sum'],ascending=False)
40/72:
i = 0
for message in all_spam:
    if 'html' in message:
        i+=1

print('i: ', i)
i/500
40/73: len(all_spam)
40/74:
x = 0
for message in spam_dicts:
    if 'html' in message.keys():
        x+=1

print('x: ', x)
x/500
40/75: len(spam_dicts)
40/76: all_spam[496]
40/77: matrix_spam.sort_values(by=['sum'],ascending=False).head()
40/78: all_ham = get_messages(spam_path)[:500]
40/79: ham_dicts = [count_words(message) for message in all_ham]
40/80:
matrix_ham = pd.DataFrame(ham_dicts)
matrix_ham = matrix_spam.transpose()
40/81:
matrix_spam['sum']=matrix_spam.sum(axis=1)
matrix_spam=matrix_spam.loc[matrix_spam['sum']>1 ]
40/82: matrix_spam['number_of_documents']=(matrix_spam.count(axis=1)-1)
40/83: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)
40/84:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
40/85:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
40/86: stopwords=["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
40/87:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
40/88: all_spam = get_messages(spam_path)
40/89:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
40/90: spam_dicts = [count_words(message) for message in all_spam]
40/91:
matrix_spam = pd.DataFrame(spam_dicts)
matrix_spam = matrix_spam.transpose()
40/92:
matrix_spam['sum']=matrix_spam.sum(axis=1)
matrix_spam=matrix_spam.loc[matrix_spam['sum']>1 ]
40/93: matrix_spam['number_of_documents']=(matrix_spam.count(axis=1)-1)
40/94: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)
40/95: matrix_spam['word_density'] = matrix_spam['sum']/matrix_spam['sum'].sum()
40/96: matrix_spam.sort_values(by=['sum'],ascending=False).head()
40/97: all_ham = get_messages(spam_path)[:500]
40/98: ham_dicts = [count_words(message) for message in all_ham]
40/99:
matrix_ham = pd.DataFrame(ham_dicts)
matrix_ham = matrix_ham.transpose()
40/100:
matrix_ham['sum']=matrix_ham.sum(axis=1)
matrix_ham=matrix_ham.loc[matrix_ham['sum']>1 ]
40/101: matrix_ham['number_of_documents']=(matrix_ham.count(axis=1)-1)
40/102: matrix_ham['documents_percentage']=matrix_ham['number_of_documents']/len(all_ham)
40/103: matrix_ham['word_density'] = matrix_ham['sum']/matrix_ham['sum'].sum()
40/104: matrix_ham.sort_values(by=['sum'],ascending=False).head()
40/105:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
40/106:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
40/107: stopwords=["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
40/108:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
40/109: all_spam = get_messages(spam_path)
40/110:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
40/111: spam_dicts = [count_words(message) for message in all_spam]
40/112:
matrix_spam = pd.DataFrame(spam_dicts)
matrix_spam = matrix_spam.transpose()
40/113:
matrix_spam['sum']=matrix_spam.sum(axis=1)
matrix_spam=matrix_spam.loc[matrix_spam['sum']>1 ]
40/114: matrix_spam['number_of_documents']=(matrix_spam.count(axis=1)-1)
40/115: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)
40/116: matrix_spam['word_density'] = matrix_spam['sum']/matrix_spam['sum'].sum()
40/117: matrix_spam.sort_values(by=['sum'],ascending=False).head()
40/118: all_ham = get_messages(ham_path)[:500]
40/119: all_ham = get_messages(easyham_path)[:500]
40/120: ham_dicts = [count_words(message) for message in all_ham]
40/121:
matrix_ham = pd.DataFrame(ham_dicts)
matrix_ham = matrix_ham.transpose()
40/122:
matrix_ham['sum']=matrix_ham.sum(axis=1)
matrix_ham=matrix_ham.loc[matrix_ham['sum']>1 ]
40/123: matrix_ham['number_of_documents']=(matrix_ham.count(axis=1)-1)
40/124: matrix_ham['documents_percentage']=matrix_ham['number_of_documents']/len(all_ham)
40/125: matrix_ham['word_density'] = matrix_ham['sum']/matrix_ham['sum'].sum()
40/126: matrix_ham.sort_values(by=['sum'],ascending=False).head()
40/127: matrix_ham.sort_values(by=['sum'],ascending=False).head(10)
40/128: matrix_ham.sort_values(by=['documents_percentage'],ascending=False).head(10)
40/129: matrix_ham['yahoo']
40/130: matrix_ham['linux']
40/131: matrix_ham['list']
40/132:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
40/133:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
40/134: stopwords=["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
40/135:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
40/136: all_spam = get_messages(spam_path)
40/137:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
40/138: spam_dicts = [count_words(message) for message in all_spam]
40/139:
matrix_spam = pd.DataFrame(spam_dicts)
matrix_spam = matrix_spam.transpose()
40/140:
matrix_spam['sum']=matrix_spam.sum(axis=1)
matrix_spam=matrix_spam.loc[matrix_spam['sum']>1 ]
40/141: matrix_spam['number_of_documents']=(matrix_spam.count(axis=1)-1)
40/142: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)
40/143: matrix_spam['word_density'] = matrix_spam['sum']/matrix_spam['sum'].sum()
40/144: matrix_spam.sort_values(by=['sum'],ascending=False).head()
40/145: all_ham = get_messages(easyham_path)[:500]
40/146: ham_dicts = [count_words(message) for message in all_ham]
40/147:
matrix_ham = pd.DataFrame(ham_dicts)
matrix_ham = matrix_ham.transpose()
40/148:
matrix_ham['sum']=matrix_ham.sum(axis=1)
matrix_ham=matrix_ham.loc[matrix_ham['sum']>1 ]
40/149: matrix_ham['number_of_documents']=(matrix_ham.count(axis=1)-1)
40/150: matrix_ham['documents_percentage']=matrix_ham['number_of_documents']/len(all_ham)
40/151: matrix_ham['word_density'] = matrix_ham['sum']/matrix_ham['sum'].sum()
40/152: matrix_ham.sort_values(by=['documents_percentage'],ascending=False).head(10)
40/153: matrix_ham['list']
40/154: matrix_ham.loc['list']
40/155: matrix_ham.loc['yahoo']
40/156: matrix_ham.loc['linux']
40/157:
def classify_email(email, prior=0.5, c=10**-6):
    pass
40/158:
def create_tdm(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix.transpose()
    matrix['sum']=matrix_spam.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
40/159: spam_matrix.index
40/160: spam_matrix.index()
40/161: matrix_spam.index()
40/162: matrix_spam.index
40/163:
def classify_emails(emails, prior=0.5, c=10**-6):
    messages = get_messages(emails)
    matrix = create_matrix(messages)
    common_words = set(spam_matrix.index).intersection(set(matrix.index))
40/164:
def classify_email(email, prior=0.5, c=10**-6):
    matrix = create_matrix(email)
    common_words = set(spam_matrix.index).intersection(set(matrix.index))
40/165:
def classify_email(email, prior=0.5, c=10**-6):
    matrix = create_matrix(email)
    common_words = set(spam_matrix.index).intersection(set(matrix.index))
    print(common_words)
40/166: classify_email(all_spam[0])
40/167:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix.transpose()
    
    matrix['sum']=matrix_spam.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
40/168: classify_email(all_spam[0])
40/169:
def classify_email(email, prior=0.5, c=10**-6):
    matrix = create_matrix(email)
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
40/170: classify_email(all_spam[0])
40/171:
def classify_email(email, prior=0.5, c=10**-6):
    matrix = create_matrix(email)
    matrix.head(10)
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
40/172: classify_email(all_spam[0])
40/173:
def classify_email(email, prior=0.5, c=10**-6):
    matrix = create_matrix(email)
    print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
40/174: classify_email(all_spam[0])
40/175: classify_email([all_spam[0]])
40/176:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix_spam.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
40/177: classify_email([all_spam[0]])
40/178:
def classify_email(email, prior=0.5, c=10**-6):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
40/179: classify_email([all_spam[0]])
40/180: matrix_spam.at['list', 'documents_percentage']
41/1: matrix_ham.sort_values(by=['documents_percentage'],ascending=False).head()
41/2:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
41/3:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
41/4: stopwords=["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
41/5:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
41/6: all_spam = get_messages(spam_path)
41/7:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
41/8:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix_spam.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
41/9: spam_dicts = [count_words(message) for message in all_spam]
41/10:
matrix_spam = pd.DataFrame(spam_dicts)
matrix_spam = matrix_spam.transpose()
41/11:
matrix_spam['sum']=matrix_spam.sum(axis=1)
matrix_spam=matrix_spam.loc[matrix_spam['sum']>1 ]
41/12: matrix_spam['number_of_documents']=(matrix_spam.count(axis=1)-1)
41/13: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)
41/14: matrix_spam['word_density'] = matrix_spam['sum']/matrix_spam['sum'].sum()
41/15: matrix_spam.sort_values(by=['sum'],ascending=False).head()
41/16: all_ham = get_messages(easyham_path)[:500]
41/17: ham_dicts = [count_words(message) for message in all_ham]
41/18:
matrix_ham = pd.DataFrame(ham_dicts)
matrix_ham = matrix_ham.transpose()
41/19:
matrix_ham['sum']=matrix_ham.sum(axis=1)
matrix_ham=matrix_ham.loc[matrix_ham['sum']>1 ]
41/20: matrix_ham['number_of_documents']=(matrix_ham.count(axis=1)-1)
41/21: matrix_ham['documents_percentage']=matrix_ham['number_of_documents']/len(all_ham)
41/22: matrix_ham['word_density'] = matrix_ham['sum']/matrix_ham['sum'].sum()
41/23: matrix_ham.sort_values(by=['documents_percentage'],ascending=False).head()
41/24:
def classify_email(email, prior=0.5, c=10**-6):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = re.findall(r'\w+', email[0])
    if common_words < 1:
        return prior*c**number_of_words
    else:
        probabilities = [matrix_spam.at[word,'documents_percentage'] for word in common_words]
        
        return(prior)
41/25: classify_email([all_spam[0]])
41/26:
def classify_email(email, prior=0.5, c=10**-6):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = re.findall(r'\w+', email[0])
    if len(common_words) < 1:
        return prior*c**number_of_words
    else:
        probabilities = [matrix_spam.at[word,'documents_percentage'] for word in common_words]
        
        return(prior)
41/27: classify_email([all_spam[0]])
41/28: classify_email([all_spam[1]])
41/29:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
41/30:
def classify_email(email, prior=0.5, c=10**-6):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = re.findall(r'\w+', email[0])
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probabilities_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        probability_uncommon = c**number_of_uncommon
        return(prior)
41/31:
def classify_email(email, prior=0.5, c=10**-6):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = re.findall(r'\w+', email[0])
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probabilities_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        probability_uncommon = c**number_of_uncommon
        return(prior*probability_common*probability_uncommon)
41/32: classify_email([all_spam[1]])
41/33:
def classify_email(email, prior=0.5, c=10**-6):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = len(re.findall(r'\w+', email[0]))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probabilities_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        probability_uncommon = c**number_of_uncommon
        return(prior*probability_common*probability_uncommon)
41/34: classify_email([all_spam[1]])
41/35:
def classify_email(email, prior=0.5, c=10**-6):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = len(re.findall(r'\w+', email[0]))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probabilities_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        probability_uncommon = c**number_of_uncommon_words
        return(prior*probability_common*probability_uncommon)
41/36: classify_email([all_spam[1]])
41/37:
def classify_email(email, prior=0.5, c=10**-6):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = len(re.findall(r'\w+', email[0]))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        probability_uncommon = c**number_of_uncommon_words
        return(prior*probability_common*probability_uncommon)
41/38: classify_email([all_spam[1]])
41/39:
def classify_email(email, prior=0.5, c=10**-6):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = len(re.findall(r'\w+', email[0]))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        print('probability_common: ', probability_common)
        probability_uncommon = c**number_of_uncommon_words
        print('probability_uncommon: ', probability_uncommon)
        return(prior*probability_common*probability_uncommon)
41/40: classify_email([all_spam[1]])
41/41: classify_email([all_spam[0]])
41/42: classify_email([all_spam[0y7]])
41/43: classify_email([all_spam[7]])
41/44:
def classify_email(email, prior=0.5, c=10**-3):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = len(re.findall(r'\w+', email[0]))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        print('probability_common: ', probability_common)
        probability_uncommon = c**number_of_uncommon_words
        print('probability_uncommon: ', probability_uncommon)
        return(prior*probability_common*probability_uncommon)
41/45: classify_email([all_spam[7]])
41/46:
def classify_email(email, prior=0.5, c=10**-1):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = len(re.findall(r'\w+', email[0]))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        print('probability_common: ', probability_common)
        probability_uncommon = c**number_of_uncommon_words
        print('probability_uncommon: ', probability_uncommon)
        return(prior*probability_common*probability_uncommon)
41/47: classify_email([all_spam[7]])
41/48:
def classify_email(email, prior=0.5, c=0.2):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = len(re.findall(r'\w+', email[0]))
    number_of_common_words = len(common_words)
    print()
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        print('probability_common: ', probability_common)
        probability_uncommon = c**number_of_uncommon_words
        print('probability_uncommon: ', probability_uncommon)
        return(prior*probability_common*probability_uncommon)
41/49: classify_email([all_spam[7]])
41/50: classify_email([all_spam[70]])
41/51:
def classify_email(email, prior=0.5, c=0.3):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = len(re.findall(r'\w+', email[0]))
    number_of_common_words = len(common_words)
    print()
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        print('probability_common: ', probability_common)
        probability_uncommon = c**number_of_uncommon_words
        print('probability_uncommon: ', probability_uncommon)
        return(prior*probability_common*probability_uncommon)
41/52: all_spam[70]
41/53: all_spam[7]
41/54: count_words(all_spam[7])
41/55: re.findall(r'\w+', all_spam[7])
41/56: re.findall(r'\w+', all_spam[7]).sorted()
41/57: sorted(re.findall(r'\w+', all_spam[7]))
41/58:
def classify_email(email, prior=0.5, c=0.):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = len(set(re.findall(r'\w+', email[0]))
    number_of_common_words = len(common_words)
    print()
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        print('probability_common: ', probability_common)
        probability_uncommon = c**number_of_uncommon_words
        print('probability_uncommon: ', probability_uncommon)
        return(prior*probability_common*probability_uncommon)
41/59:
def classify_email(email, prior=0.5, c=0.):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    print()
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        print('probability_common: ', probability_common)
        probability_uncommon = c**number_of_uncommon_words
        print('probability_uncommon: ', probability_uncommon)
        return(prior*probability_common*probability_uncommon)
41/60: classify_email([all_spam[70]])
41/61: classify_email([all_spam[7]])
41/62:
def classify_email(email, prior=0.5, c=0.1):
    matrix = create_matrix(email)
    #print(matrix.head(10))
    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    print()
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        print('probability_common: ', probability_common)
        probability_uncommon = c**number_of_uncommon_words
        print('probability_uncommon: ', probability_uncommon)
        return(prior*probability_common*probability_uncommon)
41/63: classify_email([all_spam[7]])
41/64: classify_email([all_spam[70]])
41/65: all_spam[70]
41/66: all_spam[170]
43/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
43/2:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
43/3: stopwords=["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
43/4:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
43/5:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
43/6:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix_spam.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
43/7: all_spam = get_messages(spam_path)
43/8: matrix_spam = create_matrix(all_spam)
43/9:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
43/10: all_spam = get_messages(spam_path)
43/11: matrix_spam = create_matrix(all_spam)
43/12: matrix_ham = create_matrix(all_ham)
43/13: all_ham = get_messages(easyham_path)[:500]
43/14: matrix_ham = create_matrix(all_ham)
43/15:
def classify_email_as(email, prior=0.5, c=0.1, corpus):
    matrix = create_matrix(email)

    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    print()
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
43/16: classify_email([all_spam[70]])
43/17:
def classify_email(email, prior=0.5, c=0.1, matrix):
    matrix = create_matrix(email)

    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    print()
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
43/18:
def classify_email(email,matrix, prior=0.5, c=0.1, ):
    matrix = create_matrix(email)

    common_words = set(matrix_spam.index).intersection(set(matrix.index))
    print(common_words)
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    print()
    number_of_uncommon_words = number_of_words - number_of_common_words
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])
        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
43/19:
def classify_email(email,matrix_corpus, prior=0.5, c=0.1, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
43/20: classify_email([all_spam[70]], matrix_spam)
43/21: classify_email([all_spam[70]], matrix_ham)
43/22:
spam_or_ham(email):
    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)
    if result >0:
        print('Spam!')
    else:
        print('Ham!')
43/23:
def spam_or_ham(email):
    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)
    if result >0:
        print('Spam!')
    else:
        print('Ham!')
43/24: spam_or_ham(all_spam[70])
43/25:
def spam_or_ham(email):
    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)
    print(result)
    if result >0:
        print('Spam!')
    else:
        print('Ham!')
43/26: spam_or_ham(all_spam[70])
43/27: spam_or_ham([all_spam[70]])
43/28:
for message in all_spam[:10]:
    spam_or_ham([message])
43/29:
def classify_email(email,matrix_corpus, prior=0.5, c=0.1, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
   # print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
43/30: classify_email([all_spam[70]], matrix_spam)
43/31: classify_email([all_spam[70]], matrix_ham)
43/32:
def spam_or_ham(email):
    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)
    #print(result)
    if result >0:
        print('Spam!')
    else:
        print('Ham!')
43/33: spam_or_ham([all_spam[70]])
43/34:
for message in all_spam[:10]:
    spam_or_ham([message])
43/35:
def classify_email(email,matrix_corpus, prior=0.5, c=0.1, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
   # print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
        #print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        #print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
43/36: classify_email([all_spam[70]], matrix_spam)
43/37: classify_email([all_spam[70]], matrix_ham)
43/38:
def spam_or_ham(email):
    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)
    #print(result)
    if result >0:
        print('Spam!')
    else:
        print('Ham!')
43/39: spam_or_ham([all_spam[70]])
43/40:
for message in all_spam[:10]:
    spam_or_ham([message])
43/41:
for message in all_spam[:50]:
    spam_or_ham([message])
43/42:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
   # print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
        #print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        #print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
43/43: classify_email([all_spam[70]], matrix_spam)
43/44: classify_email([all_spam[70]], matrix_ham)
43/45:
def spam_or_ham(email):
    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)
    #print(result)
    if result >0:
        print('Spam!')
    else:
        print('Ham!')
43/46: spam_or_ham([all_spam[70]])
43/47:
for message in all_spam[:50]:
    spam_or_ham([message])
43/48: [spam_or_ham([message]) for message in all_spam]
43/49:
def spam_or_ham(email):
    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)
    #print(result)
    if result >0:
        return 'Spam!'
    else:
        print 'Ham!'
43/50:
def spam_or_ham(email):
    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)
    #print(result)
    if result >0:
        return 'Spam!'
    else:
        return 'Ham!'
43/51: [spam_or_ham([message]) for message in all_spam]
43/52: spam_test_result = [spam_or_ham([message]) for message in all_spam]
43/53: Counter(spam_test_result)
43/54:
def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
   # print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
        #print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        #print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
43/55: classify_email([all_spam[70]], matrix_spam)
43/56: classify_email([all_spam[70]], matrix_ham)
43/57:
def spam_or_ham(email):
    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)
    #print(result)
    if result >0:
        return 'Spam!'
    else:
        return 'Ham!'
43/58: spam_test_result = [spam_or_ham([message]) for message in all_spam]
43/59: Counter(spam_test_result)
43/60:
def classify_email(email,matrix_corpus, prior=0.5, c=0.1, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
   # print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
        #print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        #print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
43/61: classify_email([all_spam[70]], matrix_spam)
43/62: classify_email([all_spam[70]], matrix_ham)
43/63:
def spam_or_ham(email):
    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)
    #print(result)
    if result >0:
        return 'Spam!'
    else:
        return 'Ham!'
43/64: spam_test_result = [spam_or_ham([message]) for message in all_spam]
43/65: Counter(spam_test_result)
43/66:
def classify_email(email,matrix_corpus, prior=0.2, c=0.1, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
   # print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
        #print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        #print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
43/67: classify_email([all_spam[70]], matrix_spam)
43/68: classify_email([all_spam[70]], matrix_ham)
43/69:
def spam_or_ham(email):
    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)
    #print(result)
    if result >0:
        return 'Spam!'
    else:
        return 'Ham!'
43/70: spam_test_result = [spam_or_ham([message]) for message in all_spam]
43/71: Counter(spam_test_result)
43/72:
def classify_email(email,matrix_corpus, prior=0.2, c=0.1, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
   # print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
        #print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        #print('probability_uncommon: ', probability_uncommon)
        
        return(prior-probability_common*probability_uncommon)
43/73: classify_email([all_spam[70]], matrix_spam)
43/74: classify_email([all_spam[70]], matrix_ham)
43/75:
def spam_or_ham(email):
    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)
    #print(result)
    if result >0:
        return 'Spam!'
    else:
        return 'Ham!'
43/76: spam_test_result = [spam_or_ham([message]) for message in all_spam]
43/77: Counter(spam_test_result)
43/78:
def classify_email(email,matrix_corpus, prior=0.2, c=0.1, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
   # print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
        #print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        #print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
43/79: classify_email([all_spam[70]], matrix_spam)
43/80: classify_email([all_spam[70]], matrix_ham)
43/81:
def spam_or_ham(email):
    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)
    #print(result)
    if result >0:
        return 'Spam!'
    else:
        return 'Ham!'
43/82: spam_test_result = [spam_or_ham([message]) for message in all_spam]
43/83: Counter(spam_test_result)
43/84: hard_ham = get_messages(hardham_path)
43/85: matrix_hardham = create_matrix(hard_ham)
43/86: len(hard_ham)
43/87: hardham_result = [spam_or_ham([message]) for message in hard_ham]
43/88: Counter(hardham_result)
43/89: hardham_correct = hardham_counter['Ham!']/len(hard_ham)
43/90: hardham_counter = Counter(hardham_result)
43/91: hardham_counter = Counter(hardham_result)
43/92: hardham_correct = hardham_counter['Ham!']/len(hard_ham)
43/93: hardham_correct
44/1:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham[message] for message in messages]
    classification_counter = Counter(classification)
    print(classification_counter)
44/2:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
44/3:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
44/4: stopwords=["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
44/5:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
44/6:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
44/7:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
44/8: all_spam = get_messages(spam_path)
44/9: matrix_spam = create_matrix(all_spam)
44/10: all_ham = get_messages(easyham_path)[:500]
44/11: matrix_ham = create_matrix(all_ham)
44/12:
def classify_email(email,matrix_corpus, prior=0.2, c=0.1, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
   # print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
        #print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        #print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
44/13: classify_email([all_spam[70]], matrix_spam)
44/14: classify_email([all_spam[70]], matrix_ham)
44/15:
def spam_or_ham(email):
    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)
    #print(result)
    if result >0:
        return 'Spam!'
    else:
        return 'Ham!'
44/16: hard_ham = get_messages(hardham_path)
44/17: matrix_hardham = create_matrix(hard_ham)
44/18:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham[message] for message in messages]
    classification_counter = Counter(classification)
    print(classification_counter)
44/19: evaluate_results(hard_ham)
44/20: evaluate_results(hard_ham, _)
44/21:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham[message] for message in messages]
    classification_counter = Counter(classification)
    #print(classification_counter)
44/22: evaluate_results(hard_ham, _)
44/23:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    #print(classification_counter)
44/24:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    print(classification_counter)
44/25: evaluate_results(hard_ham, _)
44/26: evaluate_results(all_spam, _)
44/27: evaluate_results(all_ham, _)
45/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
45/2:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
45/3: stopwords=["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
45/4:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
45/5:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
45/6:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
45/7: all_spam = get_messages(spam_path)
45/8: matrix_spam = create_matrix(all_spam)
45/9: all_ham = get_messages(easyham_path)[:500]
45/10: matrix_ham = create_matrix(all_ham)
45/11:
def classify_email(email,matrix_corpus, prior=0.2, c=0.1, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
   # print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
        #print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        #print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
45/12:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
45/13: hard_ham = get_messages(hardham_path)
45/14: matrix_hardham = create_matrix(hard_ham)
45/15:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
45/16: evaluate_results(hard_ham, _)
45/17: evaluate_results(all_spam, _)
45/18: evaluate_results(all_ham, _)
45/19: print(all_ham[0])
45/20: count_words([all_ham[0]])
45/21: count_words(all_ham[0])
45/22:
def classify_email(email,matrix_corpus, prior=0.2, c=0.1, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
45/23: classify_email(all_ham[0], matrix_spam)
45/24: classify_email([all_ham[0]], matrix_spam)
45/25: classify_email([all_ham[0]], matrix_ham)
45/26: matrix_spam.at(['rules', documents_percentage])
45/27: matrix_spam.at(['rules', 'documents_percentage'])
45/28: matrix_spam.at['rules', 'documents_percentage']
45/29: matrix_ham.at['rules', 'documents_percentage']
45/30: matrix_spam.at['want', 'documents_percentage']
45/31: matrix_ham.at['want', 'documents_percentage']
45/32: classify_email([all_ham[0]], matrix_ham)-classify_email([all_ham[0]], matrix_spam)
45/33: classify_email([all_ham[1]], matrix_ham)-classify_email([all_ham[1]], matrix_spam)
45/34:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
45/35: evaluate_results(all_ham, _)
45/36:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
45/37: evaluate_results(hard_ham, _)
45/38: evaluate_results(all_spam, _)
45/39: evaluate_results(all_ham, _)
45/40:
def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
45/41: evaluate_results(all_spam, _)
45/42: matrix_spam.at['html', 'documents_percentage']
45/43:
def classify_email(email,matrix_corpus, prior=0.5, c=0.0001, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
45/44:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
45/45: hard_ham = get_messages(hardham_path)
45/46: matrix_hardham = create_matrix(hard_ham)
45/47:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
45/48: evaluate_results(hard_ham, _)
45/49: evaluate_results(all_spam, _)
45/50: evaluate_results(all_ham, _)
45/51:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
45/52: stop_words = stopwords.words('english')
45/53:
import nltk
nltk.download()
45/54: stop_words = stopwords.words('english')
45/55:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
45/56: stop_words = stopwords.words('english')
45/57: stop_words = stopwords.words('english')
45/58:
import nltk
nltk.download()
45/59: stop_words = stopwords.words('english')
45/60: print(stopwords)
45/61: print(stop_words)
45/62:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
45/63:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
45/64:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
45/65: all_spam = get_messages(spam_path)
45/66: matrix_spam = create_matrix(all_spam)
45/67: all_ham = get_messages(easyham_path)[:500]
45/68: matrix_ham = create_matrix(all_ham)
45/69:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and word.lower() not in stop_words]
    occurances = Counter(words_in_message)
    return occurances
45/70:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
45/71: all_spam = get_messages(spam_path)
45/72: matrix_spam = create_matrix(all_spam)
45/73: all_ham = get_messages(easyham_path)[:500]
45/74: matrix_ham = create_matrix(all_ham)
45/75:
def classify_email(email,matrix_corpus, prior=0.5, c=0.0001, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
45/76:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
45/77: hard_ham = get_messages(hardham_path)
45/78: matrix_hardham = create_matrix(hard_ham)
45/79:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
45/80: evaluate_results(hard_ham, _)
45/81: evaluate_results(all_spam, _)
45/82: evaluate_results(all_ham, _)
45/83:
def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
45/84:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
45/85: hard_ham = get_messages(hardham_path)
45/86: matrix_hardham = create_matrix(hard_ham)
45/87:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
45/88: evaluate_results(hard_ham, _)
45/89: evaluate_results(all_spam, _)
45/90: evaluate_results(all_ham, _)
45/91:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
45/92: evaluate_results(all_spam, _)
45/93:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
45/94: help(nltk.FreqDist)
45/95:
import nltk
nltk.download()
47/1:
import nltk
nltk.download()
47/2:
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
47/3: fdist = FreqDist()
47/4:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
47/5:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
47/6: stop_words = stopwords.words('english')
47/7:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
47/8:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and word.lower() not in stop_words]
    occurances = Counter(words_in_message)
    return occurances
47/9:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
47/10: all_spam = get_messages(spam_path)
47/11: matrix_spam = create_matrix(all_spam)
47/12: all_ham = get_messages(easyham_path)[:500]
47/13: matrix_ham = create_matrix(all_ham)
47/14:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
47/15:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
47/16: hard_ham = get_messages(hardham_path)
47/17: matrix_hardham = create_matrix(hard_ham)
47/18:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
47/19: fdist = FreqDist(words.lower() for word in word_tokenize(all_spam[0]))
47/20: fdist = FreqDist(word.lower() for word in word_tokenize(all_spam[0]))
47/21: fdist
47/22: word_tokenize(all_spam[0])
47/23: print(fdist)
47/24: word_tokenize(all_spam[0])
47/25: cout_words(all_spam[0])
47/26: count_words(all_spam[0])
47/27: len(count_words(all_spam[0]).keys())
47/28: x=set(fdist)
47/29: print(x)
47/30: print(len(x)
47/31: print(len(x))
47/32: fdist = FreqDist(word.lower() for word in word_tokenize(all_spam[0]))
47/33: fdist
47/34: fdist['home']
47/35: fdist.keys()
47/36: print(len(fdist.keys()))
47/37: len(word_tokenize(all_spam[0]))
47/38: fdist.plot()
47/39: help(nltk.tokenize)
47/40: len(fdist.keys())
47/41: word_tokenize(all_spam[0])
47/42: len(word_tokenize(all_spam[0]))
47/43: fdist = FreqDist(word for word in word_tokenize(all_spam[0]))
47/44: fdist.plot()
47/45: fdist['home']
47/46: fdist.keys()
47/47: len(fdist.keys())
47/48: len(word_tokenize(all_spam[0]))
47/49: len(count_words(all_spam[0]).keys())
47/50: help(nltk.tokenize)
47/51: len(word_tokenize(all_spam[0]))
47/52:
from nltk.tokenize import word_tokenize, wordpunct_tokenize
from nltk.probability import FreqDist
47/53: wordpunt_tokenize(all_spam[0])
47/54: wordpunct_tokenize(all_spam[0])
47/55: len(wordpunct_tokenize(all_spam[0]))
48/1: stowp_words_pl = stopwords.words('polish')
48/2:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
48/3:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
48/4: stop_words = stopwords.words('english')
48/5: stowp_words_pl = stopwords.words('polish')
48/6:
import nltk
nltk.download()
48/7: help(stopwords.words)
48/8: stowp_words_pl = stopwords.words('polish')
48/9: stop_words_pl
48/10: stop_words_pl = stopwords.words('polish')
48/11: stop_words_pl
48/12: stopwords_pl = open('/Użytkownicy/wioletanytko/nltk_data/corpora/stopwords/polish')
48/13: stopwords_pl = open(r'/Użytkownicy/wioletanytko/nltk_data/corpora/stopwords/polish')
48/14: stopwords_pl = open(r'/Użytkownicy/wioletanytko/nltk_data/corpora/stopwords/german')
48/15: stopwords_pl = open('/Użytkownicy/wioletanytko//nltk_data/corpora/stopwords/german')
48/16: stopwords_pl = open('/Użytkownicy/wioletanytko/\nltk_data/corpora/stopwords/german')
48/17: stopwords_pl = open(r"/Użytkownicy/wioletanytko/nltk_data/corpora/stopwords/german")
48/18: stopwords_pl = open(r"/users/wioletanytko/nltk_data/corpora/stopwords/german")
48/19: stopwords_pl = open(r"/users/wioletanytko/nltk_data/corpora/stopwords/polish")
48/20: stopwords_txt = stopwords.read()
48/21: stopwords_pl = open(r"/users/wioletanytko/nltk_data/corpora/stopwords/polish", w)
48/22: help(open)
48/23: stopwords_pl = open(r"/users/wioletanytko/nltk_data/corpora/stopwords/polish", 'w')
48/24: stopwords_txt = stopwords_pl.read()
48/25: stopwords_pl = open(r"/users/wioletanytko/nltk_data/corpora/stopwords/polish", 'w', encoding='UTF-8')
48/26: stopwords_txt = stopwords_pl.read()
48/27: stopwords_pl = open(r"/users/wioletanytko/nltk_data/corpora/stopwords/polish", 'r+w', encoding='UTF-8')
48/28: stopwords_pl = open(r"/users/wioletanytko/nltk_data/corpora/stopwords/polish", 'r', encoding='UTF-8')
48/29: stopwords_txt = stopwords_pl.read()
48/30: stopwords_txt = stopwords_pl.read()
48/31: stopwords_pl.close()
48/32: stopwords_txt = stopwords_txt.replace('\n', '\s')
48/33: stopwords_pl = open(r"/users/wioletanytko/nltk_data/corpora/stopwords/polish", 'w', encoding='UTF-8')
48/34: stopwords_pl.write(stopwords_txt)
48/35: stopwords_pl.close()
48/36: stop_words_pl = stopwords.words('polish')
48/37: stop_words_pl
48/38: stopwords_txt
48/39: x='a\nb\nc'
48/40: print(x)
48/41: x
48/42: x = x.replace('\n','\s')
48/43: x
48/44: print(x)
48/45: a = ['a','b','c']
48/46: a
48/47: a = ['a\n','b\n','c\n']
48/48: a
48/49: print(a)
48/50: stopwords_pl = open(r"/users/wioletanytko/nltk_data/corpora/stopwords/polish", 'w+', encoding='UTF-8')
48/51: stopwords_txt = stopwords_pl.read()
49/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
49/2:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
49/3: stop_words = stopwords.words('english')
49/4: stop_words_pl = stopwords.words('polish')
49/5:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
49/6:
def count_words(message):
    words_in_message =  set(re.findall(r'\w+', message))
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and word.lower() not in stop_words]
    occurances = Counter(words_in_message)
    return occurances
49/7:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
49/8: all_spam = get_messages(spam_path)
49/9: matrix_spam = create_matrix(all_spam)
49/10: all_ham = get_messages(easyham_path)[:500]
49/11: matrix_ham = create_matrix(all_ham)
49/12:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
49/13:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
49/14: hard_ham = get_messages(hardham_path)
49/15: matrix_hardham = create_matrix(hard_ham)
49/16:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
49/17: evaluate_results(hard_ham, _)
49/18: evaluate_results(all_spam, _)
50/1:
def count_words(message):
    words_in_message =  set(re.findall(r'\w+', message))
    words_in_message = [word.lower() for word in words_in_message if len(word)>2 not word.isnumeric() and word.lower() not in stop_words]
    occurances = Counter(words_in_message)
    return occurances
50/2:
def count_words(message):
    words_in_message =  set(re.findall(r'\w+', message))
    words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]
    occurances = Counter(words_in_message)
    return occurances
50/3:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
50/4:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
50/5: stop_words = stopwords.words('english')
50/6: stop_words_pl = stopwords.words('polish')
50/7:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
50/8:
def count_words(message):
    words_in_message =  set(re.findall(r'\w+', message))
    words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]
    occurances = Counter(words_in_message)
    return occurances
50/9:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
50/10:
def count_words(message):
    #words_in_message =  set(re.findall(r'\w+', message))
    #words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]
    #occurances = Counter(words_in_message)
    occurances = FreqDist(word_tokenize(message))
    return occurances
50/11:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
50/12: all_spam = get_messages(spam_path)
50/13: matrix_spam = create_matrix(all_spam)
50/14: all_ham = get_messages(easyham_path)[:500]
50/15: matrix_ham = create_matrix(all_ham)
50/16:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
50/17:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
50/18: hard_ham = get_messages(hardham_path)
50/19: matrix_hardham = create_matrix(hard_ham)
50/20:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
50/21: evaluate_results(all_spam, _)
50/22:
def count_words(message):
    #words_in_message =  set(re.findall(r'\w+', message))
    #words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]
    #occurances = Counter(words_in_message)
    occurances = set(word_tokenize(message))
    return dict.fromkeys(occurances, 1)
50/23:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
50/24: all_spam = get_messages(spam_path)
50/25: matrix_spam = create_matrix(all_spam)
50/26: all_ham = get_messages(easyham_path)[:500]
50/27: matrix_ham = create_matrix(all_ham)
50/28:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
50/29:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
50/30: hard_ham = get_messages(hardham_path)
50/31: matrix_hardham = create_matrix(hard_ham)
50/32:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
50/33: evaluate_results(all_spam, _)
50/34: matrix_spam.head()
50/35: matrix_spam.head(20)
50/36:
def count_words(message):
    #words_in_message =  set(re.findall(r'\w+', message))
    #words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]
    #occurances = Counter(words_in_message)
    occurances = set([word.lower for word in word_tokenize(message) if len(word)>2 and not word.isnumeric and word.lower() not in stop_words])
    return dict.fromkeys(occurances, 1)
50/37: matrix_spam = create_matrix(all_spam)
50/38: matrix_ham = create_matrix(all_ham)
50/39:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
50/40:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
50/41: hard_ham = get_messages(hardham_path)
50/42: matrix_hardham = create_matrix(hard_ham)
50/43:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
50/44: evaluate_results(all_spam, _)
50/45: matrix_spam.head(20)
50/46: word_tokenize(all_spam[0])
50/47: set(word_tokenize(all_spam[0]))
50/48: dict.fromkeys(set(word_tokenize(all_spam[0])), 1)
50/49: all_spam = get_messages(spam_path)
50/50: matrix_spam = create_matrix(all_spam)
50/51: matrix_spam.head(20)
50/52:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    print(dicts)
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
50/53: all_spam = get_messages(spam_path)
50/54: matrix_spam = create_matrix(all_spam)
50/55:
def count_words(message):
    #words_in_message =  set(re.findall(r'\w+', message))
    #words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]
    #occurances = Counter(words_in_message)
    occurances = set([word.lower for word in word_tokenize(message) if len(word)>2 and not word.isnumeric and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    print('result_of_count_words:', result)
    return result
50/56:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    #print(dicts)
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
50/57: all_spam = get_messages(spam_path)
50/58: matrix_spam = create_matrix(all_spam)
50/59:
def count_words(message):
    #words_in_message =  set(re.findall(r'\w+', message))
    #words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]
    #occurances = Counter(words_in_message)
    occurances = set([word.lower for word in word_tokenize(message) if len(word)>2 and not word.isnumeric and word.lower() not in stop_words])
    print('occurances: ', occurances)
    result = dict.fromkeys(occurances, 1)
    print('result_of_count_words:', result)
    return result
50/60: matrix_spam = create_matrix(all_spam)
50/61:
def count_words(message):
    #words_in_message =  set(re.findall(r'\w+', message))
    #words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]
    #occurances = Counter(words_in_message)
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words])
    print('occurances: ', occurances)
    result = dict.fromkeys(occurances, 1)
    print('result_of_count_words:', result)
    return result
50/62: matrix_spam = create_matrix(all_spam)
50/63:
def count_words(message):
    #words_in_message =  set(re.findall(r'\w+', message))
    #words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]
    #occurances = Counter(words_in_message)
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words])
  #  print('occurances: ', occurances)
    result = dict.fromkeys(occurances, 1)
  #  print('result_of_count_words:', result)
    return result
50/64: evaluate_results(all_spam, _)
50/65: all_ham = get_messages(easyham_path)[:500]
50/66: matrix_ham = create_matrix(all_ham)
50/67:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
50/68:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
50/69: hard_ham = get_messages(hardham_path)
50/70: matrix_hardham = create_matrix(hard_ham)
50/71:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
50/72: evaluate_results(all_spam, _)
50/73: %time evaluate_results(all_spam, _)
50/74: matrix_spam.head(20)
50/75: all_ham = get_messages(easyham_path)[:500]
50/76: matrix_ham.head()
50/77:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
50/78:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
50/79: hard_ham = get_messages(hardham_path)
50/80: matrix_hardham = create_matrix(hard_ham)
50/81:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
50/82: %time evaluate_results(all_spam, _)
50/83: evaluate_results(hard_ham, _)
50/84: create_matrix(all_spam[0])
50/85: create_matrix([all_spam[0]])
50/86: cre
50/87: create_matrix([all_spam[0]])
50/88: cre
50/89: create_matrix([all_spam[1]])
50/90: all_spam
50/91: create_matrix([all_spam[9]])
50/92: all_spam
50/93: create_matrix([all_spam[9]])
50/94: create_matrix([all_spam[9]])
50/95:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    print(dicts)
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
50/96: create_matrix([all_spam[9]])
51/1: 'a b c d'.split(' ')
51/2:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
51/3: ps = PorterStemmer()
51/4:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
51/5:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
51/6: dict.fromkeys(set(word_tokenize(all_spam[0])), 1)
51/7:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
51/8:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
51/9: ps = PorterStemmer()
51/10: stop_words = stopwords.words('english')
51/11: stop_words_pl = stopwords.words('polish')
51/12:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
51/13:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
51/14:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    print(dicts)
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/15: all_spam = get_messages(spam_path)
51/16: matrix_spam = create_matrix(all_spam)
51/17: matrix_spam.head(20)
51/18: all_ham = get_messages(easyham_path)[:500]
51/19: matrix_ham = create_matrix(all_ham)
51/20:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and word.asalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
51/21:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/22: all_spam = get_messages(spam_path)
51/23: matrix_spam = create_matrix(all_spam)
51/24:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
51/25: dict.fromkeys(set(word_tokenize(all_spam[0])), 1)
51/26:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/27: all_spam = get_messages(spam_path)
51/28: matrix_spam = create_matrix(all_spam)
51/29: matrix_spam.head(20)
51/30: all_ham = get_messages(easyham_path)[:500]
51/31: all_ham = get_messages(easyham_path)[:500]
51/32: matrix_ham = create_matrix(all_ham)
51/33: matrix_ham.head()
51/34:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
51/35:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
51/36: hard_ham = get_messages(hardham_path)
51/37: matrix_hardham = create_matrix(hard_ham)
51/38:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
51/39: evaluate_results(hard_ham, _)
51/40: %time evaluate_results(all_spam, _)
51/41: count_words(all_spam[0])
51/42: matrix_ham
51/43: create_matrix([all_spam[9]])
51/44: create_matrix([all_spam[0]])
51/45:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    print(dicts)
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/46: create_matrix([all_spam[0]])
51/47: matrix_spam.head()
51/48:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    print(dicts)
    matrix = pd.DataFrame(dicts)
    matrix.head()
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/49: create_matrix([all_spam[0]])
51/50:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    print(dicts)
    matrix = pd.DataFrame.from_dict(dicts)
    matrix.head()
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/51: create_matrix([all_spam[0]])
51/52: d1 = dict(a=1, b=2)
51/53: df = pd.DataFrame([d1])
51/54: df
51/55:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    print(dicts)
    matrix = pd.DataFrame(dicts)
    matrix.head()
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/56: create_matrix([all_spam[0]])
51/57: d2 = dict(a=3, b=4)
51/58: df = pd.DataFrame([d1, d2])
51/59: df
51/60:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    print(dicts)
    matrix = pd.DataFrame(dicts)
    print(matrix)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/61: all_spam = get_messages(spam_path)
51/62: create_matrix([all_spam[0]])
51/63:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    print(dicts)
    matrix = pd.DataFrame(dicts)
    matrix.head()
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/64: create_matrix([all_spam[0]])
51/65:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    print(dicts)
    matrix = pd.DataFrame(dicts)
    matrix.head()
    matrix = matrix.transpose()
    matrix.head()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/66: create_matrix([all_spam[0]])
51/67:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    print(dicts)
    matrix = pd.DataFrame(dicts)
    matrix.head()
    matrix = matrix.transpose()
    matrix.head
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/68: create_matrix([all_spam[0]])
51/69: xxx = create_matrix([all_spam[0]])
51/70: xxx
51/71: xxx.head()
51/72:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    print(dicts)
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    print(matrix)
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/73: xxx = create_matrix([all_spam[0]])
51/74: xxx.head()
51/75:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
51/76:
import nltk
nltk.download()
51/77:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
51/78: ps = PorterStemmer()
51/79: stop_words = stopwords.words('english')
51/80: stop_words_pl = stopwords.words('polish')
51/81:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
51/82:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
51/83:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    print(dicts)
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    print(matrix)
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/84: all_spam = get_messages(spam_path)
51/85: matrix_spam = create_matrix(all_spam)
51/86: all_ham = get_messages(easyham_path)[:500]
51/87: matrix_ham = create_matrix(all_ham)
51/88:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
51/89:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
51/90: hard_ham = get_messages(hardham_path)
51/91: matrix_hardham = create_matrix(hard_ham)
51/92:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
51/93: #evaluate_results(hard_ham, _)
51/94: %time evaluate_results(all_spam, _)
51/95: #evaluate_results(all_ham, _)
51/96: xxx = create_matrix([all_spam[0]])
51/97: xxx.head()
51/98: xxx.head(10)
51/99: xxx = create_matrix([all_spam[0]])
51/100: xxx = create_matrix([all_spam[1]])
51/101: xxx.head(10)
51/102: xxx = create_matrix([all_spam[1]])
51/103: xxx.head(10)
51/104: xxx
51/105:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
51/106:
#import nltk
#nltk.download()
51/107:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
51/108: ps = PorterStemmer()
51/109: stop_words = stopwords.words('english')
51/110: stop_words_pl = stopwords.words('polish')
51/111:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
51/112:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
51/113:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/114: all_spam = get_messages(spam_path)
51/115: matrix_spam = create_matrix(all_spam)
51/116: all_ham = get_messages(easyham_path)[:500]
51/117: matrix_ham = create_matrix(all_ham)
51/118:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
51/119:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
51/120: hard_ham = get_messages(hardham_path)
51/121: matrix_hardham = create_matrix(hard_ham)
51/122:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
51/123: #evaluate_results(hard_ham, _)
51/124: %time evaluate_results(all_spam, _)
51/125:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/126:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
51/127:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
51/128: %time evaluate_results(all_spam, _)
51/129: xxx = create_matrix([all_spam[1]])
51/130: xxx.
51/131: xxx
51/132: xxx = create_message_matrix([all_spam[1]])
51/133: xxx
51/134: evaluate_results(hard_ham, _)
52/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
52/2:
#import nltk
#nltk.download()
52/3:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
52/4: ps = PorterStemmer()
52/5: stop_words = stopwords.words('english')
52/6: stop_words_pl = stopwords.words('polish')
52/7:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
52/8:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
52/9:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/10:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/11: all_spam = get_messages(spam_path)
52/12:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=0)
    
    matrix['number_of_documents']=(matrix.count(axis=0)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/13: all_spam = get_messages(spam_path)
52/14: matrix_spam = create_matrix(all_spam)
52/15:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=0)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=0)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/16:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=0)
    
    matrix['number_of_documents']=(matrix.count(axis=0)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/17: all_spam = get_messages(spam_path)
52/18: matrix_spam = create_corpus_matrix(all_spam)
52/19:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=0)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=0)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/20:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=0)
    
    matrix['number_of_documents']=(matrix.count(axis=0)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/21: all_spam = get_messages(spam_path)
52/22: matrix_spam = create_corpus_matrix(all_spam)
52/23: %time evaluate_results(all_spam, _)
52/24:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
52/25:
#import nltk
#nltk.download()
52/26:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
52/27: ps = PorterStemmer()
52/28: stop_words = stopwords.words('english')
52/29: stop_words_pl = stopwords.words('polish')
52/30:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
52/31:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
52/32:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=0)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=0)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/33:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=0)
    
    matrix['number_of_documents']=(matrix.count(axis=0)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/34: all_spam = get_messages(spam_path)
52/35: matrix_spam = create_corpus_matrix(all_spam)
52/36: all_ham = get_messages(easyham_path)[:500]
52/37: matrix_ham = create_matrix(all_ham)
52/38: matrix_ham = create_corpus_matrix(all_ham)
52/39:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
52/40:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
52/41: hard_ham = get_messages(hardham_path)
52/42: matrix_hardham = create_matrix(hard_ham)
52/43: matrix_hardham = create_corpus_matrix(hard_ham)
52/44:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
52/45: #evaluate_results(hard_ham, _)
52/46: %time evaluate_results(all_spam, _)
52/47: #evaluate_results(all_ham, _)
52/48: xxx = create_message_matrix([all_spam[1]])
52/49: xxx
52/50: evaluate_results(hard_ham, _)
52/51: matrix_all_spam.head()
52/52: matrix_spam.head()
52/53:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/54:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/55: all_spam = get_messages(spam_path)
52/56: matrix_spam = create_corpus_matrix(all_spam)
52/57: matrix_spam.head()
52/58: matrix_spam.head(10)
52/59:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame.from_dict(dicts)
   # matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/60: matrix_spam = create_corpus_matrix(all_spam)
52/61: matrix_spam.head(10)
52/62:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame.from_dict(dicts, orient='index')
   # matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/63: matrix_spam = create_corpus_matrix(all_spam)
52/64: d1 = dict(a=1, b=2)
52/65: d2=dict(b=3, c=4)
52/66: pd1 = pd.DataFrame(d1)
52/67: pd1 = pd.DataFrame([d1])
52/68: pd1.head()
52/69: pd1 = pd.DataFrame(d1, orient='index')
52/70: pd1 = pd.DataFrame.from_dict(d1, orient='index')
52/71: pd1.head()
52/72: pd2.DataFrame.from_dict(d2, orient='index')
52/73: pd2 = pd.DataFrame.from_dict(d2, orient='index')
52/74: pd2
52/75: pd1.join(pd2)
52/76: pd1.merge(pd2)
52/77: pd1.join(pd2, how='outer')
52/78: pd1.join(pd2, lsuffix='_caller', rsuffix='_other')
52/79: x=pd.DataFrame.join([pd1, pd2])
52/80: x=pd.DataFrame.join(pd1, pd2)
52/81: pd1 = pd.DataFrame.from_dict(d1)
52/82: pd1 = pd.DataFrame(d1)
52/83: pd = pd.DataFrame([d1,d2])
52/84: pd
52/85:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
52/86: pd3 = pd.DataFrame([d1,d2])
52/87: pd3
52/88: pd3.loc['summa']= pd3.count()
52/89: pd3
52/90:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix.loc['sum']=matrix.sum(axis=1)
    matrix=matrix[matrix['sum']>1 ]
    
    matrix.loc['number_of_documents']=(matrix.count(axis=1)-1)
    matrix.loc['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
52/91: all_spam = get_messages(spam_path)
52/92: matrix_spam = create_corpus_matrix(all_spam)
52/93:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix.loc['sum']=matrix.sum(axis=1)
    matrix=matrix[matrix['sum']>1 ]
    
    matrix.loc['number_of_documents']=(matrix.count(axis=1)-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
52/94: matrix_spam = create_corpus_matrix(all_spam)
52/95:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    #matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/96: all_spam = get_messages(spam_path)
52/97: matrix_spam = create_corpus_matrix(all_spam)
52/98:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    #matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/99: all_spam = get_messages(spam_path)
52/100: matrix_spam = create_corpus_matrix(all_spam)
52/101:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix.loc['sum']=matrix.sum(axis=1)
    matrix=matrix[matrix['sum']>1 ]
    
    matrix.loc['number_of_documents']=(matrix.count(axis=1)-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
52/102:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/103: all_spam = get_messages(spam_path)
52/104: matrix_spam = create_corpus_matrix(all_spam)
52/105: %time evaluate_results(all_spam, _)
52/106: matrix_spam.head(10)
52/107:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
    matrix=matrix[matrix['sum']>1 ]
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
52/108: matrix_spam = create_corpus_matrix(all_spam)
52/109: %time evaluate_results(all_spam, _)
52/110: matrix_spam.head(10)
52/111: pd3.loc['number_of_documents']=pd3.count()-1
52/112: pd3
52/113:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
   # matrix.loc['summa']=matrix.sum()
   # matrix=matrix[matrix['sum']>1 ]
    
   # matrix.loc['number_of_documents']=(matrix.count()-1)
   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
52/114: matrix_spam = create_corpus_matrix(all_spam)
52/115: matrix_spam.head()
52/116: matrix_spam.head(10)
52/117:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
52/118:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
52/119: matrix_spam = create_corpus_matrix(all_spam)
52/120: matrix_spam.head(10)
52/121:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
   # matrix=matrix[matrix['sum']>1 ]
    
   # matrix.loc['number_of_documents']=(matrix.count()-1)
   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
52/122: matrix_spam = create_corpus_matrix(all_spam)
52/123: matrix_spam.head(10)
52/124: matrix_spam.tail(10)
52/125:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
    matrix=matrix[matrix['sum']>1 ]
    
   # matrix.loc['number_of_documents']=(matrix.count()-1)
   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
52/126: matrix_spam = create_corpus_matrix(all_spam)
52/127: matrix_spam.tail(10)
52/128:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
    matrix=matrix[matrix.loc['sum']>1 ]
    
   # matrix.loc['number_of_documents']=(matrix.count()-1)
   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
52/129:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/130: all_spam = get_messages(spam_path)
52/131: matrix_spam = create_corpus_matrix(all_spam)
52/132:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
    matrix=matrix[matrix.loc['summa']>1 ]
    
   # matrix.loc['number_of_documents']=(matrix.count()-1)
   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
52/133:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
52/134: all_spam = get_messages(spam_path)
52/135: matrix_spam = create_corpus_matrix(all_spam)
53/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
53/2:
#import nltk
#nltk.download()
53/3:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
53/4: ps = PorterStemmer()
53/5: stop_words = stopwords.words('english')
53/6: stop_words_pl = stopwords.words('polish')
53/7:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
53/8:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
53/9:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
    matrix=matrix[matrix.loc['summa']>1 ]
    
   # matrix.loc['number_of_documents']=(matrix.count()-1)
   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
53/10:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
53/11: all_spam = get_messages(spam_path)
53/12: matrix_spam = create_corpus_matrix(all_spam)
53/13:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
    matrix=matrix.loc[matrix.loc['summa']>1 ]
    
   # matrix.loc['number_of_documents']=(matrix.count()-1)
   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
53/14: matrix_spam = create_corpus_matrix(all_spam)
53/15:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
   # matrix=matrix.loc[matrix.loc['summa']>1 ]
    
   # matrix.loc['number_of_documents']=(matrix.count()-1)
   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
53/16: matrix_spam = create_corpus_matrix(all_spam)
53/17: %time matrix_spam = create_corpus_matrix(all_spam)
53/18: all_ham = get_messages(easyham_path)[:500]
53/19: matrix_ham = create_corpus_matrix(all_ham)
53/20:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
53/21:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
53/22:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
53/23: %time evaluate_results(all_spam, _)
53/24: matrix_spam.tail(10)
53/25:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
   # matrix=matrix.loc[matrix.loc['summa']>1 ]
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
53/26: %time matrix_spam = create_corpus_matrix(all_spam)
53/27: matrix_spam.tail(10)
53/28: %time evaluate_results(all_spam, _)
53/29:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
53/30:
#import nltk
#nltk.download()
53/31:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
53/32: ps = PorterStemmer()
53/33:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
53/34:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha()])
    result = dict.fromkeys(occurances, 1)
    return result
53/35:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
    matrix=matrix.loc[matrix.loc['summa']>1 ]
    
   # matrix.loc['number_of_documents']=(matrix.count()-1)
   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
53/36:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
53/37: all_spam = get_messages(spam_path)
53/38: matrix_spam = create_corpus_matrix(all_spam)
54/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
54/2:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
54/3: stopwords=["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
54/4:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
54/5:
def count_words(message):
    words_in_message =  re.findall(r'\w+', message)
    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]
    occurances = Counter(words_in_message)
    return occurances
54/6:
def create_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    matrix=matrix.loc[matrix['sum']>1 ]
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
54/7: all_spam = get_messages(spam_path)
54/8: matrix_spam = create_matrix(all_spam)
54/9: all_ham = get_messages(easyham_path)[:500]
54/10: matrix_ham = create_matrix(all_ham)
54/11:
def classify_email(email,matrix_corpus, prior=0.5, c=0.0001, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
54/12:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
54/13: hard_ham = get_messages(hardham_path)
54/14: matrix_hardham = create_matrix(hard_ham)
54/15:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
54/16: %time evaluate_results(all_spam, _)
54/17:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
54/18: %time evaluate_results(all_spam, _)
55/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
55/2:
#import nltk
#nltk.download()
55/3:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
55/4: ps = PorterStemmer()
55/5:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
55/6:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
55/7:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
    matrix=matrix.loc[matrix.loc['summa']>1 ]
    
   # matrix.loc['number_of_documents']=(matrix.count()-1)
   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
55/8:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
55/9: all_spam = get_messages(spam_path)
55/10: matrix_spam = create_corpus_matrix(all_spam)
55/11: stop_words = stopwords('english')
55/12: stop_words = stopwords.stopwords('english')
55/13: stop_words = stopwords.words('english')
55/14:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
55/15:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
55/16:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
   # matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
    matrix=matrix.loc[matrix.loc['summa']>1 ]
    
   # matrix.loc['number_of_documents']=(matrix.count()-1)
   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
55/17:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
55/18: all_spam = get_messages(spam_path)
55/19: matrix_spam = create_corpus_matrix(all_spam)
55/20:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    #matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
   # matrix=matrix.loc[matrix.loc['summa']>1 ]
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
55/21: matrix_spam = create_corpus_matrix(all_spam)
55/22:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    #matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
   # matrix=matrix.loc[matrix.loc['summa']>1 ]
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
   # matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
55/23: matrix_spam = create_corpus_matrix(all_spam)
55/24: matrix_spam.head()
55/25: matrix_spam.tail()
55/26:
def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):
    matrix_email = create_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
55/27:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
55/28:
def evaluate_results(messages, correct_group):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
55/29: %time evaluate_results(all_spam, _)
55/30:
def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
55/31: %time evaluate_results(all_spam, _)
55/32: all_ham = get_messages(easyham_path)[:500]
55/33: matrix_ham = create_corpus_matrix(all_ham)
55/34: %time evaluate_results(all_spam, _)
55/35: matrix_spam.index
55/36: matrix_spam.columns
55/37:
def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.column).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
55/38: %time evaluate_results(all_spam, _)
55/39:
def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.columns).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
55/40: %time evaluate_results(all_spam, _)
55/41:
def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.columns).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at['documents_percentage', word] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
55/42:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 'Spam!'
    else:
        return 'Ham!'
55/43: %time evaluate_results(all_spam, _)
55/44:
def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.columns).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at['documents_percentage', word] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
55/45: %time evaluate_results(all_spam, _)
55/46: matrix_spam.tail()
55/47:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.columns).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at['documents_percentage', word] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
55/48: unnecessary_columns = matrix_spam.columns
55/49: unnecessry_columns
55/50: unnecessary_columns
55/51: unnecessary_indexes = matrix_spam.index - ['documents_percentage']
55/52: unnecessary_indexes = matrix_spam.index
55/53: unnecessary_indexes
55/54: matrix_spam = matrix_spam.drop(columns = list(range(500)))
55/55: spam_percentages = matrix_spam.loc['spam_percentages']
55/56: spam_percentages = matrix_spam.loc['documents_percentage']
55/57: spam_percentages
55/58: spam_percentages.min()
55/59: matrix_spam = matrix_spam.loc['documents_percentage']
55/60: matrix_spam.tail()
55/61: %time evaluate_results(all_spam, _)
55/62:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.at['documents_percentage', word] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
55/63: %time evaluate_results(all_spam, _)
55/64: matrix_spam.index
55/65: matrix_spam.loc('house')
55/66: matrix_spam.loc['house']
55/67: matrix_spam['house']
55/68: matrix_spam.head
55/69: matrix_spam.loc['abu']
55/70:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.loc[ word] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
55/71: %time evaluate_results(all_spam, _)
55/72:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
55/73: x = (x for x in 'abcs')
55/74: x = (x*2 for x in 'abcd')
55/75: Counter(x)
55/76:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
55/77: ps = PorterStemmer()
55/78: stop_words = stopwords.words('english')
55/79:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
55/80:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
55/81:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    #matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
   # matrix=matrix.loc[matrix.loc['summa']>1 ]
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
   # matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
55/82:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
55/83: all_spam = get_messages(spam_path)
55/84:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
55/85:
#import nltk
#nltk.download()
55/86:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
55/87: ps = PorterStemmer()
55/88: stop_words = stopwords.words('english')
55/89:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
55/90:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
55/91:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    #matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
   # matrix=matrix.loc[matrix.loc['summa']>1 ]
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
   # matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()
    
    return matrix
55/92:
def create_message_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()
    
    return matrix
55/93: all_spam = get_messages(spam_path)
55/94: matrix_spam = create_corpus_matrix(all_spam)
55/95: matrix_spam = matrix_spam.loc['documents_percentage']
55/96: matrix_spam.loc['abu']
55/97: all_ham = get_messages(easyham_path)[:500]
55/98: matrix_ham = create_corpus_matrix(all_ham)
55/99:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#
#    print(common_words)
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.loc[ word] for word in common_words])
#        print('probability_common: ', probability_common)
        
        probability_uncommon = c**number_of_uncommon_words
#        print('probability_uncommon: ', probability_uncommon)
        
        return(prior*probability_common*probability_uncommon)
55/100:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 0
    else:
        return 1
55/101: hard_ham = get_messages(hardham_path)
55/102: matrix_hardham = create_corpus_matrix(hard_ham)
55/103:
def evaluate_results(messages):
    classification = [spam_or_ham([message]) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter[0]/len(messages), classification_counter[1]/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
55/104: #evaluate_results(hard_ham)
55/105: %time evaluate_results(all_spam)
55/106:
def evaluate_results(messages):
    classification = (spam_or_ham([message]) for message in messages)
    classification_counter = Counter(classification)
    results = (classification_counter[0]/len(messages), classification_counter[1]/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
55/107: #evaluate_results(hard_ham)
55/108: %time evaluate_results(all_spam)
55/109:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    #matrix = matrix.transpose()
    
    matrix.loc['summa']=matrix.sum()
   # matrix=matrix.loc[matrix.loc['summa']>1 ]
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
55/110:
def create_message_matrix(messages):
    dicts = count_words(message)
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    
    return matrix
55/111:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
    
    number_of_words = len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.loc[ word] for word in common_words])
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
55/112:
def evaluate_results(messages):
    classification = (spam_or_ham(message) for message in messages)
    classification_counter = Counter(classification)
    results = (classification_counter[0]/len(messages), classification_counter[1]/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
55/113: %time evaluate_results(all_spam)
55/114:
def create_message_matrix(message):
    dicts = count_words(message)
    matrix = pd.DataFrame(dicts)
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    
    return matrix
55/115: %time evaluate_results(all_spam)
55/116:
def create_message_matrix(message):
    dicts = count_words(message)
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    
    return matrix
55/117: %time evaluate_results(all_spam)
55/118:
def create_message_matrix(message):
    dicts = count_words(message)
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    matrix['sum']=matrix.sum(axis=1)
    
    matrix['number_of_documents']=(matrix.count(axis=1)-1)
   # matrix['documents_percentage']=matrix['number_of_documents']/len(messages)
    
    return matrix
55/119: %time evaluate_results(all_spam)
55/120:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
    
    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = np.prod([matrix_corpus.loc[ word] for word in common_words])
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
55/121: %time evaluate_results(all_spam)
55/122: evaluate_results(hard_ham)
55/123:
def evaluate_results(messages):
    classification = [spam_or_ham(message) for message in messages]
    classification_counter = Counter(classification)
    results = (classification_counter[0]/len(messages), classification_counter[1]/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
55/124: %time evaluate_results(all_spam)
55/125:
def evaluate_results(messages):
    classification = (spam_or_ham(message) for message in messages)
    classification_counter = Counter(classification)
    results = (classification_counter[0]/len(messages), classification_counter[1]/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
55/126: %time evaluate_results(all_spam)
55/127: %time evaluate_results(all_spam)
55/128:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = pd.Series(set(matrix_corpus.index).intersection(set(matrix_email.index)))
    
    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
55/129: %time evaluate_results(all_spam)
55/130:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = pd.Series([set(matrix_corpus.index).intersection(set(matrix_email.index))])
    
    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
55/131: %time evaluate_results(all_spam)
55/132:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = pd.Series([set(matrix_corpus.index).intersection(set(matrix_email.index))])
    print('common_words: 'common_words)
    
    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
55/133:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = pd.Series([set(matrix_corpus.index).intersection(set(matrix_email.index))])
    print('common_words: ', common_words)
    
    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
55/134:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = pd.Series([set(matrix_corpus.index).intersection(set(matrix_email.index))])
    print('common_words_prod: ', common_words.prod())
    
    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
55/135: %time evaluate_results(all_spam)
55/136:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = pd.Series(list(set(matrix_corpus.index).intersection(set(matrix_email.index))))
    print('common_words_prod: ', common_words.prod())
    
    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
55/137: %time evaluate_results(all_spam)
55/138:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = pd.Series(list(set(matrix_corpus.index).intersection(set(matrix_email.index))))
    print('common_words: ', common_words)
    print('common_words_prod: ', common_words.prod())
    
    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
55/139: %time evaluate_results(all_spam)
55/140: matrix_spam.head()
55/141:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
    common_words_frequencies = matrix_corpus[matrix_corpus.index in common_words]
   # print('common_words: ', common_words)
   # print('common_words_prod: ', common_words.prod())
    
    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
55/142: %time evaluate_results(all_spam)
55/143:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
    common_words_frequencies = matrix_corpus.filter(item=list(common_words))
   # print('common_words: ', common_words)
   # print('common_words_prod: ', common_words.prod())
    
    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
55/144: %time evaluate_results(all_spam)
55/145:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
    common_words_frequencies = matrix_corpus.filter(items=list(common_words))
   # print('common_words: ', common_words)
   # print('common_words_prod: ', common_words.prod())
    
    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
55/146: %time evaluate_results(all_spam)
55/147:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
    common_words_frequencies = matrix_corpus.filter(items=list(common_words))
   # print('common_words: ', common_words)
   # print('common_words_prod: ', common_words.prod())
    
    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\w+', email[0])))
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = common_words_frequencies.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
55/148: %time evaluate_results(all_spam)
55/149: %time evaluate_results(all_spam)
56/1:
import pandas as pd
import matplotlib.pyplot as plt
58/1:
import pandas as pd
import matplotlib.pyplot as plt
58/2:
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
60/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
60/2:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
60/3: stop_words = stopwords.words('english')
60/4:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
60/5:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
60/6:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
60/7:
def create_message_matrix(message):
    dicts = count_words(message)
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    return matrix
60/8: all_spam = get_messages(spam_path)
60/9: matrix_spam = create_corpus_matrix(all_spam)
60/10: ps = PorterStemmer
60/11:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
60/12:
def count_words(message):
    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
60/13:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
60/14:
def create_message_matrix(message):
    dicts = count_words(message)
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    return matrix
60/15: all_spam = get_messages(spam_path)
60/16: matrix_spam = create_corpus_matrix(all_spam)
60/17: ps = PorterStemmer('english')
60/18: help(PorterStemmer)
60/19:
def count_words(message):
    occurances = set(word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
60/20:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
60/21:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
60/22:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
60/23:
def create_message_matrix(message):
    dicts = count_words(message)
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    return matrix
60/24: all_spam = get_messages(spam_path)
60/25: matrix_spam = create_corpus_matrix(all_spam)
60/26: matrix_spam.head(10)
60/27: matrix_spam = create_corpus_matrix(all_spam)
60/28: matrix_spam = matrix_spam.loc['documents_percentage']
60/29: matrix_spam.head(10)
60/30: x = count_words('Testowy tekst do zliczenia słów')
60/31: print(x)
60/32: x = count_words('Testowy tekst do zliczenia słów słów')
60/33: print(x)
60/34: x = count_words('Testowy tekst do zliczenia słów słów tekst')
60/35: x = word_tokenize('Testowy tekst do zliczenia słów słów tekst')
60/36: print(x)
60/37: all_ham = get_messages(easyham_path)[:500]
60/38: matrix_ham = create_corpus_matrix(all_ham)
60/39:
def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
    common_words_frequencies = matrix_corpus.filter(items=list(common_words))
    
    number_of_words = len(matrix_email.index)
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = common_words_frequencies.prod()
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
60/40:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 0
    else:
        return 1
60/41: hard_ham = get_messages(hardham_path)
60/42: matrix_hardham = create_corpus_matrix(hard_ham)
60/43:
def evaluate_results(messages):
    classification = (spam_or_ham(message) for message in messages)
    classification_counter = Counter(classification)
    results = (classification_counter[0]/len(messages), classification_counter[1]/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
60/44: %time evaluate_results(all_spam)
60/45:
def classify_email(email,matrix_corpus, prior=0.5, c=0.0001, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
    common_words_frequencies = matrix_corpus.filter(items=list(common_words))
    
    number_of_words = len(matrix_email.index)
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = common_words_frequencies.prod()
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
60/46: %time evaluate_results(all_spam)
60/47: evaluate_results(hard_ham)
61/1:
import pandas as pd
import matplotlib.pyplot as plt
61/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
61/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
    return all_email_headers,all_email_messages
61/4: headers, messages = get_emails(easyham_path)
61/5:
import pandas as pd
import matplotlib.pyplot as plt
import os
61/6:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
    return all_email_headers,all_email_messages
61/7: headers, messages = get_emails(easyham_path)
61/8: headers[:5]
61/9:
for header in headers[:5]:
    print(header)
61/10: headers_lines = [header.split('\n') for header in headers]
61/11: headers_lines[:2]
61/12: headers_lines[0]
61/13:
for header in headers[:5]:
    print(header,'\n\n')
61/14: len(headers_lines)
61/15: headers_lines[0]
61/16: header_lines[0]
61/17: headers_lines[0]
61/18:
for header_lines in headers_lines[:5]:
    for line in header_lines:
        if 'From:' in line:
            print(header_,'\n\n')
61/19:
for header_lines in headers_lines[:5]:
    for line in header_lines:
        if 'From:' in line:
            print(line,'\n\n')
61/20:
for header_lines,x in enumerate(headers_lines[:5]):
    for line in header_lines:
        if 'From:' in line:
            print(x, ': ',line,'\n\n')
61/21:
for x,header_lines in enumerate(headers_lines[:5]):
    for line in header_lines:
        if 'From:' in line:
            print(x, ': ',line,'\n\n')
61/22:
for x,header_lines in enumerate(headers_lines[:10]):
    for line in header_lines:
        if 'From:' in line:
            print(x, ': ',line,'\n\n')
61/23:
from_lines = []
for x,header_lines in enumerate(headers_lines[:10]):
    for line in header_lines:
        if 'From:' in line:
            from_lines.append(x, line)
61/24:
from_lines = []
for x,header_lines in enumerate(headers_lines[:10]):
    for line in header_lines:
        if 'From:' in line:
            from_lines.append((x,line))
61/25: len(from_lines)
61/26:
from_lines = []
for x,header_lines in enumerate(headers_lines):
    for line in header_lines:
        if 'From:' in line:
            from_lines.append((x,line))
61/27: len(from_lines)
61/28:
from_lines = []
for x,header_lines in enumerate(headers_lines):
    for line in header_lines:
        if line startswith('From:'):
            from_lines.append((x,line))
61/29:
from_lines = []
for x,header_lines in enumerate(headers_lines):
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append((x,line))
61/30: len(from_lines)
61/31: from_lines[:10]
61/32:
for email  in from_lines[:10]:
    x = re.findall(r"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$)", email)
    print(x)
61/33:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
61/34:
for email in from_lines[:10]:
    x = re.findall(r"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$)", email)
    print(x)
61/35:
for email in from_lines[:10]:
    x = re.findall(r"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$)", email[1])
    print(x)
61/36:
for email in from_lines[:10]:
    x = re.findall(r"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$)", email[1])
    print(email[1])
61/37:
for email in from_lines[:10]:
    x = re.findall(r"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$)", email[1])
    print(x)
61/38:
for email in from_lines[:10]:
    x = re.findall(r"(@)", email[1])
    print(x)
61/39:
for email in from_lines[:10]:
    x = re.findall(r"([a-z]+@)", email[1])
    print(x)
61/40:
for email in from_lines[:10]:
    x = re.findall(r"([a-zA-Z0-9_.+-]+@)", email[1])
    print(x)
61/41:
for email in from_lines[:20]:
    x = re.findall(r"([a-zA-Z0-9_.+-]+@)", email[1])
    print(x)
61/42:
for email in from_lines[:20]:
    x = re.findall(r"([a-zA-Z0-9_.+-]+@+\.[a-zA-Z0-9-.])", email[1])
    print(x)
61/43:
for email in from_lines[:20]:
    x = re.findall(r"([a-zA-Z0-9_.+-]+@+[a-zA-Z0-9-.])", email[1])
    print(x)
61/44:
for email in from_lines[:20]:
    x = re.findall(r"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-.])", email[1])
    print(x)
61/45:
for email in from_lines[:20]:
    x = re.findall(r"(\S+@\S+)", email[1])
    print(x)
61/46:
for email in from_lines[:20]:
    y = parseaddr(email)
    x = re.findall(r"(\S+@\S+)", email[1])
    print(y)
61/47:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re

from email.utils import parseaddr
61/48:
for email in from_lines[:20]:
    y = parseaddr(email)
    x = re.findall(r"(\S+@\S+)", email[1])
    print(y)
61/49:
for email in from_lines[:20]:
    y = parseaddr(email[1])
    x = re.findall(r"(\S+@\S+)", email[1])
    print(y)
61/50:
for email in from_lines[:20]:
    y = parseaddr(email[1])
    print(y[1])
61/51:
email_addresses = []
for email in from_lines[:20]:
    parser = parseaddr(email[1])
    email_addresses.append[parser[1]]
61/52:
email_addresses = []
for email in from_lines[:20]:
    parser = parseaddr(email[1])
    email_addresses.append(parser[1])
61/53: email_addresses[:20]
61/54: len(email_addresses)
61/55:
email_addresses = []
for email in from_lines[]:
    parser = parseaddr(email[1])
    email_addresses.append(parser[1])
61/56:
email_addresses = []
for email in from_lines:
    parser = parseaddr(email[1])
    email_addresses.append(parser[1])
61/57: len(email_addresses)
63/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re

from email.utils import parseaddr
63/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
63/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
    return all_email_headers,all_email_messages
63/4: headers, messages = get_emails(easyham_path)
63/5: headers_lines = [header.split('\n') for header in headers]
63/6:
from_lines = []
for header_lines in headers:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
63/7: from_lines[:10]
63/8:
email_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    email_addresses.append(parser_results[1])
63/9: len(email_addresses)
63/10: headers[0]
63/11:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
63/12: from_lines[:10]
63/13:
email_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    email_addresses.append(parser_results[1])
63/14: len(email_addresses)
63/15: email_addresses[:10]
63/16: headers[0]
63/17: print(headers[0])
63/18:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
63/19: print(subject_lines[:10])
63/20: len(subject_lines)
63/21:
subjects=[]
for subject_line in subjects_lines[:10]:
    subject = re.findall('From: (\w+)')
    subjects.append(subject)
63/22:
subjects=[]
for subject_line in subject_lines[:10]:
    subject = re.findall('From: (\w+)')
    subjects.append(subject)
63/23:
subjects=[]
for subject_line in subject_lines[:10]:
    subject = re.findall('From: (\w+)', subject_line)
    subjects.append(subject)
63/24: subjects
63/25:
subjects=[]
for subject_line in subject_lines[:10]:
    subject = re.findall(r'From: (\w+)', subject_line)
    subjects.append(subject)
63/26: subjects
63/27:
subjects=[]
for subject_line in subject_lines[:10]:
    subject = re.findall(r'(\w+)', subject_line)
    subjects.append(subject)
63/28: subjects
63/29:
subjects=[]
for subject_line in subject_lines[:10]:
    subject = re.findall(r'Subject: (\w+)', subject_line)
    subjects.append(subject)
63/30: subjects
63/31:
subjects=[]
for subject_line in subject_lines[:10]:
    subject = re.findall(r'Subject: (\.+)', subject_line)
    subjects.append(subject)
63/32: subjects
63/33:
subjects=[]
for subject_line in subject_lines[:10]:
    subject = re.findall(r'Subject: (\.)', subject_line)
    subjects.append(subject)
63/34: subjects
63/35:
subjects=[]
for subject_line in subject_lines[:10]:
    subject = re.findall(r'Subject: (.)', subject_line)
    subjects.append(subject)
63/36: subjects
63/37:
subjects=[]
for subject_line in subject_lines[:10]:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject)
63/38: subjects
63/39: header[0]
63/40: headers[0]
63/41: print(headers[0])
63/42:
subjects=[]
for subject_line in subject_lines[]:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject)
63/43:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject)
63/44: subjects
63/45: len(subjects)
63/46: print(subjects[100])
64/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
from collections import Counter
import numpy as np
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
64/2:
spam_path = 'Spam/data/spam/'
spam2_path = 'Spam/data/spam2/'
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
hardham_path = 'Spam/data/hard_ham/'
hardham2_path = 'Spam/data/har_ham_2/'
64/3: stop_words = stopwords.words('english')
64/4:
def get_messages(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_messages
64/5:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
64/6:
def create_corpus_matrix(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
64/7:
def create_message_matrix(message):
    dicts = count_words(message)
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    return matrix
64/8: all_spam = get_messages(spam_path)
64/9: matrix_spam = create_corpus_matrix(all_spam)
64/10: matrix_spam = matrix_spam.loc['documents_percentage']
64/11: all_ham = get_messages(easyham_path)[:500]
64/12: matrix_ham = create_corpus_matrix(all_ham)
64/13:
def classify_email(email,matrix_corpus, prior=0.5, c=0.0001, ):
    matrix_email = create_message_matrix(email)

    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))
    common_words_frequencies = matrix_corpus.filter(items=list(common_words))
    
    number_of_words = len(matrix_email.index)
    number_of_common_words = len(common_words)
    number_of_uncommon_words = number_of_words - number_of_common_words
    
    if number_of_common_words < 1:
        return prior*c**number_of_words
    else:
        probability_common = common_words_frequencies.prod()
        
        probability_uncommon = c**number_of_uncommon_words
        
        return(prior*probability_common*probability_uncommon)
64/14:
def spam_or_ham(email):
    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):
        return 0
    else:
        return 1
64/15: hard_ham = get_messages(hardham_path)
64/16: matrix_hardham = create_corpus_matrix(hard_ham)
64/17:
def evaluate_results(messages):
    classification = (spam_or_ham(message) for message in messages)
    classification_counter = Counter(classification)
    results = (classification_counter[0]/len(messages), classification_counter[1]/len(messages))
    print('Spam: ', results[0], ', Ham: ', results[1])
64/18: evaluate_results(hard_ham)
64/19: %time evaluate_results(all_spam)
63/47: len(messages)
63/48: print(messages[99])
63/49: print(messages[199])
66/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re

from email.utils import parseaddr
66/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
66/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
    return all_email_headers,all_email_messages
66/4: headers, messages = get_emails(easyham_path)
66/5: headers_lines = [header.split('\n') for header in headers]
66/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
66/7:
email_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    email_addresses.append(parser_results[1])
66/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
66/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject)
66/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startwith('Date:'):
            dates_lines.append(line)
66/11:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
66/12: print(dates_lines[:10])
66/13:
for x in dates_lines[:10]:
    print(x)
66/14: len(dates_lines)
66/15:
for x in dates_lines[:20]:
    print(x)
66/16:
for x in dates_lines[100:120]:
    print(x)
66/17:
for x in dates_lines[900:1200]:
    print(x)
66/18:
for x in dates_lines[:5]:
    print(x)
66/19: dates_lines = [datestr[7:] for datestr in date_lines]
66/20: dates_lines = [datestr[7:] for datestr in dates_lines]
66/21: dates_lines
66/22: dates_lines = [datestr[6:] for datestr in dates_lines]
66/23: dates_lines
66/24:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
66/25:
for x in dates_lines[:5]:
    print(x)
66/26: dates_lines = [datestr[:] for datestr in dates_lines]
66/27: dates_lines
66/28: dates_lines = [datestr[6:] for datestr in dates_lines]
66/29: dates_lines
66/30: t = pd.to_datetime(dates_lines[0])
66/31: t = pd.to_datetime(dates_lines[1])
66/32: t
66/33: t = pd.to_datetime(dates_lines[], error='coerce')
66/34: t = pd.to_datetime(dates_lines, error='coerce')
66/35: t = pd.to_datetime(dates_lines, errors='coerce')
66/36: len(t)
66/37: t
66/38: t.count()
66/39: t[:30]
66/40: t[:50]
66/41: t[:2]
66/42: t[:200]
66/43: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
66/44: send_datetime.info()
68/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re

from email.utils import parseaddr
68/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
68/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
    return all_email_headers,all_email_messages
68/4: headers, messages = get_emails(easyham_path)
68/5: headers_lines = [header.split('\n') for header in headers]
68/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
68/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
68/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
68/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject)
68/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
68/11:
for x in dates_lines[:5]:
    print(x)
68/12: dates_lines = [datestr[6:] for datestr in dates_lines]
68/13: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
68/14: emails_data = [send_datetime, from_addresses, subjects, messages]
68/15: all_emails_data = pd.DataFrame(emails_data)
68/16: all_emails_data.head()
68/17:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data.transpose()
68/18: all_emails_data.head()
68/19: all_emails_data.head()
68/20:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
68/21: all_emails_data.head()
68/22:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
68/23: all_emails_data.head()
68/24:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0])
68/25:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
68/26:
for x in dates_lines[:5]:
    print(x)
68/27: dates_lines = [datestr[6:] for datestr in dates_lines]
68/28: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
68/29: emails_data = [send_datetime, from_addresses, subjects, messages]
68/30:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
68/31: all_emails_data.head()
68/32:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:]
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
68/33: headers, messages = get_emails(easyham_path)
68/34: headers_lines = [header.split('\n') for header in headers]
68/35:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
68/36:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
68/37:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
68/38:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0])
68/39:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
68/40:
for x in dates_lines[:5]:
    print(x)
68/41: dates_lines = [datestr[6:] for datestr in dates_lines]
68/42: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
68/43: emails_data = [send_datetime, from_addresses, subjects, messages]
68/44:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
68/45: all_emails_data.head()
68/46:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index:]
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
68/47: headers, messages = get_emails(easyham_path)
68/48: headers_lines = [header.split('\n') for header in headers]
68/49:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
68/50:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
68/51:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
68/52:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0])
68/53:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
68/54:
for x in dates_lines[:5]:
    print(x)
68/55: dates_lines = [datestr[6:] for datestr in dates_lines]
68/56: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
68/57: emails_data = [send_datetime, from_addresses, subjects, messages]
68/58:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
68/59: all_emails_data.head()
68/60:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:]
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
68/61: headers, messages = get_emails(easyham_path)
68/62: headers_lines = [header.split('\n') for header in headers]
68/63:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
68/64:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
68/65:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
68/66:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0])
68/67:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
68/68:
for x in dates_lines[:5]:
    print(x)
68/69: dates_lines = [datestr[6:] for datestr in dates_lines]
68/70: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
68/71: emails_data = [send_datetime, from_addresses, subjects, messages]
68/72:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
68/73: all_emails_data.head()
68/74:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
68/75: headers, messages = get_emails(easyham_path)
68/76: headers_lines = [header.split('\n') for header in headers]
68/77:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
68/78:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
68/79:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
68/80:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
68/81:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
68/82:
for x in dates_lines[:5]:
    print(x)
68/83: dates_lines = [datestr[6:] for datestr in dates_lines]
68/84: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
68/85: emails_data = [send_datetime, from_addresses, subjects, messages]
68/86:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
68/87: all_emails_data.head()
68/88:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails = all_emails_data.sort_values(by=['Send_datetime'])
68/89: all_emails.head()
68/90:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
68/91:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index()
68/92: all_emails_data.head()
68/93:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
68/94: all_emails_data.head()
68/95: all_emails_training = all_emails_data.loc[:1250]
68/96: all_emails_training.head()
68/97: len(all_emails_training)
68/98: all_emails_training.tail()
68/99: all_emails_training.groupby(by=['From'])
68/100: grouped = all_emails_training.groupby(by=['From'])
68/101: grouped.head()
68/102: grouped = all_emails_training.groupby(by=['From']).count()
68/103: grouped.head()
68/104: grouped.tail()
68/105: grouped = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
68/106: grouped.tail()
68/107: plt.scatter(grouped.Message)
68/108: plt.scatter(grouped.From, grouped.Message)
68/109: grouped.index
68/110: plt.scatter(grouped.index, grouped.Message)
68/111:
plt.scatter(grouped.index, grouped.Message)
plt.ylim(6,)
68/112:
plt.scatter(grouped[grouped.Message>6].index, grouped[grouped.Message>6].Message)
plt.ylim(6,)
68/113: plt.scatter(grouped[grouped.Message>10].index, grouped[grouped.Message>10].Message)
70/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re

from email.utils import parseaddr
70/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
70/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
70/4: headers, messages = get_emails(easyham_path)
70/5: headers_lines = [header.split('\n') for header in headers]
70/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
70/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
70/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
70/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
70/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
70/11:
for x in dates_lines[:5]:
    print(x)
70/12: dates_lines = [datestr[6:] for datestr in dates_lines]
70/13: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
70/14: emails_data = [send_datetime, from_addresses, subjects, messages]
70/15:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
70/16: all_emails_training = all_emails_data.loc[:1250]
70/17: all_emails_training.tail()
70/18: len(all_emails_training)
70/19: grouped = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
70/20: grouped.tail()
70/21: plt.scatter(grouped[grouped.Message>10].index, grouped[grouped.Message>10].Message)
70/22: grouped.index
70/23: plt.bar(grouped[grouped.Message>10].index, grouped[grouped.Message>10].Message)
70/24: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
70/25: grouped_from.tail()
70/26:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np

from email.utils import parseaddr
70/27: grouped_from['Weight'] = np.log[grouped_from['Message']]
70/28: grouped_from['Weight'] = np.log(grouped_from['Message'])
70/29: grouped_from.head()
70/30: grouped_from.tail()
70/31: grouped_from['Weight'] = np.ln(grouped_from['Message'])
70/32: grouped_from.head()
70/33: grouped_from['Weight'] = np.log(grouped_from['Message']+1)
70/34: grouped_from.head()
70/35: plt.bar(grouped_from.index, grouped_from.Weight)
70/36:
plt.bar(grouped_from.index, grouped_from.Weight)
plt.bar(grouped_from.index, grouped_from.Message)
70/37:
plt.bar(grouped_from.index, grouped_from.Weight)
plt.bar(grouped_from.index, grouped_from.Message)
plt.rcParams['figure.figsize']=(16,8)
70/38:
plt.bar(grouped_from.index, grouped_from.Weight)
plt.bar(grouped_from.index, grouped_from.Message)
plt.rcParams['figure.figsize']=(24,16)
70/39:
plt.bar(grouped_from.index, grouped_from.Weight, grouped_from.Message)
#plt.bar(grouped_from.index, grouped_from.Message)
plt.rcParams['figure.figsize']=(24,16)
70/40:
plt.scatter(grouped_from.index, grouped_from.Weight, grouped_from.Message)
#plt.bar(grouped_from.index, grouped_from.Message)
plt.rcParams['figure.figsize']=(24,16)
70/41:
plt.plot(grouped_from.index, grouped_from.Weight)
plt.plot(grouped_from.index, grouped_from.Message)
plt.rcParams['figure.figsize']=(24,16)
70/42:
plt.plot(grouped_from.index, grouped_from.Weight, 'or')
plt.plot(grouped_from.index, grouped_from.Message, 'ob')
plt.rcParams['figure.figsize']=(24,16)
70/43: responses = all_emails_training.loc[all_emails_training.Subject.startswith('re:')]
70/44: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith('re:')]
70/45: responses.head()
70/46: responses.tail()
70/47: responses_2 = responses.loc[responses.Subject.str.startswith('re:')]
70/48: responses_2.head()
70/49: responses.head()
72/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np

from email.utils import parseaddr
72/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
72/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
72/4: headers, messages = get_emails(easyham_path)
72/5: headers_lines = [header.split('\n') for header in headers]
72/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
72/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
72/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
72/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
72/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
72/11:
for x in dates_lines[:5]:
    print(x)
72/12: dates_lines = [datestr[6:] for datestr in dates_lines]
72/13: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
72/14: emails_data = [send_datetime, from_addresses, subjects, messages]
72/15:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
72/16: all_emails_training = all_emails_data.loc[:1250]
72/17: all_emails_training.tail()
72/18: len(all_emails_training)
72/19: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
72/20: grouped_from.tail()
72/21: grouped_from['Weight'] = np.log(grouped_from['Message']+1)
72/22: grouped_from.head()
72/23:
plt.plot(grouped_from.index, grouped_from.Weight, 'or')
plt.plot(grouped_from.index, grouped_from.Message, 'ob')
plt.rcParams['figure.figsize']=(24,16)
72/24: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith('re:')]
72/25: len(responses)
72/26: responses_2 = responses.loc[responses.Subject.str.startswith('re:')]
72/27: responses_2.head()
72/28: responses.head()
72/29: responses_2 = [element[1:] for element in responses.subject.split('re:')]
72/30: responses_2 = [element[1:] for element in responses.Subject.split('re:')]
72/31: responses_2 = [element[1:] for element in responses.Subject.str.split('re:')]
72/32: responses_2.head()
72/33: responses_2
72/34: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith('re')]
72/35: len(responses)
72/36: responses_2 = [element[1:] for element in responses.Subject.str.split('re')]
72/37: responses_2
72/38: responses_2 = [element[1:] for element in responses.Subject.str.split(r'(re...:)')]
72/39: responses_2
72/40: responses_2 = [element[1:] for element in responses.Subject.str.split(r'(re:)')]
72/41: responses_2
72/42: responses_2 = [element for element in responses.Subject.str.split(r'(re:)')]
72/43: responses_2
72/44: responses.Subject
72/45: responses_2 = [element for element in responses.Subject.str.split(r'(re: )')]
72/46: responses_2
72/47: responses.Subject[0]
72/48: responses.Subject[1]
72/49: responses_2 = [element for element in responses.Subject[:2].str.split(r'(re: )')]
72/50: responses_2
72/51: responses.Subject[:2]
72/52: responses_2 = [element for element in responses.Subject[:2].str.split('re:')]
72/53: responses_2
72/54: responses.Subject[:2].str.split('re:')
72/55: responses.Subject[:2].str.split()
72/56: responses.Subject[:2].str.split('please')
72/57: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(r'(re[.]{0,3}:)')]
72/58: responses.Subject[:2].str.split('please')
72/59: responses
72/60: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(r'(re:)')]
72/61: responses
72/62: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(r'(re)')]
72/63: responses
72/64: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(r're')]
72/65: responses
72/66: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(r're.?')]
72/67: responses
72/68: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith('re.?')]
72/69: responses
72/70: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith('re.')]
72/71: responses
72/72: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(r're.')]
72/73: responses
72/74:
x = re.match('re')
responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(x)]
72/75: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(('re:', 're['))]
72/76: responses
72/77: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(('re', 're['))]
72/78: responses
72/79: responses.Subject[:2].str.split('re')
72/80: responses.Subject.str.split('re')
72/81: responses_2 = [element for element in responses.Subject.str.split('re')]
72/82: responses_2
72/83: x = responses.Subject.str.split('re')
72/84: responses_cut_re = responses.Subject.str.split('re')
72/85: responses_cut_re
72/86: responses_cut_re.loc(response_cut_re.str.startswith('re'))
72/87: responses_cut_re.loc(responses_cut_re.str.startswith('re'))
72/88: [x for x in responses_cut_re]
72/89: [x[1] for x in responses_cut_re]
72/90: [x[1] for x in responses_cut_re if x[1].startswith('re')]
72/91: all_emails_training.Subject.str.startswith('re')]
72/92: all_emails_training.Subject.str.startswith('re')
75/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np

from email.utils import parseaddr
75/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
75/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
75/4: headers, messages = get_emails(easyham_path)
75/5: headers_lines = [header.split('\n') for header in headers]
75/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
75/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
75/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
75/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
75/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
75/11:
for x in dates_lines[:5]:
    print(x)
75/12: dates_lines = [datestr[6:] for datestr in dates_lines]
75/13: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
75/14: emails_data = [send_datetime, from_addresses, subjects, messages]
75/15:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
75/16: all_emails_training = all_emails_data.loc[:1250]
75/17: all_emails_training.tail()
75/18: len(all_emails_training)
75/19: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
75/20: grouped_from.tail()
75/21: grouped_from['Weight'] = np.log(grouped_from['Message']+1)
75/22: grouped_from.head()
75/23:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
75/24: all_emails_training.Subject.str.startswith('re')
75/25: responses
75/26: all_emails_training['is_thread'] = all_emails_training.Subject.str.startswith('re')
75/27: all_emails_training.head()
75/28: all_emails_training.subject.str.split(':', 2)
75/29: all_emails_training.Subject.str.split(':', 2)
75/30: all_emails_training.Subject.str.split(':', 1)
75/31: all_emails_training['thread_subject'] = all_emails_training.Subject.str.split(':', 1)[-1]
75/32: all_emails_training['thread_subject'] = all_emails_training.Subject.str.split(':', 1)[0]
75/33: all_emails_training['thread_subject'] = all_emails_training.Subject.str.split(':', 1)
75/34: all_emails_training.Subject.str.split(':', 1)[0]
75/35: thread_subjects = [x[1] for x in all_emails_training.Subject.str.split(':', 1) if x[1]]
75/36: thread_subjects = [x[1] for x in all_emails_training.Subject.str.split(':', 1) if len(x)>1]
75/37: thread_subjects
75/38: thread_subjects = [x[1] for x in all_emails_training.Subject.str.split(':', 1) if len(x)>1 else 0]
75/39: all_emails_training.Subject.str.split(':', 1)[2]
75/40: len(thread_subjects)
76/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np

from email.utils import parseaddr
76/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
76/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
76/4: headers, messages = get_emails(easyham_path)
76/5: headers_lines = [header.split('\n') for header in headers]
76/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
76/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
76/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
76/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
76/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
76/11:
for x in dates_lines[:5]:
    print(x)
76/12: dates_lines = [datestr[6:] for datestr in dates_lines]
76/13: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
76/14: emails_data = [send_datetime, from_addresses, subjects, messages]
76/15:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
76/16: all_emails_training = all_emails_data.loc[:1250]
76/17: all_emails_training.tail()
76/18: len(all_emails_training)
76/19: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
76/20: grouped_from.tail()
76/21: grouped_from['Weight'] = np.log(grouped_from['Message']+1)
76/22: grouped_from.head()
76/23:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
76/24: all_emails_training['is_thread'] = all_emails_training.Subject.str.startswith('re')
76/25: all_emails_training.head()
76/26: thread_subjects = [x[1] for x in all_emails_training.Subject.str.split(':', 1) if len(x)>1]
76/27: all_emails_training['thread_subject'] = all_emails_training.Subject.str.split(':', 1)
76/28: all_emails_training.Subject.str.split(':', 1)[2]
76/29: len(thread_subjects)
76/30: all_emails_training.head()
76/31: all_emails_training.head(10)
76/32: all_emails_training.head(20)
76/33: all_emails_training.Subject.str.split(':', 1)[3]
76/34: all_emails_training.Subject.str.split(':', 2)[3]
76/35: all_emails_training.Subject.str.split(':', 1)[3]
76/36: all_emails_training.Subject.str.split(':', 1)[3][1]
76/37: all_emails_training.Subject.str.split(':', 1)[0][1]
76/38: all_emails_training.Subject.str.split(':', 1)[4][1]
76/39: all_emails_training.Subject.str.split(':', 1)[18][1]
76/40: all_emails_training['thread_subject'] = all_emails_training.Subject.str.split(':', 1)[1] if all_emails_training['is_thread'] else Nan
76/41: all_emails_training['thread_subject'] = all_emails_training.Subject.str.split(':', 1)[1] if all_emails_training['is_thread'] else 'x'
76/42: thread_subject = [x.str.split(':', 1)[1] for x in all_emails_training.subject if all_emails_training['is_thread'] else 'x']
76/43: thread_subject = [x.str.split(':', 1)[1] for x in all_emails_training.Subject if all_emails_training['is_thread'] else 'x']
76/44: thread_subject = [x.str.split(':', 1)[1] for x,y in all_emails_training.Subject,all_emails_training.is_thread if y else 'x']
76/45: thread_subject = [x.str.split(':', 1)[1] for x,y in (all_emails_training.Subject,all_emails_training.is_thread) if y else 'x']
76/46: thread_subject = [x.str.split(':', 1)[1] for (x,y) in (all_emails_training.Subject,all_emails_training.is_thread) if y else 'x']
76/47: thread_subject = [x.str.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject,all_emails_training.is_thread) if y else 'x']
76/48: thread_subject = [x.str.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject,all_emails_training.is_thread) if y else 'x']
76/49: z=[x.str.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject,all_emails_training.is_thread) if y else 'x']
76/50: z=[x.str.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject,all_emails_training.is_thread) if y]
76/51: z=[x.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject,all_emails_training.is_thread) if y]
76/52: z=[x.split(':', 1)[0] for (x,y) in zip(all_emails_training.Subject,all_emails_training.is_thread) if y]
76/53: z.head()
76/54: z[:10]
76/55: splited_subjects = [x.split(':', 1) for (x,y) in zip(all_emails_training.Subject, all_emails_training.is_thread) if y]
76/56: splited_subjects
76/57: splited_subjects_len = [len(x) for x in splitted_subjects]
76/58: splited_subjects_len = [len(x) for x in splited_subjects]
76/59: min(splited_subjects_len)
76/60: sorted(splited_subjects_len)
76/61: sorted(zip(splited_subjects_len, splited_subjects))
76/62: all_emails_training.loc[all_emails_training.Subject.contains('recomended')]
76/63: all_emails_training.loc[all_emails_training.Subject.contain('recomended')]
76/64: all_emails_training.loc[all_emails_training.Subject.str.contain('recomended')]
76/65: all_emails_training.loc[all_emails_training.Subject.str.contains('recomended')]
76/66: all_emails_training.loc[all_emails_training.Subject.str.contains('recommended')]
76/67: all_emails_training.loc[all_emails_training.Subject.str.contains('recommended viewing')]
76/68: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1\]{0,1}')
76/69: all_emails_training.head()
76/70: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re[{0,1}')
76/71: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}')
76/72: all_emails_training.head()
76/73: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}')
76/74: all_emails_training.head(10)
76/75: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}')
76/76: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}')
76/77: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
76/78: all_emails_training.head(100)
76/79: splited_subjects = [x.split(':', 1) for (x,y) in zip(all_emails_training.Subject, all_emails_training.is_thread) if y]
76/80: splited_subjects_len = [len(x) for x in splited_subjects]
76/81: all_emails_training.loc[all_emails_training.Subject.str.contains('recommended viewing')]
76/82: sorted(zip(splited_subjects_len, splited_subjects))
76/83: splited_subjects = [x.split(':', 1)[0] for (x,y) in zip(all_emails_training.Subject, all_emails_training.is_thread) if y]
76/84: splitted_subjects
76/85: splited_subjects
76/86: splited_subjects = [x.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject, all_emails_training.is_thread) if y]
76/87: splited_subjects
76/88: all_emails_training['thread_subject'] = splited_subjects
76/89: all_emails_training['thread_subject'] = x.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject, all_emails_training.is_thread) if y
76/90: all_emails_training['thread_subject'] = [(x.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject, all_emails_training.is_thread) if y) or 'x']
77/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np

from email.utils import parseaddr
77/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
77/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
77/4: headers, messages = get_emails(easyham_path)
77/5: headers_lines = [header.split('\n') for header in headers]
77/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
77/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
77/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
77/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
77/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
77/11:
for x in dates_lines[:5]:
    print(x)
77/12: dates_lines = [datestr[6:] for datestr in dates_lines]
77/13: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
77/14: emails_data = [send_datetime, from_addresses, subjects, messages]
77/15:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
77/16: all_emails_training = all_emails_data.loc[:1250]
77/17: all_emails_training.tail()
77/18: len(all_emails_training)
77/19: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
77/20: grouped_from.tail()
77/21: grouped_from['Weight'] = np.log(grouped_from['Message']+1)
77/22: grouped_from.head()
77/23:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
77/24: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
77/25: all_emails_training.head(100)
77/26:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            threads_subjects.append(thread_subject)
        else:
            threads_subject.append(None)
    
    return threads_subjects
77/27: threads_subjects = get_threads(all_emails_training)
77/28:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append(None)
    
    return threads_subjects
77/29: threads_subjects = get_threads(all_emails_training)
77/30: threads_subjects[:10]
77/31: all_emails_training['thread_subject'] = threads_subjects
77/32: all_emails_training.head()
77/33: x=Nan
77/34: x=pd.Nan
77/35: x=NaN
77/36: x=pd.NaN
77/37: x=np.nan
77/38: x=nan
77/39:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append(np.nan)
    
    return threads_subjects
77/40: threads_subjects = get_threads(all_emails_training)
77/41: threads_subjects[:10]
77/42: all_emails_training['thread_subject'] = threads_subjects
77/43: all_emails_training.head()
77/44: grouped_from = grouped_from.Weight
77/45: grouped_from.head()
77/46:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
77/47:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np

from email.utils import parseaddr
77/48:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
77/49:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
77/50: headers, messages = get_emails(easyham_path)
77/51: headers_lines = [header.split('\n') for header in headers]
77/52:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
77/53:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
77/54:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
77/55:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
77/56:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
77/57:
for x in dates_lines[:5]:
    print(x)
77/58: dates_lines = [datestr[6:] for datestr in dates_lines]
77/59: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
77/60: emails_data = [send_datetime, from_addresses, subjects, messages]
77/61:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
77/62: all_emails_training = all_emails_data.loc[:1250]
77/63: all_emails_training.tail()
77/64: len(all_emails_training)
77/65: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
77/66: grouped_from.tail()
77/67:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
77/68: grouped_from.head()
77/69:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
77/70: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
77/71: all_emails_training.head(100)
77/72:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append(np.nan)
    
    return threads_subjects
77/73: threads_subjects = get_threads(all_emails_training)
77/74: threads_subjects[:10]
77/75: all_emails_training['thread_subject'] = threads_subjects
77/76: all_emails_training.head()
77/77: grouped_from.tail()
77/78: all_emails_training.head()
77/79: emails_threads = all_emails_training[all_emails_training.is_thread=True]
77/80: emails_threads = all_emails_training[all_emails_training.is_thread==True]
77/81: emaisl_thread.head()
77/82: emails_thread.head()
77/83: emails_threads.head()
77/84:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_thread.drop(['Message', 'is_thread'], axis=1)
77/85:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1)
77/86: emails_threads.head()
77/87:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
77/88: emails_threads.head()
77/89: emails_threads_senders = emails_threads.groupby(by=['From'])
77/90: emails_threads_senders.head()
77/91: emails_threads_senders = emails_threads.groupby(by=['From'].count())
77/92: emails_threads_senders = emails_threads.groupby(by=['From']).count()
77/93: emails_threads_senders.head()
77/94: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
77/95: emails_threads_senders.head()
77/96: emails_threads_senders.tail()
77/97:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
77/98: emails_threads_senders.head()
77/99: emails_threads_senders.tail()
78/1: all_emails_training.groupby(by=['thread_subject'])
78/2:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np

from email.utils import parseaddr
78/3:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
78/4:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
78/5: headers, messages = get_emails(easyham_path)
78/6: headers_lines = [header.split('\n') for header in headers]
78/7:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
78/8:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
78/9:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
78/10:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
78/11:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
78/12:
for x in dates_lines[:5]:
    print(x)
78/13: dates_lines = [datestr[6:] for datestr in dates_lines]
78/14: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
78/15: emails_data = [send_datetime, from_addresses, subjects, messages]
78/16:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
78/17: all_emails_training = all_emails_data.loc[:1250]
78/18: len(all_emails_training)
78/19: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
78/20:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
78/21:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
78/22: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
78/23:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append(np.nan)
    
    return threads_subjects
78/24: threads_subjects = get_threads(all_emails_training)
78/25: all_emails_training['thread_subject'] = threads_subjects
78/26:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
78/27: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
78/28: emails_threads_senders.tail()
78/29:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
78/30: emails_threads_senders.tail()
78/31: all_emails_training.groupby(by=['thread_subject'])
78/32: def get_thread_intensity(emails_df):
78/33: thread_subjects = all_emails_training.groupby(by=['thread_subject'])
78/34: thread_subjects
78/35: print(thread_subjects)
78/36: thread_subjects = all_emails_training.groupby(by=['thread_subject']).count(['Send_datetime'])
78/37: thread_subjects = all_emails_training.groupby(by=['thread_subject']).count('Send_datetime')
78/38: thread_subjects = all_emails_training.groupby(by=['thread_subject']).count()
78/39: print(thread_subjects)
78/40: thread_subjects = all_emails_training.groupby(by=['thread_subject']).count('index')
78/41: thread_subjects = all_emails_training.groupby(by=['thread_subject']).size()
78/42: print(thread_subjects)
78/43: thread_number_of_messages = all_emails_training.groupby(by=['thread_subject']).size()
78/44: thread_number_of_messages
78/45: thread_number_of_messages[:5]
78/46: thread_number_of_messages = all_emails_training.groupby(by=['thread_subject']).min('Send_datetime')
78/47: thread_number_of_messages = all_emails_training.groupby(by=['thread_subject']).size()
78/48: thread_number_of_messages[:5]
78/49: grouped_by_thread_subject = all_emails_training.groupby(by=['thread_subject'])
78/50: thread_number_of_messages = .grouped_by_thread_subject.size()
78/51: thread_number_of_messages = grouped_by_thread_subject.size()
78/52: thread_number_of_messages[:5]
78/53: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform.min()
78/54: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
78/55: print(minimum_send_datetime)
78/56:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
emails_threads.head()
78/57: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
78/58: thread_number_of_messages = grouped_by_thread_subject.size()
78/59: thread_number_of_messages[:5]
78/60: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
78/61: print(minimum_send_datetime)
78/62: print(minimum_send_datetime[:5])
78/63: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
78/64: print(maximum_send_datetime[:5])
78/65: emails_threads['send_min', 'send_max'] = [minimum_send_datetime, maximum_send_datetime]
78/66: len(maximum_send_datetime)
78/67: len(minimum_send_datetime)
78/68: len(emails_threads)
78/69: emails_threads['send_min'] = minimum_send_datetime
78/70: emails_threads['send_max'] = maximum_send_datetime
78/71: emails_threads.head()
78/72: emails_threads[thread_time] = emails_threads.send_max - emails_threads.send_min
78/73: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min
78/74: emails_threads.head()
79/1: emails_threads.thread_density = 1
79/2:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np

from email.utils import parseaddr
79/3:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
79/4:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
79/5: headers, messages = get_emails(easyham_path)
79/6: headers_lines = [header.split('\n') for header in headers]
79/7:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
79/8:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
79/9:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
79/10:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
79/11:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
79/12:
for x in dates_lines[:5]:
    print(x)
79/13: dates_lines = [datestr[6:] for datestr in dates_lines]
79/14: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
79/15: emails_data = [send_datetime, from_addresses, subjects, messages]
79/16:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
79/17: all_emails_training = all_emails_data.loc[:1250]
79/18: len(all_emails_training)
79/19: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
79/20:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
79/21:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
79/22: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
79/23:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append(np.nan)
    
    return threads_subjects
79/24: threads_subjects = get_threads(all_emails_training)
79/25: all_emails_training['thread_subject'] = threads_subjects
79/26:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
emails_threads.head()
79/27: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
79/28: emails_threads_senders.tail()
79/29:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
79/30: emails_threads_senders.tail()
79/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
79/32: thread_number_of_messages = grouped_by_thread_subject.size()
79/33: thread_number_of_messages[:5]
79/34: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
79/35: len(minimum_send_datetime)
79/36: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
79/37: len(maximum_send_datetime)
79/38: len(emails_threads)
79/39: emails_threads['send_min'] = minimum_send_datetime
79/40: emails_threads['send_max'] = maximum_send_datetime
79/41: emails_threads.head()
79/42: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min
79/43: emails_threads.thread_density = 1
79/44: emails_threads.head()
79/45: emails_threads.thread_density = 1
79/46: emails_threads.head()
79/47: emails_threads['thread_density'] = 1
79/48: emails_threads.head()
79/49: emails_threads['thread_density'] = emails_threads.send_max-emails_threads.send_min
79/50: emails_threads.head()
79/51: emails_threads['thread_density'] = emails_threads.thread_time/thread_number_of_messages[emails_threads.thread_subject]
79/52:
for email in emails_threads:
    email['thread_density'] = 1 # emails_threads.thread_time/thread_number_of_messages[emails_threads.thread_subject]
79/53:
for email in emails_threads[:10]:
    print(email) #['thread_density'] = 1 # emails_threads.thread_time/thread_number_of_messages[emails_threads.thread_subject]
79/54:
for email in emails_threads[:20]:
    print(email) #['thread_density'] = 1 # emails_threads.thread_time/thread_number_of_messages[emails_threads.thread_subject]
79/55: emails_threads.head()
79/56:
for email in emails_threads.thread_density[:10]:
    print(email) #['thread_density'] = 1 # emails_threads.thread_time/thread_number_of_messages[emails_threads.thread_subject]
79/57: df = emails_threads.merge(thread_number_of_messages, on='thread_subject', how='left')
79/58: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
79/59: df = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
79/60: df
79/61: df.head()
79/62: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
79/63: emails_threads.head()
79/64: emails_threads.thread_density = emails_threads.Number_of_messages/emails_threads.thread_time
79/65: emails_threads.describe()
79/66: emails_threads.columns
79/67: emails_threads.dtypes
79/68: emails_threads.thread_density = emails_threads.Number_of_messages/emails_threads.thread_time.seconds
79/69: emails_threads.thread_density = emails_threads.Number_of_messages/emails_threads.thread_time
79/70: emails_threads.thread_density.min()
79/71: emails_threads.thread_density.max()
79/72: emails_threads.thread_time.min()
79/73: emails_threads.thread_time.max()
79/74: emails_threads.sortby(by='thread_time').head()
79/75: emails_threads.sort_by(by='thread_time').head()
79/76: emails_threads.sort_values(by='thread_time').head()
79/77: emails_threads.sort_values(by='thread_time').head(20)
79/78: emails_threads.loc(emails_threads.send_min==emails_threads.send_max)
79/79: emails_threads.loc(emails_threads.send_min=emails_threads.send_max)
79/80: emails_threads.loc(emails_threads.send_min==emails_threads.send_max)
79/81: emails_threads.sort_values(by='thread_time').head()
79/82: emails_threads.loc(emails_threads.thread_time==0)
79/83: emails_threads.loc(emails_threads.thread_time=datetime.timedelta(seconds=0))
79/84: emails_threads.loc[emails_threads.thread_time=datetime.timedelta(seconds=0)]
79/85: emails_threads[emails_threads.thread_time=datetime.timedelta(seconds=0)]
79/86: emails_threads[emails_threads.thread_time==datetime.timedelta(seconds=0)]
79/87:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr
79/88: emails_threads[emails_threads.thread_time==datetime.timedelta(seconds=0)]
79/89: all_emails_training[all_emails_training.str.contain('automated forwarding')]
79/90: all_emails_training[all_emails_training['Subject'].str.contain('automated forwarding')]
79/91: all_emails_training[all_emails_training['Subject'].str.contains('automated forwarding')]
79/92: 'alexander' in all_emails_training.Subject
79/93: 'alexander' in all_emails_training.Subject.strip()
79/94: 'alexander' in all_emails_training.Subject.rstrip()
79/95: 'alexander' in all_emails_training.Subject.str.rstrip()
79/96: 'problems with apt update' in all_emails_training.Subject.str.rstrip()
79/97: all_emails_training.Subject.str.rstrip()
79/98: all_emails_training.loc[0].Subject.str.rstrip()
79/99: all_emails_training.loc[0].Subject.rstrip()
79/100: 'please help a newbie compile mplayer :-)' in all_emails_training.Subject
79/101: 'please help a newbie compile mplayer :-)' in all_emails_training.Subject.rstrip()
79/102: 'please help a newbie compile mplayer :-)' in all_emails_training.Subject.str.rstrip()
79/103: all_emails_training.loc[0].Subject
79/104: 'please help a newbie compile mplayer :-)' in all_emails_training.Subject.str.rstrip()
79/105: all_emails_training['Subject'].str.contains(help)
79/106: all_emails_training['Subject'].str.contains('help')
79/107: all_emails_training['Subject'].str.contains('please help a newbie compile mplayer :-)')
79/108: all_emails_training['Subject'].str.contains('please help a newbie compile mplayer :-\)')
80/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr
80/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
80/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
80/4: headers, messages = get_emails(easyham_path)
80/5: headers_lines = [header.split('\n') for header in headers]
80/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
80/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
80/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
80/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
80/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
80/11:
for x in dates_lines[:5]:
    print(x)
80/12: dates_lines = [datestr[6:] for datestr in dates_lines]
80/13: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
80/14: emails_data = [send_datetime, from_addresses, subjects, messages]
80/15:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
80/16: all_emails_training = all_emails_data.loc[:1250]
80/17: len(all_emails_training)
80/18: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
80/19:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
80/20:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
80/21: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
80/22:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append(np.nan)
    
    return threads_subjects
80/23: threads_subjects = get_threads(all_emails_training)
80/24: all_emails_training['thread_subject'] = threads_subjects
80/25:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
emails_threads.head()
80/26: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
80/27: emails_threads_senders.tail()
80/28:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
80/29: emails_threads_senders.tail()
80/30: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
80/31: thread_number_of_messages = grouped_by_thread_subject.size()
80/32: thread_number_of_messages[:5]
80/33: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
80/34: len(minimum_send_datetime)
80/35: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
80/36: len(maximum_send_datetime)
80/37: len(emails_threads)
80/38: emails_threads['send_min'] = minimum_send_datetime
80/39: emails_threads['send_max'] = maximum_send_datetime
80/40: emails_threads.head()
80/41: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min
80/42: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
80/43: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
80/44: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
80/45: emails_threads.sort_values(by='thread_time').head()
80/46: emails_threads.dtypes
80/47: emails_threads.thread_time.min()
80/48: emails_threads.thread_time.max()
80/49: emails_threads[emails_threads.thread_time==datetime.timedelta(seconds=0)]
80/50: all_emails_training[all_emails_training['Subject'].str.contains('automated forwarding')]
80/51: all_emails_training.loc[0].Subject
80/52: 'please help a newbie compile mplayer :-)' in all_emails_training.Subject.str.rstrip()
80/53: all_emails_training['Subject'].str.contains('please help a newbie compile mplayer :-\)')
80/54: all_emails_training.loc[all_emails_training.is_thread].head()
80/55: all_emails_training.loc[~all_emails_training.is_thread].head()
80/56: all_emails_training.loc[~all_emails_training.is_thread].Subject
80/57:
for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:
    print(subject)
80/58:
for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:
    if subject in all_emails_training.Subject:
        print('tak')
80/59: all_emails_training.Subject
80/60: 'problems with apt update' in all_emails_training.Subject
80/61: 'problems with apt update' in all_emails_training.Subject.strip()
80/62: 'problems with apt update' in all_emails_training.Subject.str.strip()
80/63: 'problems with apt update' in all_emails_training.Subject.str.rstrip()
80/64: 're' in all_emails_training.Subject.str.rstrip()
80/65: x = pd.series('a','b', 'c')
80/66: x = pd.Series('a','b', 'c')
80/67: x = pd.Series(['a','b', 'c'])
80/68: 'a' in x
80/69: x
80/70: 'a' isin x
80/71: x.isin('a')
80/72: x.isin(['a'])
80/73: x.isin(['a']).any()
80/74: x.isin(['a']).all()
80/75:
for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:
    if all_emails_training.Subject.isin([subject]).any():
        print('tak')
80/76: all_emails_training.Subject[:5]
80/77: all_emails_training.is_thread = all_emails_training.is_thread or all_emails_training.Subject.isin(all_emails_training.thread_subject).any()
80/78:
for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:
    if all_emails_training.Subject.isin([subject]).any():
        all_emails_training.at['is_thread','subject']=True
80/79:
for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:
    if all_emails_training.Subject.isin([subject]).any():
        all_emails_training.loc['subject','is_thread']=True
80/80:
for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:
    if all_emails_training.Subject.isin([subject]).any():
        all_emails_training.loc['subject','is_thread']=True
80/81:
for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:
    if all_emails_training.Subject.isin([subject]).any():
        print('x')
        #all_emails_training.loc['subject','is_thread']=True
80/82:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
80/83:
for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:
    if all_emails_training.Subject.isin([subject]).any():
        print('x')
        #all_emails_training.loc['subject','is_thread']=True
80/84: all_emails_training.head()
80/85: all_emails_training.drop(['subject'], axis=1, inplace=True)
80/86: all_emails_training.head()
80/87:
for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:
    if all_emails_training.Subject.isin([subject]).any():
        print('x')
        #all_emails_training.loc['subject','is_thread']=True
80/88:
for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:
    #if all_emails_training.Subject.isin([subject]).any():
    print('x')
        #all_emails_training.loc['subject','is_thread']=True
80/89: all_emails_training.dtypes
80/90:
for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:
    #if all_emails_training.Subject.isin([subject]).any():
    print('x')
        #all_emails_training.loc['subject','is_thread']=True
80/91: all_emails_training.loc[all_emails_training.is_thread]
80/92: all_emails_training.is_thread
80/93: all_emails_training.tail()
80/94: all_emails_training.drop(['is_thread'], axis=0, inplace=True)
80/95: all_emails_training.tail()
80/96:
for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:
    if all_emails_training.Subject.isin([subject]).any():
        all_emails_training.at[subject,'is_thread']=True
80/97: all_emails_training.tail()
80/98:
#for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:
 #   if all_emails_training.Subject.isin([subject]).any():
  #      all_emails_training.at[subject,'is_thread']=True
80/99:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr
80/100:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
80/101:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
80/102: headers, messages = get_emails(easyham_path)
80/103: headers_lines = [header.split('\n') for header in headers]
80/104:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
80/105:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
80/106:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
80/107:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
80/108:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
80/109: dates_lines = [datestr[6:] for datestr in dates_lines]
80/110: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
80/111: emails_data = [send_datetime, from_addresses, subjects, messages]
80/112:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
80/113: all_emails_training = all_emails_data.loc[:1250]
80/114: len(all_emails_training)
80/115: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
80/116:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
80/117:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
80/118: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
80/119: x = pd.Series(['a','b', 'c'])
80/120: x
80/121: x.isin(['a']).all()
80/122: all_emails_training.drop(['subject'], axis=1, inplace=True)
80/123: all_emails_training.drop(['is_thread'], axis=0, inplace=True)
80/124: all_emails_training.tail()
80/125:
#for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:
 #   if all_emails_training.Subject.isin([subject]).any():
  #      all_emails_training.at[subject,'is_thread']=True
80/126: all_emails_training.Subject[:5]
80/127:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
80/128: threads_subjects = get_threads(all_emails_training)
80/129: all_emails_training['thread_subject'] = threads_subjects
80/130:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
emails_threads.head()
80/131: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
80/132: emails_threads_senders.tail()
80/133:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
80/134: emails_threads_senders.tail()
80/135: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
80/136: thread_number_of_messages = grouped_by_thread_subject.size()
80/137: thread_number_of_messages[:5]
80/138: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
80/139: len(minimum_send_datetime)
80/140: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
80/141: len(maximum_send_datetime)
80/142: len(emails_threads)
80/143: emails_threads['send_min'] = minimum_send_datetime
80/144: emails_threads['send_max'] = maximum_send_datetime
80/145: emails_threads.head()
80/146: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min
80/147: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
80/148: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
80/149: emails_threads.sort_values(by='thread_time').head()
80/150: emails_threads.thread_time.min()
80/151: emails_threads.thread_time.max()
80/152: emails_threads[emails_threads.thread_time==datetime.timedelta(seconds=0)]
80/153: all_emails_training[all_emails_training['Subject'].str.contains('automated forwarding')]
80/154: all_emails_training.loc[0].Subject
80/155: 'please help a newbie compile mplayer :-)' in all_emails_training.Subject.str.rstrip()
80/156: all_emails_training['Subject'].str.contains('please help a newbie compile mplayer :-\)')
80/157:
for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:
    if all_emails_training.Subject.isin([subject]).any():
        all_emails_training.at[all_emails_training.loc[all_emails_training.Subject==subject].index,'is_thread']=True
80/158: all_emails_training.head()
80/159: all_emails_training.tail()
80/160: all_emails_training.tail(20)
80/161: all_emails_training.threas_subject.isin('oh my').any()
80/162: all_emails_training.thread_subject.isin('oh my').any()
80/163: all_emails_training.thread_subject.isin(['oh my']).any()
80/164: all_emails_training.thread_subject.isin(['oh my...']).any()
80/165: all_emails_training.at[1234,'thread_subject']
80/166: all_emails_training.thread_subject.isin([' oh my...']).any()
80/167: all_emails_training.tail()
80/168: threads_subjects = get_threads(all_emails_training)
80/169: all_emails_training['thread_subject'] = threads_subjects
80/170:
for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:
    if all_emails_training.Subject.isin([subject]).any():
        all_emails_training.at[all_emails_training.loc[all_emails_training.Subject==subject].index,'is_thread']=True
80/171: all_emails_training.head()
80/172: all_emails_training.thread_subject.isin(['please help a newbie compile mplayer :-)']).any()
80/173: all_emails_training.at[1,'thread_subject']
80/174:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1].rstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
80/175: threads_subjects = get_threads(all_emails_training)
80/176:
for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:
    if all_emails_training.Subject.isin([subject]).any():
        all_emails_training.at[all_emails_training.loc[all_emails_training.Subject==subject].index,'is_thread']=True
80/177: all_emails_training.head()
80/178: all_emails_training.at[1,'thread_subject']
80/179: threads_subjects = get_threads(all_emails_training)
80/180: threads_subjects = get_threads(all_emails_training)
80/181:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1].rstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    print(threads_subjects[:10])
    return threads_subjects
80/182: threads_subjects = get_threads(all_emails_training)
80/183: x='   abc'
80/184:
print(x)
x.rstrip()
print(x)
80/185:
print(x)
x.strip()
print(x)
80/186:
print(x)
x.strip('\s')
print(x)
80/187:
print(x)
x.strip(' ')
print(x)
80/188: x='       abc'
80/189:
print(x)
x.strip(' ')
print(x)
80/190:
print(x)
x.rstrip(' ')
print(x)
80/191:
print(x)
x.lstrip()
print(x)
80/192:
print(x)
x=x.lstrip()
print(x)
80/193:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]#.rstrip()
            thread_subject = thread_subject.strip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    print(threads_subjects[:10])
    return threads_subjects
80/194: threads_subjects = get_threads(all_emails_training)
80/195:
for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:
    if all_emails_training.Subject.isin([subject]).any():
        all_emails_training.at[all_emails_training.loc[all_emails_training.Subject==subject].index,'is_thread']=True
80/196: all_emails_training.head()
80/197: all_emails_training.at[1,'thread_subject']
80/198:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]#.rstrip()
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    print(threads_subjects[:10])
    return threads_subjects
80/199: threads_subjects = get_threads(all_emails_training)
80/200: all_emails_training['thread_subject'] = threads_subjects
80/201:
for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:
    if all_emails_training.Subject.isin([subject]).any():
        all_emails_training.at[all_emails_training.loc[all_emails_training.Subject==subject].index,'is_thread']=True
80/202: all_emails_training.head()
80/203: all_emails_training.at[1,'thread_subject']
81/1: import pandas as pd
82/1: %run Priority_box
82/2: %run Priority_box.py
85/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr
85/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
85/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
85/4: headers, messages = get_emails(easyham_path)
85/5: headers_lines = [header.split('\n') for header in headers]
85/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
85/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
85/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
85/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
85/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
85/11: dates_lines = [datestr[6:] for datestr in dates_lines]
85/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
85/13: emails_data = [send_datetime, from_addresses, subjects, messages]
85/14:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
85/15: all_emails_training = all_emails_data.loc[:1250]
85/16: len(all_emails_training)
85/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
85/18:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
85/19:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
85/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
85/21: all_emails_training.thread_subject.isin(['please help a newbie compile mplayer :-)']).any()
85/22: all_emails_training?
85/23:
for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:
    if all_emails_training.Subject.isin([subject]).any():
        all_emails_training.at[all_emails_training.loc[all_emails_training.Subject==subject].index,'is_thread']=True
85/24:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    print(threads_subjects[:10])
    return threads_subjects
85/25: threads_subjects = get_threads(all_emails_training)
85/26: all_emails_training['thread_subject'] = threads_subjects
85/27:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
emails_threads.head()
85/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
85/29: emails_threads_senders.tail()
85/30:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
85/31: emails_threads_senders.tail()
85/32: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
85/33: thread_number_of_messages = grouped_by_thread_subject.size()
85/34: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
85/35: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
85/36: emails_threads['send_min'] = minimum_send_datetime
85/37: emails_threads['send_max'] = maximum_send_datetime
85/38: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min
85/39: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
85/40: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
85/41: emails_threads.thread_time.min()
85/42: emails_threads.thread_time.max()
85/43: all_emails_training.thread_subject.isin(['please help a newbie compile mplayer :-)']).any()
85/44: all_emails_training.head()
85/45: all_emails_training.head(20)
85/46: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
85/47: all_emails_training.head(2)
85/48: all_emails_training.is_thread.where(threads_subject.isin(all_emails_training.Subject), True, False)
85/49: all_emails_training.is_thread.where(threads_subjects.isin(all_emails_training.Subject), True, False)
85/50: all_emails_training.head()
85/51: all_emails_training.is_thread.where(~threads_subjects.isin(all_emails_training.Subject), True, inplace=True)
85/52: all_emails_training.head()
85/53: all_emails_training.is_thread.where(threads_subjects.isin(all_emails_training.Subject), True, inplace=True)
85/54: all_emails_training.head()
85/55: all_emails_training.is_thread.where(threads_subjects.isin(all_emails_training.Subject).any(), True, inplace=True)
85/56: threads_subjects.isin(all_emails_training.Subject)
85/57: all_emails_training.is_thread.where(threads_subjects.isin(all_emails_training.Subject), True, inplace=True)
85/58: all_emails_training.head()
85/59: ~threads_subjects.isin(all_emails_training.Subject)
85/60: all_emails_training.is_thread.where(threads_subjects.isin(all_emails_training.Subject), other=True, inplace=True)
85/61: all_emails_training.head()
85/62: all_emails_training.is_thread.where(threads_subjects.isin(all_emails_training.Subject), other=True,axis=1, inplace=True)
85/63: all_emails_training.is_thread.where(threads_subjects.isin(all_emails_training.Subject), other='xxxx', inplace=True)
85/64: all_emails_training.head()
85/65: all_emails_training.is_thread.where(all_emails_training.is_thread, other='xxxx', inplace=True)
85/66: all_emails_training.thread_subject.where(all_emails_training.is_thread, other='xxxx', inplace=True)
85/67: all_emails_training.thread_subject.where(all_emails_training.is_thread==True, other='xxxx', inplace=True)
85/68: all_emails_training.head()
85/69: all_emails_training.is_thread.where(threads_subject.isin([all_emails_training.Subject]), other=True, inplace=True)
85/70: all_emails_training.is_thread.where(threads_subjects.isin([all_emails_training.Subject]), other=True, inplace=True)
85/71: all_emails_training.head()
85/72: all_emails_training.is_thread.where(~threads_subjects.isin([all_emails_training.Subject]), other=True, inplace=True)
85/73: all_emails_training.head()
85/74: all_emails_training.head(10)
85/75: all_emails_training.head(15)
85/76: ~all_emails_training.Subject.isin(threads_subjects)
85/77: threads_subjects.isin(r"please help a newbie compile mplayer :-)")
85/78: threads_subjects.isin([r"please help a newbie compile mplayer :-)"])
85/79: threads_subjects[:10]
85/80: all_emails_training.Subject[:10]
85/81: all_emails_training.Subject[:10].isin(subjects_threads[:10])
85/82: all_emails_training.Subject[:10].isin(threads_subjects[:10])
85/83: all_emails_training.Subject[:10].isin(threads_subjects[:10]).any()
85/84: all_emails_training.Subject[:10].isin(threads_subjects[:10])
85/85: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subject), other=True, inplace=True)
85/86: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
85/87: all_emails_training.head(15)
85/88: all_emails_training.head()
85/89: all_emails_training
85/90:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr
85/91:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
85/92:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
85/93: headers, messages = get_emails(easyham_path)
85/94: headers_lines = [header.split('\n') for header in headers]
85/95:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
85/96:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
85/97:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
85/98:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
85/99:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
85/100: dates_lines = [datestr[6:] for datestr in dates_lines]
85/101: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
85/102: emails_data = [send_datetime, from_addresses, subjects, messages]
85/103:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
85/104: all_emails_training = all_emails_data.loc[:1250]
85/105: len(all_emails_training)
85/106: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
85/107:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
85/108:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
85/109: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
85/110:
for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:
    if all_emails_training.Subject.isin([subject]).any():
        all_emails_training.at[all_emails_training.loc[all_emails_training.Subject==subject].index,'is_thread']=True
85/111:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr
85/112:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
85/113:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
85/114: headers, messages = get_emails(easyham_path)
85/115: headers_lines = [header.split('\n') for header in headers]
85/116:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
85/117:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
85/118:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
85/119:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
85/120:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
85/121: dates_lines = [datestr[6:] for datestr in dates_lines]
85/122: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
85/123: emails_data = [send_datetime, from_addresses, subjects, messages]
85/124:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
85/125: all_emails_training = all_emails_data.loc[:1250]
85/126: len(all_emails_training)
85/127: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
85/128:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
85/129:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
85/130: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
85/131:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    print(threads_subjects[:10])
    return threads_subjects
85/132: threads_subjects = get_threads(all_emails_training)
85/133: all_emails_training['thread_subject'] = threads_subjects
85/134: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
85/135: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
85/136:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
emails_threads.head()
85/137: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
85/138: emails_threads_senders.tail()
85/139:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
85/140: emails_threads_senders.tail()
85/141: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
85/142: thread_number_of_messages = grouped_by_thread_subject.size()
85/143: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
85/144: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
85/145: emails_threads['send_min'] = minimum_send_datetime
85/146: emails_threads['send_max'] = maximum_send_datetime
85/147: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min
85/148: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
85/149: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
85/150: emails_threads.thread_time.min()
85/151: emails_threads.thread_time.max()
85/152: emails_threads['thread_density']=emails_threads.thread_time/emails_threads.thread_number_of_messages
85/153: emails_threads['thread_density']=emails_threads.thread_time/emails_threads.Number_of_messages
85/154: emails_threads
85/155: all_emails_training.head(15)
85/156:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr
85/157:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
85/158:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
85/159: headers, messages = get_emails(easyham_path)
85/160: headers_lines = [header.split('\n') for header in headers]
85/161:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
85/162:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
85/163:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
85/164:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
85/165:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
85/166: dates_lines = [datestr[6:] for datestr in dates_lines]
85/167: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
85/168: emails_data = [send_datetime, from_addresses, subjects, messages]
85/169:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
85/170: all_emails_training = all_emails_data.loc[:1250]
85/171: len(all_emails_training)
85/172: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
85/173:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
85/174:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
85/175: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
85/176:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    print(threads_subjects[:10])
    return threads_subjects
85/177: threads_subjects = get_threads(all_emails_training)
85/178: all_emails_training['thread_subject'] = threads_subjects
85/179: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
85/180: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
85/181: threads_subjects = get_threads(all_emails_training)
85/182: all_emails_training.head()
85/183: all_emails_training.thread_subject.where(~(all_emails_training.is_thread&all_emails_training.thread_subject=='x'),all_emails_training.Subject, inplace=True)
85/184: all_emails_training.thread_subject.where(~(all_emails_training.is_thread and all_emails_training.thread_subject=='x'),all_emails_training.Subject, inplace=True)
85/185: all_emails_training.loc[all_emails_training.is_thread==True].thread_subject.where(~(all_emails_training.thread_subject=='x'),all_emails_training.Subject, inplace=True)
85/186: all_emails_training.head()
85/187: all_emails_training.loc[all_emails_training.is_thread==True].thread_subject.where(~(all_emails_training.thread_subject=='x'),'xxx', inplace=True)
85/188: all_emails_training.head()
85/189: all_emails_training.loc[all_emails_training.is_thread==True]
85/190: all_emails_training.loc[all_emails_training.is_thread==True].thread_subject
85/191: all_emails_training.loc[all_emails_training.is_thread==True].thread_subject.where(~(thread_subject=='x'),'xxx', inplace=True)
85/192:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),'xxx', inplace=True)
85/193: all_emails_training.head()
85/194:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
85/195: all_emails_training['thread_subject'] = threads_subjects
85/196:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='xxx'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
85/197: all_emails_training.head()
85/198: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
85/199:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    print(threads_subjects[:10])
    return threads_subjects
85/200: threads_subjects = get_threads(all_emails_training)
85/201: all_emails_training['thread_subject'] = threads_subjects
85/202: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
85/203: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
85/204: all_emails_training.head()
85/205:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
85/206: all_emails_training.head()
85/207: all_emails_training['thread_subject'] = threads_subjects
85/208:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
emails_threads.head()
85/209: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
85/210: emails_threads_senders.tail()
85/211:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
85/212: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
85/213: thread_number_of_messages = grouped_by_thread_subject.size()
85/214: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
85/215: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
85/216: emails_threads['send_min'] = minimum_send_datetime
85/217: emails_threads['send_max'] = maximum_send_datetime
85/218: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min
85/219: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
85/220: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
85/221: emails_threads.thread_time.min()
85/222: emails_threads.thread_time.max()
85/223: emails_threads['thread_density']=emails_threads.thread_time/emails_threads.Number_of_messages
85/224: emails_threads
85/225: all_emails_training.head(15)
85/226: emails_threads.head(10)
85/227:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
85/228: all_emails_training.head()
85/229:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr
85/230:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
85/231:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
85/232: headers, messages = get_emails(easyham_path)
85/233: headers_lines = [header.split('\n') for header in headers]
85/234:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
85/235:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
85/236:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
85/237:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
85/238:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
85/239: dates_lines = [datestr[6:] for datestr in dates_lines]
85/240: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
85/241: emails_data = [send_datetime, from_addresses, subjects, messages]
85/242:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
85/243: all_emails_training = all_emails_data.loc[:1250]
85/244: len(all_emails_training)
85/245: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
85/246:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
85/247:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
85/248: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
85/249:
def get_threads(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    print(threads_subjects[:10])
    return threads_subjects
85/250: threads_subjects = get_threads(all_emails_training)
85/251: all_emails_training['thread_subject'] = threads_subjects
85/252: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
85/253: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
85/254: all_emails_training.head()
85/255:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
85/256:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
emails_threads.head()
85/257: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
85/258: emails_threads_senders.tail()
85/259:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
85/260: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
85/261: thread_number_of_messages = grouped_by_thread_subject.size()
85/262: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
85/263: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
85/264: emails_threads['send_min'] = minimum_send_datetime
85/265: emails_threads['send_max'] = maximum_send_datetime
85/266: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min
85/267: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
85/268: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
85/269: emails_threads.thread_time.min()
85/270: emails_threads.thread_time.max()
85/271: emails_threads['thread_density']=emails_threads.thread_time/emails_threads.Number_of_messages
85/272: emails_threads.head(10)
85/273: all_emails_training.head(15)
85/274: emails_threads.groupby(by=thread_subject)
85/275: emails_threads.groupby(by=[thread_subject])
85/276: emails_threads.head()
86/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr
86/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
86/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
86/4: headers, messages = get_emails(easyham_path)
86/5: headers_lines = [header.split('\n') for header in headers]
86/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
86/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
86/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
86/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
86/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
86/11: dates_lines = [datestr[6:] for datestr in dates_lines]
86/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
86/13: emails_data = [send_datetime, from_addresses, subjects, messages]
86/14:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
86/15: all_emails_training = all_emails_data.loc[:1250]
86/16: len(all_emails_training)
86/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
86/18:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
86/19:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
86/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
86/21:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
86/22: threads_subjects = get_threads_subjects(all_emails_training)
86/23: all_emails_training['thread_subject'] = threads_subjects
86/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
86/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
86/26:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
86/27:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
86/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
86/29: emails_threads_senders.tail()
86/30:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
86/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
86/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
86/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
86/34: emails_threads['send_min'] = minimum_send_datetime
86/35: emails_threads['send_max'] = maximum_send_datetime
86/36: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min
86/37: thread_number_of_messages = grouped_by_thread_subject.size()
86/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
86/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
86/40: emails_threads.thread_time.min()
86/41: emails_threads.thread_time.max()
86/42: emails_threads['thread_density']=emails_threads.thread_time/emails_threads.Number_of_messages
86/43: emails_threads.head()
86/44: emails_threads.sort_values(by='thread_density')
86/45: emails_threads.loc[emails_threads.thread_time=0]
86/46: emails_threads.loc[emails_threads.thread_time==0]
86/47: emails_threads.loc[emails_threads.thread_time==datetime(0)]
86/48: emails_threads.loc[emails_threads.thread_time==datetime(0,0,0)]
86/49: emails_threads.loc[emails_threads.thread_time==datetime.datetime(0,0,0)]
86/50: emails_threads.loc[emails_threads.thread_time==datetime.timedelta(0)]
86/51: 'snow' in all_emails_training.Subject
86/52: 're: snow' in all_emails_training.Subject
86/53: all_emails_training.loc[711, 'Subject']
86/54: all_emails_training.Subject[all_emails_training.Subject.contains('snow')]
86/55: all_emails_training.Subject[all_emails_training.Subject.str.contains('snow')]
86/56: all_emails_training.Subject[all_emails_training.Subject.str.contains('underground')]
86/57: all_emails_training.Subject[all_emails_training.Subject.str.contains('satalk')]
86/58: all_emails_training.Subject[all_emails_training.Subject.str.contains('spamassasin')]
86/59: all_emails_training.Subject[all_emails_training.Subject.str.contains('spamassassin')]
86/60: emails_threads.thread_density.where(emails_threads.number_of_messages>1)
86/61: emails_threads.thread_density.where(emails_threads.Number_of_messages>1)
86/62: emails_threads.thread_density.where(emails_threads.Number_of_messages>1, inplace=True)
86/63: emails_threads.loc[emails_threads.thread_time==datetime.timedelta(0)]
86/64: emails_threads.head()
86/65: emails_threads.head(10)
86/66: emails_threads['thread_density']=(emails_threads.thread_time/emails_threads.Number_of_messages).total_seconds()
86/67: emails_threads['thread_density']=(emails_threads.thread_time/emails_threads.Number_of_messages).dt.total_seconds()
86/68: emails_threads.head(10)
86/69: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
86/70: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
86/71: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
86/72: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
86/73: emails_threads.thread_time
86/74: emails_threads.sort_values(by='thread_time')
86/75: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
86/76: thread_number_of_messages = grouped_by_thread_subject.size()
86/77: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
86/78: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
86/79: emails_threads.thread_time.min()
86/80: emails_threads.thread_time.max()
86/81: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
86/82: emails_threads.sort_values(by='thread_time')
86/83:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr
86/84:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
86/85:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
86/86: headers, messages = get_emails(easyham_path)
86/87: headers_lines = [header.split('\n') for header in headers]
86/88:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
86/89:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
86/90:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
86/91:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
86/92:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
86/93: dates_lines = [datestr[6:] for datestr in dates_lines]
86/94: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
86/95: emails_data = [send_datetime, from_addresses, subjects, messages]
86/96:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
86/97: all_emails_training = all_emails_data.loc[:1250]
86/98: len(all_emails_training)
86/99: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
86/100:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
86/101:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
86/102: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
86/103:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
86/104: threads_subjects = get_threads_subjects(all_emails_training)
86/105: all_emails_training['thread_subject'] = threads_subjects
86/106: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
86/107: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
86/108:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
86/109:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
86/110: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
86/111: emails_threads_senders.tail()
86/112:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
86/113: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
86/114: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
86/115: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
86/116: emails_threads['send_min'] = minimum_send_datetime
86/117: emails_threads['send_max'] = maximum_send_datetime
86/118: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
86/119: thread_number_of_messages = grouped_by_thread_subject.size()
86/120: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
86/121: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
86/122: emails_threads.thread_time.min()
86/123: emails_threads.thread_time.max()
86/124: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
86/125: emails_threads.sort_values(by='thread_time')
86/126: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
86/127: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
86/128: emails_threads.head(10)
86/129: emails_threads.sort_values(by='thread_density')
86/130: emails_threads.sort_values(by='thread_density').max()
86/131: emails_threads.loc[emails_threads.thread_subject=='whoa}']
86/132: emails_threads.loc[emails_threads.thread_subject=='whoa']
86/133: emails_threads.loc[emails_threads.thread_subject=='whoa}']
86/134: emails_threads.tail()
86/135: emails_threads.loc[emails_threads.thread_subject=='whoa']
86/136: emails_threads.loc[2498]
86/137: emails_threads.index.max()
86/138: emails_threads.index
86/139: emails_threads.loc['index']
86/140: emails_threads.loc[:,'index']
86/141: emails_threads.loc[:,'index'].max()
86/142: emails_threads.sort_values(by='index')
86/143: emails_threads.sort_values(by='thread_denisty').max()
86/144: emails_threads.sort_values(by='thread_density').max()
86/145: emails_threads.loc[emails_threads.Number_of_messages==42]
87/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr
87/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
87/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
87/4: headers, messages = get_emails(easyham_path)
87/5: headers_lines = [header.split('\n') for header in headers]
87/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
87/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
87/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
87/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
87/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
87/11: dates_lines = [datestr[6:] for datestr in dates_lines]
87/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
87/13: emails_data = [send_datetime, from_addresses, subjects, messages]
87/14:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
87/15: all_emails_training = all_emails_data.loc[:1250]
87/16: len(all_emails_training)
87/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
87/18:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
87/19:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
87/20: all_emails_training.loc[:,'is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
87/21: all_emails_training.loc[:,['is_thread']] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
87/22: all_emails_training.loc['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
87/23:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
87/24: threads_subjects = get_threads_subjects(all_emails_training)
87/25:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr
87/26:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
87/27:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
87/28: headers, messages = get_emails(easyham_path)
87/29: headers_lines = [header.split('\n') for header in headers]
87/30:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
87/31:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
87/32:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
87/33:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
87/34:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
87/35: dates_lines = [datestr[6:] for datestr in dates_lines]
87/36: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
87/37: emails_data = [send_datetime, from_addresses, subjects, messages]
87/38:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
87/39: all_emails_training = all_emails_data.loc[:1250]
87/40: len(all_emails_training)
87/41: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
87/42:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
87/43:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
87/44: all_emails_training.loc['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
87/45:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
87/46: threads_subjects = get_threads_subjects(all_emails_training)
87/47: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
87/48:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
87/49: threads_subjects = get_threads_subjects(all_emails_training)
87/50: threads_subjects = get_threads_subjects(all_emails_training)
87/51:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr
87/52:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
87/53:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
87/54: headers, messages = get_emails(easyham_path)
87/55: headers_lines = [header.split('\n') for header in headers]
87/56:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
87/57:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
87/58:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
87/59:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
87/60:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
87/61: dates_lines = [datestr[6:] for datestr in dates_lines]
87/62: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
87/63: emails_data = [send_datetime, from_addresses, subjects, messages]
87/64:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
87/65: all_emails_training = all_emails_data.loc[:1250]
87/66: len(all_emails_training)
87/67: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
87/68:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
87/69:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
87/70: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
87/71:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
87/72: threads_subjects = get_threads_subjects(all_emails_training)
87/73: all_emails_training['thread_subject'] = threads_subjects
87/74: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
87/75: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
87/76:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
87/77:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
87/78: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
87/79: emails_threads_senders.tail()
87/80:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
87/81: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
87/82: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
87/83: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
87/84: emails_threads['send_min'] = minimum_send_datetime
87/85: emails_threads['send_max'] = maximum_send_datetime
87/86: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
87/87: thread_number_of_messages = grouped_by_thread_subject.size()
87/88: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
87/89: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
87/90: emails_threads.thread_time.min()
87/91: emails_threads.thread_time.max()
87/92: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
87/93: emails_threads.sort_values(by='thread_time')
87/94: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
87/95: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
87/96: emails_threads.tail()
87/97: emails_threads.sort_values(by='thread_density').max()
87/98: emails_threads.loc[emails_threads.Number_of_messages==42]
87/99: emails_threads.loc[:,'index'].max()
87/100: emails_threads.sort_values(by='thread_density')
87/101: help(pd.sort_values)
87/102: help(pd.DataFrame.sort_values)
87/103: emails_threads.sort_values(by='thread_density', ascending=False)
87/104: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+10
87/105: emails_threads.sort_values(by='thread_density', ascending=False)
87/106: emails_threads.head()
87/107: emails_threads.head(20)
88/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr
88/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
88/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
88/4: headers, messages = get_emails(easyham_path)
88/5: headers_lines = [header.split('\n') for header in headers]
88/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
88/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
88/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
88/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
88/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
88/11: dates_lines = [datestr[6:] for datestr in dates_lines]
88/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
88/13: emails_data = [send_datetime, from_addresses, subjects, messages]
88/14:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
88/15: all_emails_training = all_emails_data.loc[:1250]
88/16: len(all_emails_training)
88/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
88/18:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
88/19:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
88/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
88/21:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
88/22: threads_subjects = get_threads_subjects(all_emails_training)
88/23: all_emails_training['thread_subject'] = threads_subjects
88/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
88/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
88/26:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
88/27:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
88/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
88/29: emails_threads_senders.tail()
88/30:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
88/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
88/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
88/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
88/34: emails_threads['send_min'] = minimum_send_datetime
88/35: emails_threads['send_max'] = maximum_send_datetime
88/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
88/37: thread_number_of_messages = grouped_by_thread_subject.size()
88/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
88/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
88/40: emails_threads.thread_time.min()
88/41: emails_threads.thread_time.max()
88/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
88/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
88/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
88/45: emails_threads.head()
88/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+10
88/47: def thread_tdm(df):
88/48: emails_threads.head()
88/49: all_emails_training.head()
88/50:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr

from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
88/51: stop_words = stopwords.words('english')
88/52:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
88/53:
def thread_tdm(df):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
88/54:
def thread_tdm(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
88/55: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].message)
88/56: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
88/57: thread_tdm.head()
88/58: thread_tdm.head(10)
88/59: all_emails_training[all_emails_training.is_thread==True].Message
88/60: all_emails_training[all_emails_training.is_thread==True].Message[:2]
88/61: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[:1])
88/62: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[:2])
88/63: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
88/64:
def thread_tdm(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
88/65: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
88/66: thread_tdm.head(10)
88/67: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[:2])
88/68: list(all_emails_training[all_emails_training.is_thread==True].Message[:2])
88/69: thread_tdm = thread_tdm(list(all_emails_training[all_emails_training.is_thread==True].Message)[:2])
88/70:
def make_thread_tdm(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
88/71: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)[:2]
88/72: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)[:2]
88/73: thread_tdm.head(10)
88/74: delete thread_tdm
88/75: delete(thread_tdm)
88/76: del(thread_tdm)
88/77: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)[:2]
88/78: thread_tdm.head(10)
88/79:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr

from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
88/80:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
88/81:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
88/82: headers, messages = get_emails(easyham_path)
88/83: headers_lines = [header.split('\n') for header in headers]
88/84:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
88/85:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
88/86:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
88/87:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
88/88:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
88/89: dates_lines = [datestr[6:] for datestr in dates_lines]
88/90: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
88/91: emails_data = [send_datetime, from_addresses, subjects, messages]
88/92:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
88/93: all_emails_training = all_emails_data.loc[:1250]
88/94: len(all_emails_training)
88/95: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
88/96:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
88/97:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
88/98: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
88/99:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
88/100: threads_subjects = get_threads_subjects(all_emails_training)
88/101: all_emails_training['thread_subject'] = threads_subjects
88/102: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
88/103: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
88/104:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
88/105:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
88/106: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
88/107: emails_threads_senders.tail()
88/108:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
88/109: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
88/110: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
88/111: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
88/112: emails_threads['send_min'] = minimum_send_datetime
88/113: emails_threads['send_max'] = maximum_send_datetime
88/114: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
88/115: thread_number_of_messages = grouped_by_thread_subject.size()
88/116: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
88/117: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
88/118: emails_threads.thread_time.min()
88/119: emails_threads.thread_time.max()
88/120: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
88/121: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
88/122: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
88/123: emails_threads.head()
88/124: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+10
88/125: all_emails_training.head()
88/126: stop_words = stopwords.words('english')
88/127:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
88/128:
def make_thread_tdm(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
88/129: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)[:2]
88/130: thread_tdm.head(10)
88/131: list(all_emails_training[all_emails_training.is_thread==True].Message[:2])
88/132: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)[:1]
88/133: thread_tdm.head(10)
90/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr

from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
90/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
90/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
90/4: headers, messages = get_emails(easyham_path)
90/5: headers_lines = [header.split('\n') for header in headers]
90/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
90/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
90/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
90/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
90/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
90/11: dates_lines = [datestr[6:] for datestr in dates_lines]
90/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
90/13: emails_data = [send_datetime, from_addresses, subjects, messages]
90/14:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
90/15: all_emails_training = all_emails_data.loc[:1250]
90/16: len(all_emails_training)
90/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
90/18:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
90/19:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
90/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
90/21:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
90/22: threads_subjects = get_threads_subjects(all_emails_training)
90/23: all_emails_training['thread_subject'] = threads_subjects
90/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
90/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
90/26:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
90/27:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
90/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
90/29: emails_threads_senders.tail()
90/30:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
90/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
90/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
90/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
90/34: emails_threads['send_min'] = minimum_send_datetime
90/35: emails_threads['send_max'] = maximum_send_datetime
90/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
90/37: thread_number_of_messages = grouped_by_thread_subject.size()
90/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
90/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
90/40: emails_threads.thread_time.min()
90/41: emails_threads.thread_time.max()
90/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
90/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
90/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
90/45: emails_threads.head()
90/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+10
90/47: all_emails_training.head()
90/48: stop_words = stopwords.words('english')
90/49:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
90/50:
def make_thread_tdm(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
90/51: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)[:1]
90/52: thread_tdm.head(10)
90/53: list(all_emails_training[all_emails_training.is_thread==True].Message[:2])
90/54:
def make_thread_tdm(messages):
    dicts = [count_words(message) for message in messages]
    print(dicts)
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
90/55: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)[:1]
90/56: len(all_emails_training[all_emails_training.is_thread==True].Message[0])
90/57: all_emails_trainins[all_emails_training.is_thread==True].Message[0]
90/58: all_emails_trainins[all_emails_training.is_thread==True].Message[0]
90/59: all_emails_training[all_emails_training.is_thread==True].Message[0]
90/60: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[0])
90/61: thread_tdm = make_thread_tdm(list[all_emails_training[all_emails_training.is_thread==True].Message[0]])
90/62: thread_tdm = make_thread_tdm(list(all_emails_training[all_emails_training.is_thread==True].Message[0]))
90/63: all_emails_training[all_emails_training.is_thread==True].Message[:2]
90/64: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[:2])
90/65: thread_tdm.head(10)
90/66: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
90/67: thread_tdm.head(10)
90/68: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[:10])
90/69: thread_tdm.head(10)
90/70:
def make_thread_tdm(messages):
    dicts = [count_words(message) for message in messages]
    print(dicts)
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
90/71:
def make_thread_tdm(messages):
    dicts = [count_words(message) for message in messages]
    #print(dicts)
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
90/72: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[:10])
90/73: thread_tdm.head(10)
90/74: stop_words = stopwords.words('english')
90/75:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
90/76:
def make_thread_tdm(messages):
    dicts = [count_words(message) for message in messages]
    #print(dicts)
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
90/77: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[:10])
90/78: thread_tdm.head(10)
90/79: thread_tdm.head()
90/80: thread_tdm.head(20)
90/81: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
90/82: thread_tdm.head(20)
90/83: thread_tdm.tail(20)
91/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr

from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
91/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
91/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
91/4: headers, messages = get_emails(easyham_path)
91/5: headers_lines = [header.split('\n') for header in headers]
91/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
91/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
91/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
91/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
91/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
91/11: dates_lines = [datestr[6:] for datestr in dates_lines]
91/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
91/13: emails_data = [send_datetime, from_addresses, subjects, messages]
91/14:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
91/15: all_emails_training = all_emails_data.loc[:1250]
91/16: len(all_emails_training)
91/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
91/18:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
91/19:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
91/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
91/21:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
91/22: threads_subjects = get_threads_subjects(all_emails_training)
91/23: all_emails_training['thread_subject'] = threads_subjects
91/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
91/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
91/26:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
91/27:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
91/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
91/29: emails_threads_senders.tail()
91/30:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
91/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
91/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
91/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
91/34: emails_threads['send_min'] = minimum_send_datetime
91/35: emails_threads['send_max'] = maximum_send_datetime
91/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
91/37: thread_number_of_messages = grouped_by_thread_subject.size()
91/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
91/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
91/40: emails_threads.thread_time.min()
91/41: emails_threads.thread_time.max()
91/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
91/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
91/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
91/45: emails_threads.head()
91/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+10
91/47: all_emails_training.head()
91/48: stop_words = stopwords.words('english')
91/49:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
91/50:
def make_thread_tdm(messages):
    dicts = [count_words(message) for message in messages]
    #print(dicts)
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    
    return matrix
91/51: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
91/52: thread_tdm.tail(20)
91/53: len(all_emails_training[all_emails_training.is_thread==True].Message[0])
91/54: all_emails_training[all_emails_training.is_thread==True].Message[:2]
91/55: thread_tdm['help']
91/56: emails_threads.head()
91/57: emails_threads.head(8)
91/58: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12
91/59: emails_threads.head(8)
91/60:
def make_thread_tdm(messages):
    dicts = [count_words(message) for message in messages]
    #print(dicts)
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])
    
    return matrix
91/61: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
91/62: thread_tdm.tail()
91/63:
def make_thread_tdm(messages):
    dicts = [count_words(message) for message in messages]
    #print(dicts)
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10
    
    return matrix
91/64: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
91/65: thread_tdm.tail()
91/66:
emails_tdm = make_tdm(all_emails_training.Message)
emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2
91/67:
def make_tdm(messages):
    dicts = [count_words(message) for message in messages]
    #print(dicts)
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10
    
    return matrix
91/68:
emails_tdm = make_tdm(all_emails_training.Message)
emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2
91/69: grouped_from.head()
91/70: grouped_from.head(20)
91/71: grouped_from.tail(20)
91/72:
#grouped_from.tail(20 -social activity measure
emails_senders.head()
91/73:
#grouped_from.tail(20 -social activity measure
emails_threads_senders.head()
91/74:
#grouped_from.tail(20 -social activity measure
emails_threads_senders.tail()
91/75:
#grouped_from.tail(20 -social activity measure
emails_threads_senders.tail()
91/76:
emails_tdm = make_tdm(all_emails_training.Message)
emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2
emails_tdm.tail()
91/77:
#grouped_from.tail(20 -social activity measure
#emails_threads_senders.tail() -thread senders activity measure
thread_tdm.weight.tail()
92/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr

from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
92/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
92/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
92/4: headers, messages = get_emails(easyham_path)
92/5: headers_lines = [header.split('\n') for header in headers]
92/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
92/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
92/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
92/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
92/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
92/11: dates_lines = [datestr[6:] for datestr in dates_lines]
92/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
92/13: emails_data = [send_datetime, from_addresses, subjects, messages]
92/14:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
92/15: all_emails_training = all_emails_data.loc[:1250]
92/16: len(all_emails_training)
92/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
92/18:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
92/19:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
92/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
92/21:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
92/22: threads_subjects = get_threads_subjects(all_emails_training)
92/23: all_emails_training['thread_subject'] = threads_subjects
92/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
92/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
92/26:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
92/27:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
92/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
92/29: emails_threads_senders.tail()
92/30:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
92/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
92/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
92/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
92/34: emails_threads['send_min'] = minimum_send_datetime
92/35: emails_threads['send_max'] = maximum_send_datetime
92/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
92/37: thread_number_of_messages = grouped_by_thread_subject.size()
92/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
92/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
92/40: emails_threads.thread_time.min()
92/41: emails_threads.thread_time.max()
92/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
92/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
92/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
92/45: emails_threads.head(8)
92/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12
92/47: all_emails_training.head()
92/48: stop_words = stopwords.words('english')
92/49:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
92/50:
def make_tdm(messages):
    dicts = [count_words(message) for message in messages]
    #print(dicts)
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10
    
    return matrix
92/51: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
92/52: thread_tdm.tail()
92/53:
emails_tdm = make_tdm(all_emails_training.Message)
emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2
emails_tdm.tail()
92/54:
#grouped_from.tail(20 -social activity measure
#emails_threads_senders.tail() -thread senders activity measure
#emails_threads.thread_density -thread activity measure
#thread_tdm.weight -words in active threads mesure
#emails_tdm.weight -words in all messages measure
92/55:
get_from_measure(senders):
    x = [grouped_from[sender] for sender in senders if sender in grouped_from else 1]
    from_measure = grouped_from[sender]
92/56:
get_from_measure(senders):
    x = [grouped_from[sender] for sender in senders if sender in grouped_from else 1]
92/57:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
92/58:
get_from_measure(senders):
    x = [grouped_from[sender] for sender in senders if sender in grouped_from else 1]
92/59:
def get_from_measure(senders):
    x = [grouped_from[sender] for sender in senders if sender in grouped_from else 1]
94/1:
import pandas as pd
a = pd.Series({'a':1, 'b':2})
94/2: a
94/3:
def y(x):
    result = [a.get(z,1) for z in x]
    print(result)
94/4: y(['a','c','b'])
93/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr

from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
93/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
93/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
93/4: headers, messages = get_emails(easyham_path)
93/5: headers_lines = [header.split('\n') for header in headers]
93/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
93/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
93/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
93/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
93/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
93/11: dates_lines = [datestr[6:] for datestr in dates_lines]
93/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
93/13: emails_data = [send_datetime, from_addresses, subjects, messages]
93/14:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
93/15: all_emails_training = all_emails_data.loc[:1250]
93/16: len(all_emails_training)
93/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
93/18:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
93/19:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
93/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
93/21:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
93/22: threads_subjects = get_threads_subjects(all_emails_training)
93/23: all_emails_training['thread_subject'] = threads_subjects
93/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
93/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
93/26:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
93/27:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
93/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
93/29: emails_threads_senders.tail()
93/30:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
93/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
93/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
93/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
93/34: emails_threads['send_min'] = minimum_send_datetime
93/35: emails_threads['send_max'] = maximum_send_datetime
93/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
93/37: thread_number_of_messages = grouped_by_thread_subject.size()
93/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
93/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
93/40: emails_threads.thread_time.min()
93/41: emails_threads.thread_time.max()
93/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
93/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
93/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
93/45: emails_threads.head(8)
93/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12
93/47: all_emails_training.head()
93/48: stop_words = stopwords.words('english')
93/49:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
93/50:
def make_tdm(messages):
    dicts = [count_words(message) for message in messages]
    #print(dicts)
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10
    
    return matrix
93/51: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
93/52: thread_tdm.tail()
93/53:
emails_tdm = make_tdm(all_emails_training.Message)
emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2
emails_tdm.tail()
93/54:
#grouped_from.tail(20 -social activity measure
#emails_threads_senders.tail() -thread senders activity measure
#emails_threads.thread_density -thread activity measure
#thread_tdm.weight -words in active threads mesure
#emails_tdm.weight -words in all messages measure
93/55:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
93/56:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
93/57:
test_emails = all_emails_data[:10]
test_emails_data['rank']= rank_emails[test_emails_data]
test_emails_data.sort_values(by=['rank'])
93/58:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails[test_emails_data]
test_emails_data.sort_values(by=['rank'])
93/59:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = 1# get_from_thread_measure(emails.From)
    thread_activity_measure = 1# get_thread_activity_measure(emails)
    words_in_thread_measure = 1#get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
93/60:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'])
93/61:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    print(x)
93/62:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'])
93/63:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    print(x)
    return pd.Series(x)
93/64:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print(result)
    return result
93/65:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'])
93/66:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
93/67:
def get_from_thread_measure(senders):
    x = [emails_threads_senders.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print(result)
    return result
93/68:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = 1# get_from_thread_measure(emails.From)
    thread_activity_measure = 1# get_thread_activity_measure(emails)
    words_in_thread_measure = 1#get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
93/69:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = 1# get_thread_activity_measure(emails)
    words_in_thread_measure = 1#get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
93/70:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
93/71:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print(result.sort_values())
    return result
93/72:
def get_from_thread_measure(senders):
    x = [emails_threads_senders.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print(result.sort_values())
    return result
93/73:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
93/74:
def get_from_thread_measure(senders):
    x = [emails_threads_senders.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print('get_from_thread_measure: ', result.sort_values())
    return result
93/75:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print('get_from_measure: ', result.sort_values())
    return result
93/76:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
93/77:
def get_from_thread_measure(senders):
    x = [emails_threads_senders.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print('get_from_thread_measure: \n', result.sort_values())
    return result
93/78:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print('get_from_measure: \n', result.sort_values())
    return result
93/79:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
94/5:
a=pd.Series[1,2,3]
b=pd.Series[1,2,3]
z = a*b
94/6:
a=pd.Series([1,2,3])
b=pd.Series([1,2,3])
z = a*b
94/7:
a=pd.Series([1,2,3])
b=pd.Series([1,2,3])
z = a*b
print(x)
94/8:
a=pd.Series([1,2,3])
b=pd.Series([1,2,3])
z = a*b
print(z)
95/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr

from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
95/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
95/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
95/4: headers, messages = get_emails(easyham_path)
95/5: headers_lines = [header.split('\n') for header in headers]
95/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
95/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
95/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
95/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
95/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
95/11: dates_lines = [datestr[6:] for datestr in dates_lines]
95/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
95/13: emails_data = [send_datetime, from_addresses, subjects, messages]
95/14:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
95/15: all_emails_training = all_emails_data.loc[:1250]
95/16: len(all_emails_training)
95/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
95/18:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
95/19:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
95/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
95/21:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
95/22: threads_subjects = get_threads_subjects(all_emails_training)
95/23: all_emails_training['thread_subject'] = threads_subjects
95/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
95/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
95/26:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
95/27:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
95/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
95/29: #emails_threads_senders.tail()
95/30:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
95/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
95/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
95/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
95/34: emails_threads['send_min'] = minimum_send_datetime
95/35: emails_threads['send_max'] = maximum_send_datetime
95/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
95/37: thread_number_of_messages = grouped_by_thread_subject.size()
95/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
95/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
95/40: emails_threads.thread_time.min()
95/41: emails_threads.thread_time.max()
95/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
95/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
95/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
95/45: #emails_threads.head(8)
95/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12
95/47: #all_emails_training.head()
95/48: stop_words = stopwords.words('english')
95/49:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
95/50:
def make_tdm(messages):
    dicts = [count_words(message) for message in messages]
    #print(dicts)
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10
    
    return matrix
95/51: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
95/52: thread_tdm.tail()
95/53:
emails_tdm = make_tdm(all_emails_training.Message)
emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2
emails_tdm.tail()
95/54:
#grouped_from.tail(20 -social activity measure
#emails_threads_senders.tail() -thread senders activity measure
#emails_threads.thread_density -thread activity measure
#thread_tdm.weight -words in active threads mesure
#emails_tdm.weight -words in all messages measure
95/55:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print('get_from_measure: \n', result.sort_values())
    return result
95/56:
def get_from_thread_measure(senders):
    x = [emails_threads_senders.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print('get_from_thread_measure: \n', result.sort_values())
    return result
95/57:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    all_emails_training['thread_subject'] = threads_subjects
95/58:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = 1# get_thread_activity_measure(emails)
    words_in_thread_measure = 1#get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
95/59:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
95/60:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    print(emails)
95/61: x = all_emails_data[-10:]
95/62: x
95/63:
#test_emails_data = all_emails_data[:10]
#test_emails_data['rank']= rank_emails(test_emails_data)
#test_emails_data.sort_values(by=['rank'], ascending=False)
95/64: get_thread_activity_measure(x)
95/65:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    emails
95/66: get_thread_activity_measure(x)
95/67: x
95/68:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails_threads = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails_threads.thread_density)+12
95/69: x = all_emails_data[-10:]
95/70: x.head()
95/71: get_thread_activity_measure(x)
95/72:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails_threads = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails_threads.thread_density)+12
95/73:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails_threads = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails_threads.thread_density)+12
95/74: get_thread_activity_measure(x)
95/75:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails= emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails_threads.thread_density)+12
95/76: get_thread_activity_measure(x)
95/77: x
95/78:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails= emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
95/79: get_thread_activity_measure(x)
95/80: x
96/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr

from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
96/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
96/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
96/4: headers, messages = get_emails(easyham_path)
96/5: headers_lines = [header.split('\n') for header in headers]
96/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
96/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
96/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
96/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
96/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
96/11: dates_lines = [datestr[6:] for datestr in dates_lines]
96/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
96/13: emails_data = [send_datetime, from_addresses, subjects, messages]
96/14:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
96/15: all_emails_training = all_emails_data.loc[:1250]
96/16: len(all_emails_training)
96/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
96/18:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
96/19:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
96/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
96/21:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
96/22: threads_subjects = get_threads_subjects(all_emails_training)
96/23: all_emails_training['thread_subject'] = threads_subjects
96/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
96/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
96/26:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
96/27:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
96/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
96/29: #emails_threads_senders.tail()
96/30:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
96/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
96/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
96/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
96/34: emails_threads['send_min'] = minimum_send_datetime
96/35: emails_threads['send_max'] = maximum_send_datetime
96/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
96/37: thread_number_of_messages = grouped_by_thread_subject.size()
96/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
96/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
96/40: emails_threads.thread_time.min()
96/41: emails_threads.thread_time.max()
96/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
96/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
96/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
96/45: #emails_threads.head(8)
96/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12
96/47: #all_emails_training.head()
96/48: stop_words = stopwords.words('english')
96/49:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
96/50:
def make_tdm(messages):
    dicts = [count_words(message) for message in messages]
    #print(dicts)
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10
    
    return matrix
96/51: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
96/52: thread_tdm.tail()
96/53:
emails_tdm = make_tdm(all_emails_training.Message)
emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2
emails_tdm.tail()
96/54:
#grouped_from.tail(20 -social activity measure
#emails_threads_senders.tail() -thread senders activity measure
#emails_threads.thread_density -thread activity measure
#thread_tdm.weight -words in active threads mesure
#emails_tdm.weight -words in all messages measure
96/55:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print('get_from_measure: \n', result.sort_values())
    return result
96/56:
def get_from_thread_measure(senders):
    x = [emails_threads_senders.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print('get_from_thread_measure: \n', result.sort_values())
    return result
96/57:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails= emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
96/58:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = 1# get_thread_activity_measure(emails)
    words_in_thread_measure = 1#get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
96/59:
#test_emails_data = all_emails_data[:10]
#test_emails_data['rank']= rank_emails(test_emails_data)
#test_emails_data.sort_values(by=['rank'], ascending=False)
96/60: x = all_emails_data[-10:]
96/61: x.head()
96/62: get_thread_activity_measure(x)
96/63: x
96/64:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    print(thread_number_of_messages)
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails= emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
96/65: get_thread_activity_measure(x)
96/66:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    print(thread_number_of_messages_df)
    emails= emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
96/67: get_thread_activity_measure(x)
96/68:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails= emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
96/69: x = all_emails_data[-10:]
96/70: x.head()
96/71: get_thread_activity_measure(x)
96/72: x
96/73: x.head()
96/74:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails= emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
96/75: x = all_emails_data[-10:]
96/76: x.head()
96/77: get_thread_activity_measure(x)
96/78:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
96/79: x = all_emails_data[-10:]
96/80: x.head()
96/81: get_thread_activity_measure(x)
96/82:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    #print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    #print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
    
    return emails.thread_weight
96/83: get_thread_activity_measure(x)
96/84:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    #print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    #print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
    
    return emails.thread_weight
96/85:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = 1# get_thread_activity_measure(emails)
    words_in_thread_measure = 1#get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
96/86:
#test_emails_data = all_emails_data[:10]
#test_emails_data['rank']= rank_emails(test_emails_data)
#test_emails_data.sort_values(by=['rank'], ascending=False)
96/87: x = all_emails_data[-10:]
96/88: x.head()
96/89:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    #print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    #print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
    
    return emails.thread_weight
96/90: x = all_emails_data[-10:]
96/91: x.head()
96/92: get_thread_activity_measure(x)
96/93: x.head()
96/94:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    #print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    #print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
    emails.thread_weight.fillna(value=1)
    
    return emails.thread_weight
96/95: get_thread_activity_measure(x)
96/96:
def get_thread_activity_measure(emails):    
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    #print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    #print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
    emails.thread_weight.fillna(value=1, inplace=True)
    
    return emails.thread_weight
96/97: get_thread_activity_measure(x)
96/98:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = 1#get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
96/99:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
96/100:
def get_thread_activity_measure(emails):
    emails = emails.copy(deep+True)
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    #print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    #print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
    emails.thread_weight.fillna(value=1, inplace=True)
    
    return emails.thread_weight
96/101:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = 1#get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
96/102:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
96/103:
def get_thread_activity_measure(emails):
    emails = emails.copy()
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    #print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    #print(emails)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
    emails.thread_weight.fillna(value=1, inplace=True)
    
    return emails.thread_weight
96/104:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = 1#get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
96/105:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
96/106:
def get_thread_activity_measure(emails):
    emails = emails.copy()
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
    emails.thread_weight.fillna(value=1, inplace=True)
    
    print(emails.thread_weight)
    
    return emails.thread_weight
96/107:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = 1#get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
96/108:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
97/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr

from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
97/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
97/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
97/4: headers, messages = get_emails(easyham_path)
97/5: headers_lines = [header.split('\n') for header in headers]
97/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
97/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
97/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
97/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
97/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
97/11: dates_lines = [datestr[6:] for datestr in dates_lines]
97/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
97/13: emails_data = [send_datetime, from_addresses, subjects, messages]
97/14:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
97/15: all_emails_training = all_emails_data.loc[:1250]
97/16: len(all_emails_training)
97/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
97/18:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
97/19:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
97/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
97/21:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
97/22: threads_subjects = get_threads_subjects(all_emails_training)
97/23: all_emails_training['thread_subject'] = threads_subjects
97/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
97/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
97/26:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
97/27:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
97/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
97/29: #emails_threads_senders.tail()
97/30:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
97/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
97/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
97/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
97/34: emails_threads['send_min'] = minimum_send_datetime
97/35: emails_threads['send_max'] = maximum_send_datetime
97/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
97/37: thread_number_of_messages = grouped_by_thread_subject.size()
97/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
97/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
97/40: emails_threads.thread_time.min()
97/41: emails_threads.thread_time.max()
97/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
97/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
97/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
97/45: #emails_threads.head(8)
97/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12
97/47: #all_emails_training.head()
97/48: stop_words = stopwords.words('english')
97/49:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
97/50:
def make_tdm(messages):
    dicts = [count_words(message) for message in messages]
    #print(dicts)
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10
    
    return matrix
97/51: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
97/52: thread_tdm.tail()
97/53:
emails_tdm = make_tdm(all_emails_training.Message)
emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2
emails_tdm.tail()
97/54:
#grouped_from.tail(20 -social activity measure
#emails_threads_senders.tail() -thread senders activity measure
#emails_threads.thread_density -thread activity measure
#thread_tdm.weight -words in active threads mesure
#emails_tdm.weight -words in all messages measure
97/55:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print('get_from_measure: \n', result.sort_values())
    return result
97/56:
def get_from_thread_measure(senders):
    x = [emails_threads_senders.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print('get_from_thread_measure: \n', result.sort_values())
    return result
97/57:
def get_thread_activity_measure(emails):
    emails = emails.copy()
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
    emails.thread_weight.fillna(value=1, inplace=True)
    
    print(emails.thread_weight)
    
    return emails.thread_weight
97/58:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = 1#get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
97/59:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
97/60:
def get_words_in_thread_measure(emails):
    messages = emails.message.copy()
97/61:
def get_words_in_thread_measure(emails):
    messages = emails.message.copy()
    print(messages)
97/62: get_words_in_thread_measure(test_emails_data)
97/63: test_emails_data
97/64:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    print(messages)
97/65: get_words_in_thread_measure(test_emails_data)
97/66:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    dicts = count_words(messages)
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    print(matrix)
97/67: get_words_in_thread_measure(test_emails_data)
97/68:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    dicts = [count_words(message) for message in messages]
    #matrix = pd.DataFrame([dicts])
    #matrix = matrix.transpose()
    
    print(matrix)
97/69:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    dicts = [count_words(message) for message in messages]
    #matrix = pd.DataFrame([dicts])
    #matrix = matrix.transpose()
    
    print(dicts)
97/70: get_words_in_thread_measure(test_emails_data)
97/71:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    dicts = [count_words(message) for message in messages]
    #matrix = pd.DataFrame([dicts])
    #matrix = matrix.transpose()
    
    print(len(dicts)
97/72: get_words_in_thread_measure(test_emails_data)
97/73:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    dicts = [count_words(message) for message in messages]
    #matrix = pd.DataFrame([dicts])
    #matrix = matrix.transpose()
    
    print(len(dicts))
97/74: get_words_in_thread_measure(test_emails_data)
97/75:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    print(matrix)
97/76: get_words_in_thread_measure(test_emails_data)
97/77:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    print(matrix.loc[0])
97/78: get_words_in_thread_measure(test_emails_data)
97/79: test_emails_data.Message.loc[0]
97/80:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    return matrix
97/81: get_words_in_thread_measure(test_emails_data)
97/82: x = get_words_in_thread_measure(test_emails_data)
97/83: x
97/84: x.loc[0]
97/85:
for z in x:
    print(z)
97/86:
for z in x.loc[0:
    print(z)
97/87:
for z in x.loc[0]:
    print(z)
97/88:
for z in x.loc[0].sorted():
    print(z)
97/89:
for z in x.loc[0].sort():
    print(z)
97/90:
for z in x.loc[0].sort_values():
    print(z)
99/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr

from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
99/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
99/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
99/4: headers, messages = get_emails(easyham_path)
99/5: headers_lines = [header.split('\n') for header in headers]
99/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
99/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
99/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
99/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
99/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
99/11: dates_lines = [datestr[6:] for datestr in dates_lines]
99/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
99/13: emails_data = [send_datetime, from_addresses, subjects, messages]
99/14:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
99/15: all_emails_training = all_emails_data.loc[:1250]
99/16: len(all_emails_training)
99/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
99/18:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
99/19:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
99/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
99/21:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
99/22: threads_subjects = get_threads_subjects(all_emails_training)
99/23: all_emails_training['thread_subject'] = threads_subjects
99/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
99/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
99/26:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
99/27:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
99/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
99/29: #emails_threads_senders.tail()
99/30:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
99/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
99/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
99/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
99/34: emails_threads['send_min'] = minimum_send_datetime
99/35: emails_threads['send_max'] = maximum_send_datetime
99/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
99/37: thread_number_of_messages = grouped_by_thread_subject.size()
99/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
99/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
99/40: emails_threads.thread_time.min()
99/41: emails_threads.thread_time.max()
99/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
99/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
99/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
99/45: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12
99/46: stop_words = stopwords.words('english')
99/47:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
99/48:
def make_tdm(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10
    
    return matrix
99/49: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
99/50: thread_tdm.tail()
99/51:
emails_tdm = make_tdm(all_emails_training.Message)
emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2
emails_tdm.tail()
99/52:
#grouped_from.tail(20 -social activity measure
#emails_threads_senders.tail() -thread senders activity measure
#emails_threads.thread_density -thread activity measure
#thread_tdm.weight -words in active threads mesure
#emails_tdm.weight -words in all messages measure
99/53:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print('get_from_measure: \n', result.sort_values())
    return result
99/54:
def get_from_thread_measure(senders):
    x = [emails_threads_senders.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print('get_from_thread_measure: \n', result.sort_values())
    return result
99/55:
def get_thread_activity_measure(emails):
    emails = emails.copy()
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
    emails.thread_weight.fillna(value=1, inplace=True)
    
    print(emails.thread_weight)
    
    return emails.thread_weight
99/56:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    return matrix
99/57: x = get_words_in_thread_measure(test_emails_data)
99/58:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = 1#get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
99/59:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
99/60: x = get_words_in_thread_measure(test_emails_data)
99/61:
for z in x.loc[0].sort_values():
    print(z)
99/62: test_emails_data.Message.loc[0]
99/63: thread_tdm.player
99/64: thread_tdm.linux
99/65: x
99/66: x.loc[0]
99/67: x
99/68:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    matrix.weight = 0
    
    return matrix
99/69: x = get_words_in_thread_measure(test_emails_data)
99/70: x
99/71:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    matrix['weight'] = 0
    
    return matrix
99/72: x = get_words_in_thread_measure(test_emails_data)
99/73: x
99/74: thread_tdm.get('linux')
99/75: thread_tdm.weight.get('lina',9)
99/76: thread_tdm.weight.get('linux',9)
99/77: thread_tdm.weight.get('linux')
99/78: a=thread_tdm.weight.get('linux')
99/79:
a=thread_tdm.weight.get('linux')
a
99/80: a
99/81: thread_tdm.weight
99/82: thread_tdm.get('linux', 'weight')
99/83: thread_tdm.get(['linux', 'weight'])
99/84: thread_tdm.get(['weight', 'linux'])
99/85: thread_tdm.at('weight','linux')
99/86: thread_tdm.loc['weight']
99/87: thread_tdm.loc['weight']['linux']
99/88: thread_tdm.get(['weight']['linux'])
100/1:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr

from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
100/2:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
100/3:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
100/4: headers, messages = get_emails(easyham_path)
100/5: headers_lines = [header.split('\n') for header in headers]
100/6:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
100/7:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
100/8:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
100/9:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
100/10:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
100/11: dates_lines = [datestr[6:] for datestr in dates_lines]
100/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
100/13: emails_data = [send_datetime, from_addresses, subjects, messages]
100/14:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
100/15: all_emails_training = all_emails_data.loc[:1250]
100/16: len(all_emails_training)
100/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
100/18:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
100/19:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
100/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
100/21:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
100/22: threads_subjects = get_threads_subjects(all_emails_training)
100/23: all_emails_training['thread_subject'] = threads_subjects
100/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
100/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
100/26:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
100/27:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
100/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
100/29: #emails_threads_senders.tail()
100/30:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
100/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
100/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
100/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
100/34: emails_threads['send_min'] = minimum_send_datetime
100/35: emails_threads['send_max'] = maximum_send_datetime
100/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
100/37: thread_number_of_messages = grouped_by_thread_subject.size()
100/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
100/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
100/40: emails_threads.thread_time.min()
100/41: emails_threads.thread_time.max()
100/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
100/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
100/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
100/45: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12
100/46: stop_words = stopwords.words('english')
100/47:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
100/48:
def make_tdm(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10
    
    return matrix
100/49: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
100/50: thread_tdm.tail()
100/51: thread_tdm.get(['weight']['linux'])
100/52: #thread_tdm.get(['weight']['linux'])
100/53:
emails_tdm = make_tdm(all_emails_training.Message)
emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2
emails_tdm.tail()
100/54:
#grouped_from.tail(20 -social activity measure
#emails_threads_senders.tail() -thread senders activity measure
#emails_threads.thread_density -thread activity measure
#thread_tdm.weight -words in active threads mesure
#emails_tdm.weight -words in all messages measure
100/55:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print('get_from_measure: \n', result.sort_values())
    return result
100/56:
def get_from_thread_measure(senders):
    x = [emails_threads_senders.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print('get_from_thread_measure: \n', result.sort_values())
    return result
100/57:
def get_thread_activity_measure(emails):
    emails = emails.copy()
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
    emails.thread_weight.fillna(value=1, inplace=True)
    
    print(emails.thread_weight)
    
    return emails.thread_weight
100/58:
def count_words_in_thread_weight():
    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = {word, thread_tdm.get}
    return result
100/59:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    matrix['weight'] = calculate_wor
    
    return matrix
100/60: x = get_words_in_thread_measure(test_emails_data)
100/61:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = 1#get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/62:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/63: x = get_words_in_thread_measure(test_emails_data)
100/64:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    #matrix['weight'] = calculate_wor
    
    return matrix
100/65: x = get_words_in_thread_measure(test_emails_data)
100/66: x
100/67:
def count_words_in_thread_weight():
    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = {word, thread_tdm.get}
    return words
100/68:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    #matrix['weight'] = calculate_wor
    
    return matrix
100/69: x = get_words_in_thread_measure(test_emails_data)
100/70: x
100/71:
def count_words_in_thread_weight():
    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = {word, thread_tdm.get}
    return words
100/72:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame([dicts])
    matrix = matrix.transpose()
    
    #matrix['weight'] = calculate_wor
    
    return matrix
100/73: x = get_words_in_thread_measure(test_emails_data)
100/74: x
100/75:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words(message) for message in messages]
    #matrix = pd.DataFrame([dicts])
   # matrix = matrix.transpose()
    
    #matrix['weight'] = calculate_wor
    
    return messages_words
100/76: x = get_words_in_thread_measure(test_emails_data)
100/77: x
100/78:
def count_words_in_thread_weight():
    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = {word, thread_tdm.get}
    return words
100/79:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    #matrix = pd.DataFrame([dicts])
   # matrix = matrix.transpose()
    
    #matrix['weight'] = calculate_wor
    
    return messages_words
100/80: x = get_words_in_thread_measure(test_emails_data)
100/81:
def count_words_in_thread_weight(message):
    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = {word, thread_tdm.get}
    return words
100/82:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    #matrix = pd.DataFrame([dicts])
   # matrix = matrix.transpose()
    
    #matrix['weight'] = calculate_wor
    
    return messages_words
100/83: x = get_words_in_thread_measure(test_emails_data)
100/84:
def count_words_in_thread_weight(message):
    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    #result = {word, thread_tdm.get}
    return words
100/85:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    #matrix = pd.DataFrame([dicts])
   # matrix = matrix.transpose()
    
    #matrix['weight'] = calculate_wor
    
    return messages_words
100/86: x = get_words_in_thread_measure(test_emails_data)
100/87: x
100/88:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.DataFrame(message_words)
    #matrix = pd.DataFrame([dicts])
   # matrix = matrix.transpose()
    
    #matrix['weight'] = calculate_wor
    
    return z
100/89: x = get_words_in_thread_measure(test_emails_data)
100/90:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.DataFrame(messages_words)
    #matrix = pd.DataFrame([dicts])
   # matrix = matrix.transpose()
    
    #matrix['weight'] = calculate_wor
    
    return z
100/91: x = get_words_in_thread_measure(test_emails_data)
100/92: x
100/93:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    #matrix['weight'] = calculate_wor
    
    return z
100/94: x = get_words_in_thread_measure(test_emails_data)
100/95: x
100/96: emails_tdm.columns
100/97: emails_tdm[emails_tdm.columns in x[0]]
100/98: emails_tdm[emails_tdm.columns == 'linux']
100/99: emails_tdm[emails_tdm.columns == 'aaron']
100/100: emails_tdm[emails_tdm.columns == 'zoo']
100/101: emails_tdm['zoo']
100/102: emails_tdm['mamka']
100/103: emails_tdm[{'aaron'}]
100/104: emails_tdm[list({'aaron'})]
100/105: set(emails_tdm.columns)
100/106: emails_tdm[list(set(emails_tdm.columns).intersection{'aaron'})]
100/107: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron'})]
100/108: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron'}))]
100/109: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron', 'sales'}))]
100/110: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron', 'sales'}))].product()
100/111: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron', 'sales'}))]
100/112: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron', 'sales'}))].product()
100/113: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron', 'sales'}))].product(axis=0)
100/114: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron', 'sales'}))].product(axis=1)
100/115: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron', 'sales'}))].product(axis=1)['weight']
100/116:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame()
    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(z))].product(axis=1)['weight']
    
    return z
100/117:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame()
    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(z))].product(axis=1)['weight']
    
    return x
100/118: x = get_words_in_thread_measure(test_emails_data)
100/119:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame()
    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(z))].product(axis=1)['weight']
    
    return z
100/120: x = get_words_in_thread_measure(test_emails_data)
100/121:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame()
 #   x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(z))].product(axis=1)['weight']
    
    return z
100/122: x = get_words_in_thread_measure(test_emails_data)
100/123: x
100/124: emails_tdm[list(set(emails_tdm.columns).intersection(z))].product(axis=1)['weight']
100/125: emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']
100/126:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
 #   x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(z))].product(axis=1)['weight']
    
    return z
100/127: x = get_words_in_thread_measure(test_emails_data)
100/128: x
100/129:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
 #   x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(z))].product(axis=1)['weight']
    
    return x
100/130: x = get_words_in_thread_measure(test_emails_data)
100/131: x
100/132:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(x[0]))].product(axis=1)['weight']
    
    return x
100/133: x = get_words_in_thread_measure(test_emails_data)
100/134:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = emails_tdm[list(set(emails_tdm.columns)).intersection(x[0])].product(axis=1)['weight']
    
    return x
100/135: x = get_words_in_thread_measure(test_emails_data)
100/136:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(x[0]))].product(axis=1)['weight']
    
    return x
100/137: x = get_words_in_thread_measure(test_emails_data)
100/138:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    set(emails_tdm.columns).intersection(x[0]
    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(x[0]))].product(axis=1)['weight']
    
    return x
100/139:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    set(emails_tdm.columns).intersection(x[0])
    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(x[0]))].product(axis=1)['weight']
    
    return x
100/140: x = get_words_in_thread_measure(test_emails_data)
100/141: x[0]
100/142: emails_tdm[list(set(emails_tdm.columns).intersection(x[0]))].product(axis=1)['weight']
100/143: emails_tdm[list(set(emails_tdm.columns).intersection({'zoo'}))].product(axis=1)['weight']
100/144: emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']
100/145: x
100/146:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    set(emails_tdm.columns).intersection(x[0])
    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']
    
    return x
100/147: x = get_words_in_thread_measure(test_emails_data)
100/148:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    #set(emails_tdm.columns).intersection(x[0])
    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']
    
    return x
100/149: x = get_words_in_thread_measure(test_emails_data)
100/150: x
100/151:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    #set(emails_tdm.columns).intersection(x[0])
    x['weight'] = x[0]*2 #emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']
    
    return x
100/152: x = get_words_in_thread_measure(test_emails_data)
100/153:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    #set(emails_tdm.columns).intersection(x[0])
    x['weight'] = x[0].intersection('now') #emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']
    
    return x
100/154: x = get_words_in_thread_measure(test_emails_data)
100/155: x[0]
100/156:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    #set(emails_tdm.columns).intersection(x[0])
    x['weight'] = min(x[0])
    
    return x
100/157: x = get_words_in_thread_measure(test_emails_data)
100/158:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    #set(emails_tdm.columns).intersection(x[0])
    x['weight'] = min(x)
    
    return x
100/159: x = get_words_in_thread_measure(test_emails_data)
100/160: x[0]
100/161: x
100/162:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    #set(emails_tdm.columns).intersection(x[0])
    x['weight'] = max(x)
    
    return x
100/163:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    #set(emails_tdm.columns).intersection(x[0])
    x['weight'] = calculate_weight(x[0])
    
    return x
100/164: x = get_words_in_thread_measure(test_emails_data)
100/165:
def calculate_weight(words_set):
    result = emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']
    return result
100/166:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    #set(emails_tdm.columns).intersection(x[0])
    x['weight'] = calculate_weight(x[0])
    
    return x
100/167: x = get_words_in_thread_measure(test_emails_data)
100/168: x
100/169:
test_emails_data = all_emails_data[:30]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/170: x = get_words_in_thread_measure(test_emails_data)
100/171: x
100/172:
def calculate_weight(words_set):
    result = emails_tdm[list(set(emails_tdm.columns).intersection(word_set))].product(axis=1)['weight']
    return result
100/173:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    #set(emails_tdm.columns).intersection(x[0])
    x['weight'] = calculate_weight(x[0])
    
    return x
100/174: x = get_words_in_thread_measure(test_emails_data)
100/175:
def calculate_weight(words_set):
    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].product(axis=1)['weight']
    return result
100/176:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    #set(emails_tdm.columns).intersection(x[0])
    x['weight'] = calculate_weight(x[0])
    
    return x
100/177: x = get_words_in_thread_measure(test_emails_data)
100/178:
def calculate_weight(words_set):
    print(words_set)
    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].product(axis=1)['weight']
    return result
100/179:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    #set(emails_tdm.columns).intersection(x[0])
    x['weight'] = calculate_weight(x[0])
    
    return x
100/180: x = get_words_in_thread_measure(test_emails_data)
100/181:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    #set(emails_tdm.columns).intersection(x[0])
    x['weight'] = x[0].apply(calculate_weight)
    
    return x
100/182: x = get_words_in_thread_measure(test_emails_data)
100/183: x
100/184:
test_emails_data = all_emails_data[:2]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/185:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_weight)
    
    return x
100/186: x = get_words_in_thread_measure(test_emails_data)
100/187: x
100/188: emails_tdm['source']
100/189:
def calculate_weight(words_set):
    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].mean(axis=1)['weight']
    return result
100/190:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_weight)
    
    return x
100/191: x = get_words_in_thread_measure(test_emails_data)
100/192: x
100/193:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/194:
test_emails_data = all_emails_data[:2]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/195:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_weight)
    
    return x.weight
100/196: x = get_words_in_thread_measure(test_emails_data)
100/197: x
100/198: emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']
100/199:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/200:
test_emails_data = all_emails_data[:2]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/201:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/202:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)/2
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/203:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/204:
def calculate_weight(words_set, tdm):
    result = tdm[list(set(tdm.columns).intersection(words_set))].mean(axis=1)['weight']
    return result
100/205:
def get_words_in_thread_measure(emails, tdm):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_weight,tdm)
    
    return x.weight
100/206:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails,)/2
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/207:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/208:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails, emails_tdm)/2
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/209:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/210:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails, thread_tdm)/2
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/211:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/212:
def get_words_in_thread_measure(emails, tdm):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_weight(tdm))
    
    return x.weight
100/213: x = get_words_in_thread_measure(test_emails_data)
100/214:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/215: #x = get_words_in_thread_measure(test_emails_data)
100/216:
def get_words_in_thread_measure(emails, tdm):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_weight(tdm))
    
    return x.weight
100/217: #x = get_words_in_thread_measure(test_emails_data)
100/218: #x
100/219:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails, thread_tdm)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/220:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/221:
def calculate_weight(words_set):
    result = thread_tdm[list(set(thread_tdm.columns).intersection(words_set))].mean(axis=1)['weight']
    return result
100/222:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_weight)
    
    return x.weight
100/223: #x = get_words_in_thread_measure(test_emails_data)
100/224: #x
100/225:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/226:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/227:
def calculate_thread_weight(words_set):
    result = thread_tdm[list(set(thread_tdm.columns).intersection(words_set))].mean(axis=1)['weight']
    return result
100/228:
def calculate_thread_weight(words_set):
    result = thread_tdm[list(set(thread_tdm.columns).intersection(words_set))].mean(axis=1)['weight']
    return result
100/229:
def calculate_all_messages_weight(words_set):
    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].mean(axis=1)['weight']
    return result
100/230:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_weight)
    
    return x.weight
100/231:
def get_words_in_all_messages_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_all_messages_weight)
    
    return x.weight
100/232: #x = get_words_in_thread_measure(test_emails_data)
100/233: #x
100/234:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/235:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/236:
def get_thread_activity_measure(emails):
    emails = emails.copy()
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+12
    emails.thread_weight.fillna(value=1, inplace=True)
    
    #print(emails.thread_weight)
    
    return emails.thread_weight
100/237:
def get_from_thread_measure(senders):
    x = [emails_threads_senders.get(sender,1) for sender in senders]
    result = pd.Series(x)
    #print('get_from_thread_measure: \n', result.sort_values())
    return result
100/238:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    3print('get_from_measure: \n', result.sort_values())
    return result
100/239:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    #print('get_from_measure: \n', result.sort_values())
    return result
100/240:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/241:
test_emails_data = all_emails_data[:100]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/242:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure, from_thread_measure])
    print('Test rank: ', test_rank)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/243:
test_emails_data = all_emails_data[:100]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/244:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure, from_thread_measure]).T
    print('Test rank: ', test_rank)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/245:
test_emails_data = all_emails_data[:100]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/246:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure, from_thread_measure]).T
    test_rank.columns = ['from_measure', 'from_thread_measure']
    print('Test rank: ', test_rank)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/247:
test_emails_data = all_emails_data[:100]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/248:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure, from_thread_measure]).T
    test_rank.columns = ['from_measure', 'from_thread_measure']
    print('Test rank: \n', test_rank)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/249:
test_emails_data = all_emails_data[:100]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/250:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure']
    print('Test rank: \n', test_rank)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/251:
test_emails_data = all_emails_data[:100]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/252:
def get_thread_activity_measure(emails):
    emails = emails.copy()
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+15
    emails.thread_weight.fillna(value=1, inplace=True)
    
    #print(emails.thread_weight)
    
    return emails.thread_weight
100/253:
def count_words_in_thread_weight(message):
    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    return words
100/254:
def calculate_thread_weight(words_set):
    result = thread_tdm[list(set(thread_tdm.columns).intersection(words_set))].mean(axis=1)['weight']
    return result
100/255:
def calculate_all_messages_weight(words_set):
    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].mean(axis=1)['weight']
    return result
100/256:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_weight)
    
    return x.weight
100/257:
def get_words_in_all_messages_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_all_messages_weight)
    
    return x.weight
100/258:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure']
    print('Test rank: \n', test_rank)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/259:
test_emails_data = all_emails_data[:100]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/260:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    test_rank['final_rank']=test_rank.product()
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']
    
    print('Test rank: \n', test_rank)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/261:
test_emails_data = all_emails_data[:100]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/262:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    test_rank['final_rank']=test_rank.product(axis=0)
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']
    
    print('Test rank: \n', test_rank)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/263:
test_emails_data = all_emails_data[:100]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/264:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    #test_rank['final_rank']=test_rank.product(axis=0)
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure',]# 'final_rank']
    
    print('Test rank: \n', test_rank)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/265:
test_emails_data = all_emails_data[:100]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/266: a = DataFrame([1,2,3])
100/267: a = pd.DataFrame([1,2,3])
100/268: a
100/269: a = pd.DataFrame([1,2,3], [4,5,6])
100/270: a
100/271: a = pd.DataFrame([[1,2,3],[4,5,6]])
100/272: a
100/273: a['suma'] = a.sum()
100/274: a
100/275:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    test_rank['final_rank']=test_rank.product(axis=0)
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure',]# 'final_rank']
    
    print('Test rank: \n', test_rank)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/276:
test_emails_data = all_emails_data[:100]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/277:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    test_rank['final_rank']=test_rank.product(axis=1)
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure',]# 'final_rank']
    
    print('Test rank: \n', test_rank)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/278:
test_emails_data = all_emails_data[:100]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/279: a['suma'] = a.sum(axis=0)
100/280: a = pd.DataFrame([[1,2,3],[4,5,6]])
100/281: a
100/282: a['suma'] = a.sum(axis=0)
100/283: a
100/284: a = pd.DataFrame([[1,2,3],[4,5,6]])
100/285: a
100/286: a['suma'] = a.sum(axis=1)
100/287: a
100/288: a.loc['suma'] = a.sum(axis=1)
100/289: a
100/290: a = pd.DataFrame([[1,2,3],[4,5,6]])
100/291: a
100/292: a.loc['suma'] = a.sum()
100/293: a
100/294: a = pd.DataFrame([[1,2,3],[4,5,6]])
100/295: a
100/296: a['prod'] = a.product()
100/297: a
100/298: a = pd.DataFrame([[1,2,3],[4,5,6]])
100/299: a
100/300: a['prod'] = a.product(axis=0)
100/301: a
100/302: a = pd.DataFrame([[1,2,3],[4,5,6]])
100/303: a
100/304: a['prod'] = a.product(axis=1)
100/305: a
100/306:
test_emails_data = all_emails_data[:100]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/307:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    test_rank['final_rank']=test_rank.product(axis=1)
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']
    
    print('Test rank: \n', test_rank)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/308:
test_emails_data = all_emails_data[:100]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/309: a = pd.DataFrame([[1,2,3],[4,5,6]])
100/310: a
100/311: a['prod'] = a.product()
100/312: a
100/313:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    test_rank['final_rank']=test_rank.product(axis=1)
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']
    
    #print('Test rank: \n', test_rank)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/314:
test_emails_data = all_emails_data[:100]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/315: all_emails_data.describe
100/316:
test_emails_data = all_emails_data[1250:]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/317:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/318:
test_emails_data = all_emails_data[1240:1260]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/319:
test_emails_data = all_emails_data[1000:1260]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/320:
test_emails_data = all_emails_data[:10]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/321:
test_emails_data = all_emails_data[:1000]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/322:
test_emails_data = all_emails_data[800:1500]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/323:
test_emails_data = all_emails_data[800:810]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/324:
test_emails_data = all_emails_data[700:800]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/325:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    test_rank['final_rank']=test_rank.product(axis=1)
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']
    
    print('Test rank: \n', test_rank)
    
    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return rank
100/326:
test_emails_data = all_emails_data[700:800]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/327:
test_emails_data = all_emails_data[0:800]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/328: test_emails_data.loc[790]
100/329:
test_emails_data = all_emails_data[790:791]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/330:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    test_rank['final_rank']=test_rank.product(axis=1)
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']
    
    print('Test rank: \n', test_rank)
    
    #rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return test_rank['final_rank']
100/331:
test_emails_data = all_emails_data[790:791]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/332:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    test_rank['final_rank']=test_rank.product(axis=1)
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']
    
    print('Test rank: \n', test_rank)
    
    #rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return test_rank['final_rank']
100/333:
test_emails_data = all_emails_data[790:791]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/334:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    test_rank['final_rank']=test_rank.product(axis=1)
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']
    
    print('Test rank: \n', test_rank['final_rank'])
    
    #rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return test_rank['final_rank']
100/335:
test_emails_data = all_emails_data[790:791]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/336:
test_emails_data = all_emails_data[0:1]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/337:
test_emails_data = all_emails_data[100:101]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/338:
test_emails_data = all_emails_data[90:101]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/339:
test_emails_data = all_emails_data[0:101]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/340:
test_emails_data = all_emails_data[675:676]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/341: test_emails_data.loc[675]
100/342: test_emails_data.loc['562']
100/343: test_emails_data.index
100/344: test_emails_data.head()
100/345: all_emails_data.head()
100/346:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
#all_emails_data.reset_index(inplace=True)
100/347: all_emails_training = all_emails_data.loc[:1250]
100/348: len(all_emails_training)
100/349: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
100/350:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
100/351:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
100/352: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
100/353:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
100/354: threads_subjects = get_threads_subjects(all_emails_training)
100/355: all_emails_training['thread_subject'] = threads_subjects
100/356: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
100/357: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
100/358:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
100/359:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
100/360: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
100/361: #emails_threads_senders.tail()
100/362:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
100/363: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
100/364: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
100/365: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
100/366: emails_threads['send_min'] = minimum_send_datetime
100/367: emails_threads['send_max'] = maximum_send_datetime
100/368: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
100/369: thread_number_of_messages = grouped_by_thread_subject.size()
100/370: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
100/371: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
100/372: emails_threads.thread_time.min()
100/373: emails_threads.thread_time.max()
100/374: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
100/375: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
100/376: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
100/377: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12
100/378: stop_words = stopwords.words('english')
100/379:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
100/380:
def make_tdm(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10
    
    return matrix
100/381: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
100/382: thread_tdm.tail()
100/383:
emails_tdm = make_tdm(all_emails_training.Message)
emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2
emails_tdm.tail()
100/384:
#grouped_from.tail(20 -social activity measure
#emails_threads_senders.tail() -thread senders activity measure
#emails_threads.thread_density -thread activity measure
#thread_tdm.weight -words in active threads mesure
#emails_tdm.weight -words in all messages measure
100/385:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    #print('get_from_measure: \n', result.sort_values())
    return result
100/386:
def get_from_thread_measure(senders):
    x = [emails_threads_senders.get(sender,1) for sender in senders]
    result = pd.Series(x)
    #print('get_from_thread_measure: \n', result.sort_values())
    return result
100/387:
def get_thread_activity_measure(emails):
    emails = emails.copy()
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+15
    emails.thread_weight.fillna(value=1, inplace=True)
    
    #print(emails.thread_weight)
    
    return emails.thread_weight
100/388:
def count_words_in_thread_weight(message):
    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    return words
100/389:
def calculate_thread_weight(words_set):
    result = thread_tdm[list(set(thread_tdm.columns).intersection(words_set))].mean(axis=1)['weight']
    return result
100/390:
def calculate_all_messages_weight(words_set):
    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].mean(axis=1)['weight']
    return result
100/391:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_weight)
    
    return x.weight
100/392:
def get_words_in_all_messages_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_all_messages_weight)
    
    return x.weight
100/393:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    test_rank['final_rank']=test_rank.product(axis=1)
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']
    
    print('Test rank: \n', test_rank['final_rank'])
    
    #rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return test_rank['final_rank']
100/394:
test_emails_data = all_emails_data[675:676]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/395: test_emails_data.head()
100/396: all_emails_data.head()
100/397:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr

from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer
100/398:
easyham_path = 'Spam/data/easy_ham/'
easyham2_path = 'Spam/data/easy_ham2/'
100/399:
def get_emails(path_to_directory):
    file_names = os.listdir(path_to_directory)
    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']
    all_email_messages = []
    all_email_headers = []
    
    for file in path_to_all_files:
        email = open(file, encoding='latin1').read()
        header_end_index = email.find('\n\n')
        
        email_header = email[:header_end_index]
        all_email_headers.append(email_header)
        
        email_message = email[header_end_index+2:].lower()
        all_email_messages.append(email_message)
        
    return all_email_headers,all_email_messages
100/400: headers, messages = get_emails(easyham_path)
100/401: headers_lines = [header.split('\n') for header in headers]
100/402:
from_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('From:'):
            from_lines.append(line)
100/403:
from_addresses = []
for email in from_lines:
    parser_results = parseaddr(email)
    from_addresses.append(parser_results[1])
100/404:
subject_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Subject:'):
            subject_lines.append(line)
100/405:
subjects=[]
for subject_line in subject_lines:
    subject = re.findall(r'Subject: (.+)', subject_line)
    subjects.append(subject[0].lower())
100/406:
dates_lines = []
for header_lines in headers_lines:
    for line in header_lines:
        if line.startswith('Date:'):
            dates_lines.append(line)
100/407: dates_lines = [datestr[6:] for datestr in dates_lines]
100/408: send_datetime = pd.to_datetime(dates_lines, errors='coerce')
100/409: emails_data = [send_datetime, from_addresses, subjects, messages]
100/410:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
#all_emails_data.reset_index(inplace=True)
100/411: all_emails_training = all_emails_data.loc[:1250]
100/412: len(all_emails_training)
100/413: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
100/414:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
100/415:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
100/416: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
100/417:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
100/418: threads_subjects = get_threads_subjects(all_emails_training)
100/419: all_emails_training['thread_subject'] = threads_subjects
100/420: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
100/421: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
100/422:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
100/423:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
100/424: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
100/425: #emails_threads_senders.tail()
100/426:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
100/427: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
100/428: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
100/429: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
100/430: emails_threads['send_min'] = minimum_send_datetime
100/431: emails_threads['send_max'] = maximum_send_datetime
100/432: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
100/433: thread_number_of_messages = grouped_by_thread_subject.size()
100/434: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
100/435: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
100/436: emails_threads.thread_time.min()
100/437: emails_threads.thread_time.max()
100/438: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
100/439: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
100/440: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
100/441: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12
100/442: stop_words = stopwords.words('english')
100/443:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
100/444:
def make_tdm(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10
    
    return matrix
100/445: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
100/446: thread_tdm.tail()
100/447:
emails_tdm = make_tdm(all_emails_training.Message)
emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2
emails_tdm.tail()
100/448:
#grouped_from.tail(20 -social activity measure
#emails_threads_senders.tail() -thread senders activity measure
#emails_threads.thread_density -thread activity measure
#thread_tdm.weight -words in active threads mesure
#emails_tdm.weight -words in all messages measure
100/449:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    #print('get_from_measure: \n', result.sort_values())
    return result
100/450:
def get_from_thread_measure(senders):
    x = [emails_threads_senders.get(sender,1) for sender in senders]
    result = pd.Series(x)
    #print('get_from_thread_measure: \n', result.sort_values())
    return result
100/451:
def get_thread_activity_measure(emails):
    emails = emails.copy()
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+15
    emails.thread_weight.fillna(value=1, inplace=True)
    
    #print(emails.thread_weight)
    
    return emails.thread_weight
100/452:
def count_words_in_thread_weight(message):
    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    return words
100/453:
def calculate_thread_weight(words_set):
    result = thread_tdm[list(set(thread_tdm.columns).intersection(words_set))].mean(axis=1)['weight']
    return result
100/454:
def calculate_all_messages_weight(words_set):
    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].mean(axis=1)['weight']
    return result
100/455:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_weight)
    
    return x.weight
100/456:
def get_words_in_all_messages_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_all_messages_weight)
    
    return x.weight
100/457:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    test_rank['final_rank']=test_rank.product(axis=1)
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']
    
    print('Test rank: \n', test_rank['final_rank'])
    
    #rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure
    
    return test_rank['final_rank']
100/458:
test_emails_data = all_emails_data[675:676]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/459: test_emails_data.head()
100/460: all_emails_data.head()
100/461:
test_emails_data = all_emails_data[675:676]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/462:
test_emails_data = all_emails_data[675:676]
test_emails_data['rank']= 69 # rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/463:
test_emails_data = all_emails_data[675:676]
print('returned from function: ', rank_emails(test_emails_data))
test_emails_data['rank']= 69 # rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/464:
test_emails_data = all_emails_data[675:676]
print('returned from function: \n', rank_emails(test_emails_data))
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/465:
test_emails_data = all_emails_data[675:676]
print('returned from function: \n', rank_emails(test_emails_data))
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/466: test_emails_data
100/467:
test_emails_data = all_emails_data[0:6]
print('returned from function: \n', rank_emails(test_emails_data))
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/468:
test_emails_data = all_emails_data[:6]
print('returned from function: \n', rank_emails(test_emails_data))
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/469:
test_emails_data = all_emails_data[:10]
print('returned from function: \n', rank_emails(test_emails_data))
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/470:
test_emails_data = all_emails_data[:10]
#print('returned from function: \n', rank_emails(test_emails_data))
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/471:
test_emails_data = all_emails_data[:1]
#print('returned from function: \n', rank_emails(test_emails_data))
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/472:
test_emails_data = all_emails_data[:100]
#print('returned from function: \n', rank_emails(test_emails_data))
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/473:
all_emails_data = pd.DataFrame(emails_data)
all_emails_data = all_emails_data.transpose()
all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']
all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])
all_emails_data.reset_index(inplace=True)
100/474: all_emails_training = all_emails_data.loc[:1250]
100/475: len(all_emails_training)
100/476: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])
100/477:
grouped_from['Weight'] = np.log(grouped_from['Message']+1)
grouped_from = grouped_from.Weight
100/478:
#plt.plot(grouped_from.index, grouped_from.Weight, 'or')
#plt.plot(grouped_from.index, grouped_from.Message, 'ob')
#plt.rcParams['figure.figsize']=(24,16)
100/479: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
100/480:
def get_threads_subjects(emails_df):
    threads_subjects=[]
    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
    
    return threads_subjects
100/481: threads_subjects = get_threads_subjects(all_emails_training)
100/482: all_emails_training['thread_subject'] = threads_subjects
100/483: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]
100/484: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)
100/485:
is_thread = all_emails_training.is_thread==True
thread_subject_x = all_emails_training.thread_subject=='x'
all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)
100/486:
emails_threads = all_emails_training[all_emails_training.is_thread==True]
emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)
100/487: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])
100/488: #emails_threads_senders.tail()
100/489:
emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)
emails_threads_senders = emails_threads_senders.Weight
100/490: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])
100/491: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
100/492: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
100/493: emails_threads['send_min'] = minimum_send_datetime
100/494: emails_threads['send_max'] = maximum_send_datetime
100/495: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()
100/496: thread_number_of_messages = grouped_by_thread_subject.size()
100/497: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
100/498: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')
100/499: emails_threads.thread_time.min()
100/500: emails_threads.thread_time.max()
100/501: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
100/502: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()
100/503: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)
100/504: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12
100/505: stop_words = stopwords.words('english')
100/506:
def count_words(message):
    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    result = dict.fromkeys(occurances, 1)
    return result
100/507:
def make_tdm(messages):
    dicts = [count_words(message) for message in messages]
    matrix = pd.DataFrame(dicts)
    
    matrix.loc['summa']=matrix.sum()
    
    matrix.loc['number_of_documents']=(matrix.count()-1)
    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)
    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10
    
    return matrix
100/508: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)
100/509: thread_tdm.tail()
100/510:
emails_tdm = make_tdm(all_emails_training.Message)
emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2
emails_tdm.tail()
100/511:
#grouped_from.tail(20 -social activity measure
#emails_threads_senders.tail() -thread senders activity measure
#emails_threads.thread_density -thread activity measure
#thread_tdm.weight -words in active threads mesure
#emails_tdm.weight -words in all messages measure
100/512:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    #print('get_from_measure: \n', result.sort_values())
    return result
100/513:
def get_from_thread_measure(senders):
    x = [emails_threads_senders.get(sender,1) for sender in senders]
    result = pd.Series(x)
    #print('get_from_thread_measure: \n', result.sort_values())
    return result
100/514:
def get_thread_activity_measure(emails):
    emails = emails.copy()
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+15
    emails.thread_weight.fillna(value=1, inplace=True)
    
    #print(emails.thread_weight)
    
    return emails.thread_weight
100/515:
def count_words_in_thread_weight(message):
    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])
    return words
100/516:
def calculate_thread_weight(words_set):
    result = thread_tdm[list(set(thread_tdm.columns).intersection(words_set))].mean(axis=1)['weight']
    return result
100/517:
def calculate_all_messages_weight(words_set):
    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].mean(axis=1)['weight']
    return result
100/518:
def get_words_in_thread_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_weight)
    
    return x.weight
100/519:
def get_words_in_all_messages_measure(emails):
    messages = emails.Message.copy()
    messages_words = [count_words_in_thread_weight(message) for message in messages]
    z=pd.Series(messages_words)
    
    x = pd.DataFrame(z)
    x['weight'] = x[0].apply(calculate_all_messages_weight)
    
    return x.weight
100/520:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    test_rank['final_rank']=test_rank.product(axis=1)
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']
    
    print('Test rank: \n', test_rank['final_rank'])
    
    return test_rank['final_rank']
100/521:
test_emails_data = all_emails_data[:100]
#print('returned from function: \n', rank_emails(test_emails_data))
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/522: test_emails_data
100/523:
test_emails_data = all_emails_data[:100]
test_emails_data.head()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/524:
test_emails_data = all_emails_data[:100]
test_emails_data.reset_index()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/525:
test_emails_data = all_emails_data[:1]
test_emails_data.reset_index()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/526:
test_emails_data = all_emails_data[670:671]
test_emails_data.reset_index()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/527:
test_emails_data = all_emails_data[0:1]
test_emails_data.reset_index()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/528:
test_emails_data = all_emails_data[0:2]
test_emails_data.reset_index()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/529:
test_emails_data = all_emails_data[0:20]
test_emails_data.reset_index()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/530:
test_emails_data = all_emails_data[10:20]
test_emails_data.reset_index()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/531:
test_emails_data = all_emails_data[0:20]
test_emails_data.reset_index()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/532:
test_emails_data = all_emails_data[:20]
test_emails_data.reset_index()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/533:
test_emails_data = all_emails_data[1:20]
test_emails_data.reset_index()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/534:
test_emails_data = all_emails_data[:20]
test_emails_data.reset_index()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/535:
test_emails_data = all_emails_data[1:20]
test_emails_data.reset_index()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/536:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    test_rank['final_rank']=test_rank.product(axis=1)
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']
    
    print('Test rank: \n', test_rank)
    
    return test_rank['final_rank']
100/537:
test_emails_data = all_emails_data[1:20]
test_emails_data.reset_index()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/538:
test_emails_data = all_emails_data[1:20]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/539:
test_emails_data = all_emails_data[10:20]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/540:
test_emails_data = all_emails_data[5:20]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/541:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    print('get_from_measure: \n', result.sort_values())
    return result
100/542:
test_emails_data = all_emails_data[5:20]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/543:
def get_thread_activity_measure(emails):
    emails = emails.copy()
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+15
    emails.thread_weight.fillna(value=1, inplace=True)
    
    print(emails.thread_weight)
    
    return emails.thread_weight
100/544:
test_emails_data = all_emails_data[5:20]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/545:
def get_thread_activity_measure(emails):
    #emails = emails.copy()
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+15
    emails.thread_weight.fillna(value=1, inplace=True)
    
    print(emails.thread_weight)
    
    return emails.thread_weight
100/546:
test_emails_data = all_emails_data[5:20]
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/547:
test_emails_data = all_emails_data[5:20].copy()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/548:
test_emails_data = all_emails_data[5:20].copy()
test_emails_data.reset_index()
test_emails_data['rank']= rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/549:
test_emails_data = all_emails_data[5:20]
rank_df= rank_emails(test_emails_data)
result = concat([test_emails_data, rank_df],)
result.sort_values(by=['rank'], ascending=False)
100/550:
test_emails_data = all_emails_data[5:20]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],)
result.sort_values(by=['rank'], ascending=False)
100/551:
test_emails_data = all_emails_data[5:20]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],)
#result.sort_values(by=['rank'], ascending=False)
100/552: result
100/553:
test_emails_data = all_emails_data[5:20]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df], join='inner')
#result.sort_values(by=['rank'], ascending=False)
100/554:
test_emails_data = all_emails_data[5:20]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1, join='inner')
#result.sort_values(by=['rank'], ascending=False)
100/555:
test_emails_data = all_emails_data[5:20]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_axis=True)
#result.sort_values(by=['rank'], ascending=False)
100/556:
test_emails_data = all_emails_data[5:20]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=0, join='inner', ignore_axis=True)
#result.sort_values(by=['rank'], ascending=False)
100/557:
test_emails_data = all_emails_data[5:20]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=0, join='inner', ignore_index=True)
#result.sort_values(by=['rank'], ascending=False)
100/558:
test_emails_data = all_emails_data[5:20]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)
#result.sort_values(by=['rank'], ascending=False)
100/559: result
100/560:
test_emails_data = all_emails_data[5:20]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)
result
#result.sort_values(by=['rank'], ascending=False)
100/561:
test_emails_data = all_emails_data[5:20]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)
result
result.sort_values(by=[10], ascending=False)
100/562:
test_emails_data = all_emails_data[5:20]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)
result.sort_values(by=[10], ascending=False)
100/563:
def rank_emails(emails):
    from_measure = get_from_measure(emails.From)
    from_thread_measure = get_from_thread_measure(emails.From)
    thread_activity_measure = get_thread_activity_measure(emails)
    words_in_thread_measure = get_words_in_thread_measure(emails)
    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)
    
    test_rank = pd.DataFrame([from_measure,
                              from_thread_measure,
                              thread_activity_measure,
                              words_in_thread_measure,
                              words_in_all_messages_measure]).T
    
    test_rank['final_rank']=test_rank.product(axis=1)
    
    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',
                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']
    
    #print('Test rank: \n', test_rank)
    
    return test_rank['final_rank']
100/564:
def get_thread_activity_measure(emails):
    #emails = emails.copy()
    emails['is_thread'] = emails.Subject.str.contains('re\[{0,1}[1-9]{0,1}[1-9]{0,1}\]{0,1}:')
    
    threads_subjects=[]
    for subject,isthread in zip(emails.Subject, emails.is_thread):
        if isthread:
            thread_subject = subject.split(':',1)[1]
            thread_subject = thread_subject.lstrip()
            threads_subjects.append(thread_subject)
        else:
            threads_subjects.append('x')
            
    emails['thread_subject'] = threads_subjects
    threads_subjects = emails.thread_subject.loc[emails.is_thread]
    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)
    
    is_thread = emails.is_thread==True
    thread_subject_x = emails.thread_subject=='x'
    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)
    
    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])
    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)
    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)
    emails['send_min'] = minimum_send_datetime
    emails['send_max'] = maximum_send_datetime
    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()
    
    thread_number_of_messages = grouped_by_thread_subject.size()
    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})
    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)
    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)
    emails['thread_weight'] = np.log(emails.thread_density)+15
    emails.thread_weight.fillna(value=1, inplace=True)
    
    #print(emails.thread_weight)
    
    return emails.thread_weight
100/565:
def get_from_measure(senders):
    x = [grouped_from.get(sender,1) for sender in senders]
    result = pd.Series(x)
    #print('get_from_measure: \n', result.sort_values())
    return result
100/566:
test_emails_data = all_emails_data[5:20]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)
result.sort_values(by=[10], ascending=False)
100/567:
test_emails_data = all_emails_data[1250:]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)
result.sort_values(by=[10], ascending=False)
100/568:
test_emails_data = all_emails_data[1250:]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)
result
#result.sort_values(by=[10], ascending=False)
100/569:
test_emails_data = all_emails_data[:10]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)
result
#result.sort_values(by=[10], ascending=False)
100/570:
test_emails_data = all_emails_data[5:10]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)
result
#result.sort_values(by=[10], ascending=False)
100/571: result
100/572:
test_emails_data = all_emails_data[5:10]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df], ignore_index=True)
#result.sort_values(by=[10], ascending=False)
100/573: result
100/574:
test_emails_data = all_emails_data[5:10]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1, ignore_index=True)
#result.sort_values(by=[10], ascending=False)
100/575: result
100/576:
test_emails_data = all_emails_data[5:10]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1,join=inner ignore_index=True)
#result.sort_values(by=[10], ascending=False)
100/577:
test_emails_data = all_emails_data[5:10]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1,join=inner, ignore_index=True)
#result.sort_values(by=[10], ascending=False)
100/578: result
100/579:
test_emails_data = all_emails_data[5:10]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1,join='inner', ignore_index=True)
#result.sort_values(by=[10], ascending=False)
100/580: result
100/581:
test_emails_data = all_emails_data[5:10]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1,join='inner', ignore_index=True)
#result.sort_values(by=[10], ascending=False)
100/582: result
100/583:
test_emails_data = all_emails_data[0:10]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1,join='inner', ignore_index=True)
#result.sort_values(by=[10], ascending=False)
100/584: result
100/585:
test_emails_data = all_emails_data[3:10]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1,join='inner', ignore_index=True)
#result.sort_values(by=[10], ascending=False)
100/586: result
100/587:
test_emails_data = all_emails_data[3:10]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1, ignore_index=True)
#result.sort_values(by=[10], ascending=False)
100/588: result
100/589:
test_emails_data = all_emails_data[3:10]
rank_df= rank_emails(test_emails_data)
result = pd.concat([test_emails_data, rank_df],axis=1)
#result.sort_values(by=[10], ascending=False)
100/590: result
100/591:
test_emails_data = all_emails_data[3:10]
rank_df= rank_emails(test_emails_data)
result = test_emails_data.join(rank_df)
#result.sort_values(by=[10], ascending=False)
100/592: result
100/593:
test_emails_data = all_emails_data[3:10]
test_emails_data.reset_index()
rank_df= rank_emails(test_emails_data)
result = test_emails_data.join(rank_df)
#result.sort_values(by=[10], ascending=False)
100/594: result
100/595:
test_emails_data = all_emails_data[3:10].copy()
test_emails_data.reset_index()
rank_df= rank_emails(test_emails_data)
result = test_emails_data.join(rank_df)
#result.sort_values(by=[10], ascending=False)
100/596: result
100/597: test_emails_data.head()
100/598:
test_emails_data = all_emails_data[3:10].copy()
test_emails_data.reset_index(inplace=True)
rank_df= rank_emails(test_emails_data)
result = test_emails_data.join(rank_df)
#result.sort_values(by=[10], ascending=False)
100/599: test_emails_data.head()
100/600:
test_emails_data = all_emails_data[3:10].reset_index(inplace=True)
test_emails_data['rank']=rank_emails(test_emails_data)
test_emails_data.sort_values(by=[10], ascending=False)
100/601:
test_emails_data = all_emails_data[3:10].reset_index(inplace=True)
test_emails_data['rank']=rank_emails(test_emails_data)
test_emails_data.sort_values(by=[10], ascending=False)
100/602: test_emails_data.head()
100/603:
test_emails_data = all_emails_data[3:10].copy()
test_emails_data.reset_index(inplace=True)
test_emails_data['rank']=rank_emails(test_emails_data)
test_emails_data.sort_values(by=[10], ascending=False)
100/604:
test_emails_data = all_emails_data[3:10].copy()
test_emails_data.reset_index(inplace=True)
test_emails_data['rank']=rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/605:
test_emails_data = all_emails_data[3:10].copy()
test_emails_data.reset_index(inplace=True)
test_emails_data['rank']=rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/606:
test_emails_data = all_emails_data[:1250].copy()
test_emails_data.reset_index(inplace=True)
test_emails_data['rank']=rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False)
100/607: plt.plot(test_emails_data.rank)
100/608: plt.plot(test_emails_data.index, test_emails_data.rank)
100/609: plt.plot(test_emails_data.index, test_emails_data['rank'])
100/610:
test_emails_data = all_emails_data[:1250].copy()
test_emails_data.reset_index(inplace=True)
test_emails_data['rank']=rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False, inplace=True)
100/611: plt.plot(test_emails_data.index, test_emails_data['rank'])
100/612: test_emails_data.head()
100/613:
test_emails_data = all_emails_data[:125].copy()
test_emails_data.reset_index(inplace=True)
test_emails_data['rank']=rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False, inplace=True)
test_emails_data.reset_index()
100/614:
test_emails_data = all_emails_data[:125].copy()
test_emails_data.reset_index(inplace=True)
test_emails_data['rank']=rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False, inplace=True)
#test_emails_data.reset_index()
100/615: plt.plot(test_emails_data.index, test_emails_data['rank'])
100/616: plt.plot([test_emails_data.index, test_emails_data['rank']])
100/617: plt.plot(test_emails_data['rank'], test_emails_data.index)
100/618: plt.plot( test_emails_data.index, test_emails_data['rank'])
100/619: test_emails_data.head()
100/620:
test_emails_data = all_emails_data[:125].copy()
test_emails_data.reset_index(inplace=True)
test_emails_data['rank']=rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False, inplace=True)
test_emails_data.reset_index(inplace=True)
100/621: plt.plot(test_emails_data['rank'])
100/622:
test_emails_data = all_emails_data[:125].copy()
test_emails_data.reset_index(inplace=True)
test_emails_data['rank']=rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False, inplace=True)
#test_emails_data.reset_index(inplace=True)
100/623: plt.plot(test_emails_data['rank'])
100/624: plt.plot(range(0,125),test_emails_data['rank'])
100/625:
histogram = test_emails_data['rank'].hist(bins=20, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
100/626:
histogram = test_emails_data['rank'].hist(bins=200, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
100/627:
histogram = test_emails_data['rank'].hist(bins=10, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
100/628:
histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
100/629:
histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
plt.rc('figure', figsize=(10,10))
100/630:
histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
plt.rc('figure', figsize=(20,20))
100/631:
histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
plt.rc('figure', figsize=(20,40))
100/632:
histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
plt.rc('figure', figsize=(15,30))
100/633:
histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
plt.rc('figure', figsize=(10,10))
100/634:
histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
plt.rc('figure', figsize=(10,5))
100/635:
histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
plt.rc('figure', figsize=(15,5))
100/636:
histogram = test_emails_data['rank'].hist(bins=30, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
plt.rc('figure', figsize=(15,5))
100/637: test_emails_data['rank'].plot.density()
100/638: test_emails_data['rank'].plot.density(grid=True)
100/639: test_emails_data.tail()
100/640:
histogram = test_emails_data['rank'].hist(bins=20, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
plt.rc('figure', figsize=(15,5))
100/641: test_emails_data['rank'].describe()
100/642:
histogram = test_emails_data['rank'].hist(bins=20, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
test_emails_data['rank'].plot.density(grid=True)
plt.rc('figure', figsize=(15,5))
100/643:
histogram = test_emails_data['rank'].hist(bins=20, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
plt.rc('figure', figsize=(15,5))
100/644:
test_emails_data = all_emails_data[:1250].copy()
test_emails_data.reset_index(inplace=True)
test_emails_data['rank']=rank_emails(test_emails_data)
test_emails_data.sort_values(by=['rank'], ascending=False, inplace=True)
#test_emails_data.reset_index(inplace=True)
100/645:
histogram = test_emails_data['rank'].hist(bins=20, label=['rank', 'number'])
plt.title('Emails rank')
plt.xlabel('rank')
plt.ylabel('Observations')
plt.rc('figure', figsize=(15,5))
100/646: test_emails_data['rank'].plot.density(grid=True)
100/647:
test_emails_data['rank'].plot.density(grid=True)
plt.axvline(x=750)
100/648:
test_emails_data['rank'].plot.density(grid=True)
plt.axvline(x=750, 'r')
100/649:
test_emails_data['rank'].plot.density(grid=True)
plt.axvline(x=750, color='r')
100/650: test_emails_data['rank'].mean()
100/651:
test_emails_data['rank'].plot.density(grid=True)
rank_mean = test_email_data[rank].mean()
plt.axvline(x=rank_mean, color='r')
100/652:
test_emails_data['rank'].plot.density(grid=True)
rank_mean = test_email_data['rank'].mean()
plt.axvline(x=rank_mean, color='r')
100/653:
test_emails_data['rank'].plot.density(grid=True)
rank_mean = test_emails_data['rank'].mean()
plt.axvline(x=rank_mean, color='r')
100/654:
emails_data = all_emails_data[1250:].copy()
emails_data.reset_index(inplace=True)
emails_data['rank']=rank_emails(test_emails_data)
emails_data.sort_values(by=['rank'], ascending=False, inplace=True)
100/655:
emails_data['rank'].plot.density(grid=True)
rank_mean = test_emails_data['rank'].mean()
plt.axvline(x=rank_mean, color='r')
100/656:
emails_data = all_emails_data[1250:].copy()
emails_data.reset_index(inplace=True)
emails_data['rank']=rank_emails(emails_data)
emails_data.sort_values(by=['rank'], ascending=False, inplace=True)
100/657:
emails_data['rank'].plot.density(grid=True)
rank_mean = test_emails_data['rank'].mean()
plt.axvline(x=rank_mean, color='r')
100/658:
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import numpy as np
import datetime

from email.utils import parseaddr

from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import PorterStemmer

from scipy import stats
100/659: percentile = stats.percentileofscore(rank_mean, emails_data['rank'])
100/660: percentile = stats.percentileofscore(emails_data['rank'], rank_mean)
100/661: percentile
103/1: path_to_data = 'Regression/longevity.csv'
104/1:
import pandas as pd
import matplotlib.pyplot as plt
104/2: path_to_data = 'Regression/longevity.csv'
104/3: longevity = pd.read_csv(path_to_data)
104/4: longevity.head()
104/5: longevity.loc[longevity.Smokes==1].plot.density()
104/6: longevity.AgeAtDeath.loc[longevity.Smokes==1]#.plot.density()
104/7: longevity.AgeAtDeath.loc[longevity.Smokes==1].plot.density()
104/8: longevity.AgeAtDeath.loc[longevity.Smokes==1].plot.density(grid=True)
104/9:
longevity.AgeAtDeath.loc[longevity.Smokes==1].plot.density(grid=True)
longevity.AgeAtDeath.loc[longevity.Smokes==0].plot.density(grid=True)
104/10:
longevity.AgeAtDeath.loc[longevity.Smokes==1].plot.density(grid=True)
longevity.AgeAtDeath.loc[longevity.Smokes==0].plot.density(grid=True)
legend()
104/11:
longevity.AgeAtDeath.loc[longevity.Smokes==1].plot.density(grid=True, label='Smokers')
longevity.AgeAtDeath.loc[longevity.Smokes==0].plot.density(grid=True, label='NoSmokers')
plt.legend()
104/12:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn
104/13:
longevity.AgeAtDeath.loc[longevity.Smokes==1].plot.density(grid=True, label='Smokers')
longevity.AgeAtDeath.loc[longevity.Smokes==0].plot.density(grid=True, label='NoSmokers')
plt.legend()
104/14: mean_age_at_death = longevity.AgeAtDeath.mean()
104/15: mean_age_at_death
104/16:
mean_age_at_death = longevity.AgeAtDeath.mean()
mean_age_at_death
104/17: guess = 73
104/18: longevity.AgeAtDeath-guess
104/19: (longevity.AgeAtDeath-guess)^2
104/20: (longevity.AgeAtDeath-guess)**2
104/21: ((longevity.AgeAtDeath-guess)**2).mean()
104/22: [((longevity.AgeAtDeath-x)**2).mean() for x in range(63,84)]
104/23: guesses = [((longevity.AgeAtDeath-x)**2).mean() for x in range(63,84)]
104/24: guesses.plot()
104/25: plot(list(range(63,84)), guesses)
104/26: plt.plot(list(range(63,84)), guesses)
104/27: plt.plot(list(range(63,84)), guesses, grid=True)
104/28: plt.plot(list(range(63,84)), guesses)
104/29:
plt.plot(list(range(63,84)), guesses)
plt.grid()
106/1:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn
106/2: path_to_data = 'Regression/longevity.csv'
106/3: longevity = pd.read_csv(path_to_data)
106/4: longevity.head()
106/5:
longevity.AgeAtDeath.loc[longevity.Smokes==1].plot.density(grid=True, label='Smokers')
longevity.AgeAtDeath.loc[longevity.Smokes==0].plot.density(grid=True, label='NoSmokers')
plt.legend()
106/6:
mean_age_at_death = longevity.AgeAtDeath.mean()
mean_age_at_death
106/7: guess = 73
106/8:
#rms
((longevity.AgeAtDeath-guess)**2).mean()
106/9:
# rms for random guesses
guesses = [((longevity.AgeAtDeath-x)**2).mean() for x in range(63,84)]
106/10:
plt.plot(list(range(63,84)), guesses)
plt.grid()
106/11: ((longevity.AgeAtDeath-guess)**2).mean()**(0.5)
106/12: smokers_guess = longevity.loc[smokes==1].AgeAtDeath.mean()
106/13: smokers_guess = longevity.loc[Smokes==1].AgeAtDeath.mean()
106/14: smokers_guess = longevity.loc[longevity.Smokes==1].AgeAtDeath.mean()
106/15:
smokers_guess = longevity.loc[longevity.Smokes==1].AgeAtDeath.mean()
smokers_guess
106/16:
no_smokers_guess = longevity.loc[longevity.Smokes==0].AgeAtDeath.mean()
no_smokers_guess
106/17: longevity.loc[longevity.Smokes==1]-smokers_guess
106/18: smokers = longevity.loc[longevity.Smokes==1]-smokers_guess
106/19:
smokers = longevity.loc[longevity.Smokes==1]-smokers_guess
smokers
106/20:
smokers = longevity.loc[longevity.Smokes==1].AgeAtDeath-smokers_guess
smokers
106/21:
smokers = (longevity.loc[longevity.Smokes==1].AgeAtDeath-smokers_guess)**2
smokers
106/22:
smokers = (longevity.loc[longevity.Smokes==1].AgeAtDeath-smokers_guess)**2
no_smokers = (longevity.loc[longevity.Smokes==0].AgeAtDeath-no_smokers_guess)**2
106/23: smokers.append(no_smokers)
106/24: longevity.describe()
106/25: smokers.append(no_smokers).mean().**(0.5)
106/26: smokers.append(no_smokers).mean()**(0.5)
107/1:
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.optimize import curve_fit
import numpy as np

import statsmodels.api as sm

%matplotlib inline
107/2:
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.optimize import curve_fit
import numpy as np

import statsmodels.api as sm

%matplotlib inline
107/3: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'
107/4:
hwg_df = pd.read_csv(
    heights_weights_gender_data)
107/5: hwg_df.head()
107/6: hwg_df.Height =hwg_df.Height * 2.54
107/7: hwg_df.Weight = hwg_df.Weight * 0.454
107/8: hwg_df.describe()
107/9: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)
107/10:
def my_var(x):
    m = sum(x)/len(x)
    return sum([(element-m)**2 for element in x])/(len(x)-1)
107/11:
def my_sd(x):
    return my_var(x)**(0.5)
107/12: my_var([1,2,3,4,5])
107/13: my_var(hwg_df.Height)
107/14: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()
107/15: my_sd(hwg_df.Height) - hwg_df.Height.std()
107/16: df = hwg_df
107/17: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()
107/18: from scipy import stats
107/19: stats.percentileofscore(df.Height, 158.8)
107/20: stats.percentileofscore(df.Height, 178.34)
107/21: 82.5-17.31
107/22: df.describe()
107/23: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])
107/24:
kde = df.Height.plot.kde()
plt.title('KDE')
plt.xlabel('Height')
plt.ylabel('Probability')
107/25: df.head()
107/26:
df.Height[df.Gender=='Male'].plot.kde(label='Male')
df.Height[df.Gender=='Female'].plot.kde(label='Female')
plt.legend()
plt.xlabel('Height')
107/27:
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.legend()
plt.xlabel('Weight')
107/28:
plt.figure(figsize=(10,10))
ax1 = plt.subplot(2,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.grid(True)
plt.legend()
ax2 = plt.subplot(2,1,2, sharex = ax1)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.grid(True)
plt.xlabel('Weight')
plt.legend()
107/29:
mu = 10
variance = 5
sigma = variance**0.5
x = np.linspace(mu-3*sigma, mu+3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Gauss')
plt.plot(x, stats.cauchy.pdf(x, mu, sigma), label='Cauchy')
plt.legend()
107/30:
plt.scatter(df.Height, df.Weight)
def f1(x,a,b):
    return a*x + b

popt,pcov = curve_fit(f1, df.Height, df.Weight )
plt.plot(df.Height, f1(df.Height, *popt), color = 'red')
plt.xlabel('Height [cm]')
plt.ylabel('Weight [kg]')
107/31: popt
107/32: 1.38*160-159
107/33: colors = dict(Male='red', Female='blue')
107/34: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x]))
107/35: colors
107/36: df.dtypes
107/37: df.Gender.loc[9970]
107/38:
plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')
plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')
plt.legend()
plt.xlabel('Height')
plt.ylabel('Weight')
107/39: df.head()
107/40:
# Note the difference in argument order
model = sm.OLS(df.Weight, df.Height).fit()
predictions = model.predict(df.Height) # make the predictions by the model

# Print out the statistics
model.summary()
107/41:
# Note the difference in argument order
model = sm.OLS(df.Weight, df.Height).fit()
predictions = model.predict(df.Height) # make the predictions by the model
df.Height = sm.add_constant(df.Height)

# Print out the statistics
model.summary()
107/42:
# Note the difference in argument order
model = sm.OLS(df.Weight, df.Height).fit()
predictions = model.predict(df.Height) # make the predictions by the model
df.Height = sm.add_constant(df.Height)

# Print out the statistics
model.summary()
107/43:
# Note the difference in argument order
df.Height = sm.add_constant(df.Height)
model = sm.OLS(df.Weight, df.Height).fit()
predictions = model.predict(df.Height) # make the predictions by the model

# Print out the statistics
model.summary()
107/44:
df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
model = sm.OLS(df.Weight, df.Height).fit()
predictions = model.predict(df.Height) # make the predictions by the model

# Print out the statistics
model.summary()
107/45:
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.optimize import curve_fit
import numpy as np

import statsmodels.api as sm

%matplotlib inline
107/46: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'
107/47:
hwg_df = pd.read_csv(
    heights_weights_gender_data)
107/48: hwg_df.head()
107/49: hwg_df.Height =hwg_df.Height * 2.54
107/50: hwg_df.Weight = hwg_df.Weight * 0.454
107/51: hwg_df.describe()
107/52: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)
107/53:
def my_var(x):
    m = sum(x)/len(x)
    return sum([(element-m)**2 for element in x])/(len(x)-1)
107/54:
def my_sd(x):
    return my_var(x)**(0.5)
107/55: my_var([1,2,3,4,5])
107/56: my_var(hwg_df.Height)
107/57: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()
107/58: my_sd(hwg_df.Height) - hwg_df.Height.std()
107/59: df = hwg_df
107/60: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()
107/61: from scipy import stats
107/62: stats.percentileofscore(df.Height, 158.8)
107/63: stats.percentileofscore(df.Height, 178.34)
107/64: 82.5-17.31
107/65: df.describe()
107/66: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])
107/67:
kde = df.Height.plot.kde()
plt.title('KDE')
plt.xlabel('Height')
plt.ylabel('Probability')
107/68: df.head()
107/69:
df.Height[df.Gender=='Male'].plot.kde(label='Male')
df.Height[df.Gender=='Female'].plot.kde(label='Female')
plt.legend()
plt.xlabel('Height')
107/70:
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.legend()
plt.xlabel('Weight')
107/71:
plt.figure(figsize=(10,10))
ax1 = plt.subplot(2,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.grid(True)
plt.legend()
ax2 = plt.subplot(2,1,2, sharex = ax1)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.grid(True)
plt.xlabel('Weight')
plt.legend()
107/72:
mu = 10
variance = 5
sigma = variance**0.5
x = np.linspace(mu-3*sigma, mu+3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Gauss')
plt.plot(x, stats.cauchy.pdf(x, mu, sigma), label='Cauchy')
plt.legend()
107/73:
plt.scatter(df.Height, df.Weight)
def f1(x,a,b):
    return a*x + b

popt,pcov = curve_fit(f1, df.Height, df.Weight )
plt.plot(df.Height, f1(df.Height, *popt), color = 'red')
plt.xlabel('Height [cm]')
plt.ylabel('Weight [kg]')
107/74: popt
107/75: 1.38*160-159
107/76: colors = dict(Male='red', Female='blue')
107/77: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x]))
107/78: colors
107/79: df.dtypes
107/80: df.Gender.loc[9970]
107/81:
plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')
plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')
plt.legend()
plt.xlabel('Height')
plt.ylabel('Weight')
107/82:
df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
model = sm.OLS(df.Weight, df.Height).fit()
predictions = model.predict(df.Height) # make the predictions by the model

# Print out the statistics
model.summary()
107/83: df.head()
107/84:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

model = sm.OLS(y, y).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
model.summary()
107/85:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
model.summary()
107/86:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
model.summary()
107/87:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

#x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
model.summary()
107/88:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

#x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/89:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/90:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

#x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/91:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height[:10]
y = df.Weight[:10]

#x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/92:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height[:10]
y = df.Weight[:10]

x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/93:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height[:20]
y = df.Weight[:20]

x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/94:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height[:20]
y = df.Weight[:20]

x = sm.tools.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/95:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height[:20]
y = df.Weight[:20]

x = sm.tools.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/96:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height[:20]
y = df.Weight[:20]

x = sm.tools.tools.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/97:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height[:20]
y = df.Weight[:20]

x = sm.tools.tools.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/98:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height[:]
y = df.Weight[]

x = sm.tools.tools.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/99:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height[]
y = df.Weight[]

x = sm.tools.tools.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/100:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

x = sm.tools.tools.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/101:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/102:
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.optimize import curve_fit
import numpy as np

import statsmodels as sm

%matplotlib inline
107/103:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/104:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

x = sm.tools.tools.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/105:
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.optimize import curve_fit
import numpy as np

import statsmodels.api as sm

%matplotlib inline
107/106:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/107: x = df.Height
107/108:
x = df.Height
x
107/109: x = df.Height
107/110:
x = df.Height
x
107/111:
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.optimize import curve_fit
import numpy as np

import statsmodels.api as sm

%matplotlib inline
107/112: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'
107/113:
hwg_df = pd.read_csv(
    heights_weights_gender_data)
107/114: hwg_df.head()
107/115: hwg_df.Height =hwg_df.Height * 2.54
107/116: hwg_df.Weight = hwg_df.Weight * 0.454
107/117: hwg_df.describe()
107/118: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)
107/119:
def my_var(x):
    m = sum(x)/len(x)
    return sum([(element-m)**2 for element in x])/(len(x)-1)
107/120:
def my_sd(x):
    return my_var(x)**(0.5)
107/121: my_var([1,2,3,4,5])
107/122: my_var(hwg_df.Height)
107/123: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()
107/124: my_sd(hwg_df.Height) - hwg_df.Height.std()
107/125: df = hwg_df
107/126: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()
107/127: from scipy import stats
107/128: stats.percentileofscore(df.Height, 158.8)
107/129: stats.percentileofscore(df.Height, 178.34)
107/130: 82.5-17.31
107/131: df.describe()
107/132: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])
107/133:
kde = df.Height.plot.kde()
plt.title('KDE')
plt.xlabel('Height')
plt.ylabel('Probability')
107/134: df.head()
107/135:
df.Height[df.Gender=='Male'].plot.kde(label='Male')
df.Height[df.Gender=='Female'].plot.kde(label='Female')
plt.legend()
plt.xlabel('Height')
107/136:
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.legend()
plt.xlabel('Weight')
107/137:
plt.figure(figsize=(10,10))
ax1 = plt.subplot(2,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.grid(True)
plt.legend()
ax2 = plt.subplot(2,1,2, sharex = ax1)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.grid(True)
plt.xlabel('Weight')
plt.legend()
107/138:
mu = 10
variance = 5
sigma = variance**0.5
x = np.linspace(mu-3*sigma, mu+3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Gauss')
plt.plot(x, stats.cauchy.pdf(x, mu, sigma), label='Cauchy')
plt.legend()
107/139:
plt.scatter(df.Height, df.Weight)
def f1(x,a,b):
    return a*x + b

popt,pcov = curve_fit(f1, df.Height, df.Weight )
plt.plot(df.Height, f1(df.Height, *popt), color = 'red')
plt.xlabel('Height [cm]')
plt.ylabel('Weight [kg]')
107/140: popt
107/141: 1.38*160-159
107/142: colors = dict(Male='red', Female='blue')
107/143: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x]))
107/144: colors
107/145: df.dtypes
107/146: df.Gender.loc[9970]
107/147:
plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')
plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')
plt.legend()
plt.xlabel('Height')
plt.ylabel('Weight')
107/148:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
print(print_model)
107/149:
x = df.Height
x
107/150: x = sm.add_constant(x)
107/151:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
#print(print_model)
107/152:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
print_model = model.summary()
#print(print_model)
107/153:
#df.Height = sm.add_constant(df.Height)
# Note the difference in argument order
x = df.Height
y = df.Weight

x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
model.summary()
#print(print_model)
107/154: model.__dir__()
107/155: model.coef
107/156: model.coef()
107/157: model['coeff']
107/158:
# Note the difference in argument order
x = df.Height
y = df.Weight

x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
model.summary()
107/159: model
107/160: model.params
107/161: print(model.params)
107/162: model.params[0]
107/163: model.params[1]
107/164:
model.params
model.params[0]
model.params[1]
107/165:
model.params
model.params[0]
model.params[1]
107/166:
print(model.params)
model.params[0]
model.params[1]
107/167:
print(model.params)
print(model.params[0])
print(model.params[1])
107/168:
print('''
model.params
model.params[0]
model.params[1]
''')
107/169: model.params
107/170:
#Linear regression calculation: statsmodels
x = df.Height
y = df.Weight

x = sm.add_constant(x)
model = sm.OLS(y, x).fit()
predictions = model.predict(x) # make the predictions by the model

# Print out the statistics
model.summary()
107/171:
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.optimize import curve_fit
import numpy as np

import statsmodels.api as sm

from sklearn import linear_model

%matplotlib inline
107/172:
lm = linear_model.LinearRegression()
model = lm.fit(x,y)
107/173:
predictions = lm.predict(X)
print(predictions)[0:5]
107/174:
predictions = lm.predict(x)
print(predictions)[0:5]
107/175:
predictions = lm.predict(x)
print(predictions[0:5])
107/176: df.head()
107/177:
#Linear regression calculation: statsmodels
x = df.Height
y = df.Weight

x = sm.add_constant(x)
model_statsmodels = sm.OLS(y, x).fit()
predictions = model_statmodels.predict(x) # make the predictions by the model

# Print out the statistics
model_statmodels.summary()
107/178:
#Linear regression calculation: statsmodels
x = df.Height
y = df.Weight

x = sm.add_constant(x)
model_statsmodels = sm.OLS(y, x).fit()
predictions = model_statsmodels.predict(x) # make the predictions by the model

# Print out the statistics
model_statsmodels.summary()
107/179: model_statsmodels.params
107/180:
lm = linear_model.LinearRegression()
model_sklearn = lm.fit(x,y)
107/181: model_sklearn
107/182: model_sklearn.coef
107/183: model_sklearn._coef
107/184: model_sklearn.__dir__
107/185: model_sklearn.__dir__()
107/186: model_sklearn.coef_#__dir__()
107/187: model_sklearn.coef_ * 2#__dir__()
107/188: model_sklearn.coef_ #__dir__()
107/189: model_sklearn.intercept
107/190: help(model_sklearn)
107/191: model_sklearn.intercept_
107/192: plt.plot(df.Height, df.Weight)
107/193: df.sort_values(by='Height')
107/194: df.sort_values(by='Height', inplace=True)
107/195: plt.plot(df.Height, df.Weight)
107/196:
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.optimize import curve_fit
import numpy as np

import statsmodels.api as sm

from sklearn import linear_model

%matplotlib inline
107/197: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'
107/198:
hwg_df = pd.read_csv(
    heights_weights_gender_data)
107/199: hwg_df.head()
107/200: hwg_df.Height =hwg_df.Height * 2.54
107/201: hwg_df.Weight = hwg_df.Weight * 0.454
107/202: hwg_df.describe()
107/203: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)
107/204:
def my_var(x):
    m = sum(x)/len(x)
    return sum([(element-m)**2 for element in x])/(len(x)-1)
107/205:
def my_sd(x):
    return my_var(x)**(0.5)
107/206: my_var([1,2,3,4,5])
107/207: my_var(hwg_df.Height)
107/208: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()
107/209: my_sd(hwg_df.Height) - hwg_df.Height.std()
107/210: df = hwg_df
107/211: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()
107/212: from scipy import stats
107/213: stats.percentileofscore(df.Height, 158.8)
107/214: stats.percentileofscore(df.Height, 178.34)
107/215: 82.5-17.31
107/216: df.describe()
107/217: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])
107/218:
kde = df.Height.plot.kde()
plt.title('KDE')
plt.xlabel('Height')
plt.ylabel('Probability')
107/219: df.head()
107/220:
df.Height[df.Gender=='Male'].plot.kde(label='Male')
df.Height[df.Gender=='Female'].plot.kde(label='Female')
plt.legend()
plt.xlabel('Height')
107/221:
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.legend()
plt.xlabel('Weight')
107/222:
plt.figure(figsize=(10,10))
ax1 = plt.subplot(2,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.grid(True)
plt.legend()
ax2 = plt.subplot(2,1,2, sharex = ax1)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.grid(True)
plt.xlabel('Weight')
plt.legend()
107/223:
mu = 10
variance = 5
sigma = variance**0.5
x = np.linspace(mu-3*sigma, mu+3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Gauss')
plt.plot(x, stats.cauchy.pdf(x, mu, sigma), label='Cauchy')
plt.legend()
107/224:
plt.scatter(df.Height, df.Weight)
def f1(x,a,b):
    return a*x + b

popt,pcov = curve_fit(f1, df.Height, df.Weight )
plt.plot(df.Height, f1(df.Height, *popt), color = 'red')
plt.xlabel('Height [cm]')
plt.ylabel('Weight [kg]')
107/225: popt
107/226: 1.38*160-159
107/227: colors = dict(Male='red', Female='blue')
107/228: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x]))
107/229: colors
107/230: df.dtypes
107/231: df.Gender.loc[9970]
107/232:
plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')
plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')
plt.legend()
plt.xlabel('Height')
plt.ylabel('Weight')
107/233:
#Linear regression calculation: statsmodels
x = df.Height
y = df.Weight

x = sm.add_constant(x)
model_statsmodels = sm.OLS(y, x).fit()
predictions = model_statsmodels.predict(x) # make the predictions by the model

# Print out the statistics
model_statsmodels.summary()
107/234: model_statsmodels.params
107/235:
lm = linear_model.LinearRegression()
model_sklearn = lm.fit(x,y)
107/236:
predictions = lm.predict(x)
print(predictions[0:5])
107/237: model_sklearn.coef_ #__dir__()
107/238: model_sklearn.intercept_
107/239: squares=[df.weights]
107/240: squares=df.Weight - predictions
107/241: squares
107/242: squares.mean()
107/243: squares
107/244: squares[500:520]
107/245: squares[500:550]
107/246: squares**2
107/247: squares**2.sum()
107/248: (squares**2).sum()
107/249: (squares**2).sum()**(0.5)
107/250: (squares**2).sum()/len(squares)
107/251: ((squares**2).sum()/len(squares))**0.5
107/252: difference=df.Weight - predictions
107/253: rms = ((difference**2).sum()/len(difference))**0.5
107/254:
rms = ((difference**2).sum()/len(difference))**0.5
rms
107/255: nominator = rms**2
107/256:
mean_weight = df.Weight.mean()
mean_weight
107/257:
nominator = ((predictions-mean_weight)**2).sum()
nominator
107/258: denominator = ((df.Weight-mean_weight)**2).sum()
107/259:
denominator = ((df.Weight-mean_weight)**2).sum()
denominator
107/260: r_square = nominator/denominator
107/261: r_square
107/262:
r_square = nominator/denominator
r_square
107/263:
difference=df.Weight - predictions
rms = ((difference**2).sum()/len(difference))**0.5
rms
107/264: (difference**2).sum()
107/265: (difference**2)[1:10].sum()
107/266: (difference**2)[0:10].sum()
107/267: (difference**2)[0:11].sum()
107/268: (difference**2)[1:10].sum()
107/269:
difference=df.Weight - predictions
rms_model = ((difference**2).sum()/len(difference))**0.5
rms_model
107/270:
rmse_mean = (((mean_weight - df.Weight)**2).mean())**(0.5)
rmse_mean
107/271: r_square_2 = 1-(rms_model/rmse_mean)
107/272:
r_square_2 = 1-(rms_model/rmse_mean)
r_square_2
107/273:
rmse_mean = (((mean_weight - df.Weight)**2).sum()/len(df.Weight))**(0.5)
rmse_mean
107/274:
r_square_2 = 1-(rms_model/rmse_mean)
r_square_2
107/275: mean_weight - df.Weight
107/276: (mean_weight - df.Weight)**2.sum()
107/277: ((mean_weight - df.Weight)**2).sum()
107/278: ((mean_weight - df.Weight)**2).sum()/len(mean_weight)
107/279: ((mean_weight - df.Weight)**2).sum()/len(df.Weight)
108/1:
import pandas as pd
import matplotlib.pyplot as plt
108/2: help(pd.read_csv)
108/3: top_sites = pd.read_csv('\Regression\top_1000_sites.tsv', delimiter='\t' )
108/4: import os
108/5: help(os)
108/6: os.get_cwd()
108/7: os.__dir__
108/8: os.__dir__()
108/9: os.getcwd()
108/10: top_sites = pd.read_csv('Regression\top_1000_sites.tsv', delimiter='\t' )
108/11: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\t' )
108/12: top_sites.head()
108/13: top_sites = pd.read_csv('Regression/top_1000_sites.tsv')#, delimiter='\t' )
108/14: top_sites.head()
108/15: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\t' )
108/16: top_sites.head()
108/17: help(plt.scatter)
108/18: plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )
108/19:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )
grid=True
108/20: help(plt.grid)
108/21:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )
grid(True)
108/22:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )
plt.grid
108/23:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )
plt.grid = True
108/24:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )
plt.grid = on
108/25:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )
plt.grid = True
108/26:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )
plt.grid()
110/1:
import pandas as pd
import matplotlib.pyplot as plt
110/2: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\t' )
110/3: top_sites.head()
110/4: help(plt.grid)
110/5:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )
plt.grid()
110/6: help(plt.xlabel)
110/7:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )
plt.grid()
plt(xlabel='Unique_visitors')
110/8:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )
plt.grid()
plt.xlabel='Unique_visitors'
plt.show()
110/9:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )
plt.grid()
plt.xlabel('Unique_visitors')
plt.show()
110/10: help(plt.set_xlabel)
111/1:
import pandas as pd
import matplotlib.pyplot as plt
111/2: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\t' )
111/3: top_sites.head()
111/4: help(plt.set_xlabel)
111/5:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )
plt.grid()
plt.xlabel('Unique_visitors')
plt.show()
111/6:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )
plt.grid()
plt.xlabel('Unique_visitors')
plt.ylabel('Page_views')
plt.show()
111/7: help(plt.kde)
111/8: help(kde)
111/9: top_sites.PageViews.plot()
111/10: top_sites.PageViews.plot.kde()
111/11:
top_sites.PageViews.plot.kde()
xlabe('Unique_visitors')
ylabel('Density')
111/12:
top_sites.PageViews.plot.kde()
plt.xlabe('Unique_visitors')
plt.ylabel('Density')
111/13:
top_sites.PageViews.plot.kde()
plt.xlabel('Unique_visitors')
plt.ylabel('Density')
111/14:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('Unique_visitors')
plt.ylabel('Page_views')
plt.show()
111/15:
import pandas as pd
import matplotlib.pyplot as plt
from numpy import log
111/16:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('Unique_visitors')
plt.ylabel('Page_views')
plt.show()
111/17:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.show()
111/18:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.show()
111/19:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)
plt.yscale('log')
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.show()
111/20:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)
plt.yscale('log')
plt.xscale('log')
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.show()
111/21:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.show()
111/22:
import pandas as pd
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm
111/23: help(sm)
111/24: help(sm.ols)
111/25: help(sm.OLS)
111/26:
x = top_sites.UniqueVisitors
y = top_sites.PageViews

x = sm.add_constant(x)

model = sm.OLS.fit(y,x)
111/27:
x = top_sites.UniqueVisitors
y = top_sites.PageViews

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()
111/28:
x = top_sites.UniqueVisitors
y = top_sites.PageViews

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

print(model)
111/29:
x = top_sites.UniqueVisitors
y = top_sites.PageViews

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

model.summary()
111/30:
import pandas as pd
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm

import seaborn as sns
111/31: sns.regplot(data=top_sites[columns=['UniqueVisitor','PageViews']])
111/32: top_sites(columns='UniqueVisitors')
111/33: top_sites[columns='UniqueVisitors']
111/34: top_sites[columns=='UniqueVisitors']
111/35: top_sites['UniqueVisitors']
111/36: top_sites['UniqueVisitors', 'PageViews']
111/37: top_sites[['UniqueVisitors', 'PageViews']]
111/38: sns.regplot(data=top_sites[['UniqueVisitor','PageViews']])
111/39: sns.regplot(data=top_sites[['UniqueVisitors','PageViews']])
111/40: sns.regplot(x='Unique_visitors', y='Page_views', data=top_sites[['UniqueVisitors','PageViews']])
111/41: sns.regplot(x='UniqueVisitors', y='PageViews', data=top_sites[['UniqueVisitors','PageViews']])
111/42: sns.regplot(x='UniqueVisitors', y='PageViews', data=top_sites)
112/1: z = sns.regplot(x='UniqueVisitors', y='PageViews', data=top_sites)
112/2:
import pandas as pd
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm

import seaborn as sns
112/3: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\t' )
112/4: top_sites.head()
112/5:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.show()
112/6:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)
plt.yscale('log')
plt.xscale('log')
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.show()
112/7:
x = top_sites.UniqueVisitors
y = top_sites.PageViews

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

model.summary()
112/8: help(sm.OLS).
112/9: z
112/10: z = sns.regplot(x='UniqueVisitors', y='PageViews', data=top_sites)
112/11:
z = sns.regplot(x='UniqueVisitors', y='PageViews', data=top_sites)
z.set_yscale('log')
z.set_xscale('log')
112/12: logs = log(top_sites['Unigue'])
112/13: logs = log(top_sites['UniqueVisitors'])
112/14:
logs = log(top_sites['UniqueVisitors'])
logs
112/15:
logs = log(top_sites['UniqueVisitors', 'PageViews'])
logs
112/16:
logs = log(top_sites[['UniqueVisitors', 'PageViews']])
logs
112/17:
z = sns.regplot(x='UniqueVisitors', y='PageViews', data=logs)
#z.set_yscale('log')
#z.set_xscale('log')
112/18: logs = log(top_sites[['UniqueVisitors', 'PageViews']])
112/19:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)
plt.yscale('log')
plt.xscale('log')
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.show()
112/20: plt.gcf().set_size_inches(8,4)
112/21:
import pandas as pd
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm

import seaborn as sns

%matplotlib inline
112/22: plt.gcf().set_size_inches(8,4)
112/23:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)
plt.yscale('log')
plt.xscale('log')
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.show()
112/24: plt.gcf().set_size_inches(8,4)
112/25: plt.gcf().set_size_inches(10,5)
112/26: plt.gcf().set_size_inches(12,6)
112/27:
plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)
plt.yscale('log')
plt.xscale('log')
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.gcf().set_size_inches(12,6)
plt.show()
112/28:
figure = plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)
plt.yscale('log')
plt.xscale('log')
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.gcf().set_size_inches(12,6)
plt.show()
112/29: figure
112/30: help(figure)
112/31:
x_min = log(top_sites.UniqueVisiotors).min()
x_min
112/32:
x_min = log(top_sites.UniqueVisitors).min()
x_min
112/33:
x_min = log(top_sites.UniqueVisitors).min()
x_max = log(top_sites.UniqueVisitors).max()
x_min, x_max
112/34:
x = log(top_sites.UniqueVisitors)
y = log(top_sites.PageViews)

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

model.summary()
112/35: model.predict([x_min, x_max])
112/36:
predictions = model.predict([x_min, x_max])
predictions
112/37:
predictions = model.predict([x_min, x_max])
print(predictions)
112/38:
predictions = model.predict([x_min, x_max])
print(predictions)
help(predictions)
112/39:
predictions = model.predict([x_min, x_max])
print(predictions)
help(model.predict())
112/40:
predictions = model.predict([x_min, x_max])
print(predictions)
help(model.predict())
112/41:
predictions = model.predict([x_min, x_max])
print(predictions)
help(model)
112/42:
predictions = model.predict(x_min, x_max)
print(predictions)
#help(model)
112/43:
predictions = model.predict([x_min, x_max])
print(predictions)
#help(model)
112/44:
predictions = model.predict([x_min, x_max])
predictions
#help(model)
112/45:
predictions = model.predict([1, 2])
predictions
#help(model)
112/46:
x = log(top_sites.UniqueVisitors)
y = log(top_sites.PageViews)

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

model.summary()
112/47:
predictions = model.predict([15, 20])
predictions
#help(model)
112/48:
predictions = model.predict([15, 20])
predictions = 1.3363*15 - 2.8344
#help(model)
112/49:
predictions = model.predict([15, 20])
predictions = 1.3363*15 - 2.8344
predictions
112/50:
predictions = model.predict([15, 20])
predictions = 1.3363*20 - 2.8344
predictions
112/51:
predictions = model.predict([15, 20])
predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]
predictions
112/52:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
112/53:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.plot([x_min, x_max],predictions)
112/54:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.plot([x_min, x_max],predictions, color='red')
112/55:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.plot(minmax,predictions, color='red')
112/56:
x_min = log(top_sites.UniqueVisitors).min()
x_max = log(top_sites.UniqueVisitors).max()
minmax = [x_min, x_max]
112/57:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.plot(minmax,predictions, color='red')
112/58:
predictions = model.predict([15, 20])
predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]
predictions_2 = model.predict([15,20])
112/59:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.plot(minmax,predictions_2, color='red')
112/60:
predictions = model.predict(minmax)
predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]
predictions_2 = model.predict(minmax)
predictions_2
112/61: minmax_2 = pd.DataFrame(minmax)
112/62:
predictions = model.predict(minmax)
predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]
predictions_2 = model.predict(minmax_2)
predictions_2
112/63:
minmax_2 = pd.DataFrame(minmax)
minmax_2
112/64:
minmax_2 = pd.DataFrame(minmax)
minmax_2.transpose()
112/65:
minmax_2 = pd.DataFrame(minmax)
minmax_2.transpose(inplace=True)
112/66:
minmax_2 = pd.DataFrame(minmax)
minmax_2 = minmax_2.transpose()
112/67:
predictions = model.predict(minmax)
predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]
predictions_2 = model.predict(minmax_2)
predictions_2
112/68: minmax_2 = pd.DataFrame(minmax)
112/69:
predictions = model.predict(minmax)
predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]
predictions_2 = model.predict(minmax_2)
predictions_2
112/70: minmax_2 = x
112/71:
predictions = model.predict(minmax)
predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]
predictions_2 = model.predict(minmax_2)
predictions_2
112/72: minmax_2 = sm.add_constant(minmax)
112/73:
predictions = model.predict(minmax)
predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]
predictions_2 = model.predict(minmax_2)
predictions_2
112/74:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.plot(minmax,predictions_2, color='red')
112/75:
minmax_2 = sm.add_constant(minmax)
predictions = model.predict(minmax_2)
predictions
112/76:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.plot(minmax,predictions, color='red')
112/77:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.plot(minmax,predictions, color='red')
plt.gcf.set_size_inches(12,6)
112/78:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.plot(minmax,predictions, color='red')
plt.gcf().set_size_inches(12,6)
112/79:
x = log(top_sites.UniqueVisitors)
y = log(top_sites.PageViews)

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

model.summary()
model.information()
112/80:
x = log(top_sites.UniqueVisitors)
y = log(top_sites.PageViews)

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

model.summary()
112/81: help(model)
112/82:
x = log(top_sites.UniqueVisitors)
y = log(top_sites.PageViews)

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

y_predictions = model.predict(x)
rmse = rmse(y, y_predictions)
print('rmse: ', rmse)
model.summary()
112/83:
x = log(top_sites.UniqueVisitors)
y = log(top_sites.PageViews)

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

y_predictions = model.predict(x)
rmse = sm.rmse(y, y_predictions)
print('rmse: ', rmse)
model.summary()
112/84:
import pandas as pd
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm
from statsmodels.tools.eval_measures import rmse

import seaborn as sns

%matplotlib inline
112/85:
x = log(top_sites.UniqueVisitors)
y = log(top_sites.PageViews)

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

y_predictions = model.predict(x)
rmse = sm.rmse(y, y_predictions)
print('rmse: ', rmse)
model.summary()
112/86:
x = log(top_sites.UniqueVisitors)
y = log(top_sites.PageViews)

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

y_predictions = model.predict(x)
rmse = rmse(y, y_predictions)
print('rmse: ', rmse)
model.summary()
112/87:
x = [log(top_sites.UniqueVisitors), top_sites.InEnglish]
y = log(top_sites.PageViews)

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

y_predictions = model.predict(x)
rmse = rmse(y, y_predictions)
print('rmse: ', rmse)
model.summary()
112/88:
x = [log(top_sites.UniqueVisitors), top_sites.InEnglish]
y = log(top_sites.PageViews)

#x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

y_predictions = model.predict(x)
rmse = rmse(y, y_predictions)
print('rmse: ', rmse)
model.summary()
112/89:
m,c = np.polyfit(x,y)
plt.plot(x, m*x+c)
112/90:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm
from statsmodels.tools.eval_measures import rmse

import seaborn as sns

%matplotlib inline
112/91:
m,c = np.polyfit(x,y)
plt.plot(x, m*x+c)
112/92:
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c)
112/93:
m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)
plt.plot(x, m*x+c)
112/94:
m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)
plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c)
112/95:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)
plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c)
112/96:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)
plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c, color='red')
112/97:
x = [log(top_sites.UniqueVisitors)]
y = log(top_sites.PageViews)

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

y_predictions = model.predict(x)
rmse = rmse(y, y_predictions)
print('rmse: ', rmse)
model.summary()
112/98:
x = log(top_sites.UniqueVisitors)
y = log(top_sites.PageViews)

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

y_predictions = model.predict(x)
rmse = rmse(y, y_predictions)
print('rmse: ', rmse)
model.summary()
112/99:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.plot(log(top_sites.UniqueVisitors),model.predict(log(top_sites.UniqueVisitors)), color='red')
plt.gcf().set_size_inches(12,6)
112/100:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.plot(minmax,predictions, color='red')
plt.gcf().set_size_inches(12,6)
112/101:
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)
plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c, color='red')
plt.gcf().set_size_inches(12,6)
112/102: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).append(tope_sites['IsEnglish'])
112/103: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).append(top_sites['IsEnglish'])
112/104: top_sites.head()
112/105: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).append(top_sites['InEnglish'])
112/106: logs.head()
112/107: logs = log(top_sites[['UniqueVisitors', 'PageViews']])#.append(top_sites['InEnglish'])
112/108: logs.head()
112/109: logs.summary()
112/110: logs.describe()
112/111: logs.info()
112/112: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).append(top_sites['InEnglish'], axis=1)
112/113: help(pd.DataFrame.append())
112/114: help(pd.DataFrame.append
112/115: help(pd.DataFrame.append)
112/116: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish)
112/117: logs.head()
112/118: z = sns.regplot(x='UniqueVisitors', y='PageViews',hue='InEnglish' data=logs)
112/119: z = sns.regplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)
112/120: z = sns.lmplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)
112/121:
#x = log(top_sites.UniqueVisitors)
#y = log(top_sites.PageViews)

#x = sm.add_constant(x)

model = sm.OLS(formula='PageViews ~ UniqueVisitors + InEnglish').fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/122:
#x = log(top_sites.UniqueVisitors)
#y = log(top_sites.PageViews)

#x = sm.add_constant(x)

model = sm.OLS(formula='PageViews ~ UniqueVisitors + InEnglish', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/123:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm
from statsmodels.tools.eval_measures import rmse
import statsmodels.formula.api as smf

import seaborn as sns

%matplotlib inline
112/124:
#x = log(top_sites.UniqueVisitors)
#y = log(top_sites.PageViews)

#x = sm.add_constant(x)

model = smf.OLS(formula='PageViews ~ UniqueVisitors + InEnglish', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/125:
#x = log(top_sites.UniqueVisitors)
#y = log(top_sites.PageViews)

#x = sm.add_constant(x)

model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/126:
x = logs[['UniqueVisitors', 'InEnglish']]
y = log(top_sites.PageViews)

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

#model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/127:
import smpi.statsmodels as ssm
x = logs[['UniqueVisitors', 'InEnglish']]
y = log(top_sites.PageViews)

x = ssm.add_constant(x)

model = ssm.OLS(y,x).fit()

#model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/128:
#import smpi.statsmodels as ssm
#x = logs[['UniqueVisitors', 'InEnglish']]
#y = log(top_sites.PageViews)

#x = ssm.add_constant(x)

#model = ssm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ UniqueVisitors + c(InEnglish)', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/129:
#import smpi.statsmodels as ssm
#x = logs[['UniqueVisitors', 'InEnglish']]
#y = log(top_sites.PageViews)

#x = ssm.add_constant(x)

#model = ssm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ UniqueVisitors + C(InEnglish)', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/130:
#import smpi.statsmodels as ssm
#x = logs[['UniqueVisitors', 'InEnglish']]
#y = log(top_sites.PageViews)

#x = ssm.add_constant(x)

#model = ssm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/131:
logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'], drop_first=True)), axis=1)

model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/132:
logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'], drop_first=True)), axis=1)

x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

#model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/133:
logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'], drop_first=True)), axis=1)

x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']

x = sm.add_constant(x)

model = sm.OLS(y,x.astype(float)).fit()

#model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/134: logs.info()
112/135:
logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'])), axis=1)

x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']

x = sm.add_constant(x)

model = sm.OLS(y,x.astype(float)).fit()

#model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/136: logs.info()
112/137: logs.head()
112/138: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish)
112/139: logs.head()
112/140:
#logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'])), axis=1)
logs['IsEnglish']=logs.InEnglish.map({'Yes':0,'No':1})

x = logs[['UniqueVisitors', 'IsEnglish']]
y = logs['PageViews']

x = sm.add_constant(x)

model = sm.OLS(y,x.astype(float)).fit()

#model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/141:
#logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'])), axis=1)
logs.dropna()
logs['IsEnglish']=logs.InEnglish.map({'Yes':0,'No':1})

x = logs[['UniqueVisitors', 'IsEnglish']]
y = logs['PageViews']

x = sm.add_constant(x)

model = sm.OLS(y,x.astype(float)).fit()

#model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/142:
#logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'])), axis=1)
logs.dropna()
logs['IsEnglish']=logs.InEnglish.map({'Yes':0,'No':1})

x = logs[['UniqueVisitors', 'IsEnglish']]
y = logs['PageViews']

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

#model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/143: logs.group_by(by='InEnglish').count()
112/144: logs.groupby(by='InEnglish').count()
112/145: logs.groupby(by='UniqueVisitors').count()
112/146:
#logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'])), axis=1)
logs.dropna()
logs['IsEnglish']=logs.InEnglish.map({'Yes':0,'No':1})

x = logs[['UniqueVisitors', 'IsEnglish']]
y = logs['PageViews']

x = sm.add_constant(x)

#model = sm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ C(IsEnglish)', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/147:
x = logs[['UniqueVisitors', 'IsEnglish']]
y = logs['PageViews']

x = sm.add_constant(x)

#model = sm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ C(IsEnglish)', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/148:
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']

x = sm.add_constant(x)

#model = sm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ InEnglish', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/149:
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']

x = sm.add_constant(x)

#model = sm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/150: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish)
112/151: logs.head()
112/152:
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']
logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish).labels
#x = sm.add_constant(x)

#model = sm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish_ord', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/153:
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']
logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)
#x = sm.add_constant(x)

#model = sm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish_ord', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/154: logs.head()
112/155: logs.info()
112/156:
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']
logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)
#x = sm.add_constant(x)

#model = sm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ UniqueVisitors + C(InEnglish_ord)', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
112/157:
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']
logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)
#x = sm.add_constant(x)

#model = sm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ (InEnglish_ord)', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
113/1:
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.optimize import curve_fit
import numpy as np

import statsmodels.api as sm

from sklearn import linear_model

%matplotlib inline
113/2: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'
113/3:
hwg_df = pd.read_csv(
    heights_weights_gender_data)
113/4: hwg_df.head()
113/5: hwg_df.Height =hwg_df.Height * 2.54
113/6: hwg_df.Weight = hwg_df.Weight * 0.454
113/7: hwg_df.describe()
113/8: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)
113/9:
def my_var(x):
    m = sum(x)/len(x)
    return sum([(element-m)**2 for element in x])/(len(x)-1)
113/10:
def my_sd(x):
    return my_var(x)**(0.5)
113/11: my_var([1,2,3,4,5])
113/12: my_var(hwg_df.Height)
113/13: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()
113/14: my_sd(hwg_df.Height) - hwg_df.Height.std()
113/15: df = hwg_df
113/16: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()
113/17: from scipy import stats
113/18: stats.percentileofscore(df.Height, 158.8)
113/19: stats.percentileofscore(df.Height, 178.34)
113/20: 82.5-17.31
113/21: df.describe()
113/22: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])
113/23:
kde = df.Height.plot.kde()
plt.title('KDE')
plt.xlabel('Height')
plt.ylabel('Probability')
113/24: df.head()
113/25:
df.Height[df.Gender=='Male'].plot.kde(label='Male')
df.Height[df.Gender=='Female'].plot.kde(label='Female')
plt.legend()
plt.xlabel('Height')
113/26:
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.legend()
plt.xlabel('Weight')
113/27:
plt.figure(figsize=(10,10))
ax1 = plt.subplot(2,1,1)
df.Weight[df.Gender=='Male'].plot.kde(label='Male')
plt.grid(True)
plt.legend()
ax2 = plt.subplot(2,1,2, sharex = ax1)
df.Weight[df.Gender=='Female'].plot.kde(label='Female')
plt.grid(True)
plt.xlabel('Weight')
plt.legend()
113/28:
mu = 10
variance = 5
sigma = variance**0.5
x = np.linspace(mu-3*sigma, mu+3*sigma, 100)
plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Gauss')
plt.plot(x, stats.cauchy.pdf(x, mu, sigma), label='Cauchy')
plt.legend()
113/29:
plt.scatter(df.Height, df.Weight)
def f1(x,a,b):
    return a*x + b

popt,pcov = curve_fit(f1, df.Height, df.Weight )
plt.plot(df.Height, f1(df.Height, *popt), color = 'red')
plt.xlabel('Height [cm]')
plt.ylabel('Weight [kg]')
113/30: popt
113/31: 1.38*160-159
113/32: colors = dict(Male='red', Female='blue')
113/33: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x]))
113/34: colors
113/35: df.dtypes
113/36: df.Gender.loc[9970]
113/37:
plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')
plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')
plt.legend()
plt.xlabel('Height')
plt.ylabel('Weight')
113/38:
#Linear regression calculation: statsmodels
x = df.Height
y = df.Weight

x = sm.add_constant(x)
model_statsmodels = sm.OLS(y, x).fit()
predictions = model_statsmodels.predict(x) # make the predictions by the model

# Print out the statistics
model_statsmodels.summary()
113/39: model_statsmodels.params
113/40:
lm = linear_model.LinearRegression()
model_sklearn = lm.fit(x,y)
113/41:
predictions = lm.predict(x)
print(predictions[0:5])
113/42: model_sklearn.coef_ #__dir__()
113/43: model_sklearn.intercept_
113/44:
difference=df.Weight - predictions
rms_model = ((difference**2).sum()/len(difference))**0.5
rms_model
113/45: (difference**2)[1:10].sum()
113/46:
mean_weight = df.Weight.mean()
mean_weight
113/47:
nominator = ((predictions-mean_weight)**2).sum()
nominator
113/48:
denominator = ((df.Weight-mean_weight)**2).sum()
denominator
113/49:
r_square = nominator/denominator
r_square
113/50:
rmse_mean = (((mean_weight - df.Weight)**2).sum()/len(df.Weight))**(0.5)
rmse_mean
113/51:
r_square_2 = 1-(rms_model/rmse_mean)
r_square_2
113/52: ((mean_weight - df.Weight)**2).sum()/len(df.Weight)
114/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm
from statsmodels.tools.eval_measures import rmse
import statsmodels.formula.api as smf

import seaborn as sns

%matplotlib inline
114/2: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\t' )
114/3: top_sites.head()
114/4:
x_min = log(top_sites.UniqueVisitors).min()
x_max = log(top_sites.UniqueVisitors).max()
minmax = [x_min, x_max]
114/5:
#regression plot from statsmodels predict function
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
plt.plot(minmax,predictions, color='red')
plt.gcf().set_size_inches(12,6)
114/6:
#regression from numpy polyfit function
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)
plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c, color='red')
plt.gcf().set_size_inches(12,6)
114/7:
x = log(top_sites.UniqueVisitors)
y = log(top_sites.PageViews)

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

y_predictions = model.predict(x)
rmse = rmse(y, y_predictions)
print('rmse: ', rmse)
model.summary()
114/8:
minmax_2 = sm.add_constant(minmax)
predictions = model.predict(minmax_2)
predictions
114/9: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish)
114/10: z = sns.lmplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)
114/11: logs.info()
114/12:
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']
logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)
#x = sm.add_constant(x)

#model = sm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ (InEnglish_ord)', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
114/13: logs.head()
114/14: logs.info()
114/15:
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']
logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)
#x = sm.add_constant(x)

#model = sm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~UniqueVisitors + InEnglish_ord', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
114/16:
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']
logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)
#x = sm.add_constant(x)

#model = sm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~UniqueVisitors + InEnglish', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
114/17:
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']
logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)
#x = sm.add_constant(x)

#model = sm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ InEnglish', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
114/18:
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']
logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)
#x = sm.add_constant(x)

#model = sm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ InEnglish + UniqueVisitors', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
114/19:
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']
logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)
#x = sm.add_constant(x)

#model = sm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ UniqueVisitors', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
114/20: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish, top_sites.HasAdvertising)
114/21: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).concat(top_sites.InEnglish, top_sites.HasAdvertising)
114/22: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish, top_sites.HasAdvertising)
114/23: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish)
114/24:
#regression plot from statsmodels predict function
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
plt.grid()
plt.xlabel('log(Unique_visitors)')
plt.ylabel('log(Page_views)')
#plt.plot(minmax,predictions, color='red')
plt.gcf().set_size_inches(12,6)
114/25:
#regression from numpy polyfit function
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)
plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c, color='red')
plt.gcf().set_size_inches(12,6)
114/26: logs = pd.concat(log(top_sites[['UniqueVisitors', 'PageViews']]),(top_sites.InEnglish))
114/27: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish)
114/28: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish, top_sitez.HasAdvertising)
114/29: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish, top_sites.HasAdvertising)
114/30: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish).join(top_sites.HasAdvertising)
114/31:
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']
logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)
#x = sm.add_constant(x)

#model = sm.OLS(y,x).fit()

model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish + HasAdvertising', data=logs).fit()

#y_predictions = model.predict(x)
#rmse = rmse(y, y_predictions)
#print('rmse: ', rmse)
model.summary()
114/32: top_sites.info()
115/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm
from statsmodels.tools.eval_measures import rmse
import statsmodels.formula.api as smf

import seaborn as sns

%matplotlib inline
115/2: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\t' )
115/3: top_sites.info()
115/4:
x_min = log(top_sites.UniqueVisitors).min()
x_max = log(top_sites.UniqueVisitors).max()
minmax = [x_min, x_max]
115/5:
#regression from numpy polyfit function
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)
plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c, color='red')
plt.gcf().set_size_inches(12,6)
115/6:
x = log(top_sites.UniqueVisitors)
y = log(top_sites.PageViews)

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

y_predictions = model.predict(x)
rmse = rmse(y, y_predictions)
print('rmse: ', rmse)
model.summary()
115/7:
minmax_2 = sm.add_constant(minmax)
predictions = model.predict(minmax_2)
predictions
115/8: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish).join(top_sites.HasAdvertising)
115/9: z = sns.lmplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)
115/10: logs.info()
115/11:
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']

model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish + HasAdvertising', data=logs).fit()

model.summary()
115/12: logs.head()
115/13:
z = sns.lmplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)
help(z)
115/14: z = sns.lmplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)
115/15:
#multiple regression model with statsmodels -formula interface
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']

model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish + HasAdvertising', data=logs).fit()

model.summary()
115/16:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm
from statsmodels.tools.eval_measures import rmse
import statsmodels.formula.api as smf

from sklearn import linearmodel

import seaborn as sns

%matplotlib inline
115/17:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm
from statsmodels.tools.eval_measures import rmse
import statsmodels.formula.api as smf

from sklearn import linear_model

import seaborn as sns

%matplotlib inline
115/18:
y_sk = logs.PageViews
x_sk = logs.UniqueVisitors
lm = linera_model.LinearRegression()
model = lm.fit(x,y)
lm.coef()
115/19:
y_sk = logs.PageViews
x_sk = logs.UniqueVisitors
lm = linear_model.LinearRegression()
model = lm.fit(x,y)
lm.coef()
115/20:
y_sk = logs.PageViews
x_sk = logs.UniqueVisitors
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
lm.coef()
115/21:
y_sk = [1,2,3,4] #logs.PageViews
x_sk = [1,4,6,8] #logs.UniqueVisitors
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
lm.coef()
115/22:
Stock_Market = {'Year': [2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016],
                'Month': [12, 11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1],
                'Interest_Rate': [2.75,2.5,2.5,2.5,2.5,2.5,2.5,2.25,2.25,2.25,2,2,2,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75],
                'Unemployment_Rate': [5.3,5.3,5.3,5.3,5.4,5.6,5.5,5.5,5.5,5.6,5.7,5.9,6,5.9,5.8,6.1,6.2,6.1,6.1,6.1,5.9,6.2,6.2,6.1],
                'Stock_Index_Price': [1464,1394,1357,1293,1256,1254,1234,1195,1159,1167,1130,1075,1047,965,943,958,971,949,884,866,876,822,704,719]        
                }

df = pd.DataFrame(Stock_Market,columns=['Year','Month','Interest_Rate','Unemployment_Rate','Stock_Index_Price'])

X = df[['Interest_Rate','Unemployment_Rate']] # here we have 2 variables for multiple regression. If you just want to use one variable for simple linear regression, then use X = df['Interest_Rate'] for example.Alternatively, you may add additional variables within the brackets
Y = df['Stock_Index_Price']
115/23: X
115/24: Y
115/25:
y_sk = logs.PageViews
x_sk = logs.UniqueVisitors
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
lm.coef()
115/26:
y_sk = logs.PageViews.to_numpy()
x_sk = logs.UniqueVisitors.to_numpy()
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
lm.coef()
115/27:
y_sk = logs.PageViews.as_matrix()
x_sk = logs.UniqueVisitors.as_matrix()
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
lm.coef()
115/28:
y_sk = logs.PageViews.reshape(-1,1)
x_sk = logs.UniqueVisitors.reshape(-1,1)
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
lm.coef()
115/29:
y_sk = logs.PageViews#.reshape(-1,1)
x_sk = logs.UniqueVisitors#.reshape(-1,1)
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
lm.coef()
115/30:
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs.UniqueVisitors.values.reshape(-1,1)
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
lm.coef()
115/31:
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs.UniqueVisitors.values.reshape(-1,1)
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
lm.coef_()
115/32:
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs.UniqueVisitors.values.reshape(-1,1)
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
lm.coef_
115/33:
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs.UniqueVisitors.values.reshape(-1,1)
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
lm.coef_, lm.intercept_
115/34:
#linear regression with scikit-learn
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs.UniqueVisitors.values.reshape(-1,1)
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
print('model coefficient and intereception: ',lm.coef_, lm.intercept_)
115/35:
#linear regression with scikit-learn
y_sk = logs.PageViews.values.to_numpy()
x_sk = logs.UniqueVisitors.values.reshape(-1,1)
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)
115/36:
#linear regression with scikit-learn
y_sk = pd.DataFrame(logs.PageViews).to_numpy()#.values.to_numpy()
x_sk = logs.UniqueVisitors.values.reshape(-1,1)
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)
115/37: pd.DataFrame(logs.PageViews)
115/38: pd.DataFrame(logs.PageViews).to_numpy()
115/39:
#linear regression with scikit-learn
y_sk = logs.PageViews.values.to_numpy()
x_sk = logs.UniqueVisitors.values.reshape(-1,1)
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)
115/40:
#linear regression with scikit-learn
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs.UniqueVisitors.values.reshape(-1,1)
lm = linear_model.LinearRegression()
model = lm.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)
115/41: y_sk
115/42: y_sk, x_sk
115/43: pd.version()
115/44:
#linear regression with scikit-learn
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs.UniqueVisitors.values.reshape(-1,1)
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)
115/45: model_sk.predict(x_sk)
115/46:
#linear regression with scikit-learn
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs.UniqueVisitors.values.reshape(-1,1)
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)
print('r2 for the model: ', model_sk.score())
115/47:
#linear regression with scikit-learn
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs.UniqueVisitors.values.reshape(-1,1)
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)
print('r2 for the model: ', model_sk.score)
115/48:
#linear regression with scikit-learn
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs.UniqueVisitors.values.reshape(-1,1)
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
115/49:
#linear regression with scikit-learn
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs['UniqueVisitors','InEnglish'].values.reshape(-1,1)
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
115/50:
#linear regression with scikit-learn
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs['UniqueVisitors','InEnglish']
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
115/51:
#linear regression with scikit-learn
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs[['UniqueVisitors','InEnglish']]
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
115/52:
#linear regression with scikit-learn
dummies = logs.get_dummies(logs.InEnglish, drop_first=True)
logs.join(dummies)
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs[['UniqueVisitors','InEnglish']]
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
115/53:
#linear regression with scikit-learn
dummies = pd.get_dummies(data=logs.InEnglish, drop_first=True)
logs.join(dummies)
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs[['UniqueVisitors','InEnglish']]
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
115/54:
#linear regression with scikit-learn
dummies = pd.get_dummies(data=logs.InEnglish, drop_first=True)
logs.join(dummies)
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = dummies#logs[['UniqueVisitors','InEnglish']]
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
115/55: dummies
115/56:
#linear regression with scikit-learn
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs.UniqueVisitors.values.reshape(-1,1)
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
115/57:
#linear regression with scikit-learn
dummies = pd.get_dummies(data=logs.InEnglish, drop_first=True)
logs.join(dummies)
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = dummies#logs[['UniqueVisitors','InEnglish']]
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
115/58: logs
115/59: logs.join(dummies)
115/60:
#linear regression with scikit-learn
dummies = pd.get_dummies(data=logs.InEnglish, drop_first=True)
logs=logs.join(dummies)
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs[['UniqueVisitors','Yes']]
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
115/61:
#linear regression with scikit-learn
dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)
logs=logs.join(dummies_english)

dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)
logs=logs.join(dummies_english)

y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs[['UniqueVisitors','Yes']]
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
115/62: logs
115/63:
dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)
logs=logs.join(dummies_english)
logs
115/64:
dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)
logs=logs.join(dummies_advertising)
logs
115/65:
#linear regression with scikit-learn
dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)
logs=logs.join(dummies_english)
logs.rename(columns={'Yes': 'English_yes'})

dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)
logs=logs.join(dummies_advertising)

y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs[['UniqueVisitors','Yes']]
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
115/66:
#linear regression with scikit-learn
dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)
logs=logs.join(dummies_english)
logs.rename(columns={'Yes': 'English_yes'})

dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)
logs=logs.join(dummies_advertising)
logs.rename(columns={'Yes': 'Advertising_yes'})

y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs[['UniqueVisitors','Yes']]
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
115/67:
#linear regression with scikit-learn
dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)
logs=logs.join(dummies_english)
logs.rename(columns={'Yes': 'English_yes'})
115/68: logs.head()
115/69: logs.drop_column('Yes')
115/70: logs.drop(column=['Yes'])
115/71: logs.drop(columns=['Yes'])
115/72:
#linear regression with scikit-learn
dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)
logs=logs.join(dummies_english)
logs.rename(columns={'Yes': 'English_yes'})
115/73: logs.drop(columns=['Yes'], inplace=True)
115/74:
#linear regression with scikit-learn
dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)
logs=logs.join(dummies_english)
logs.rename(columns={'Yes': 'English_yes'})
115/75:
dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)
logs=logs.join(dummies_advertising)
logs.rename(columns={'Yes': 'Advertising_yes'})

y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs[['UniqueVisitors','English_yes']]
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
115/76: logs.head()
115/77: logs.drop(columns=['Yes'], inplace=True)
115/78:
#multiple linear regression with scikit-learn
dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)
logs=logs.join(dummies_english)
logs.rename(columns={'Yes': 'English_yes'}, inplae=True)
115/79:
#multiple linear regression with scikit-learn
dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)
logs=logs.join(dummies_english)
logs.rename(columns={'Yes': 'English_yes'}, inplace=True)
115/80: logs.drop(columns=['Yes'], inplace=True)
115/81:
#multiple linear regression with scikit-learn
dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)
logs=logs.join(dummies_english)
logs.rename(columns={'Yes': 'English_yes'}, inplace=True)
115/82:
dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)
logs=logs.join(dummies_advertising)
logs.rename(columns={'Yes': 'Advertising_yes'}, inplace=True)

y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs[['UniqueVisitors','English_yes']]
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
115/83:
dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)
logs=logs.join(dummies_advertising)
logs.rename(columns={'Yes': 'Advertising_yes'}, inplace=True)

y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs[['UniqueVisitors','English_yes', 'Advertising_yes']]
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
117/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm
from statsmodels.tools.eval_measures import rmse
import statsmodels.formula.api as smf

from sklearn import linear_model

import seaborn as sns

%matplotlib inline
117/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm
from statsmodels.tools.eval_measures import rmse
import statsmodels.formula.api as smf

from sklearn import linear_model

import seaborn as sns

%matplotlib inline
117/3: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\t' )
117/4:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm
from statsmodels.tools.eval_measures import rmse
import statsmodels.formula.api as smf

from sklearn import linear_model

import seaborn as sns

%matplotlib inline
117/5:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm
from statsmodels.tools.eval_measures import rmse
import statsmodels.formula.api as smf

from sklearn import linear_model

import seaborn as sns

%matplotlib inline
117/6:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm
from statsmodels.tools.eval_measures import rmse
import statsmodels.formula.api as smf

from sklearn import linear_model

import seaborn as sns

%matplotlib inline
117/7:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy import log

import statsmodels.api as sm
from statsmodels.tools.eval_measures import rmse
import statsmodels.formula.api as smf

from sklearn import linear_model

import seaborn as sns

%matplotlib inline
117/8: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\t' )
117/9: top_sites.info()
117/10:
x_min = log(top_sites.UniqueVisitors).min()
x_max = log(top_sites.UniqueVisitors).max()
minmax = [x_min, x_max]
117/11:
#regression from numpy polyfit function
plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))
m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)
plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c, color='red')
plt.gcf().set_size_inches(12,6)
117/12:
#regression with statsmodels
x = log(top_sites.UniqueVisitors)
y = log(top_sites.PageViews)

x = sm.add_constant(x)

model = sm.OLS(y,x).fit()

y_predictions = model.predict(x)
rmse = rmse(y, y_predictions)
print('rmse: ', rmse)
model.summary()
117/13:
minmax_2 = sm.add_constant(minmax)
predictions = model.predict(minmax_2)
predictions
117/14: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish).join(top_sites.HasAdvertising)
117/15: z = sns.lmplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)
117/16:
#multiple regression model with statsmodels -formula interface
x = logs[['UniqueVisitors', 'InEnglish']]
y = logs['PageViews']

model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish + HasAdvertising', data=logs).fit()

model.summary()
117/17: logs.head()
117/18:
#linear regression with scikit-learn
y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs.UniqueVisitors.values.reshape(-1,1)
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
117/19:
#multiple linear regression with scikit-learn
dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)
logs=logs.join(dummies_english)
logs.rename(columns={'Yes': 'English_yes'}, inplace=True)
117/20:
dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)
logs=logs.join(dummies_advertising)
logs.rename(columns={'Yes': 'Advertising_yes'}, inplace=True)

y_sk = logs.PageViews.values.reshape(-1,1)
x_sk = logs[['UniqueVisitors','English_yes', 'Advertising_yes']]
lm_sk = linear_model.LinearRegression()
model_sk = lm_sk.fit(x_sk,y_sk)
print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)
print('r2 for the model: ', model_sk.score(x_sk, y_sk))
117/21: logs.head()
117/22: z = sns.lmplot(x='Log UniqueVisitors', y='Log PageViews',hue='InEnglish', data=logs)
117/23: z = sns.lmplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)
117/24:
model = smf.ols(formula='PageViews ~ UniqueVisitors', data=logs).fit()

model.summary()
117/25:
model = smf.ols(formula='PageViews ~InEnglish', data=logs).fit()

model.summary()
117/26:
#Linear regression -Unique visitors correlation
model = smf.ols(formula='PageViews ~ UniqueVisitors', data=logs).fit()
model.summary()
117/27:
#Linear regression -In English correlation
model = smf.ols(formula='PageViews ~InEnglish', data=logs).fit()
model.summary()
117/28:
#Linear regression -Has advertising correlation
model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()
model.summary()
117/29: np.correlate(logs.UniqueVisitors, logs.PageViews)
117/30: np.corrcoef(logs.UniqueVisitors, logs.PageViews)
117/31: 0.6794**2
117/32:
#Współczynnik korelacji
np.corrcoef(logs.UniqueVisitors, logs.PageViews)
117/33:
#Linear regression -Has advertising correlation
model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()
model.summary()
model.raquared
117/34:
#Linear regression -Has advertising correlation
model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()
model.summary()
model.raquared()
117/35:
#Linear regression -Has advertising correlation
model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()
model.summary()
model.rsquared()
117/36:
#Linear regression -Has advertising correlation
model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()
model.summary()
model.rsquared
117/37:
#Linear regression -Has advertising correlation
model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()
print(model.summary())
model.rsquared
117/38:
#Linear regression -Has advertising correlation
model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()
print(model.summary())
#r2 parameter  = model.rsquared
117/39:
#Linear regression -Has advertising correlation
model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()
print(model.summary())
#r2 parameter  = model.rsquared
model.resid_pearson
117/40:
#Linear regression -Has advertising correlation
model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()
print(model.summary())
#r2 parameter  = model.rsquared
118/1:
import pandas as pd
import numpy as np
118/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
118/3: normal_distribution = np.random.normal()
118/4: plt.plot(normal_distribution)
118/5: plt.scatter(normal_distribution)
118/6: normal_distribution
118/7: normal_distribution = np.random.normal(0, 0.1, 100)
118/8: plt.scatter(normal_distribution)
118/9: normal_distribution
118/10: x = pd.series[list(range(100))]
118/11: x = pd.Series[list(range(100))]
118/12: x = pd.Series(list(range(100)))
118/13: x
118/14: normal_distribution = np.random.normal(0, 0.1, len(x))
118/15: plt.scatter(normal_distribution)
118/16: normal_distribution
118/17: plt.scatter(x,normal_distribution)
118/18: normal_distribution = np.random.normal(0, 1, len(x))
118/19: plt.scatter(x,normal_distribution)
118/20: y = 1 - x**2 + normal_distribution
118/21: plt.scatter(x,y)
118/22: x = pd.Series(list(range(-10, 10)))
118/23: normal_distribution = np.random.normal(0, 1, len(x))
118/24: y = 1 - x**2 + normal_distribution
118/25: plt.scatter(x,y)
118/26: x = pd.Series(list(range(-10, 10, 0.1)))
118/27: x = pd.Series(np.arange(-10, 10, 0.1))
118/28: normal_distribution = np.random.normal(0, 1, len(x))
118/29: y = 1 - x**2 + normal_distribution
118/30: plt.scatter(x,y)
118/31: x = pd.Series(np.arange(-10, 10, 0.01))
118/32: normal_distribution = np.random.normal(0, 5, len(x))
118/33: y = 1 - x**2 + normal_distribution
118/34: plt.scatter(x,y)
118/35: plt.rcParams["figure.figsize"] = (8,4)
118/36: plt.scatter(x,y)
118/37: plt.rcParams["figure.figsize"] = (12,6)
118/38: plt.scatter(x,y)
118/39:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
118/40:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid = True
118/41:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
118/42:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid
118/43:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid(True)
118/44:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid(b=True)
118/45:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
118/46:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
118/47: plt.rcParams["figure.figsize"] = (12,6)
118/48: x = pd.Series(np.arange(-10, 10, 0.01))
118/49: normal_distribution = np.random.normal(0, 5, len(x))
118/50: y = 1 - x**2 + normal_distribution
118/51:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
119/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
119/2: plt.rcParams["figure.figsize"] = (12,6)
119/3: x = pd.Series(np.arange(-10, 10, 0.01))
119/4: normal_distribution = np.random.normal(0, 5, len(x))
119/5: y = 1 - x**2 + normal_distribution
119/6:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
119/7: normal_distribution
119/8: x
119/9:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=2)
plt.plot(x, m*x+c, color='red')
plt.grid()
119/10:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
119/11: x_squared = x**2
119/12: plt.scatter(x_squared, y)
120/1:
m,c = np.polyfit(x_squared, y)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y_model)
r**2
120/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
120/3: plt.rcParams["figure.figsize"] = (12,6)
120/4: x = pd.Series(np.arange(-10, 10, 0.01))
120/5: normal_distribution = np.random.normal(0, 5, len(x))
120/6: y = 1 - x**2 + normal_distribution
120/7:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
120/8: x_squared = x**2
120/9: plt.scatter(x_squared, y)
120/10:
m,c = np.polyfit(x_squared, y)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y_model)
r**2
120/11:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y_model)
r**2
120/12:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y_model)
print(r)
120/13: y_model - x_squared
120/14: y_model - y
120/15:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
120/16: (y_model - y).mean()
120/17: (y_model - y).max()
120/18: df = pd.DataFrame(x_squared, y)
120/19: df = pd.DataFrame(x_squared, y, columns=['x_squared', 'y'])
120/20: df = pd.DataFrame([x_squared, y], columns=['x_squared', 'y'])
120/21: df
120/22: df = pd.DataFrame(x_squared, columns=['x_squared'])
120/23: df
120/24: df = pd.DataFrame(x_squared).join(y, inplace=True)
120/25: df = pd.DataFrame(x_squared).join(y)
120/26: y
120/27: df = pd.DataFrame({'x_squared': x_squared, 'y': y})
120/28: df
120/29:
model = smf.ols(formula='y ~ x_squared', model=df)
model.summary()
120/30:
model = smf.ols(formula='y ~ x_squared', data=df)
model.summary()
120/31:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
120/32: x_squared.corr(y)
120/33: x_squared.corr(y)**2
120/34: x_squared.corr(y)
120/35:
#r2
x_squared.corr(y)**2
120/36:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
print(r)
120/37:
#r2
x_squared.corr(y)
120/38:
#r2
x_squared.corr(y)**2
120/39:
#r2
x_squared.corr(y_model)**2
120/40:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
#print(r)
print('m: ', m, ', c: ', c)
120/41:
model = smf.ols(formula='y_model ~ x_squared', data=df).fit()
model.summary()
120/42:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
120/43: my_r2 = (y - y.mean()).sum()
120/44: my_r2
120/45: my_r2 = (y - y_model).sum() / (y - y.mean()).sum()
120/46: my_r2
120/47: my_r2 = (y_model - y.mean()).sum() / (y - y.mean()).sum()
120/48: my_r2
120/49: y_model - y.mean()
120/50: (y_model - y.mean()).sum()
120/51:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
120/52: plt.rcParams["figure.figsize"] = (12,6)
120/53: x = pd.Series(np.arange(-10, 10, 0.1))
120/54: normal_distribution = np.random.normal(0, 5, len(x))
120/55: y = 1 - x**2 + normal_distribution
120/56:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
120/57: x_squared = x**2
120/58: plt.scatter(x_squared, y)
120/59:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
#print(r)
print('m: ', m, ', c: ', c)
120/60:
#r2
x_squared.corr(y_model)**2
120/61: df = pd.DataFrame({'x_squared': x_squared, 'y': y})
120/62: df
120/63:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
120/64: my_r2 = (y_model - y.mean()).sum() / (y - y.mean()).sum()
120/65: my_r2
120/66: (y_model - y.mean()).sum()
120/67: (y-y.mean()).sum()
120/68: (y_model - y.mean())**2.sum()
120/69: ((y_model - y.mean())**2).sum()
120/70: my_r2 = ((y_model - y.mean()))**2.sum() / ((y - y.mean()))**2.sum()
120/71: my_r2 = ((y_model - y.mean()))**2.sum() / ((y - y.mean()))**2.sum()
120/72: my_r2 = ((y_model - y.mean())**2.sum()) / ((y - y.mean())**2).sum()
120/73: my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())
120/74: my_r2
120/75: my_r2**(0.5)
120/76: my_r2
120/77:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
print(r)
print('m: ', m, ', c: ', c)
120/78:
r2
#x_squared.corr(y_model)**2
120/79:
r**2
#x_squared.corr(y_model)**2
120/80:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
120/81: plt.rcParams["figure.figsize"] = (12,6)
120/82: x = pd.Series(np.arange(-10, 10, 0.01))
120/83: normal_distribution = np.random.normal(0, 5, len(x))
120/84: y = 1 - x**2 + normal_distribution
120/85:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
120/86: x_squared = x**2
120/87: plt.scatter(x_squared, y)
120/88:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
print(r)
print('m: ', m, ', c: ', c)
120/89: r**2
120/90: df = pd.DataFrame({'x_squared': x_squared, 'y': y})
120/91: df
120/92:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
120/93: my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())
120/94: my_r2
120/95:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef([x_squared, y])
print(r)
print('m: ', m, ', c: ', c)
120/96:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef([x_squared, y_model])
print(r)
print('m: ', m, ', c: ', c)
120/97: x = pd.Series(np.arange(-10, 10, 0.1))
120/98: normal_distribution = np.random.normal(0, 5, len(x))
120/99: y = 1 - x**2 + normal_distribution
120/100:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
120/101: x_squared = x**2
120/102: plt.scatter(x_squared, y)
120/103:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef([x_squared, y_model])
print(r)
print('m: ', m, ', c: ', c)
120/104: r**2
120/105: r**2
120/106: df = pd.DataFrame({'x_squared': x_squared, 'y': y})
120/107: df
120/108:
model = smf.ols(formula='y_model ~ x_squared', data=df).fit()
model.summary()
120/109:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
120/110:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef([x_squared, y])
print(r)
print('m: ', m, ', c: ', c)
120/111: r**2
120/112: df = pd.DataFrame({'x_squared': x_squared, 'y': y})
120/113: df
120/114:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
120/115: my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())
120/116: my_r2
120/117: df.head()
120/118:
plt.scatter(x_squared, y)
plt.plot(x_squared, y_model)
120/119:
plt.scatter(x_squared, y)
plt.plot(x_squared, y_model, color = 'red')
122/1: df.head()
122/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
122/3: plt.rcParams["figure.figsize"] = (12,6)
122/4: x = pd.Series(np.arange(-10, 10, 0.01))
122/5: normal_distribution = np.random.normal(0, 5, len(x))
122/6: y = 1 - x**2 + normal_distribution
122/7:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
122/8: x_squared = x**2
122/9: plt.scatter(x_squared, y)
122/10:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
print(r)
print('m: ', m, ', c: ', c)
122/11: r**2
122/12: df = pd.DataFrame({'x_squared': x_squared, 'y': y})
122/13: df.head()
122/14:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
122/15: my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())
122/16: my_r2
122/17:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
print('r matrix: ', r)
print('m: ', m, ', c: ', c)
122/18: r**2
122/19: x = pd.Series(np.arange(0,1,0.01))
122/20:
my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())
my_r2
122/21: np.pi
122/22: np.sin(pi)
122/23: np.sin(np.pi)
122/24: pd.pi
122/25: pd.sin(np.pi)
122/26: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))
122/27: plt.plot(x,y)
122/28: plt.scatter(x,y)
122/29: m,c = np.polyfit(x,y, deg=1)
122/30: plt.plot(x*m+c)
122/31:
cf = plt.gcf()
cf.plot(x*m+c)
122/32:
plt.gcf()
plt.plot(x*m+c)
122/33: fig, ax = plt.scatter(x,y)
122/34:
ax=plt.gca()
ax.plot(x*m+c)
122/35:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf

%matplotlib inline
122/36:
ax=plt.gca()
ax.plot(x*m+c)
122/37: plt.scatter(x,y)
122/38:
ax=plt.gca()
ax.plot(x*m+c)
122/39:
fig, ax = plt.subplot()
ax.scatter(x,y)
122/40:
fig, ax = plt.subplots()
ax.scatter(x,y)
122/41:
#ax=plt.gca()
ax.plot(x*m+c)
122/42:
#ax=plt.gca()
ax.plot(x*m+c)
plt.show()
122/43:
fig, ax = plt.subplots()
ax.scatter(x,y)
122/44: m,c = np.polyfit(x,y, deg=1)
122/45:
#ax=plt.gca()
ax.plot(x*m+c)
plt.show()
122/46:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf

#%matplotlib inline
122/47:
fig, ax = plt.subplots()
ax.scatter(x,y)
122/48:
#ax=plt.gca()
ax.plot(x*m+c)
plt.show()
122/49:
#ax=plt.gca()
ax.plot(x*m+c)
fig.show()
122/50:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf

%matplotlib inline
122/51:
#ax=plt.gca()
ax.plot(x*m+c)
fig.show()
122/52: ax.plot(x*m+c)
122/53:
%matplotlib inline
fig, ax = plt.subplots()
ax.scatter(x,y)
122/54: ax.plot(x*m+c)
122/55:
ax.plot(x*m+c)
fig
122/56: x*m+c
122/57:
ax.plot(x,x*m+c)
fig
122/58:
ax.plot(x,x*m+c)
fig
122/59:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf

%matplotlib inline
122/60: plt.rcParams["figure.figsize"] = (12,6)
122/61: x = pd.Series(np.arange(-10, 10, 0.01))
122/62: normal_distribution = np.random.normal(0, 5, len(x))
122/63: y = 1 - x**2 + normal_distribution
122/64:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
122/65: x_squared = x**2
122/66: plt.scatter(x_squared, y)
122/67:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
print('r matrix: ', r)
print('m: ', m, ', c: ', c)
122/68: r**2
122/69: df = pd.DataFrame({'x_squared': x_squared, 'y': y})
122/70: df.head()
122/71:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
122/72:
my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())
my_r2
122/73: x = pd.Series(np.arange(0,1,0.01))
122/74: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))
122/75:
%matplotlib inline
fig, ax = plt.subplots()
ax.scatter(x,y)
122/76: m,c = np.polyfit(x,y, deg=1)
122/77:
ax.plot(x,x*m+c)
fig
122/78: x*m+c
122/79:
ax.plot(x,x*m+c, color=red)
fig
122/80:
ax.plot(x,x*m+c, color='red')
fig
122/81:
fig, ax = plt.subplots()
ax.scatter(x,y)
122/82:
ax.plot(x,x*m+c, color='red')
fig
122/83:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model

%matplotlib inline
122/84: sk_model = linear_model()
122/85: sk_model = linear_model.linear_regression()
122/86: sk_model = linear_model.LinearRegression()
122/87: sk_model(x,y).fit()
122/88: sk_model.fit(x,y)
122/89:
y_sk = y.reshape(-1,1)
sk_model.fit(x,y)
122/90:
y_sk = y.values.reshape(-1,1)
sk_model.fit(x,y)
122/91:
y_sk = y.values.reshape(-1,1)
x.sk = x.values.reshape(-1,1)
sk_model.fit(x,y)
122/92: x_sk
122/93:
y_sk = y.values.reshape(-1,1)
x_sk = x
sk_model.fit(x_sk,y_sk)
122/94: x_sk
122/95: y_sk
122/96:
y_sk = y.values.reshape(-1,1)
x_sk = x_sk.values.reshape(-1,1)
sk_model.fit(x_sk,y_sk)
122/97:
y_sk = y.values.reshape(-1,1)
x_sk = x_sk.values.reshape(-1,1)
sk_result = sk_model.fit(x_sk,y_sk)
122/98:
y_sk = y.values.reshape(-1,1)
x_sk = x.values.reshape(-1,1)
sk_result = sk_model.fit(x_sk,y_sk)
122/99: x_sk
122/100:
y_sk = y.values.reshape(-1,1)
x_sk = x.values.reshape(-1,1)
sk_result = sk_model.fit(x_sk,y_sk)
print('r2 for sk_model: ', sk_result.score(x_sk,y_sk))
122/101:
y_sk = y.values.reshape(-1,1)
x_sk = x.values.reshape(-1,1)
sk_result = sk_model.fit(x_sk,y_sk)
print('r2 for sk_model: ', sk_result.score(x_sk,y_sk))
print('coeffs for sk_model: ', sk_result.coef_)
122/102:
m,c = np.polyfit(x,y, deg=1)
print(m,c)
122/103:
y_sk = y.values.reshape(-1,1)
x_sk = x.values.reshape(-1,1)
sk_result = sk_model.fit(x_sk,y_sk)
print('r2 for sk_model: ', sk_result.score(x_sk,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/104:
m,c = np.polyfit(x,y, deg=1)
print('Coeff and intercept for np model: 'm,c)
122/105:
m,c = np.polyfit(x,y, deg=1)
print('Coeff and intercept for np model: ',m,c)
122/106:
fig, ax = plt.subplots()
ax.scatter(x,y)
fig.grid()
122/107:
fig, ax = plt.subplots()
ax.scatter(x,y)
ax.grid()
122/108:
ax.plot(x,x*m+c, color='red')
fig
122/109: x_sk2 = x_sk**2
122/110: x_sk2
122/111:
X = [x_sk, x_sk2]
X
122/112:
y_sk = y.values.reshape(-1,1)
x_sk = x.values.reshape(-1,1)
sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(x_sk,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/113: X = pd.DataFrame({'x1':x_sk, 'x2': x_sk2})
122/114: x_sk
122/115: help(x_sk)
122/116: X = np.concatenate(x_sk, x_sk2)
122/117: X = np.concatenate(x_sk, x_sk2, axis=1)
122/118: X = np.concatenate((x_sk, x_sk2), axis=1)
122/119: X
122/120:
y_sk = y.values.reshape(-1,1)
x_sk = x.values.reshape(-1,1)
sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(x_sk,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/121:
y_sk = y.values.reshape(-1,1)
x_sk = x.values.reshape(-1,1)
sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/122:
x_sk2 = x_sk**2
x_sk3 = x_sk**3
122/123: X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)
122/124: X
122/125:
y_sk = y.values.reshape(-1,1)
x_sk = x.values.reshape(-1,1)
sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/126: X.head()
122/127: np.concatenate(X, x_sk)
122/128: np.concatenate(x_sk, x_sk)
122/129: np.concatenate(x_sk, x_sk2)
122/130:
y_sk = y.values.reshape(-1,1)

x_sk = x.values.reshape(-1,1)
x_sk2 = x_sk**2
x_sk3 = x_sk**3
X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)

sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/131: np.concatenate(x_sk, x_sk2)
122/132: np.concatenate((X, x_sk2), axis=1)
122/133:
X = x_sk
for i in range(2,15):
    xi = x_sk**i
    X = np.concatenate((X, xi), axis=1)
122/134: X
122/135:
X = x_sk
for i in range(2,15):
    xi = x_sk**i
    X = np.concatenate((X, xi), axis=1)
    
sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/136:
ax.plot(x, sk_result.predict(x))
fig
122/137:
ax.plot(x_sk, sk_result.predict(x_sk))
fig
122/138: sk_result.predict(x_sk)
122/139: x_sk
122/140: sk_result.predict([0])
122/141:
ax.plot(x_sk, sk_result.predict(X))
fig
122/142:
ax.plot(x_sk, sk_result.predict(X), color='green')
fig
122/143:
ax.plot(x_sk, sk_result.predict(X), color='green')
fig
122/144:
ax.plot(x_sk, sk_result.predict(X), color='red')
fig
122/145:
ax.plot(x_sk, sk_result.predict(X), color='red', label='pow 14')
fig
122/146: help(fig)
122/147:
ax.plot(x_sk, sk_result.predict(X), color='red', label='pow 14')
ax.legend()
fig.leg
122/148:
ax.plot(x_sk, sk_result.predict(X), color='red', label='pow 14')
ax.legend()
fig
122/149:
ax.plot(x,x*m+c, color='red', legend='linear')
ax.legend()
fig
122/150:
ax.plot(x,x*m+c, color='red', label='linear')
ax.legend()
fig
122/151:
ax.plot(x_sk, sk_result.predict(X), color='red', label='pow 14')
ax.legend()
fig
122/152:
ax.plot(x,x*m+c, color='red', label='linear')
ax.legend()
fig
122/153:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model

%matplotlib inline
122/154: x = pd.Series(np.arange(-10, 10, 0.01))
122/155: normal_distribution = np.random.normal(0, 5, len(x))
122/156: y = 1 - x**2 + normal_distribution
122/157:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
122/158: x_squared = x**2
122/159: plt.scatter(x_squared, y)
122/160:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
print('r matrix: ', r)
print('m: ', m, ', c: ', c)
122/161: r**2
122/162: df = pd.DataFrame({'x_squared': x_squared, 'y': y})
122/163: df.head()
122/164:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
122/165:
my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())
my_r2
122/166: x = pd.Series(np.arange(0,1,0.01))
122/167: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))
122/168:
fig, ax = plt.subplots()
ax.scatter(x,y)
ax.grid()
122/169:
m,c = np.polyfit(x,y, deg=1)
print('Coeff and intercept for np model: ',m,c)
122/170:
ax.plot(x,x*m+c, color='red', label='linear')
ax.legend()
fig
122/171: sk_model = linear_model.LinearRegression()
122/172:
y_sk = y.values.reshape(-1,1)

x_sk = x.values.reshape(-1,1)
x_sk2 = x_sk**2
x_sk3 = x_sk**3
X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)

sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/173:
X = x_sk
for i in range(2,15):
    xi = x_sk**i
    X = np.concatenate((X, xi), axis=1)
    
sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/174:
ax.plot(x_sk, sk_result.predict(X), color='red',)
ax.legend()
fig
122/175: help(fig)
122/176: label
122/177:
fig, ax = plt.subplots(figsize=(12,6))
ax.scatter(x,y)
ax.grid()
122/178:
ax.plot(x,x*m+c, color='red', label='linear')
ax.legend()
fig
122/179:
ax.plot(x_sk, sk_result.predict(X), color='green', label=label.append)
ax.legend()
fig
122/180:
ax.plot(x_sk, sk_result.predict(X), color='green')
ax.legend()
fig
122/181:
fig, ax = plt.subplots(figsize=(12,6))
line, = ax.scatter(x,y)
line.set_label('signal')
ax.grid()
122/182:
fig, ax = plt.subplots(figsize=(12,6))
line = ax.scatter(x,y)
line.set_label('signal')
ax.grid()
122/183:
fig, ax = plt.subplots(figsize=(12,6))
line = ax.scatter(x,y)
line.set_label('signal')
ax.legend()
ax.grid()
122/184:
ax.plot(x,x*m+c, color='red', label='linear')
ax.legend()
fig
122/185:
ax.plot(x,x*m+c, color='red', label='linear regression')
ax.legend()
fig
122/186:
fig, ax = plt.subplots(figsize=(12,6))
line = ax.scatter(x,y)
line.set_label('signal')
ax.legend()
ax.grid()
122/187:
line = ax.plot(x,x*m+c, color='red')
line.set_label('linear regression')
ax.legend()
fig
122/188:
line, = ax.plot(x,x*m+c, color='red')
line.set_label('linear regression')
ax.legend()
fig
122/189:
line, = ax.plot(x_sk, sk_result.predict(X), color='green')
line.set_label('polynomial regression')
ax.legend()
fig
122/190:
fig, ax = plt.subplots(figsize=(12,6))
ax.set_xlabel('x')
line = ax.scatter(x,y)
line.set_label('signal')
ax.legend()
ax.grid()
122/191:
fig, ax = plt.subplots(figsize=(12,6))
ax.set_xlabel('x')
ax.set_ylabel('y')
line = ax.scatter(x,y)
line.set_label('signal')
ax.legend()
ax.grid()
122/192:
m,c = np.polyfit(x,y, deg=1)
print('Coeff and intercept for np model: ',m,c)
122/193:
line, = ax.plot(x,x*m+c, color='red')
line.set_label('linear regression')
ax.legend()
fig
122/194: sk_model = linear_model.LinearRegression()
122/195:
y_sk = y.values.reshape(-1,1)

x_sk = x.values.reshape(-1,1)
x_sk2 = x_sk**2
x_sk3 = x_sk**3
X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)

sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/196:
X = x_sk
for i in range(2,15):
    xi = x_sk**i
    X = np.concatenate((X, xi), axis=1)
    
sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/197:
line, = ax.plot(x_sk, sk_result.predict(X), color='green')
line.set_label('polynomial regression')
ax.legend()
fig
122/198: X_ort = PolynomialFeature(2).fit(x_sk)
122/199:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model
from sklearn.preprocessing import PolynomialFeature

%matplotlib inline
122/200:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model
from sklearn.preprocessing import PolynomialFeatures

%matplotlib inline
122/201: X_ort = PolynomialFeature(2).fit(x_sk)
122/202: X_ort = PolynomialFeatures(2).fit(x_sk)
122/203: X_ort
122/204: X_ort = PolynomialFeatures(2).fit_transform(x_sk)
122/205: X_ort
122/206:
sk_ort_result = sk_model.fit(X_ort,y_sk)
print('r2 for sk_model: ', sk_ort_result.score(X_ort,y_sk))
print('coeff and intercept for sk_model: ', sk_ort_result.coef_, sk_ort_result.intercept_)
122/207:
line, = ax.plot(x_sk, sk_ort_result.predict(X_ort), color='orange')
line.set_label('ort polynomial regression')
ax.legend()
fig
122/208: X_ort = PolynomialFeatures(3).fit_transform(x_sk)
122/209:
sk_ort_result = sk_model.fit(X_ort,y_sk)
print('r2 for sk_model: ', sk_ort_result.score(X_ort,y_sk))
print('coeff and intercept for sk_model: ', sk_ort_result.coef_, sk_ort_result.intercept_)
122/210:
line, = ax.plot(x_sk, sk_ort_result.predict(X_ort), color='orange')
line.set_label('ort polynomial regression')
ax.legend()
fig
122/211: ax.lines
122/212: ax.lines[2].pop()
122/213: ax.lines.pop(2)
122/214: fig
122/215: ax.labels()
122/216: ax.label
122/217: ax.lagend
122/218: ax.legend()
122/219: ax.legend.pop(2)
122/220: help(ax.legend)
122/221: help(ax.legends)
122/222: help(ax.legend)
122/223: ax.legend.labels()
122/224: ax.legend.labels
122/225: ax.legend().labels
122/226: h,l - ax.get_legend_handels_labels()
122/227: h,l = ax.get_legend_handels_labels()
122/228: h,l = ax.get_legend_handles_labels()
122/229: h
122/230: l
122/231: fig
122/232: fig
122/233:
line, = ax.plot(x_sk, sk_ort_result.predict(X_ort), color='orange')
line.set_label('ort polynomial regression')
ax.legend()
fig
122/234: fig
122/235:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model
from sklearn.preprocessing import PolynomialFeatures

%matplotlib inline
122/236: x = pd.Series(np.arange(-10, 10, 0.01))
122/237: normal_distribution = np.random.normal(0, 5, len(x))
122/238: y = 1 - x**2 + normal_distribution
122/239:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
122/240: x_squared = x**2
122/241: plt.scatter(x_squared, y)
122/242:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
print('r matrix: ', r)
print('m: ', m, ', c: ', c)
122/243: r**2
122/244: df = pd.DataFrame({'x_squared': x_squared, 'y': y})
122/245: df.head()
122/246:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
122/247:
my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())
my_r2
122/248: x = pd.Series(np.arange(0,1,0.01))
122/249: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))
122/250:
fig, ax = plt.subplots(figsize=(12,6))
ax.set_xlabel('x')
ax.set_ylabel('y')
line = ax.scatter(x,y)
line.set_label('signal')
ax.legend()
ax.grid()
122/251:
m,c = np.polyfit(x,y, deg=1)
print('Coeff and intercept for np model: ',m,c)
122/252:
line, = ax.plot(x,x*m+c, color='red')
line.set_label('linear regression')
ax.legend()
fig
122/253: sk_model = linear_model.LinearRegression()
122/254:
y_sk = y.values.reshape(-1,1)

x_sk = x.values.reshape(-1,1)
x_sk2 = x_sk**2
x_sk3 = x_sk**3
X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)

sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/255:
X = x_sk
for i in range(2,15):
    xi = x_sk**i
    X = np.concatenate((X, xi), axis=1)
    
sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/256:
line, = ax.plot(x_sk, sk_result.predict(X), color='green')
line.set_label('polynomial regression')
ax.legend()
fig
122/257: X_ort = PolynomialFeatures(3).fit_transform(x_sk)
122/258:
sk_ort_result = sk_model.fit(X_ort,y_sk)
print('r2 for sk_model: ', sk_ort_result.score(X_ort,y_sk))
print('coeff and intercept for sk_model: ', sk_ort_result.coef_, sk_ort_result.intercept_)
122/259:
line, = ax.plot(x_sk, sk_ort_result.predict(X_ort), color='orange')
line.set_label('ort polynomial regression')
ax.legend()
fig
122/260: ax.lines.pop(2)
122/261:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model
from sklearn.preprocessing import PolynomialFeatures

%matplotlib inline
122/262: x = pd.Series(np.arange(-10, 10, 0.01))
122/263: normal_distribution = np.random.normal(0, 5, len(x))
122/264: y = 1 - x**2 + normal_distribution
122/265:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
122/266: x_squared = x**2
122/267: plt.scatter(x_squared, y)
122/268:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
print('r matrix: ', r)
print('m: ', m, ', c: ', c)
122/269: r**2
122/270: df = pd.DataFrame({'x_squared': x_squared, 'y': y})
122/271: df.head()
122/272:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
122/273:
my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())
my_r2
122/274: x = pd.Series(np.arange(0,1,0.01))
122/275: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))
122/276:
fig, ax = plt.subplots(figsize=(12,6))
ax.set_xlabel('x')
ax.set_ylabel('y')
line = ax.scatter(x,y)
line.set_label('signal')
ax.legend()
ax.grid()
122/277:
m,c = np.polyfit(x,y, deg=1)
print('Coeff and intercept for np model: ',m,c)
122/278:
line, = ax.plot(x,x*m+c, color='red')
line.set_label('linear regression')
ax.legend()
fig
122/279: sk_model = linear_model.LinearRegression()
122/280:
y_sk = y.values.reshape(-1,1)

x_sk = x.values.reshape(-1,1)
x_sk2 = x_sk**2
x_sk3 = x_sk**3
X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)

sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/281:
X = x_sk
for i in range(2,15):
    xi = x_sk**i
    X = np.concatenate((X, xi), axis=1)
    
sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/282:
line, = ax.plot(x_sk, sk_result.predict(X), color='green')
line.set_label('polynomial regression')
ax.legend()
fig
122/283: X_ort = PolynomialFeatures(3).fit_transform(x_sk)
122/284:
sk_ort_result = sk_model.fit(X_ort,y_sk)
print('r2 for sk_model: ', sk_ort_result.score(X_ort,y_sk))
print('coeff and intercept for sk_model: ', sk_ort_result.coef_, sk_ort_result.intercept_)
122/285:
line, = ax.plot(x_sk, sk_ort_result.predict(X_ort), color='orange')
line.set_label('ort polynomial regression')
ax.legend()
fig
122/286:
line, = ax.plot(x_sk, sk_result.predict(X), color='black')
line.set_label('polynomial regression')
ax.legend()
fig
122/287:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model
from sklearn.preprocessing import PolynomialFeatures

%matplotlib inline
122/288: x = pd.Series(np.arange(-10, 10, 0.01))
122/289: normal_distribution = np.random.normal(0, 5, len(x))
122/290: y = 1 - x**2 + normal_distribution
122/291:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
122/292: x_squared = x**2
122/293: plt.scatter(x_squared, y)
122/294:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
print('r matrix: ', r)
print('m: ', m, ', c: ', c)
122/295: r**2
122/296: df = pd.DataFrame({'x_squared': x_squared, 'y': y})
122/297: df.head()
122/298:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
122/299:
my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())
my_r2
122/300: x = pd.Series(np.arange(0,1,0.01))
122/301: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))
122/302:
fig, ax = plt.subplots(figsize=(12,6))
ax.set_xlabel('x')
ax.set_ylabel('y')
line = ax.scatter(x,y)
line.set_label('signal')
ax.legend()
ax.grid()
122/303:
m,c = np.polyfit(x,y, deg=1)
print('Coeff and intercept for np model: ',m,c)
122/304:
line, = ax.plot(x,x*m+c, color='red')
line.set_label('linear regression')
ax.legend()
fig
122/305: sk_model = linear_model.LinearRegression()
122/306:
y_sk = y.values.reshape(-1,1)

x_sk = x.values.reshape(-1,1)
x_sk2 = x_sk**2
x_sk3 = x_sk**3
X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)

sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/307:
line, = ax.plot(x_sk, sk_result.predict(X), color='black')
line.set_label('polynomial regression')
ax.legend()
fig
122/308:
X = x_sk
for i in range(2,15):
    xi = x_sk**i
    X = np.concatenate((X, xi), axis=1)
    
sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
122/309:
line, = ax.plot(x_sk, sk_result.predict(X), color='green')
line.set_label('polynomial regression')
ax.legend()
fig
122/310: X_ort = PolynomialFeatures(3).fit_transform(x_sk)
122/311:
sk_ort_result = sk_model.fit(X_ort,y_sk)
print('r2 for sk_model: ', sk_ort_result.score(X_ort,y_sk))
print('coeff and intercept for sk_model: ', sk_ort_result.coef_, sk_ort_result.intercept_)
122/312:
line, = ax.plot(x_sk, sk_ort_result.predict(X_ort), color='orange')
line.set_label('ort polynomial regression')
ax.legend()
fig
122/313:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model
from sklearn.model_selection import train_test_split

%matplotlib inline
122/314:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = test_train_split(X, y, test_size=0.7)
122/315:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.7)
122/316: plt.plot(X_train,  Y_train)
122/317: plt.scater(X_train,  Y_train)
122/318: plt.scatter(X_train,  Y_train)
122/319: plt.scatter(X_test,  Y_test)
122/320: X_train.size()
122/321:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
122/322: plt.scatter(X_test,  Y_test)
122/323: plt.scatter(X_train,  Y_train)
122/324: sk_result.predict(X)
122/325: Y
122/326: y_sk
122/327:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

%matplotlib inline
122/328:
x_sk2 = x_sk**2
x_sk3 = x_sk**3
X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)
sk_result = sk_model.fit(X,y_sk)

mean_squared_error(y_sk, sk_result.predict(X))
122/329: X_train
122/330: pd.concatenate(X_train, X_train**2)
122/331: np.concatenate(X_train, X_train**2)
122/332: np.concatenate(X_train, X_train**2, axis=1)
122/333: np.concatenate((X_train, X_train**2), axis=1)
122/334:
errors = []
for i in range(2,15):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i))
    sk_result = sk_model.fit(Xtrain, y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
    plt.scatter(i, error)
122/335:
errors = []
for i in range(2,15):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i))
    sk_result = sk_model.fit(X_train, y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
    plt.scatter(i, error)
122/336:
errors = []
for i in range(2,15):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i))
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
    plt.scatter(i, error)
122/337:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
122/338:
errors = []
for i in range(2,15):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i))
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
    plt.scatter(i, error)
122/339: X_train
122/340: X_test
122/341:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
122/342:
errors = []
for i in range(2,15):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
    plt.scatter(i, error)
122/343:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
122/344:
errors = []
for i in range(2,5):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
    plt.scatter(i, error)
122/345:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))
plt.scatter(1, error)

errors = []
for i in range(2,5):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
    plt.scatter(i, error)
122/346:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))
plt.scatter(1, error)

errors = []
for i in range(2,5):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
    plt.scatter(i, error)
    plt.grid()
122/347:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))
plt.scatter(1, error)

errors = []
for i in range(2,8):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
    plt.scatter(i, error)
    plt.grid()
122/348:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))
plt.scatter(1, error)

errors = []
for i in range(2,8):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
    plt.scatter(i, error)
    plt.grid()
122/349:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))
plt.scatter(1, error)

errors = []
for i in range(2,6):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
    plt.scatter(i, error)
    plt.grid()
122/350:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))

errors = [error]
for i in range(2,6):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
plt.scatter(errors)
plt.grid()
122/351:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))

errors = [error]
for i in range(2,6):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
plt.scatter(range(6),errors)
plt.grid()
122/352: errors
122/353:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))

errors = [error]
for i in range(2,6):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
plt.scatter(range(5),errors)
plt.grid()
122/354:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 7

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))

errors = [error]
for i in range(2,MAX_POWER):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
plt.scatter(range(MAX_POWER),errors)
plt.grid()
122/355:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 7

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))

errors = [error]
for i in range(2,MAX_POWER):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
plt.scatter(range(MAX_POWER-1),errors)
plt.grid()
122/356:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 7

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))

errors = [error]
for i in range(2,MAX_POWER):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
plt.scatter(range(1,MAX_POWER),errors)
plt.grid()
122/357:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 6

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))

errors = [error]
for i in range(2,MAX_POWER):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
plt.scatter(range(1,MAX_POWER),errors)
plt.grid()
122/358:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 6

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))

errors = [error]
for i in range(2,MAX_POWER):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
plt.plot(range(1,MAX_POWER),errors)
plt.grid()
122/359:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 7

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))

errors = [error]
for i in range(2,MAX_POWER):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
plt.plot(range(1,MAX_POWER),errors)
plt.grid()
122/360:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 6

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))

errors = [error]
for i in range(2,MAX_POWER):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
plt.plot(range(1,MAX_POWER),errors)
plt.grid()
122/361:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 6

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))

errors = [error]
for i in range(2,MAX_POWER):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
plt.plot(range(1,MAX_POWER),errors)
plt.xscale('X to power n')
plt.yscale('MSE')
plt.grid()
122/362:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 6

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))

errors = [error]
for i in range(2,MAX_POWER):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
plt.plot(range(1,MAX_POWER),errors)
plt.x_label('X to power n')
plt.y_label('MSE')
plt.grid()
122/363:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 6

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))

errors = [error]
for i in range(2,MAX_POWER):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
plt.plot(range(1,MAX_POWER),errors)
plt.xlabel('X to power n')
plt.ylabel('MSE')
plt.grid()
122/364: errors
124/1: sk_result.coef_
124/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

%matplotlib inline
124/3: x = pd.Series(np.arange(-10, 10, 0.01))
124/4: normal_distribution = np.random.normal(0, 5, len(x))
124/5: y = 1 - x**2 + normal_distribution
124/6:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
124/7: x_squared = x**2
124/8: plt.scatter(x_squared, y)
124/9:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
print('r matrix: ', r)
print('m: ', m, ', c: ', c)
124/10: r**2
124/11: df = pd.DataFrame({'x_squared': x_squared, 'y': y})
124/12: df.head()
124/13:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
124/14:
my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())
my_r2
124/15: x = pd.Series(np.arange(0,1,0.01))
124/16: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))
124/17:
fig, ax = plt.subplots(figsize=(12,6))
ax.set_xlabel('x')
ax.set_ylabel('y')
line = ax.scatter(x,y)
line.set_label('signal')
ax.legend()
ax.grid()
124/18:
m,c = np.polyfit(x,y, deg=1)
print('Coeff and intercept for np model: ',m,c)
124/19:
line, = ax.plot(x,x*m+c, color='red')
line.set_label('linear regression')
ax.legend()
fig
124/20: sk_model = linear_model.LinearRegression()
124/21:
y_sk = y.values.reshape(-1,1)

x_sk = x.values.reshape(-1,1)
x_sk2 = x_sk**2
x_sk3 = x_sk**3
X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)

sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
124/22:
line, = ax.plot(x_sk, sk_result.predict(X), color='black')
line.set_label('polynomial regression: pow=3')
ax.legend()
fig
124/23:
X = x_sk
for i in range(2,15):
    xi = x_sk**i
    X = np.concatenate((X, xi), axis=1)
    
sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
124/24:
line, = ax.plot(x_sk, sk_result.predict(X), color='green')
line.set_label('polynomial regression: pow=14')
ax.legend()
fig
124/25:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
124/26:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 6

sk_result = sk_model.fit(X_train, Y_train)
error = mean_squared_error(Y_test, sk_result.predict(X_test))

errors = [error]
for i in range(2,MAX_POWER):
    X_train = np.concatenate((X_train, X_train**i), axis=1)
    X_test = np.concatenate((X_test, X_test**i), axis=1)
    sk_result = sk_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
plt.plot(range(1,MAX_POWER),errors)
plt.xlabel('X to power n')
plt.ylabel('MSE')
plt.grid()
124/27: sk_result.coef_
124/28: l2_model_complexity = sk_result.coef_
124/29: l2_model_complexity = sk_result.coef_**2.sum()
124/30: l2_model_complexity = (sk_result.coef_)**2.sum()
124/31: (sk_result.coef_)**2
124/32: sk_result.coef_
124/33: l2_model_complexity = (sk_result.coef_)[0]**2.sum()
124/34: sk_result.coef_[0]
124/35: sk_result.coef_[0].sum()
124/36: sk_result.coef_[0]**2
124/37: sk_result.coef_[0]**2.sum()
124/38: (sk_result.coef_[0]**2).sum()
124/39: l2_model_complexity = (sk_result.coef_[0]**2).sum()
124/40: l1_model_complexity = (abs(sk_result.coef_)).sum()
124/41: l1_model_complexity, l2_model_complxity
124/42: l1_model_complexity, l2_model_complexity
124/43:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from skelearn.preprocessing import PolynomialFeatures

%matplotlib inline
124/44:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures

%matplotlib inline
124/45: x_sk
124/46:
poly = PolynomialFeatures(degree=1)
poly.fit_transport(x_sk)
124/47:
poly = PolynomialFeatures(degree=1)
poly.fit_transform(x_sk)
124/48: X_sk
124/49: x_sk
124/50:
poly = PolynomialFeatures(degree=0)
poly.fit_transform(x_sk)
124/51:
poly = PolynomialFeatures(degree=1)
poly.fit_transform(x_sk)
124/52:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 6
errors = []

for i in range(1,MAX_POWER):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors.append(error)
plt.plot(range(1,MAX_POWER),errors)
plt.xlabel('X to power n')
plt.ylabel('MSE')
plt.grid()
124/53:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 6
errors = []

for i in range(1,MAX_POWER):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(1,MAX_POWER),errors)
plt.xlabel('X to power n')
plt.ylabel('MSE')
plt.grid()
124/54:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 7
errors = []

for i in range(1,MAX_POWER):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(1,MAX_POWER),errors)
plt.xlabel('X to power n')
plt.ylabel('MSE')
plt.grid()
124/55:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 10
errors = []

for i in range(1,MAX_POWER):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(1,MAX_POWER),errors)
plt.xlabel('X to power n')
plt.ylabel('MSE')
plt.grid()
124/56:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 15
errors = []

for i in range(1,MAX_POWER):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(1,MAX_POWER),errors)
plt.xlabel('X to power n')
plt.ylabel('MSE')
plt.grid()
124/57:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 15
errors = []

for i in range(4,MAX_POWER):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(1,MAX_POWER),errors)
plt.xlabel('X to power n')
plt.ylabel('MSE')
plt.grid()
124/58:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 15
errors = []

for i in range(4,MAX_POWER):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(4,MAX_POWER),errors)
plt.xlabel('X to power n')
plt.ylabel('MSE')
plt.grid()
124/59:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 20
errors = []

for i in range(4,MAX_POWER):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(4,MAX_POWER),errors)
plt.xlabel('X to power n')
plt.ylabel('MSE')
plt.grid()
124/60:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MIN_DEGREE = 1
MAX_DEGREE = 20
errors = []

for i in range(MIN_DEGREE,MAX_POWER):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)
plt.xlabel('X to power n')
plt.ylabel('MSE')
plt.grid()
124/61:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MIN_DEGREE = 3
MAX_DEGREE = 20
errors = []

for i in range(MIN_DEGREE,MAX_POWER):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)
plt.xlabel('X to power n')
plt.ylabel('MSE')
plt.grid()
124/62: sk_model_ridge = linear_model.Ridge()
125/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures

%matplotlib inline
125/2: x = pd.Series(np.arange(-10, 10, 0.01))
125/3: normal_distribution = np.random.normal(0, 5, len(x))
125/4: y = 1 - x**2 + normal_distribution
125/5:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
125/6: x_squared = x**2
125/7: plt.scatter(x_squared, y)
125/8:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
print('r matrix: ', r)
print('m: ', m, ', c: ', c)
125/9: r**2
125/10: df = pd.DataFrame({'x_squared': x_squared, 'y': y})
125/11: df.head()
125/12:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
125/13:
my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())
my_r2
125/14: x = pd.Series(np.arange(0,1,0.01))
125/15: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))
125/16:
fig, ax = plt.subplots(figsize=(12,6))
ax.set_xlabel('x')
ax.set_ylabel('y')
line = ax.scatter(x,y)
line.set_label('signal')
ax.legend()
ax.grid()
125/17:
m,c = np.polyfit(x,y, deg=1)
print('Coeff and intercept for np model: ',m,c)
125/18:
line, = ax.plot(x,x*m+c, color='red')
line.set_label('linear regression')
ax.legend()
fig
125/19: sk_model = linear_model.LinearRegression()
125/20:
y_sk = y.values.reshape(-1,1)

x_sk = x.values.reshape(-1,1)
x_sk2 = x_sk**2
x_sk3 = x_sk**3
X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)

sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
125/21:
line, = ax.plot(x_sk, sk_result.predict(X), color='black')
line.set_label('polynomial regression: pow=3')
ax.legend()
fig
125/22:
X = x_sk
for i in range(2,15):
    xi = x_sk**i
    X = np.concatenate((X, xi), axis=1)
    
sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
125/23:
line, = ax.plot(x_sk, sk_result.predict(X), color='green')
line.set_label('polynomial regression: pow=14')
ax.legend()
fig
125/24:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
125/25:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MIN_DEGREE = 3
MAX_DEGREE = 20
errors = []

for i in range(MIN_DEGREE,MAX_POWER):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)
plt.xlabel('X to power n')
plt.ylabel('MSE')
plt.grid()
125/26:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MIN_DEGREE = 3
MAX_DEGREE = 20
errors = []

for i in range(MIN_DEGREE,MAX_DEGREE):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)
plt.xlabel('X to power n')
plt.ylabel('MSE')
plt.grid()
125/27:
sk_model_ridge = linear_model.Ridge()

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 6

ridge_result = sk_model_ridge.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_result.predict(X_test))
125/28: error
125/29:
sk_model_ridge = linear_model.Ridge()

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MAX_POWER = 6

ridge_result = sk_model_ridge.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_result.predict(X_test))

print('Ridge MSE: ', error)
print('Ridge coeffs: ', ridge_result.coef_)
125/30:
l = []
l.append(a,b)
125/31:
l = []
l.append(1,2)
125/32:
l = []
l.append((1,2))
125/33:
l = []
l.append((1,2))
l.append((3,4))
125/34: l.min()
125/35: min(l)
125/36:
l = []
l.append((1,2))
l.append((3,4))
l.append(0,10)
125/37:
l = []
l.append((1,2))
l.append((3,4))
l.append((0,10))
125/38: min(l)
125/39:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,1, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errorrs_alpha[1])
#print('Ridge coeffs: ', ridge_result.coef_)
125/40:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,1, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errorrs_alpha[1])
#print('Ridge coeffs: ', ridge_result.coef_)
125/41:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,1, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errorrs_alpha[1])
#print('Ridge coeffs: ', ridge_result.coef_)
125/42:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,1, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
#print('Ridge coeffs: ', ridge_result.coef_)
125/43:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MIN_DEGREE = 3
MAX_DEGREE = 8
errors = []

for i in range(MIN_DEGREE,MAX_DEGREE):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)
plt.xlabel('X to power n')
plt.ylabel('MSE')
plt.grid()
125/44:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
#print('Ridge coeffs: ', ridge_result.coef_)
125/45:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,1, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
#print('Ridge coeffs: ', ridge_result.coef_)
125/46:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.001,1, 0.001)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
#print('Ridge coeffs: ', ridge_result.coef_)
125/47:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.00001,1, 0.001)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
#print('Ridge coeffs: ', ridge_result.coef_)
125/48: ridge_cv_result.coef_
125/49:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.1,1, 0.001)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
#print('Ridge coeffs: ', ridge_result.coef_)
125/50:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.001,1, 0.001)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
#print('Ridge coeffs: ', ridge_result.coef_)
125/51: errors_alphas
125/52: errors, alphas = zip(errors_alphas)
125/53: zip(*errors_alphas)
125/54: list(zip(*errors_alphas))
125/55: er,al = zip(*errors_alphas)
125/56: er
125/57:
er,al = zip(*errors_alphas)
plt.plot(al, er)
125/58:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.0001,0.01, 0.001)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
#print('Ridge coeffs: ', ridge_result.coef_)
125/59:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
125/60: ridge_cv_result.coef_
125/61:
er,al = zip(*errors_alphas)
plt.plot(al, er)
125/62:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.0001,1, 0.001)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
#print('Ridge coeffs: ', ridge_result.coef_)
125/63:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
125/64:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.0001,1, 0.001)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
#print('Ridge coeffs: ', ridge_result.coef_)
125/65:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
125/66:
er,al = zip(*errors_alphas)
plt.plot(al, er)
125/67:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, y_sk))
125/68:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/69:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.0001,1, 0.001)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
125/70:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.0001,1, 0.001)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
125/71:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/72:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.0001,1, 0.001)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
125/73:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/74:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/75: ridge_cv_result.coef_
125/76:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/77:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/78: ridge_cv_result.coef_
125/79:
er,al = zip(*errors_alphas)
plt.plot(al, er)
125/80:
#Ridge regression 

poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.0001,1, 0.001)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
125/81:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/82:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/83: ridge_cv_result.coef_
125/84:
er,al = zip(*errors_alphas)
plt.plot(al, er)
125/85:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.0001,1, 0.001)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
125/86:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/87:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MIN_DEGREE = 3
MAX_DEGREE = 8
errors = []

for i in range(MIN_DEGREE,MAX_DEGREE):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)
plt.xlabel('Degree of polynomial')
plt.ylabel('MSE')
plt.grid()
125/88:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,10, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
125/89:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/90:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/91:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/92:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,10, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
125/93:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,1, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
125/94: x = pd.Series(np.arange(0,1,0.001))
125/95: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))
125/96:
fig, ax = plt.subplots(figsize=(12,6))
ax.set_xlabel('x')
ax.set_ylabel('y')
line = ax.scatter(x,y)
line.set_label('signal')
ax.legend()
ax.grid()
125/97:
m,c = np.polyfit(x,y, deg=1)
print('Coeff and intercept for np model: ',m,c)
125/98:
line, = ax.plot(x,x*m+c, color='red')
line.set_label('linear regression')
ax.legend()
fig
125/99: sk_model = linear_model.LinearRegression()
125/100:
y_sk = y.values.reshape(-1,1)

x_sk = x.values.reshape(-1,1)
x_sk2 = x_sk**2
x_sk3 = x_sk**3
X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)

sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
125/101:
line, = ax.plot(x_sk, sk_result.predict(X), color='black')
line.set_label('polynomial regression: pow=3')
ax.legend()
fig
125/102:
X = x_sk
for i in range(2,15):
    xi = x_sk**i
    X = np.concatenate((X, xi), axis=1)
    
sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
125/103:
line, = ax.plot(x_sk, sk_result.predict(X), color='green')
line.set_label('polynomial regression: pow=14')
ax.legend()
fig
125/104:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
125/105:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MIN_DEGREE = 3
MAX_DEGREE = 8
errors = []

for i in range(MIN_DEGREE,MAX_DEGREE):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)
plt.xlabel('Degree of polynomial')
plt.ylabel('MSE')
plt.grid()
125/106:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,1, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
125/107:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/108: ridge_cv_result.coef_
125/109:
er,al = zip(*errors_alphas)
plt.plot(al, er)
125/110:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,1, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
125/111:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/112:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,1, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
125/113:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/114:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,1, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
125/115:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/116:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,1, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
125/117:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
125/118:
er,al = zip(*errors_alphas)
plt.plot(al, er)
plt.xlabel('alpha')
plt.ylabel('MSE')
125/119:
paht_to_file = 'Regularization/oreilly.csv'
ranks = pd.read(path_to_file)
ranks.head()
125/120:
paht_to_file = 'Regularization/oreilly.csv'
ranks = pd.read_csv(path_to_file)
ranks.head()
125/121:
path_to_file = 'Regularization/oreilly.csv'
ranks = pd.read_csv(path_to_file)
ranks.head()
125/122:
with open(path_to_file) as f:
    print(f)
125/123:
path_to_file = 'Regularization/oreilly.csv'
ranks = pd.read_csv(path_to_file, error_bad_lines=False)
ranks.head()
125/124:
with open(path_to_file) as f:
    for line in f:
        print(line)
125/125:
path_to_file = 'Regularization/oreilly.csv'
ranks = pd.read_csv(path_to_file, encoding='latin1')
ranks.head()
125/126: ranks.isnull.sum()
125/127: ranks.isnull().sum()
125/128:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_extraction.text import CountVectorizer

%matplotlib inline
125/129: count = CountVectorizer()
125/130:
bag = count(ranks[0,'BOOK title'])
bag
125/131:
bag = count(ranks.loc[0,'BOOK title'])
bag
125/132:
bag = count.fit_transform(ranks.loc[0,'BOOK title'])
bag
125/133: count.fit_transform('aa bb cc aa')
125/134: count.fit_transform(['aa bb cc aa'])
125/135: list('abc')
125/136:
x = count.fit_transform(['aa bb cc aa'])
x
125/137:
x = count.fit_transform(['aa bb cc aa'])
print(x.vocabulary_)
125/138:
x = count.fit_transform(['aa bb cc aa'])
print(count.vocabulary_)
125/139: ranks.loc([0, 'BOOK title'])
125/140: ranks.loc[0, 'BOOK title']
125/141:
bag = count.fit_transform([ranks.loc[0,'BOOK title']])
bag
125/142: count.vocabulary_
125/143:
bag = count.fit_transform([ranks.loc[0,'BOOK title']])
count.vocabulary_
125/144: count = CountVectorizer()
125/145: ranks.loc['BOOK title']
125/146:
bag = count.fit_transform([ranks['BOOK title']])
count.vocabulary_
125/147: count.fit_transform(['aa', 'bb', 'aa'])
125/148: count.vocabulary_
125/149: count.fit_transform(['aa', 'bb', 'aa', 'bb'])
125/150: count.vocabulary_
125/151: count = CountVectorizer()
125/152: count.fit_transform(['aa', 'bb', 'aa', 'bb'])
125/153: count.vocabulary_
125/154: count.fit_transform(['aa', 'bb', 'aa', 'bb', 'cc'])
125/155: count.vocabulary_
125/156: count.fit_transform(['cc', 'bb', 'aa', 'bb', 'cc'])
125/157: count.vocabulary_
125/158: count.fit_transform(['cc', 'bb', 'aaa', 'bb', 'cc'])
125/159: count.vocabulary_
125/160: count.fit_transform(['cc', 'bb', 'aaa', 'bbb', 'cc'])
125/161: count.vocabulary_
125/162: print(count.vocabulary_)
125/163:
bag = count.fit_transform([ranks['BOOK title']])
count.vocabulary_
125/164:
bag = count.fit_transform(ranks['BOOK title'])
count.vocabulary_
125/165:
count = CountVectorizer()
bag = count.fit_transform(ranks['Long Desc'])
count.vocabulary_
125/166:
count = CountVectorizer()
bag = count.fit_transform(ranks['Long Desc.'])
count.vocabulary_
128/1:
count = CountVectorizer()
count.fit_transform(['cc', 'bb', 'aaa', 'bbb', 'cc'])
print(count.vocabulary_)
128/2:
count = CountVectorizer()
bag = count.fit_transform(ranks['Long Desc.'])
count.vocabulary_
128/3:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_extraction.text import CountVectorizer

%matplotlib inline
128/4: x = pd.Series(np.arange(-10, 10, 0.01))
128/5: normal_distribution = np.random.normal(0, 5, len(x))
128/6: y = 1 - x**2 + normal_distribution
128/7:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
128/8: x_squared = x**2
128/9: plt.scatter(x_squared, y)
128/10:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
print('r matrix: ', r)
print('m: ', m, ', c: ', c)
128/11: r**2
128/12: df = pd.DataFrame({'x_squared': x_squared, 'y': y})
128/13: df.head()
128/14:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
128/15:
my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())
my_r2
128/16: x = pd.Series(np.arange(0,1,0.001))
128/17: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))
128/18:
fig, ax = plt.subplots(figsize=(12,6))
ax.set_xlabel('x')
ax.set_ylabel('y')
line = ax.scatter(x,y)
line.set_label('signal')
ax.legend()
ax.grid()
128/19:
m,c = np.polyfit(x,y, deg=1)
print('Coeff and intercept for np model: ',m,c)
128/20:
line, = ax.plot(x,x*m+c, color='red')
line.set_label('linear regression')
ax.legend()
fig
128/21: sk_model = linear_model.LinearRegression()
128/22:
y_sk = y.values.reshape(-1,1)

x_sk = x.values.reshape(-1,1)
x_sk2 = x_sk**2
x_sk3 = x_sk**3
X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)

sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
128/23:
line, = ax.plot(x_sk, sk_result.predict(X), color='black')
line.set_label('polynomial regression: pow=3')
ax.legend()
fig
128/24:
X = x_sk
for i in range(2,15):
    xi = x_sk**i
    X = np.concatenate((X, xi), axis=1)
    
sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
128/25:
line, = ax.plot(x_sk, sk_result.predict(X), color='green')
line.set_label('polynomial regression: pow=14')
ax.legend()
fig
128/26:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
128/27:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MIN_DEGREE = 3
MAX_DEGREE = 8
errors = []

for i in range(MIN_DEGREE,MAX_DEGREE):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)
plt.xlabel('Degree of polynomial')
plt.ylabel('MSE')
plt.grid()
128/28:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,1, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
128/29:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
128/30: ridge_cv_result.coef_
128/31:
er,al = zip(*errors_alphas)
plt.plot(al, er)
plt.xlabel('alpha')
plt.ylabel('MSE')
128/32:
path_to_file = 'Regularization/oreilly.csv'
ranks = pd.read_csv(path_to_file, encoding='latin1')
ranks.head()
128/33: ranks.isnull().sum()
128/34:
count = CountVectorizer()
bag = count.fit_transform(ranks['Long Desc.'])
count.vocabulary_
128/35:
count = CountVectorizer()
count.fit_transform(['cc', 'bb', 'aaa', 'bbb', 'cc'])
print(count.vocabulary_)
128/36:
count = CountVectorizer()
count.fit_transform(['ja', 'ty', 'ona', 'ty', 'ja'])
print(count.vocabulary_)
128/37:
count = CountVectorizer()
count.fit_transform(['mama', 'ja', 'ty', 'ona', 'ty', 'ja'])
print(count.vocabulary_)
128/38:
count = CountVectorizer()
count.fit_transform(['mama', 'ja', 'ty', 'ona', 'ty', 'ona'])
print(count.vocabulary_)
128/39:
count = CountVectorizer()
count.fit_transform(['mama', 'ja', 'ty', 'ona', 'ty', 'mama'])
print(count.vocabulary_)
128/40: ranks.shape()
128/41: ranks.shape
128/42:
x = CountVectorizer()
x.fit_transform(['mama', 'ja', 'ty', 'ona', 'ty', 'mama'])
print(count.vocabulary_)
128/43:
x = CountVectorizer()
x.fit_transform(['This is the first document.',
...     'This document is the second document.',
...     'And this is the third one.',
...     'Is this the first document?',])
print(count.vocabulary_)
128/44:
x = CountVectorizer()
x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])
print(count.vocabulary_)
128/45:
x = CountVectorizer()
x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])
print(x.vocabulary_)
128/46:
x = CountVectorizer()
X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])
print(X.vocabulary_)
128/47:
x = CountVectorizer()
X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])
print(X)
128/48:
x = CountVectorizer()
X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])
print(x.vocabulary)
128/49:
x = CountVectorizer()
X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])
print(x.vocabulary_)
128/50: count.vocabulary_.get('second')
128/51: count.vocabulary_.get('the')
128/52: count.get_features_names()
128/53: count.get_feature_names()
128/54: x.get_feature_names()
128/55: x.vocabulary_.get('second')
128/56:
x = CountVectorizer()
X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])
print(x.vocabulary_)
128/57:
x = CountVectorizer()
X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])
print(x.vocabulary_)
128/58: x.vocabulary_.get('second')
128/59:
x = CountVectorizer()
X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])
print(x.vocabulary_)
128/60: x.vocabulary_.get('second')
128/61:
count = CountVectorizer()
bag = count.fit_transform(ranks['Long Desc.'])
count.vocabulary_
128/62:
count = CountVectorizer()
bag = count.fit_transform(ranks['Long Desc.'])
count.vocabulary_
128/63: count.vocabulary_.get('the')
128/64: count.vocabulary_.get('perl')
128/65: count.vocabulary_.get('python')
128/66:
x = CountVectorizer()
X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])
print(x.vocabulary_)
128/67:
x = CountVectorizer()
X = x.fit_transform(['This is the document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])
print(x.vocabulary_)
128/68:
x = CountVectorizer()
X = x.fit_transform(['This is the document.', 'This document is the document.','And this is the third one.','Is this the first document?',])
print(x.vocabulary_)
128/69:
x = CountVectorizer()
X = x.fit_transform(['This is second the document.', 'This document is the document.','And this is the third one.','Is this the first document?',])
print(x.vocabulary_)
128/70:
x = CountVectorizer()
X = x.fit_transform(['This is the first document.', 'This is the second document.','And this is the third one.','Is this the first document?',])
print(x.vocabulary_)
128/71: X
128/72: X.to_array()
128/73: X.toarray()
128/74: X.toarray()[5]
128/75: X.toarray()[:,5]
128/76: X.toarray()[:]
128/77: X.toarray()[:,5]
128/78:
count = CountVectorizer()
bag = count.fit_transform(ranks['Long Desc.'])
128/79:
count = CountVectorizer()
bag = count.fit_transform(ranks['Long Desc.'], stopwords='english')
128/80:
count = CountVectorizer(stopwords='english')
bag = count.fit_transform(ranks['Long Desc.'])
128/81:
count = CountVectorizer(stop_words='english')
bag = count.fit_transform(ranks['Long Desc.'])
128/82: bag
128/83: bag.to_array()
128/84: bag.toarray()
128/85: pd.DataFrame.sparse.from_matrix(bag)
128/86: pd.DataFrame.sparse.from_spmatrix(bag)
128/87: bag_df = pd.DataFrame.sparse.from_spmatrix(bag)
128/88: bsg_df.columns = count.get_feature_names()
128/89: bag_df.columns = count.get_feature_names()
128/90: bag_df.head()
128/91: bag_df.head(10)
128/92: bag_df.index
128/93:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
128/94:
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
128/95:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,1, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
128/96: ridge_cv_result.coef_
128/97:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,10, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
128/98:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,10, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
128/99:
#Ridge regression cross validation
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
128/100:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.01,10, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
128/101:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.001,1, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
128/102:
#Ridge regression cross validation
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])
poly= PolynomialFeatures(degree=5)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
128/103:
#Ridge regression cross validation
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])
poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
128/104:
#Ridge regression cross validation
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])
poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
128/105:
er,al = zip(*errors_alphas)
plt.plot(al, er)
plt.xlabel('alpha')
plt.ylabel('MSE')
128/106: np.arange(1,100).reshape(-1,1)
128/107:
clf_rank = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])
poly_rank= PolynomialFeatures(degree=10)

X_rank = bag.to_array
y_rank = np.arange(1,100).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
X_train = poly_rank.fit_transform(X_train)
X_test = poly_rank.fit_transform(X_test)

ridge_cv_rank_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_rank_result.score(X_test, Y_test))
128/108:
clf_rank = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])
poly_rank= PolynomialFeatures(degree=10)

X_rank = bag.toarray()
y_rank = np.arange(1,100).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
X_train = poly_rank.fit_transform(X_train)
X_test = poly_rank.fit_transform(X_test)

ridge_cv_rank_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))

print('Ridge cv rank min MSE: ', error)
print('Ridge cv rank r2: ', ridge_cv_rank_result.score(X_test, Y_test))
128/109:
clf_rank = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])
poly_rank= PolynomialFeatures(degree=10)

X_rank = bag.toarray()
y_rank = np.arange(1,101).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
X_train = poly_rank.fit_transform(X_train)
X_test = poly_rank.fit_transform(X_test)

ridge_cv_rank_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))

print('Ridge cv rank min MSE: ', error)
print('Ridge cv rank r2: ', ridge_cv_rank_result.score(X_test, Y_test))
128/110:
clf_rank = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])
poly_rank= PolynomialFeatures(degree=2)

X_rank = bag.toarray()
y_rank = np.arange(1,101).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
X_train = poly_rank.fit_transform(X_train)
X_test = poly_rank.fit_transform(X_test)

ridge_cv_rank_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))

print('Ridge cv rank min MSE: ', error)
print('Ridge cv rank r2: ', ridge_cv_rank_result.score(X_test, Y_test))
128/111:
%time
clf_rank = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])
poly_rank= PolynomialFeatures(degree=2)

X_rank = bag.toarray()
y_rank = np.arange(1,101).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
X_train = poly_rank.fit_transform(X_train)
X_test = poly_rank.fit_transform(X_test)

ridge_cv_rank_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))

print('Ridge cv rank min MSE: ', error)
print('Ridge cv rank r2: ', ridge_cv_rank_result.score(X_test, Y_test))
128/112:
%timeit
clf_rank = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])
poly_rank= PolynomialFeatures(degree=2)

X_rank = bag.toarray()
y_rank = np.arange(1,101).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
X_train = poly_rank.fit_transform(X_train)
X_test = poly_rank.fit_transform(X_test)

ridge_cv_rank_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))

print('Ridge cv rank min MSE: ', error)
print('Ridge cv rank r2: ', ridge_cv_rank_result.score(X_test, Y_test))
128/113:
%timeit
clf_rank = linear_model.RidgeCV(alphas=[1e-2, 1e-1, 1, 10])
poly_rank= PolynomialFeatures(degree=2)

X_rank = bag.toarray()
y_rank = np.arange(1,101).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
X_train = poly_rank.fit_transform(X_train)
X_test = poly_rank.fit_transform(X_test)

ridge_cv_rank_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))

print('Ridge cv rank min MSE: ', error)
print('Ridge cv rank mean accuracy: ', ridge_cv_rank_result.score(X_test, Y_test))
128/114:
%%timeit
clf_rank = linear_model.RidgeCV(alphas=[1e-2, 1e-1, 1, 10])
poly_rank= PolynomialFeatures(degree=2)

X_rank = bag.toarray()
y_rank = np.arange(1,101).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
X_train = poly_rank.fit_transform(X_train)
X_test = poly_rank.fit_transform(X_test)

ridge_cv_rank_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))

print('Ridge cv rank min MSE: ', error)
print('Ridge cv rank mean accuracy: ', ridge_cv_rank_result.score(X_test, Y_test))
128/115:
X_rank = bag.toarray()
y_rank = np.arange(1,101).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
X_train = poly_rank.fit_transform(X_train)
X_test = poly_rank.fit_transform(X_test)
128/116:
clf_rank = linear_model.RidgeCV(alphas=[1e-2, 1e-1, 1, 10])
poly_rank= PolynomialFeatures(degree=2)

X_rank = bag.toarray()
y_rank = np.arange(1,101).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
X_train = poly_rank.fit_transform(X_train)
X_test = poly_rank.fit_transform(X_test)

%time ridge_cv_rank_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))

print('Ridge cv rank min MSE: ', error)
print('Ridge cv rank mean accuracy: ', ridge_cv_rank_result.score(X_test, Y_test))
128/117:
alphas = [0.1, 0.25, 0.5, 1, 2, 5]
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
128/118:
er,al = zip(*errors_alphas)
plt.plot(al, er)
plt.xlabel('alpha')
plt.ylabel('MSE')
128/119:
er,al = zip(*errors_alphas)
plt.scatter(al, er)
plt.xlabel('alpha')
plt.ylabel('MSE')
128/120: er
128/121: er**(0.5)
128/122: list(er)**(0.5)
128/123:
er,al = zip(*errors_alphas)
plt.scatter(al, [e*0.5 for e in er])
plt.xlabel('alpha')
plt.ylabel('MSE')
128/124:
er,al = zip(*errors_alphas)
plt.scatter(al, er)
plt.xlabel('alpha')
plt.ylabel('MSE')
128/125:
er,al = zip(*errors_alphas)
plt.scatter(al, [e**0.5 for e in er])
plt.xlabel('alpha')
plt.ylabel('MSE')
128/126: [e**0.5 for e in er]
128/127: al
128/128: er
128/129:
er,al = zip(*errors_alphas)
plt.scatter(al, [e**0.5 for e in er])
plt.xlabel('alpha')
plt.ylabel('MSE')
plt.grid()
128/130:
alphas = [0.1, 0.5, 1, 10, 20]
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
128/131:
er,al = zip(*errors_alphas)
plt.scatter(al, [e**0.5 for e in er])
plt.xlabel('alpha')
plt.ylabel('MSE')
plt.grid()
128/132: er
128/133: [e**0.5 for e in er]
128/134:
alphas = [1, 10, 20]
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
128/135:
er,al = zip(*errors_alphas)
plt.scatter(al, [e**0.5 for e in er])
plt.xlabel('alpha')
plt.ylabel('MSE')
plt.grid()
128/136: er
128/137: [e**0.5 for e in er]
128/138:
alphas = [0.005, 0.01, 0.1]
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
128/139:
X_rank = bag.toarray()
y_rank = np.arange(1,101).reshape(-1,1)
128/140: 50*[0]
128/141: np.arange(1,50).join(50*[0])
128/142: np.arange(1,50)
128/143: 50*[1]+50*[0]
128/144: (50*[1]+50*[0]).reshape(-1,1)
128/145: np.arange(1,50)
128/146: np.array(50*[1]+50*[0]).reshape(-1,1)
128/147:
X_rank = bag.toarray()
y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)
128/148: clf = linear_model.LogisticRegression(X_rank,y_rank)
128/149: X_rank[39]
128/150: clf.predict(X_rank[39])
128/151: clf = linear_model.LogisticRegression.fit(X_rank,y_rank)
128/152: clf = linear_model.LogisticRegression().fit(X_rank,y_rank)
128/153: clf.predict(X_rank[39])
128/154:
X_rank = bag.toarray()
y_rank = np.arange(1,101).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
X_train = poly_rank.fit_transform(X_train)
X_test = poly_rank.fit_transform(X_test)
128/155: y_rank
128/156:
X_rank = bag.toarray()
y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)
y_rank
128/157: clf = linear_model.LogisticRegression(random_state=0).fit(X_rank,y_rank)
128/158: np.arange(1,10)
128/159: np.arange(1,10).reshape(-1,1)
128/160: clf.predict(X_rank[39])
128/161:
X_rank = bag.toarray()
y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)
128/162: clf.predict(X_rank)
128/163: clf.score()
128/164: clf.score(X_rank, y_rank)
128/165: clf = linear_model.LogisticRegression(random_state=0).fit(X_train,y_train)
128/166: clf = linear_model.LogisticRegression(random_state=0).fit(X_train,Y_train)
128/167: clf = linear_model.LogisticRegression(random_state=0).fit(X_rank,y_rank)
128/168: clf = linear_model.LogisticRegression().fit(X_train,y_train)
128/169: clf = linear_model.LogisticRegression().fit(X_train,Y_train)
129/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_extraction.text import CountVectorizer

%matplotlib inline
129/2: x = pd.Series(np.arange(-10, 10, 0.01))
129/3: normal_distribution = np.random.normal(0, 5, len(x))
129/4: y = 1 - x**2 + normal_distribution
129/5:
plt.scatter(x,y)
m,c = np.polyfit(x,y, deg=1)
plt.plot(x, m*x+c, color='red')
plt.grid()
129/6: x_squared = x**2
129/7: plt.scatter(x_squared, y)
129/8:
m,c = np.polyfit(x_squared, y, deg=1)
y_model = x_squared*m +c
r = np.corrcoef(x_squared, y)
print('r matrix: ', r)
print('m: ', m, ', c: ', c)
129/9: r**2
129/10: df = pd.DataFrame({'x_squared': x_squared, 'y': y})
129/11: df.head()
129/12:
model = smf.ols(formula='y ~ x_squared', data=df).fit()
model.summary()
129/13:
my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())
my_r2
129/14: x = pd.Series(np.arange(0,1,0.001))
129/15: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))
129/16:
fig, ax = plt.subplots(figsize=(12,6))
ax.set_xlabel('x')
ax.set_ylabel('y')
line = ax.scatter(x,y)
line.set_label('signal')
ax.legend()
ax.grid()
129/17:
m,c = np.polyfit(x,y, deg=1)
print('Coeff and intercept for np model: ',m,c)
129/18:
line, = ax.plot(x,x*m+c, color='red')
line.set_label('linear regression')
ax.legend()
fig
129/19: sk_model = linear_model.LinearRegression()
129/20:
y_sk = y.values.reshape(-1,1)

x_sk = x.values.reshape(-1,1)
x_sk2 = x_sk**2
x_sk3 = x_sk**3
X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)

sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
129/21:
line, = ax.plot(x_sk, sk_result.predict(X), color='black')
line.set_label('polynomial regression: pow=3')
ax.legend()
fig
129/22:
X = x_sk
for i in range(2,15):
    xi = x_sk**i
    X = np.concatenate((X, xi), axis=1)
    
sk_result = sk_model.fit(X,y_sk)
print('r2 for sk_model: ', sk_result.score(X,y_sk))
print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)
129/23:
line, = ax.plot(x_sk, sk_result.predict(X), color='green')
line.set_label('polynomial regression: pow=14')
ax.legend()
fig
129/24:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
129/25:
X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)

MIN_DEGREE = 3
MAX_DEGREE = 8
errors = []

for i in range(MIN_DEGREE,MAX_DEGREE):
    poly= PolynomialFeatures(degree=i)
    X_train_i = poly.fit_transform(X_train)
    X_test_i = poly.fit_transform(X_test)
    
    sk_result = sk_model.fit(X_train_i, Y_train)
    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))
    errors.append(error)
plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)
plt.xlabel('Degree of polynomial')
plt.ylabel('MSE')
plt.grid()
129/26:
#Ridge regression 

poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

alphas = np.arange(0.001,1, 0.01)
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
129/27:
#Ridge regression cross validation
clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])
poly= PolynomialFeatures(degree=10)

X = x_sk
y = y_sk
X_train, X_test, Y_train, Y_test = train_test_split(X, y)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

ridge_cv_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))

print('Ridge cv min MSE: ', error)
print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))
129/28: ridge_cv_result.coef_
129/29:
er,al = zip(*errors_alphas)
plt.plot(al, er)
plt.xlabel('alpha')
plt.ylabel('MSE')
129/30:
path_to_file = 'Regularization/oreilly.csv'
ranks = pd.read_csv(path_to_file, encoding='latin1')
ranks.head()
129/31: ranks.isnull().sum()
129/32:
count = CountVectorizer(stop_words='english')
bag = count.fit_transform(ranks['Long Desc.'])
129/33: bag.toarray()
129/34: bag_df = pd.DataFrame.sparse.from_spmatrix(bag)
129/35: bag_df.columns = count.get_feature_names()
129/36: bag_df.head(10)
129/37: bag_df.index
129/38:
X_rank = bag.toarray()
y_rank = np.arange(1,101).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
X_train = poly_rank.fit_transform(X_train)
X_test = poly_rank.fit_transform(X_test)
129/39:
X_rank = bag.toarray()
y_rank = np.arange(1,101).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
129/40: y_rank
129/41:
clf_rank = linear_model.RidgeCV(alphas=[1e-2, 1e-1, 1, 10], cv=5)
poly_rank= PolynomialFeatures(degree=2)

X_rank = bag.toarray()
y_rank = np.arange(1,101).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
X_train = poly_rank.fit_transform(X_train)
X_test = poly_rank.fit_transform(X_test)

%time ridge_cv_rank_result = clf.fit(X_train, Y_train)
error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))

print('Ridge cv rank min MSE: ', error)
print('Ridge cv rank mean accuracy: ', ridge_cv_rank_result.score(X_test, Y_test))
129/42:
alphas = [0.005, 0.01, 0.1]
errors_alphas = []

for alpha in alphas:
    ridge_model = linear_model.Ridge(alpha=alpha)
    ridge_result = ridge_model.fit(X_train, Y_train)
    error = mean_squared_error(Y_test, ridge_result.predict(X_test))
    errors_alphas.append((error, alpha))
    
min_errors_alpha = min(errors_alphas)

print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])
print('Ridge r2: ', ridge_result.score(X_test,Y_test))
129/43:
er,al = zip(*errors_alphas)
plt.scatter(al, [e**0.5 for e in er])
plt.xlabel('alpha')
plt.ylabel('MSE')
plt.grid()
129/44: er
129/45: [e**0.5 for e in er]
129/46:
X_rank = bag.toarray()
y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
129/47: clf = linear_model.LogisticRegression().fit(X_train,Y_train)
129/48: clf.predict(X_rank)
129/49: clf.score(X_test, T_test)
129/50: clf.score(X_test, Y_test)
129/51: logistic_regression_cv = linear_model.LogisticRegressionCV().fit(X_train,Y_train)
129/52: logistic_regression_cv.score(X_test, Y_test)
129/53: logistic_regression_cv = linear_model.LogisticRegressionCV(cv=5).fit(X_train,Y_train)
129/54: logistic_regression_cv.score(X_test, Y_test)
129/55: logistic_regression_cv = linear_model.LogisticRegressionCV(cv=5, penalty='elasticnet').fit(X_train,Y_train)
129/56: logistic_regression_cv = linear_model.LogisticRegressionCV(cv=5, penalty='l1').fit(X_train,Y_train)
129/57: clf = linear_model.LogisticRegression().fit(X_train,Y_train)
129/58: clf.predict(X_rank)
129/59: clf.score(X_test, Y_test)
129/60: logistic_regression_cv = linear_model.LogisticRegressionCV(cv=5, penalty='l1').fit(X_train,Y_train)
129/61: logistic_regression_cv = linear_model.LogisticRegressionCV().fit(X_train,Y_train)
129/62: logistic_regression_cv.score(X_test, Y_test)
129/63:
X_rank = bag.toarray()
y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)
129/64: clf = linear_model.LogisticRegression().fit(X_train,Y_train)
129/65: clf.predict(X_rank)
129/66: clf.score(X_test, Y_test)
129/67: logistic_regression_cv = linear_model.LogisticRegressionCV().fit(X_train,Y_train)
129/68: logistic_regression_cv.score(X_test, Y_test)
129/69: logistic_regression_cv.score(X_train, Y_train)
129/70: X_test
129/71: Y_test.shape()
129/72: Y_test.shape
129/73:
X_rank = bag.toarray()
y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank, test_size=0.15)
129/74: clf = linear_model.LogisticRegression().fit(X_train,Y_train)
129/75: clf.predict(X_rank)
129/76: clf.score(X_test, Y_test)
129/77: logistic_regression_cv = linear_model.LogisticRegressionCV().fit(X_train,Y_train)
129/78: logistic_regression_cv.score(X_test, Y_test)
129/79: logistic_regression_cv.score(X_train, Y_train)
129/80: Y_test.shape
129/81:
X_rank = bag.toarray()
y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank, test_size=0.5)
129/82: clf = linear_model.LogisticRegression().fit(X_train,Y_train)
129/83: clf.predict(X_rank)
129/84: clf.score(X_test, Y_test)
129/85: logistic_regression_cv = linear_model.LogisticRegressionCV().fit(X_train,Y_train)
129/86: logistic_regression_cv.score(X_test, Y_test)
129/87: logistic_regression_cv.score(X_train, Y_train)
129/88:
X_rank = bag.toarray()
y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)
X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank, test_size=0.1)
129/89: clf = linear_model.LogisticRegression().fit(X_train,Y_train)
129/90: clf.predict(X_rank)
129/91: clf.score(X_test, Y_test)
129/92: logistic_regression_cv = linear_model.LogisticRegressionCV().fit(X_train,Y_train)
129/93: logistic_regression_cv.score(X_test, Y_test)
129/94: logistic_regression_cv.score(X_train, Y_train)
129/95: Y_test.shape
130/1: print('3')
130/2:
import pandas as pd
import matplotlib.pyplot as plt
130/3:
def one_dict(list_dict):
    keys=list_dict[0].keys()
    out_dict={key:[] for key in keys}
    for dict_ in list_dict:
        for key, value in dict_.items():
            out_dict[key].append(value)
    return out_dict
130/4:
from nba_api.stats.static import teams
import matplotlib.pyplot as plt
130/5: !pip3 install nba_api
130/6:
from nba_api.stats.static import teams
import matplotlib.pyplot as plt
130/7: from nba_api.stats.endpoints import playercareerstats
130/8: nba_api
130/9:
import pandas as pd
import matplotlib.pyplot as plt
import nba_api
130/10: !pip install nba_api
130/11:
import pandas as pd
import matplotlib.pyplot as plt
import nba_api
131/1:
import pandas as pd
import matplotlib.pyplot as plt
import nba_api
132/1:
import pandas as pd
import matplotlib.pyplot as plt
import nba_api
133/1:
import pandas as pd
import matplotlib.pyplot as plt
import nba_api
134/1:
import pandas as pd
import matplotlib.pyplot as plt
import nba_api
134/2: !pip install nba_api
134/3:
import pandas as pd
import matplotlib.pyplot as plt
import nba_api
134/4:
def one_dict(list_dict):
    keys=list_dict[0].keys()
    out_dict={key:[] for key in keys}
    for dict_ in list_dict:
        for key, value in dict_.items():
            out_dict[key].append(value)
    return out_dict
134/5:
import pandas as pd
import matplotlib.pyplot as plt
import nba_api
135/1:
import pandas as pd
import matplotlib.pyplot as plt
import nba_api
135/2: !pip install nba_api
135/3:
import pandas as pd
import matplotlib.pyplot as plt
import nba_api
135/4:
def one_dict(list_dict):
    keys=list_dict[0].keys()
    out_dict={key:[] for key in keys}
    for dict_ in list_dict:
        for key, value in dict_.items():
            out_dict[key].append(value)
    return out_dict
135/5:
from nba_api.stats.static import teams
import matplotlib.pyplot as plt
135/6: from nba_api.stats.endpoints import playercareerstats
135/7: nba_teams = teams.get_teams()
135/8: nba_teams[0:3]
135/9:
dict_nba_team=one_dict(nba_teams)
df_teams=pd.DataFrame(dict_nba_team)
df_teams.head()
135/10: dict_nba_team
135/11:
df_warriors=df_teams[df_teams['nickname']=='Warriors']
df_warriors
135/12:
id_warriors=df_warriors[['id']].values[0][0]
#we now have an integer that can be used   to request the Warriors information 
id_warriors
135/13: id_warriors=df_warriors[['id']]
135/14:
id_warriors=df_warriors[['id']]
id_warriors
135/15:
id_warriors=df_warriors[['id']].values
id_warriors
135/16:
id_warriors=df_warriors[['id']].values[0]
id_warriors
135/17:
id_warriors=df_warriors[['id']].values[0][0]
id_warriors
135/18: from nba_api.stats.endpoints import leaguegamefinder
135/19:
# Since https://stats.nba.com does lot allow api calls from Cloud IPs and Skills Network Labs uses a Cloud IP.
# The following code is comment out, you can run it on jupyter labs on your own computer.
gamefinder = leaguegamefinder.LeagueGameFinder(team_id_nullable=id_warriors)
135/20:
# Since https://stats.nba.com does lot allow api calls from Cloud IPs and Skills Network Labs uses a Cloud IP.
# The following code is comment out, you can run it on jupyter labs on your own computer.
gamefinder.get_json()
135/21:
# Since https://stats.nba.com does lot allow api calls from Cloud IPs and Skills Network Labs uses a Cloud IP.
# The following code is comment out, you can run it on jupyter labs on your own computer.
games = gamefinder.get_data_frames()[0]
games.head()
135/22:
games_home=games [games ['MATCHUP']=='GSW vs. TOR']
games_away=games [games ['MATCHUP']=='GSW @ TOR']
135/23: games_home.mean()['PLUS_MINUS']
135/24: games_away.mean()['PLUS_MINUS']
135/25:
fig, ax = plt.subplots()

games_away.plot(x='GAME_DATE',y='PLUS_MINUS', ax=ax)
games_home.plot(x='GAME_DATE',y='PLUS_MINUS', ax=ax)
ax.legend(["away", "home"])
plt.show()
136/1:
import pandas as pd
from bokeh.plotting import figure, output_file, show, output_notebook
output_notebook()
136/2:
def make_dashboard(x, gdp_change, unemployment, title, file_name):
    output_file(file_name)
    p = figure(title=title, x_axis_label='year', y_axis_label='%')
    p.line(x.squeeze(), gdp_change.squeeze(), color="firebrick", line_width=4, legend="% GDP change")
    p.line(x.squeeze(), unemployment.squeeze(), line_width=4, legend="% unemployed")
    show(p)
136/3:
links={'GDP':'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/PY0101EN/projects/coursera_project/clean_gdp.csv',\
       'unemployment':'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/PY0101EN/projects/coursera_project/clean_unemployment.csv'}
136/4:
# Type your code here
df_gdp = pd.read_csv(links['GDP'])
136/5:
# Type your code here
df_gdp.head()
136/6:
# Type your code here
df_unemployment = pd.read_csv(links['unemployment'])
136/7:
# Type your code here
df_unemployment.head()
136/8: df_unemployment.sort_values(by='unemployment')
136/9: df_unemployment.sort_values(by='unemployment', ascending=False)
136/10: df_unemployment.sort_values(by='unemployment', ascending=False)[:10]
137/1:
import pandas as pd
import numpy as np

%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
import folium
137/2: path_to_file = 'Police_Department_Incidents_-_Previous_Year__2016_'
137/3: df_incidents = pd.read_csv(path_to_file)
137/4: path_to_file = 'Police_Department_Incidents_-_Previous_Year__2016_.csv'
137/5: df_incidents = pd.read_csv(path_to_file)
137/6: df_incidents.head()
137/7: df_incidents.PdDistrict.unique()
137/8: df_incidents.groupby(['PdDistrict']).count()
137/9: df_by_district = df_incidents.groupby(['PdDistrict']).count()
137/10: df_by_district.head()
137/11: df_by_district
137/12: df_by_district['Total'] = df_by_district['Location']
137/13: df_by_district
137/14: df_by_district = df_incidents.groupby(['PdDistrict']).count()
137/15: df_by_district.reset_index
137/16: df_by_district.reset_index()
137/17: df_by_district.reset_index(inplace=True)
137/18: df_by_district = df_by_district[['Neighborhood', 'Count']]
137/19: df_by_district.rename(columns={'PdDistrict': 'Neighborhood'})
137/20: df_by_district = df_by_district[['Neighborhood', 'Count']]
137/21: df_by_district = df_incidents.groupby(['PdDistrict']).count()
137/22: df_by_district['Count'] = df_by_district['Location']
137/23: df_by_district.reset_index(inplace=True)
137/24: df_by_district.rename(columns={'PdDistrict': 'Neighborhood'})
137/25: df_by_district = df_by_district[['Neighborhood', 'Count']]
137/26: df_by_district = df_by_district['Neighborhood', 'Count']
137/27: df_by_district = df_by_district.rename(columns={'PdDistrict': 'Neighborhood'})
137/28: df_by_district = df_by_district['Neighborhood', 'Count']
137/29: df_by_district = df_by_district[['Neighborhood', 'Count']]
137/30: df_by_district
137/31: df_by_district = df_by_district.sort_values(by='Neighborhood')
137/32: df_by_district
137/33: df_by_district = df_by_district.sort_values(by='Count')
137/34: df_by_district
137/35: df_by_district = df_by_district[['Neighborhood', 'Count']]
137/36: df_by_district
137/37:
latitude = 37.77
longitude = -122.42
sf_map = folium.map(location=[latitude, longitude], zoom_start=12, tiles='Mapbox Bright')
137/38: sf_gjson_file = 'san-francisco.geojson'
137/39:
latitude = 37.77
longitude = -122.42
sf_map = folium.Map(location=[latitude, longitude], zoom_start=12, tiles='Mapbox Bright')
137/40: sf_geodata = 'san-francisco.geojson'
137/41:
sf_map.choropleth(geo_data=sf_geodata,
                 data=df_by_district,
                 columns=['Neighborhood', 'Count'],
                 key_on='feature.properties.DISTRICT',
                 fill_color='YlOrRd',
                 fill_opacity=0.7,
                 line_opacity=0.2,
                 )
sf_map
137/42:
sf_map.choropleth(geo_data=sf_geodata,
                 data=df_by_district,
                 columns=['Neighborhood', 'Count'],
                 key_on='feature.properties.DISTRICT',
                 fill_color='YlOrRd',
                 fill_opacity=0.7,
                 line_opacity=0.2,
                  legend_name='Crime rate in San Francisco'
                 )
sf_map
138/1:
import pandas as pd
import numpy as np

%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
import folium
138/2: path_to_file = 'Police_Department_Incidents_-_Previous_Year__2016_.csv'
138/3: df_incidents = pd.read_csv(path_to_file)
138/4: df_incidents.head()
138/5: df_by_district = df_incidents.groupby(['PdDistrict']).count()
138/6: df_by_district['Count'] = df_by_district['Location']
138/7: df_by_district.reset_index(inplace=True)
138/8: df_by_district = df_by_district.rename(columns={'PdDistrict': 'Neighborhood'})
138/9: df_by_district = df_by_district[['Neighborhood', 'Count']]
138/10: df_by_district
138/11: sf_geodata = 'san-francisco.geojson'
138/12:
latitude = 37.77
longitude = -122.42
sf_map = folium.Map(location=[latitude, longitude], zoom_start=12, tiles='Mapbox Bright')
138/13:
sf_map.choropleth(geo_data=sf_geodata,
                 data=df_by_district,
                 columns=['Neighborhood', 'Count'],
                 key_on='feature.properties.DISTRICT',
                 fill_color='YlOrRd',
                 fill_opacity=0.7,
                 line_opacity=0.2,
                  legend_name='Crime rate in San Francisco'
                 )
sf_map
138/14:
latitude = 37.77
longitude = -122.42
sf_map = folium.Map(location=[latitude, longitude], zoom_start=12)
138/15:
sf_map.choropleth(geo_data=sf_geodata,
                 data=df_by_district,
                 columns=['Neighborhood', 'Count'],
                 key_on='feature.properties.DISTRICT',
                 fill_color='YlOrRd',
                 fill_opacity=0.,
                 line_opacity=0.2,
                  legend_name='Crime rate in San Francisco'
                 )
sf_map
138/16:
sf_map.choropleth(geo_data=sf_geodata,
                 data=df_by_district,
                 columns=['Neighborhood', 'Count'],
                 key_on='feature.properties.DISTRICT',
                 fill_color='YlOrRd',
                 fill_opacity=0.,
                 line_opacity=0.2,
                  legend_name='Crime rate in San Francisco'
                 )
sf_map
138/17:
sf_map.choropleth(geo_data=sf_geodata,
                 data=df_by_district,
                 columns=['Neighborhood', 'Count'],
                 key_on='feature.properties.DISTRICT',
                 fill_color='YlOrRd',
                 fill_opacity=0.7,
                 line_opacity=0.2,
                  legend_name='Crime rate in San Francisco'
                 )
sf_map
138/18:
latitude = 37.77
longitude = -122.42
sf_map = folium.Map(location=[latitude, longitude], zoom_start=12)
138/19:
sf_map.choropleth(geo_data=sf_geodata,
                 data=df_by_district,
                 columns=['Neighborhood', 'Count'],
                 key_on='feature.properties.DISTRICT',
                 fill_color='YlOrRd',
                 fill_opacity=0.7,
                 line_opacity=0.2,
                  legend_name='Crime rate in San Francisco'
                 )
sf_map
139/1:
import pandas as pd
import numpy as np
139/2: print('Hello Capstone Project Course!')
140/1: df = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')
140/2:
import pandas as pd
import numpy as np
140/3: print('Hello Capstone Project Course!')
140/4: df = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')
140/5: df = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')
141/1:
import pandas as pd
import numpy as np
141/2: print('Hello Capstone Project Course!')
141/3: df = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')
141/4: df
141/5: df.columns
141/6: type(df)
141/7: df[0]
141/8: df = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')[0]
141/9: type(df)
141/10: df[0]
141/11: df
141/12: df['Borough'].isnull().sum()
141/13: df['Borough'].isnull().count()
141/14: df['Borough'].isnull()
141/15: df['Borough']=='Not assigned'
141/16: df[df['Borough']=='Not assigned']
141/17: df[df['Borough']=='Not assigned'].count()
141/18: df[df['Borough']=='Not assigned'].sum()
141/19: df[df['Borough']=='Not assigned']
141/20: df[df['Borough']=='Not assigned'].shape
141/21: df[~df['Borough']=='Not assigned']
141/22: df[~df['Borough']=='Not assigned']
141/23: df[~(df['Borough']=='Not assigned')]
141/24: df[~(df['Borough']=='Not assigned')].isnull()
141/25: df[~(df['Borough']=='Not assigned')].isnull().count()
141/26: df = df[~(df['Borough']=='Not assigned')]
141/27: df.isnull()
141/28: df.isnull().sum()
141/29: df = df[(df['Borough']=='Not assigned')]
141/30: df.isnull().sum()
141/31: df
141/32: df = df[(df['Borough']=='Not assigned')]
141/33: df.isnull().sum()
141/34: df
141/35: df = df[~(df['Borough']=='Not assigned')]
141/36: df.isnull().sum()
141/37: df
141/38: df = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')[0]
141/39: type(df)
141/40: df
141/41: df = df[~(df['Borough']=='Not assigned')]
141/42: df.isnull().sum()
141/43: df
141/44: df.count_values()
141/45: df.unique()
141/46: df.PostalCode.unique()
141/47: df.PostalCode.unique()
141/48: df.PostalCode
141/49: df['Postal Code'].unique()
141/50: df['Postal Code'].unique().count()
141/51: df['Postal Code'].unique().count_values()
141/52: df['Postal Code'].value_counts()
141/53: df['Postal Code'].value_counts()>1
141/54: df[df['Postal Code'].value_counts()>1]
141/55: df['Postal Code'].value_counts().sort()
141/56: df['Postal Code'].value_counts().sorted()
141/57: df['Postal Code'].value_counts().sort_values()
141/58: df.loc['M5A']
141/59: df[df['Postal Code']=='M5A']
141/60: df_list = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')
141/61: df_postal_codes = df_list[0]
141/62: df_postal_code
141/63: df_postal_codes
141/64: >This method return *list*, and correct dataframe is it's first element
141/65: df_postal_codes.head()
141/66: df_list = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')
141/67: df_postal_codes = df_list[0]
141/68: df.postal_codes.isnull().sum()
141/69: df_postal_codes.isnull().sum()
141/70: df_postal_codes.isnull().count()
141/71: df_postal_codes.isnull().sum()
141/72: df_postal_codes['Neighborhood']
141/73: df_postal_codes['Neighborhood']=='Not assigned'
141/74: dfdf_[postal_codes['Neighborhood']=='Not assigned']
141/75: df_postal_codes[df_postal_codes['Neighborhood']=='Not assigned']
141/76: df_postal_codes[df_postal_codes['Neighborhood']=='Not assigned']
141/77: df_postal_codes[df_postal_codes['Neighborhood']=='Not assigned'].count()
141/78: df_postal_codes = df_postal_codes[~(df_postal_codes['Borough']=='Not assigned')]
141/79: df_postal_codes.isnull().sum()
141/80: df_postal_codes[df_postal_codes['Neighborhood']=='Not assigned'].count()
141/81: df[df['Postal Code']=='M5A']
141/82: df_postal_codes[df_postal_codes['Postal Code']=='M5A']
141/83: df_postal_codes.shape
141/84:
import pandas as pd
import numpy as np
import geocoder
141/85: g = geocoder.google('M5A, Toronto, Ontario')
141/86: g
141/87: g = geocoder.google('M5A, Toronto, Ontario')
141/88: g
141/89: g = geocoder.google('M1A, Toronto, Ontario')
141/90: g
141/91: g.json
141/92: g = geocoder.google('M1A, Toronto, Ontario')
141/93: g.json
141/94:  g = geocoder.google('Mountain View, CA')
141/95: g
141/96: g.city
141/97: g.__dir__
141/98: g.__dir__()
141/99: g.status
141/100: g = geocoder.google('M1A, Toronto, Ontario')
141/101: g.status
141/102:  g = geocoder.google('Mountain View, CA')
141/103: g.geojson
141/104: g.geojson['features']
141/105: g = geocoder.canadapost('M1A, Toronto, Ontario')
141/106: g
141/107: g.json
141/108: g = geocoder.arcgis('M1A, Toronto, Ontario')
141/109: g.json
141/110:  g = geocoder.arcgis('M5A, Toronto, Ontario')
141/111: g.geojson['features']
141/112:
for postal_code in df_postal_codes['Postal Codes']:
    print(postal_code)
141/113:
for postal_code in df_postal_codes['Postal Code']:
    print(postal_code)
141/114:
for postal_code in df_postal_codes['Postal Code']:
    print(type(postal_code))
141/115:
for postal_code in df_postal_codes['Postal Code']:
    print('{}, Toronto, Ontario'.format(postal_code))
141/116:
for postal_code in df_postal_codes['Postal Code']:
    address = '{}, Toronto, Ontario'.format(postal_code)
    postal_code_coordinates = geocoder.arcgis(address)
    postal_codes_coordinates.append(postal_code_coordinates)
141/117: postal_codes_coordinates=[]
141/118:
for postal_code in df_postal_codes['Postal Code']:
    address = '{}, Toronto, Ontario'.format(postal_code)
    postal_code_coordinates = geocoder.arcgis(address)
    postal_codes_coordinates.append(postal_code_coordinates)
141/119: postal_codes_coordinates
141/120: df_postal_codes['Postal Code']
141/121: df_postal_codes['Postal Code'][-30:-10]
141/122: df_postal_codes['Postal Code'][-50:-10]
141/123: len(postal_codes_coordinates)
141/124: df_postal_codes['Postal Code']
141/125: zipped_postal_codes_coordinates = list(zip(df_postal_codes['Postal Code'], postal_codes_coordinates)
141/126: zipped_postal_codes_coordinates = list(zip(df_postal_codes['Postal Code'], postal_codes_coordinates))
141/127: zipped_postal_codes_coordinates
141/128: print(postal_codes_coordinates[0])
141/129: postal_codes_coordinates[0]
141/130: postal_codes_coordinates[0].geojson
141/131:
longitudes = []
latitudes = []
141/132: postal_codes_coordinates[0].geojson.keys()
141/133: postal_codes_coordinates[0].geojson['features']   #.keys()
141/134: postal_codes_coordinates[0].geojson['features']['coordinates']   #.keys()
141/135: postal_codes_coordinates[0].geojson['features']   #.keys()
141/136: len(postal_codes_coordinates[0].geojson['features'])   #.keys()
141/137: postal_codes_coordinates[0].geojson['features'][0]['coordinates']   #.keys()
141/138: postal_codes_coordinates[0].geojson['features'][0]['coordinate']   #.keys()
141/139: postal_codes_coordinates[0].geojson['features'][0]#['coordinate']   #.keys()
141/140: postal_codes_coordinates[0].geojson['features'][0].keys() #['coordinate']   #.keys()
141/141: postal_codes_coordinates[0].geojson['features'][0]['geometry'] #['coordinate']   #.keys()
141/142: postal_codes_coordinates[0].geojson['features'][0]['geometry']['coordinates'] #['coordinate']   #.keys()
141/143:
for coordinates in postal_codes_coordinates:
    longitude = coordinates.geojson['features'][0]['geometry']['coordinates'][0]
    longitudes.append(longitude)
    
    latitude = coordinates.geojson['features'][0]['geometry']['coordinates'][1]
    latitudes.append(lattitude)
141/144:
for coordinates in postal_codes_coordinates:
    longitude = coordinates.geojson['features'][0]['geometry']['coordinates'][0]
    longitudes.append(longitude)
    
    latitude = coordinates.geojson['features'][0]['geometry']['coordinates'][1]
    latitudes.append(latitude)
141/145: longitudes
141/146: df_postal_codes = df_postal_code.append(latitudes, axis=1)
141/147: df_postal_codes = df_postal_codes.append(latitudes, axis=1)
141/148: df_postal_codes['Latitude'] = pd.Series(latitudes)
141/149: df_postal_codes
141/150: df_postal_codes.loc[:,'Longitude'] = pd.Series(longitudes)
141/151: df_postal_codes
141/152: df_postal_codes.isnull().sum()
141/153: latitudes
141/154: len(latitudes)
141/155: df_postal_codes.shape
141/156: df_postal_codes
141/157: df_postal_codes.iloc[-10:]
141/158: df_postal_codes.iloc[-20:]
141/159: df_postal_codes.iloc[-40:]
141/160: postal_codes_coordinates=[]
141/161:
for postal_code in df_postal_codes['Postal Code']:
    address = '{}, Toronto, Ontario'.format(postal_code)
    postal_code_coordinates = geocoder.arcgis(address)
    postal_codes_coordinates.append(postal_code_coordinates)
141/162: len(postal_codes_coordinates)
141/163: df_postal_codes['Postal Code']
141/164: zipped_postal_codes_coordinates = list(zip(df_postal_codes['Postal Code'], postal_codes_coordinates))
141/165: postal_codes_coordinates[0].geojson['features'][0]['geometry']['coordinates'] #['coordinate']   #.keys()
141/166:
longitudes = []
latitudes = []
141/167:
for coordinates in postal_codes_coordinates:
    longitude = coordinates.geojson['features'][0]['geometry']['coordinates'][0]
    longitudes.append(longitude)
    
    latitude = coordinates.geojson['features'][0]['geometry']['coordinates'][1]
    latitudes.append(latitude)
141/168: df_postal_codes['Latitude'] = pd.Series(latitudes)
141/169: df_postal_codes.loc[:,'Longitude'] = pd.Series(longitudes)
141/170: df_postal_codes.isnull().sum()
141/171:  g = geocoder.baidu('M5A, Toronto, Ontario')
141/172:  g = geocoder.geocodefarm('M5A, Toronto, Ontario')
141/173: g.geojson['features']
141/174:  g = geocoder.geocodefarm('M5A, Toronto, Ontario')
141/175:  g = geocoder.geolytica('M5A, Toronto, Ontario')
141/176: g.geojson['features']
141/177: postal_codes_coordinates=[]
141/178:
for postal_code in df_postal_codes['Postal Code']:
    address = '{}, Toronto, Ontario'.format(postal_code)
    postal_code_coordinates = geocoder.arcgis(address)
    postal_codes_coordinates.append(postal_code_coordinates)
141/179: len(postal_codes_coordinates)
141/180: df_postal_codes['Postal Code']
141/181: zipped_postal_codes_coordinates = list(zip(df_postal_codes['Postal Code'], postal_codes_coordinates))
141/182: postal_codes_coordinates[0].geojson['features'][0]['geometry']['coordinates'] #['coordinate']   #.keys()
141/183:
longitudes = []
latitudes = []
141/184:
for coordinates in postal_codes_coordinates:
    longitude = coordinates.geojson['features'][0]['geometry']['coordinates'][0]
    longitudes.append(longitude)
    
    latitude = coordinates.geojson['features'][0]['geometry']['coordinates'][1]
    latitudes.append(latitude)
141/185: df_postal_codes['Latitude'] = pd.Series(latitudes)
141/186: df_postal_codes.loc[:,'Longitude'] = pd.Series(longitudes)
141/187: df_postal_codes.isnull().sum()
141/188: len(latitudes)
141/189: df_postal_codes.iloc[-40:]
141/190:  g = geocoder.geolytica('M5P, Toronto, Ontario')
141/191: g.geojson['features']
141/192:  g = geocoder.geolytica('M6P, Toronto, Ontario')
141/193: g.geojson['features']
141/194:  g = geocoder.geolytica('M5P, Toronto, Ontario')
141/195: g.geojson['features']
141/196:  g = geocoder.geolytica('M7R, Toronto, Ontario')
141/197: g.geojson['features']
141/198: postal_codes_coordinates[100].geojson['features'][0]['geometry']['coordinates'] #['coordinate']   #.keys()
141/199: postal_codes_coordinates[101].geojson['features'][0]['geometry']['coordinates'] #['coordinate']   #.keys()
141/200: postal_codes_coordinates[80].geojson['features'][0]['geometry']['coordinates'] #['coordinate']   #.keys()
141/201: df_postal_codes.reset_index(inplace=True)
141/202: df_postal_codes['Latitude'] = pd.Series(latitudes)
141/203: df_postal_codes.loc[:,'Longitude'] = pd.Series(longitudes)
141/204: df_postal_codes.isnull().sum()
141/205: len(latitudes)
141/206: df_postal_codes.iloc[-40:]
141/207: df_postal_codes.to_csv('Toronto_postal_codes.csv')
141/208: pd.DataFrame(Series(latitudes))
141/209: pd.DataFrame(pd.Series(latitudes))
141/210: df_postal_codes
141/211: df_postal_codes.drop(columns=['Latitude', 'Longitutde'])
141/212: df_postal_codes.drop(columns=['Latitude', 'Longitude'])
141/213: df_test = df_postal_codes.drop(columns=['Latitude', 'Longitude'])
141/214: df_test
141/215: df_test = df_postal_codes.drop(columns=['Latitude', 'Longitude', 'index'])
141/216: df_test
141/217: df_test.columns
141/218: df_test.columns = ['x','y','z']
141/219: df_test
141/220: df_test.columns = ['x','y','z','h']
141/221: df_test.reindex(columns = ['x','y','z','h'])
141/222: df_test['h'] = Longitudes
141/223: df_test['h'] = longitudes
141/224: df_test
141/225: df_test.reindex(columns = ['x','y','z','h','l'])
141/226: df_test[['h','l']] = [longitudes, latitudes]
141/227: df_long = pd.DataFrame(pd.Series(longitudes))
141/228: df_test.append(df_long)
141/229: df_test
141/230: df_test.append(df_long, inplace=True)
141/231: df_3 = df_test.append(df_long)
141/232: df_3
141/233: df_test = df_postal_codes.drop(columns=['Latitude', 'Longitude', 'index'])
141/234: df_test.reindex(columns = ['x','y','z','h','l'])
141/235: df_test = df_postal_codes.drop(columns=['Latitude', 'Longitude', 'index'])
141/236: df_test.reindex(columns = ['x','y','z','h','l'])
141/237: df_postal_codes
141/238: df_test = df_postal_codes.drop(columns=['Latitude', 'Longitude', 'index'])
141/239: df_test.reindex(columns = df_test.columns+['h','l'])
141/240: df_test.columns
141/241: df_test.columns + ['a']
141/242: df_test.columns.append('a')
141/243: df_test['a']=np.NaN
141/244: df_test['a']=longitudes
141/245: df_test['r']=longitudes
141/246: df_postal_codes['Latitude'] = latitudes
141/247: df_postal_codes.loc[:,['Longitude']] = pd.Series(longitudes)
141/248: df_postal_codes.loc[:,['Longitude']] = longitudes
141/249: df_postal_codes[:,['Latitude']] = latitudes
141/250: df_postal_codes.loc[:,['Latitude']] = latitudes
141/251: g.geojson
141/252: postal_codes_coordinates[0].geojson['features'][0]['geometry']['coordinates'] #['coordinate']   #.keys()
141/253: df_postal_codes
141/254: postal_codes_coordinates[0].geojson['features'][0]['geometry']['coordinates']
141/255: df_postal_codes.loc[df_postal_codes['Postal Code'].isin['M5G', 'M2H','M4M','M9L', 'M1B']]
141/256: df_postal_codes.loc[df_postal_codes['Postal Code'].isin(['M5G', 'M2H','M4M','M9L', 'M1B'])]
141/257: postal_codes_coordinates[1]
141/258: x = geocoder.geoname('M1B, Toronto, Ontario')
141/259: x = geocoder.geonames('M1B, Toronto, Ontario')
141/260: x = geocoder.here('M1B, Toronto, Ontario')
141/261: x = geocoder.osm('M1B, Toronto, Ontario')
141/262: x
141/263: x.geojson
141/264: x = geocoder.yahoo('M1B, Toronto, Ontario')
141/265: x = geocoder.yahoo('M1B, Toronto, Ontario')
141/266: x = geocoder.yandex('M1B, Toronto, Ontario')
141/267: x = geocoder.tgos('M1B, Toronto, Ontario')
141/268: x = geocoder.canadapost('M1B, Toronto, Ontario')
141/269: x.geojson
141/270: x = geocoder.google('M1B, Toronto, Ontario')
141/271: x.geojson
141/272: x
141/273:  g = geocoder.geolytica('M7R, Toronto, Ontario')
141/274: g.geojson
141/275:  g = geocoder.geolytica('M1B, Toronto, Ontario')
141/276: g.geojson
141/277:  g = geocoder.arcgis('M1B, Toronto, Ontario')
141/278: g.geojson
141/279:  g = geocoder.arcgis('M5G, Toronto, Ontario')
141/280: g.geojson
141/281:
toronto_coordinates = ['43.7325', '-79.3993']
map_toronto = folium.Map(location=toronto_coordinates, zoom_start=10)

# add markers to map
for lat, lng, postal_code, borough, neighborhood in zip(df_postal_codes['Latitude'], df_postal_codes['Longitude'],df_postal_codes['Postal Code'], df_postal_codes['Borough'], df_postal_codes['Neighborhood']):
    label = '{}, {}, {}'.format(postal_code,neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_tronto)  
    
map_toronto
141/282:
import pandas as pd
import numpy as np
import geocoder
import folium
141/283:
toronto_coordinates = ['43.7325', '-79.3993']
map_toronto = folium.Map(location=toronto_coordinates, zoom_start=10)

# add markers to map
for lat, lng, postal_code, borough, neighborhood in zip(df_postal_codes['Latitude'], df_postal_codes['Longitude'],df_postal_codes['Postal Code'], df_postal_codes['Borough'], df_postal_codes['Neighborhood']):
    label = '{}, {}, {}'.format(postal_code,neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_tronto)  
    
map_toronto
141/284:
toronto_coordinates = [43.7325, -79.3993]
map_toronto = folium.Map(location=toronto_coordinates, zoom_start=10)

# add markers to map
for lat, lng, postal_code, borough, neighborhood in zip(df_postal_codes['Latitude'], df_postal_codes['Longitude'],df_postal_codes['Postal Code'], df_postal_codes['Borough'], df_postal_codes['Neighborhood']):
    label = '{}, {}, {}'.format(postal_code,neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_tronto)  
    
map_toronto
141/285:
toronto_coordinates = [43.7325, -79.3993]
map_toronto = folium.Map(location=toronto_coordinates, zoom_start=10)

# add markers to map
for lat, lng, postal_code, borough, neighborhood in zip(df_postal_codes['Latitude'], df_postal_codes['Longitude'],df_postal_codes['Postal Code'], df_postal_codes['Borough'], df_postal_codes['Neighborhood']):
    label = '{}, {}, {}'.format(postal_code,neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_toronto)  
    
map_toronto
141/286: postal_codes_coordinates[0].geojson
141/287:  g = geocoder.arcgis('Scarborough, Ontario M1J')
141/288: g.geojson
141/289: postal_codes_coordinates[0].geojson['features'][0]['properties']['quality']
141/290:
for z in postal_codes_coordinates:
    print(z.geojson['features'][0]['properties']['quality'])
141/291:
for i,z in enumerate(postal_codes_coordinates):
    print(i,' ',z.geojson['features'][0]['properties']['quality'])
141/292:
pcc = []
for n,z in zip(df_postal_codes['Neighborhood'], df_postal_codes['Postal Code']:
    address = '{}, Ontario, {}'.format(n, z)
    geojson = geocoder.arcgis(address)           
    pcc.append(geojson)
141/293:
pcc = []
for n,z in zip(df_postal_codes['Neighborhood'], df_postal_codes['Postal Code']):
    address = '{}, Ontario, {}'.format(n, z)
    geojson = geocoder.arcgis(address)           
    pcc.append(geojson)
141/294: pcc
141/295:
for i,z in enumerate(pcc):
    print(i,' ',z.geojson['features'][0]['properties']['quality'])
141/296:
import pandas as pd
import numpy as np
import geocoder
import folium
import os
141/297:
CLIENT_ID = os.environ.get('FOURSQAUREID')
CLIENT_SECRET = os.environ.get('FOURSQARESECRET')
VERSION = '20180605'
LIMIT = 50
141/298:
def get_nearby_venues(neighborhood, latitudes, longitudes, radius=500):
    venues = []
    
    for name, lat, lng in zip(names, latitudes, longitudes):
    print(name)

    # create the API request URL
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
        CLIENT_ID, 
        CLIENT_SECRET, 
        VERSION, 
        lat, 
        lng, 
        radius, 
        LIMIT)
    
    results = requests.get(url).json()["response"]['groups'][0]['items']

    venues_list.append([(
        name, 
        lat, 
        lng, 
        v['venue']['name'], 
        v['venue']['location']['lat'], 
        v['venue']['location']['lng'],  
        v['venue']['categories'][0]['name']) for v in results])

nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
nearby_venues.columns = ['Neighborhood', 
              'Neighborhood Latitude', 
              'Neighborhood Longitude', 
              'Venue', 
              'Venue Latitude', 
              'Venue Longitude', 
              'Venue Category']

return(nearby_venues)
141/299:
def get_nearby_venues(neighborhood, latitudes, longitudes, radius=500):
    venues = []
    
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)

        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)

        results = requests.get(url).json()["response"]['groups'][0]['items']

        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
              'Neighborhood Latitude', 
              'Neighborhood Longitude', 
              'Venue', 
              'Venue Latitude', 
              'Venue Longitude', 
              'Venue Category']

    return(nearby_venues)
141/300:
toronto_venues = getNearbyVenues(names=df_postal_codes['Neighborhood'],
                                   latitudes=df_postal_codes['Latitude'],
                                   longitudes=df_postal_codes['Longitude']
                                  )
141/301:
toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'],
                                   latitudes=df_postal_codes['Latitude'],
                                   longitudes=df_postal_codes['Longitude']
                                  )
141/302:
def get_nearby_venues(names, latitudes, longitudes, radius=500):
    venues = []
    
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)

        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)

        results = requests.get(url).json()["response"]['groups'][0]['items']

        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
              'Neighborhood Latitude', 
              'Neighborhood Longitude', 
              'Venue', 
              'Venue Latitude', 
              'Venue Longitude', 
              'Venue Category']

    return(nearby_venues)
141/303:
toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'],
                                   latitudes=df_postal_codes['Latitude'],
                                   longitudes=df_postal_codes['Longitude']
                                  )
141/304:
import pandas as pd
import numpy as np
import geocoder
import folium
import os
import requests
141/305:
toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'],
                                   latitudes=df_postal_codes['Latitude'],
                                   longitudes=df_postal_codes['Longitude']
                                  )
141/306:
toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'][:1],
                                   latitudes=df_postal_codes['Latitude'][:1],
                                   longitudes=df_postal_codes['Longitude'][:1]
                                  )
141/307:
def get_nearby_venues(names, latitudes, longitudes, radius=500):
    venues = []
    
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)

        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)

        results = requests.get(url).json()#["response"]['groups'][0]['items']
        print(result)
        
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
              'Neighborhood Latitude', 
              'Neighborhood Longitude', 
              'Venue', 
              'Venue Latitude', 
              'Venue Longitude', 
              'Venue Category']

    return(nearby_venues)
141/308:
toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'][:1],
                                   latitudes=df_postal_codes['Latitude'][:1],
                                   longitudes=df_postal_codes['Longitude'][:1]
                                  )
141/309:
def get_nearby_venues(names, latitudes, longitudes, radius=500):
    venues = []
    
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)

        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)

        results = requests.get(url).json()#["response"]['groups'][0]['items']
        print(results)
        
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
              'Neighborhood Latitude', 
              'Neighborhood Longitude', 
              'Venue', 
              'Venue Latitude', 
              'Venue Longitude', 
              'Venue Category']

    return(nearby_venues)
141/310:
toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'][:1],
                                   latitudes=df_postal_codes['Latitude'][:1],
                                   longitudes=df_postal_codes['Longitude'][:1]
                                  )
141/311: print(CLIENT_ID)
141/312:
CLIENT_ID = os.environ.get('FOURSQAUREID')
CLIENT_SECRET = os.environ.get('FOURSQARESECRET')
VERSION = '20180605'
LIMIT = 50
141/313: print(CLIENT_ID)
141/314:
CLIENT_ID = os.environ.get('FOURSQAUREID')
CLIENT_SECRET = os.environ.get('FOURSQARESECRET')
VERSION = '20180605'
LIMIT = 50
141/315: print(CLIENT_ID)
141/316: print(CLIENT_ID)
142/1:
import pandas as pd
import numpy as np
import geocoder
import folium
import os
import requests
142/2: df_list = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')
142/3: df_postal_codes = df_list[0]
142/4: df_postal_codes.head()
142/5: df_postal_codes = df_postal_codes[~(df_postal_codes['Borough']=='Not assigned')]
142/6: df_postal_codes.isnull().sum()
142/7: df_postal_codes[df_postal_codes['Neighborhood']=='Not assigned'].count()
142/8: df_postal_codes[df_postal_codes['Postal Code']=='M5A']
142/9: df_postal_codes.shape
142/10:  g = geocoder.arcgis('M5G, Toronto, Ontario')
142/11: g.geojson
142/12: postal_codes_coordinates[0].geojson['features'][0]['geometry']['coordinates']
142/13: postal_codes_coordinates=[]
142/14:
for postal_code in df_postal_codes['Postal Code']:
    address = '{}, Toronto, Ontario'.format(postal_code)
    postal_code_coordinates = geocoder.arcgis(address)
    postal_codes_coordinates.append(postal_code_coordinates)
142/15: len(postal_codes_coordinates)
142/16: df_postal_codes['Postal Code']
142/17:
longitudes = []
latitudes = []
142/18:
for coordinates in postal_codes_coordinates:
    longitude = coordinates.geojson['features'][0]['geometry']['coordinates'][0]
    longitudes.append(longitude)
    
    latitude = coordinates.geojson['features'][0]['geometry']['coordinates'][1]
    latitudes.append(latitude)
142/19: df_postal_codes.reset_index(inplace=True)
142/20: df_postal_codes.loc[:,['Latitude']] = latitudes
142/21: df_postal_codes[['Latitude']] = latitudes
142/22: df_postal_codes['Latitude'] = latitudes
142/23: df_postal_codes.loc[:,['Longitude']] = longitudes
142/24: df_postal_codes.loc['Longitude'] = longitudes
142/25: df_postal_codes['Longitude'] = longitudes
142/26: df_postal_codes.isnull().sum()
142/27: df_postal_codes
142/28: df_postal_codes.to_csv('Toronto_postal_codes.csv')
142/29: df_postal_codes.loc[df_postal_codes['Postal Code'].isin(['M5G', 'M2H','M4M','M9L', 'M1B'])]
142/30:
toronto_coordinates = [43.7325, -79.3993]
map_toronto = folium.Map(location=toronto_coordinates, zoom_start=10)

# add markers to map
for lat, lng, postal_code, borough, neighborhood in zip(df_postal_codes['Latitude'], df_postal_codes['Longitude'],df_postal_codes['Postal Code'], df_postal_codes['Borough'], df_postal_codes['Neighborhood']):
    label = '{}, {}, {}'.format(postal_code,neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_toronto)  
    
map_toronto
142/31:
CLIENT_ID = os.environ.get('FOURSQAUREID')
CLIENT_SECRET = os.environ.get('FOURSQARESECRET')
VERSION = '20180605'
LIMIT = 50
142/32:
def get_nearby_venues(names, latitudes, longitudes, radius=500):
    venues = []
    
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)

        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)

        results = requests.get(url).json()#["response"]['groups'][0]['items']
        print(results)
        
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
              'Neighborhood Latitude', 
              'Neighborhood Longitude', 
              'Venue', 
              'Venue Latitude', 
              'Venue Longitude', 
              'Venue Category']

    return(nearby_venues)
142/33:
def get_nearby_venues(names, latitudes, longitudes, radius=500):
    venues_list = []
    
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)

        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)

        results = requests.get(url).json()#["response"]['groups'][0]['items']
        print(results)
        
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
              'Neighborhood Latitude', 
              'Neighborhood Longitude', 
              'Venue', 
              'Venue Latitude', 
              'Venue Longitude', 
              'Venue Category']

    return(nearby_venues)
142/34:
toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'][:1],
                                   latitudes=df_postal_codes['Latitude'][:1],
                                   longitudes=df_postal_codes['Longitude'][:1]
                                  )
142/35: print(CLIENT_ID)
143/1:
CLIENT_ID = os.environ.get('FOURSQAUREID')
CLIENT_SECRET = os.environ.get('FOURSQARESECRET')
VERSION = '20180605'
LIMIT = 50
143/2:
import pandas as pd
import numpy as np
import geocoder
import folium
import os
import requests
144/1:
import pandas as pd
import numpy as np
import geocoder
import folium
import os
import requests
144/2:
CLIENT_ID = os.environ.get('FOURSQAUREID')
CLIENT_SECRET = os.environ.get('FOURSQARESECRET')
VERSION = '20180605'
LIMIT = 50
144/3: print(CLIENT_ID)
144/4:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20180605'
LIMIT = 50
144/5: print(CLIENT_ID)
144/6:
import pandas as pd
import numpy as np
import geocoder
import folium
import os
import requests
144/7: df_list = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')
144/8: df_postal_codes = df_list[0]
144/9: df_postal_codes.head()
144/10: df_postal_codes = df_postal_codes[~(df_postal_codes['Borough']=='Not assigned')]
144/11: df_postal_codes.isnull().sum()
144/12: df_postal_codes[df_postal_codes['Neighborhood']=='Not assigned'].count()
144/13: df_postal_codes[df_postal_codes['Postal Code']=='M5A']
144/14: df_postal_codes.shape
144/15:  g = geocoder.arcgis('M5G, Toronto, Ontario')
144/16: g.geojson
144/17: postal_codes_coordinates=[]
144/18:
for postal_code in df_postal_codes['Postal Code']:
    address = '{}, Toronto, Ontario'.format(postal_code)
    postal_code_coordinates = geocoder.arcgis(address)
    postal_codes_coordinates.append(postal_code_coordinates)
144/19: len(postal_codes_coordinates)
144/20: df_postal_codes['Postal Code']
144/21:
longitudes = []
latitudes = []
144/22:
for coordinates in postal_codes_coordinates:
    longitude = coordinates.geojson['features'][0]['geometry']['coordinates'][0]
    longitudes.append(longitude)
    
    latitude = coordinates.geojson['features'][0]['geometry']['coordinates'][1]
    latitudes.append(latitude)
144/23: df_postal_codes.reset_index(inplace=True)
144/24: df_postal_codes['Latitude'] = latitudes
144/25: df_postal_codes['Longitude'] = longitudes
144/26: df_postal_codes.isnull().sum()
144/27: df_postal_codes
144/28: df_postal_codes.to_csv('Toronto_postal_codes.csv')
144/29: df_postal_codes.loc[df_postal_codes['Postal Code'].isin(['M5G', 'M2H','M4M','M9L', 'M1B'])]
144/30:
toronto_coordinates = [43.7325, -79.3993]
map_toronto = folium.Map(location=toronto_coordinates, zoom_start=10)

# add markers to map
for lat, lng, postal_code, borough, neighborhood in zip(df_postal_codes['Latitude'], df_postal_codes['Longitude'],df_postal_codes['Postal Code'], df_postal_codes['Borough'], df_postal_codes['Neighborhood']):
    label = '{}, {}, {}'.format(postal_code,neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_toronto)  
    
map_toronto
144/31:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20180605'
LIMIT = 50
144/32:
def get_nearby_venues(names, latitudes, longitudes, radius=500):
    venues_list = []
    
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)

        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)

        results = requests.get(url).json()#["response"]['groups'][0]['items']
        print(results)
        
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
              'Neighborhood Latitude', 
              'Neighborhood Longitude', 
              'Venue', 
              'Venue Latitude', 
              'Venue Longitude', 
              'Venue Category']

    return(nearby_venues)
144/33:
toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'][:1],
                                   latitudes=df_postal_codes['Latitude'][:1],
                                   longitudes=df_postal_codes['Longitude'][:1]
                                  )
144/34:
def get_nearby_venues(names, latitudes, longitudes, radius=500):
    venues_list = []
    
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)

        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)

        results = requests.get(url).json()["response"]['groups'][0]['items']
        #print(results)
        
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
              'Neighborhood Latitude', 
              'Neighborhood Longitude', 
              'Venue', 
              'Venue Latitude', 
              'Venue Longitude', 
              'Venue Category']

    return(nearby_venues)
144/35:
toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'],
                                   latitudes=df_postal_codes['Latitude'],
                                   longitudes=df_postal_codes['Longitude']
                                  )
144/36: toronto_venues.shape
144/37: toronto_venues.head()
144/38: toronto_venues.head(20)
144/39: df_postal_codes.concat(toronto_venues, axis=1)
144/40: df_postal_codes.append(toronto_venues, axis=1)
144/41: df_postal_codes.join(toronto_venues)
144/42: df_postal_codes.join(toronto_venues, how='left')
144/43: pd.merge([df_postal_codes, toronto_venues])
144/44: df_postal_codes.merge(toronto_venues)
144/45: toronto_venues.to_csv('toronto_venues.csv')
144/46: toronto_venues.groupby('Neighborhood').count().sort_values(by='Venue')
144/47:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20180605'
LIMIT = 100
144/48:
def get_nearby_venues(names, latitudes, longitudes, radius=500):
    venues_list = []
    
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)

        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)

        results = requests.get(url).json()["response"]['groups'][0]['items']
        #print(results)
        
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
              'Neighborhood Latitude', 
              'Neighborhood Longitude', 
              'Venue', 
              'Venue Latitude', 
              'Venue Longitude', 
              'Venue Category']

    return(nearby_venues)
144/49:
toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'],
                                   latitudes=df_postal_codes['Latitude'],
                                   longitudes=df_postal_codes['Longitude']
                                  )
144/50: toronto_venues.shape
144/51: toronto_venues.head(20)
144/52: toronto_venues.to_csv('toronto_venues.csv')
144/53: toronto_venues.groupby('Neighborhood').count().sort_values(by='Venue')
144/54: toronto_venues.count_values()
144/55: toronto_venues.Neighborhood.unique()
144/56: len(toronto_venues.Neighborhood.unique())
144/57: df_postal_codes.loc[df.Neighborhood.isin[toronto_venues.Neighborhood.unique()]]
144/58: df_postal_codes.loc[df_postl_codes.Neighborhood.isin[toronto_venues.Neighborhood.unique()]]
144/59: df_postal_codes.loc[df_postal_codes.Neighborhood.isin[toronto_venues.Neighborhood.unique()]]
144/60: df_postal_codes.loc[df_postal_codes.Neighborhood.isin(toronto_venues.Neighborhood.unique())]
144/61: df_postal_codes.loc[~df_postal_codes.Neighborhood.isin(toronto_venues.Neighborhood.unique())]
144/62: postal_codes_coordinates[95]
144/63: postal_codes_coordinates[95].geojson
144/64: toronto_venues.loc['Scarborough']
144/65: toronto_venues.head()
144/66: df_postal_codes.loc[df_postal_codes.Neighborhood=='Scarborough']
144/67:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
    CLIENT_ID, 
    CLIENT_SECRET, 
    VERSION, 
    '43.834768',
    '79.204101' 
    radius, 
    LIMIT)

results = requests.get(url).json()["response"]['groups'][0]['items']
144/68:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
    CLIENT_ID, 
    CLIENT_SECRET, 
    VERSION, 
    '43.834768',
    '79.204101', 
    radius, 
    LIMIT)

results = requests.get(url).json()["response"]['groups'][0]['items']
144/69:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
    CLIENT_ID, 
    CLIENT_SECRET, 
    VERSION, 
    '43.834768',
    '79.204101', 
    500, 
    LIMIT)

results = requests.get(url).json()["response"]['groups'][0]['items']
144/70: results
144/71:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
    CLIENT_ID, 
    CLIENT_SECRET, 
    VERSION, 
    '43.834768',
    '79.204101', 
    500, 
    LIMIT)

results = requests.get(url).json()#["response"]['groups'][0]['items']
144/72: results
144/73: toronto_venues.groupby('Venue Category').count().sort_values(by='Venue', ascending=False)
144/74:
# one hot encoding
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
toronto_onehot['Neighborhood'] = manhattan_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])
toronto_onehot = toronto_onehot[fixed_columns]

toronto_onehot.head()
144/75:
# one hot encoding
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])
toronto_onehot = toronto_onehot[fixed_columns]

toronto_onehot.head()
144/76: toronto_onehot.size
144/77: toronto_onehot.shape
144/78:
# one hot encoding
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])
toronto_onehot = toronto_onehot[fixed_columns]

toronto_onehot.head()
toronto_onehot.Neighborhood
144/79:
# one hot encoding
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])
toronto_onehot = toronto_onehot[fixed_columns]

toronto_onehot.head()
144/80:
# one hot encoding
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])
toronto_onehot = toronto_onehot[fixed_columns]

toronto_onehot.columns
144/81:
# one hot encoding
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])
toronto_onehot = toronto_onehot[fixed_columns]

toronto_onehot.head()
144/82:
# one hot encoding
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")
toronto_onehot.head()
144/83:
# add neighborhood column back to dataframe
toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] 
toronto_onehot.head()
144/84:
# add neighborhood column back to dataframe
toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] 
toronto_onehot.Neighborhood
144/85:
# move neighborhood column to the first column
fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])
fixed_columns
144/86:
# add neighborhood column back to dataframe
toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] 
toronto_onehot.columns[-1]
144/87:
# add neighborhood column back to dataframe
toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] 
toronto_onehot.head()
144/88:
# one hot encoding
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")
toronto_onehot.head()
144/89:
# add neighborhood column back to dataframe
toronto_onehot= pd.DataFrame(toronto_venues['Neighborhood']).join(toronto_onehot)
toronto_onehot.head()
144/90:
# add neighborhood column back to dataframe
toronto_onehot= pd.DataFrame(toronto_venues['Neighborhood']).merge(toronto_onehot)
toronto_onehot.head()
144/91:
# add neighborhood column back to dataframe
toronto_neighborhood_df = pd.DataFrame(toronto_venues['Neighborhood'])
toronto_onehot= pd.concat([toronto_neighborhood_df,toronto_onehot], axis=1)
toronto_onehot.head()
144/92:
manhattan_grouped = manhattan_onehot.groupby('Neighborhood').mean().reset_index()
manhattan_grouped
144/93:
toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()
toronto_grouped
144/94:
# one hot encoding
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")
toronto_onehot.head()
144/95:
# concatenate Neighborhood and onehot
toronto_neighborhood_df = pd.DataFrame(toronto_venues['Neighborhood'])
toronto_onehot= pd.concat([toronto_neighborhood_df,toronto_onehot], axis=1)
toronto_onehot.head()
144/96: toronto_onehot.shape
144/97:
toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()
toronto_grouped
144/98: toronto_onehot['Neighborhood']
144/99:
# one hot encoding
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")
toronto_onehot.columns
144/100: toronto_venues.head()
144/101:
# one hot encoding
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")
toronto_onehot['Neighborhood']
144/102: toronto_venues[['Venue Category']]
144/103: 'Neighborhood'.isin(toronto_venues[['Venue Category']])
144/104: toronto_venues[['Venue Category']]
144/105: toronto_venues[['Venue Category']].loc[toronto_venues[['Venue Category']]=='Neighborhood']
144/106: toronto_venues[['Venue Category']]
144/107: type(toronto_venues[['Venue Category']])
144/108: toronto_venues[['Venue Category']]=='Neighborhood'
144/109: toronto_venues[['Venue Category']].loc[toronto_venues[['Venue Category']]=='Neighborhood']
144/110: toronto_venues[['Venue Category']]=='Neighborhood'
144/111: toronto_venues[['Venue Category']]=='Neighborhood'.any()
144/112: (toronto_venues[['Venue Category']]=='Neighborhood').any()
144/113: [False, False].any()
144/114: any([False, False])
144/115: print(toronto_venues[['Venue Category']])
144/116: print(toronto_venues[['Venue Category']].to_string())
144/117: print(toronto_venues[['Venue Category']].sort_values().to_string())
144/118: print(toronto_venues[['Venue Category']].sort_values(by='Venue Category').to_string())
144/119: toronto_venues.iloc[[619, 967, 512]]
144/120: toronto_venues = toronto_venues.loc[~toronto_venues['Venue Category']=='Neighborhood']
144/121: toronto_venues = toronto_venues.loc[~(toronto_venues['Venue Category']=='Neighborhood')]
144/122: print(toronto_venues[['Venue Category']].sort_values(by='Venue Category').to_string())
144/123:
# one hot encoding
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")
toronto_onehot.head()
144/124:
# concatenate Neighborhood and onehot
toronto_neighborhood_df = pd.DataFrame(toronto_venues['Neighborhood'])
toronto_onehot= pd.concat([toronto_neighborhood_df,toronto_onehot], axis=1)
toronto_onehot.head()
144/125:
toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()
toronto_grouped
144/126:
toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()
toronto_grouped.head()
144/127: toronto_grouped.shape()
144/128: toronto_grouped.shape
144/129:
num_top_venues = 5

for hood in toronto_grouped['Neighborhood']:
    print("----"+hood+"----")
    temp = toronto_grouped[toronto_grouped['Neighborhood'] == hood].T.reset_index()
    temp.columns = ['venue','freq']
    temp = temp.iloc[1:]
    temp['freq'] = temp['freq'].astype(float)
    temp = temp.round({'freq': 2})
    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))
    print('\n')
144/130:
def return_most_common_venues(row, num_top_venues):
    row_categories = row.iloc[1:]
    row_categories_sorted = row_categories.sort_values(ascending=False)
    
    return row_categories_sorted.index.values[0:num_top_venues]
144/131:
num_top_venues = 10

indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Neighborhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
neighborhoods_venues_sorted = pd.DataFrame(columns=columns)
neighborhoods_venues_sorted['Neighborhood'] = toronto_grouped['Neighborhood']

for ind in np.arange(toronto_grouped.shape[0]):
    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(toronto_grouped.iloc[ind, :], num_top_venues)

neighborhoods_venues_sorted.head()
144/132:
# set number of clusters
kclusters = 5

toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_[0:10]
144/133:
import pandas as pd
import numpy as np
import geocoder
import folium
import os
import requests
from sklearn.cluster import KMeans
144/134:
import pandas as pd
import numpy as np
import geocoder
import folium
import os
import requests
from sklearn.cluster import KMeans
import matplotlib.cm as cm
import matplotlib.colors as colors
144/135:
# set number of clusters
kclusters = 5

toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_[0:10]
144/136:
# add clustering labels
neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)

toronto_merged = toronto_data

# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood
toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')

toronto_merged.head() # check the last columns!
144/137: df_postal_codes.drop('index')
144/138: df_postal_codes.drop(column = 'index')
144/139: df_postal_codes.drop('index', axis=1)
144/140: df_postal_codes.loc[df_postal_codes['Postal Code'].isin(['M5G', 'M2H','M4M','M9L', 'M1B'])]
144/141: df_postal_codes.drop('index', axis=1, inplace=True)
144/142: df_postal_codes.loc[df_postal_codes['Postal Code'].isin(['M5G', 'M2H','M4M','M9L', 'M1B'])]
144/143:
# add clustering labels
neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)

toronto_merged = df_postal_codes

# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood
toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')

toronto_merged.head() # check the last columns!
144/144:
num_top_venues = 10

indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Neighborhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
neighborhoods_venues_sorted = pd.DataFrame(columns=columns)
neighborhoods_venues_sorted['Neighborhood'] = toronto_grouped['Neighborhood']

for ind in np.arange(toronto_grouped.shape[0]):
    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(toronto_grouped.iloc[ind, :], num_top_venues)

neighborhoods_venues_sorted.head()
144/145:
# set number of clusters
kclusters = 5

toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_[0:10]
144/146:
# add clustering labels
neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)

toronto_merged = df_postal_codes

# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood
toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')

toronto_merged.head() # check the last columns!
144/147:
# create map
map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)

# set color scheme for the clusters
x = np.arange(kclusters)
ys = [i + x + (i*x)**2 for i in range(kclusters)]
colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))
rainbow = [colors.rgb2hex(i) for i in colors_array]

# add markers to the map
markers_colors = []
for lat, lon, poi, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'], toronto_merged['Neighborhood'], toronto_merged['Cluster Labels']):
    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=5,
        popup=label,
        color=rainbow[cluster-1],
        fill=True,
        fill_color=rainbow[cluster-1],
        fill_opacity=0.7).add_to(map_clusters)
       
map_clusters
144/148:
# create map
map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)

# set color scheme for the clusters
x = np.arange(kclusters)
ys = [i + x + (i*x)**2 for i in range(kclusters)]
colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))
rainbow = [colors.rgb2hex(i) for i in colors_array]

# add markers to the map
markers_colors = []
for lat, lon, poi, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'], toronto_merged['Neighborhood'], toronto_merged['Cluster Labels']):
    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=5,
        popup=label,
        color=rainbow[int(cluster)-1],
        fill=True,
        fill_color=rainbow[int(cluster)-1],
        fill_opacity=0.7).add_to(map_clusters)
       
map_clusters
144/149: toronto_merged.isnull().sum()
144/150: toronto_merged.sort_values(by='Cluster Labels')
144/151: toronto_merged[toronto_merged['Cluster Labels'].isnull()]
144/152: neighborhoods_venues_sorted.head()
144/153: neighborhoods_venues_sorted[neighborhood_venus_sorted.Neighborhood=='Scarborough']
144/154: neighborhoods_venues_sorted[neighborhood_venus_sorted.Neighborhood=='Scarborough']
144/155: neighborhoods_venues_sorted[neighborhoods_venus_sorted.Neighborhood=='Scarborough']
144/156: neighborhoods_venues_sorted[neighborhoods_venues_sorted.Neighborhood=='Scarborough']
144/157: neighborhoods_venues_sorted[neighborhoods_venues_sorted.Borough=='Scarborough']
144/158: neighborhood_venues_sorted.columns
144/159: neighborhood_venues_sorted.columns()
144/160: neighborhoods_venues_sorted.columns()
144/161: neighborhoods_venues_sorted.columns
144/162: neighborhoods_venues_sorted[neighborhoods_venues_sorted.Neighborhood=='Upper Rouge']
144/163: toronto_neighborhood_df[toronot_neighborhood_df.Neighborhood=='Scarborough']
144/164: toronto_neighborhood_df[toronto_neighborhood_df.Neighborhood=='Scarborough']
144/165: toronto_neighborhood_df[toronto_neighborhood_df.Neighborhood=='Parkwoods']
144/166: toronto_csv = pd.read_csv('toronto_venues.csv')
144/167: toronto_csv.head()
144/168: toronto_csv[toronto_csv.Neighborhood=='Scarborough']
144/169: toronto_csv[toronto_csv.Borough=='Scarborough']
144/170: toronto_csv[toronto_csv.Neighborhood=='Upper Rouge']
144/171: neighborhoods_venues_sorted.loc[neighborhoods_venues_sorted.Neighborhood=='Upper Rouge']
144/172: toronto_merged.loc[toronto_merged.Neighborhood=='Upper Rouge']
144/173: neighborhoods_venues_sorted.loc[neighborhoods_venues_sorted.Neighborhood=='Upper Rouge']
144/174: toronto_grouped.loc[toronto_grouped.Neighborhood=='Upper Rouge']
144/175: toronto_neighborhood_df[toronto_neighborhood_df.Neighborhood=='Upper Rouge']
144/176: toronto_venues.loc[toronto_venues.Neighborhood=='Upper Rouge']
144/177: toronto_csv.loc[toronto_csv.Neighborhood=='Upper Rouge']
144/178: toronto_csv = pd.read_csv('Toronto_postal_codes.csv')
144/179: toronto_csv.loc[toronto_csv.Neighborhood=='Upper Rouge']
144/180:
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            '43.834768', 
            '-79.204101', 
            radius, 
            LIMIT)

        results = requests.get(url).json()["response"]['groups'][0]['items']
144/181:
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            '43.834768', 
            '-79.204101', 
            500, 
            LIMIT)

        results = requests.get(url).json()["response"]['groups'][0]['items']
144/182: results
144/183:
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            '43.834768', 
            '-79.204101', 
            500, 
            LIMIT)

        results = requests.get(url).json()
144/184: results
144/185: toronto_merged.sort_values(by='Cluster Labels').head()
144/186: toronto_merged[toronto_merged['Cluster Labels'].isnull()]
144/187:
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            '43.834768', 
            '-79.204101', 
            500, 
            LIMIT)

        print(requests.get(url).json())
144/188:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
    CLIENT_ID, 
    CLIENT_SECRET, 
    VERSION, 
    '43.834768', 
    '-79.204101', 
    500, 
    LIMIT)

print(requests.get(url).json())
144/189: toronto_merged[~toronto_merged.loc['Cluster Labels'].isnull()]
144/190: toronto_merged.loc[~toronto_merged['Cluster Labels'].isnull()]
144/191: toronto merged = toronto_merged.loc[~toronto_merged['Cluster Labels'].isnull()]
144/192: toronto_merged = toronto_merged.loc[~toronto_merged['Cluster Labels'].isnull()]
144/193:
# create map
map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)

# set color scheme for the clusters
x = np.arange(kclusters)
ys = [i + x + (i*x)**2 for i in range(kclusters)]
colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))
rainbow = [colors.rgb2hex(i) for i in colors_array]

# add markers to the map
markers_colors = []
for lat, lon, poi, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'], toronto_merged['Neighborhood'], toronto_merged['Cluster Labels']):
    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=5,
        popup=label,
        color=rainbow[int(cluster)-1],
        fill=True,
        fill_color=rainbow[int(cluster)-1],
        fill_opacity=0.7).add_to(map_clusters)
       
map_clusters
144/194:
c1 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 0, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c1
144/195:
c0 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 0, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c0.head()
144/196: c0.iloc[:,1:].stack().value_counts()[:10]
144/197:
c1 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 1, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c1.head()
144/198: c0.iloc[:,1:].stack().value_counts()[:5]
144/199: c1.iloc[:,1:].stack().value_counts()[:5]
144/200:
c2 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 2, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c2.head()
144/201: toronto_merged.iloc[[6,39]]
144/202: toronto_merged.iloc[[6,39]]
144/203:
c1 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 3, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c1.head()
144/204:
c3 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 3, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c3.head()
144/205: c3.iloc[:,1:].stack().value_counts()[:5]
144/206:
c4 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 4, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c4.head()
144/207:
import pandas as pd
import numpy as np
import geocoder
import folium
import os
import requests
from sklearn.cluster import KMeans
import matplotlib.cm as cm
import matplotlib.colors as colors
144/208: toronto_venues.shape
144/209: toronto_venues.head()
144/210: toronto_venues.to_csv('toronto_venues.csv')
144/211: (toronto_venues[['Venue Category']]=='Neighborhood').any()
144/212: toronto_venues = toronto_venues.loc[~(toronto_venues['Venue Category']=='Neighborhood')]
144/213:
def return_most_common_venues(row, num_top_venues):
    row_categories = row.iloc[1:]
    row_categories_sorted = row_categories.sort_values(ascending=False)
    
    return row_categories_sorted.index.values[0:num_top_venues]
144/214: toronto_merged.isnull().count()
144/215: toronto_merged.isnull().sum()
144/216: toronto_venues.groupby('Neighborhood').count().sort_values(by='Venue')
144/217: len(toronto_venues.Neighborhood.unique())
144/218: toronto_venues.groupby('Venue Category').count().sort_values(by='Venue', ascending=False)
144/219: (toronto_venues[['Venue Category']]=='Neighborhood').any()
144/220: toronto_venues = toronto_venues.loc[~(toronto_venues['Venue Category']=='Neighborhood')]
144/221: toronto_venue.read_csv('toronto_venues.csv')
144/222: toronto_venues = pd.read_csv('toronto_venues.csv')
144/223: toronto_venues.groupby('Neighborhood').count().sort_values(by='Venue')
144/224: len(toronto_venues.Neighborhood.unique())
144/225: toronto_venues.groupby('Venue Category').count().sort_values(by='Venue', ascending=False)
144/226: (toronto_venues[['Venue Category']]=='Neighborhood').any()
144/227: toronto_venues = toronto_venues.loc[~(toronto_venues['Venue Category']=='Neighborhood')]
144/228:
toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'],
                                   latitudes=df_postal_codes['Latitude'],
                                   longitudes=df_postal_codes['Longitude']
                                  )
144/229: toronto_venues.shape
144/230: toronto_venues.head()
144/231: toronto_venues.to_csv('toronto_venues.csv')
144/232: #toronto_venues = pd.read_csv('toronto_venues.csv')
144/233: toronto_venues.groupby('Neighborhood').count().sort_values(by='Venue')
144/234: len(toronto_venues.Neighborhood.unique())
144/235: toronto_venues.groupby('Venue Category').count().sort_values(by='Venue', ascending=False)
144/236: (toronto_venues[['Venue Category']]=='Neighborhood').any()
144/237: toronto_venues = toronto_venues.loc[~(toronto_venues['Venue Category']=='Neighborhood')]
144/238:
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")
toronto_onehot.head()
144/239:
toronto_neighborhood_df = pd.DataFrame(toronto_venues['Neighborhood'])
toronto_onehot= pd.concat([toronto_neighborhood_df,toronto_onehot], axis=1)
toronto_onehot.head()
144/240: toronto_onehot.shape
144/241:
toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()
toronto_grouped.head()
144/242: toronto_grouped.shape
144/243:
num_top_venues = 5

for hood in toronto_grouped['Neighborhood']:
    print("----"+hood+"----")
    temp = toronto_grouped[toronto_grouped['Neighborhood'] == hood].T.reset_index()
    temp.columns = ['venue','freq']
    temp = temp.iloc[1:]
    temp['freq'] = temp['freq'].astype(float)
    temp = temp.round({'freq': 2})
    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))
    print('\n')
144/244:
def return_most_common_venues(row, num_top_venues):
    row_categories = row.iloc[1:]
    row_categories_sorted = row_categories.sort_values(ascending=False)
    
    return row_categories_sorted.index.values[0:num_top_venues]
144/245:
num_top_venues = 10

indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Neighborhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
neighborhoods_venues_sorted = pd.DataFrame(columns=columns)
neighborhoods_venues_sorted['Neighborhood'] = toronto_grouped['Neighborhood']

for ind in np.arange(toronto_grouped.shape[0]):
    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(toronto_grouped.iloc[ind, :], num_top_venues)

neighborhoods_venues_sorted.head()
144/246:
kclusters = 5

toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)

kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)

kmeans.labels_[0:10]
144/247:
neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)

toronto_merged = df_postal_codes

toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')

toronto_merged.head()
144/248: toronto_merged.isnull().sum()
144/249: toronto_merged.loc[toronot_merged.isnull()]
144/250: toronto_merged.loc[toronot_merged['Clister labels'].isnull()]
144/251: toronto_merged.loc[toronot_merged['Claster labels'].isnull()]
144/252: toronto_merged.loc[toronto_merged['Claster labels'].isnull()]
144/253: toronto_merged.loc[toronto_merged['Claster Labels'].isnull()]
144/254: toronto_merged.loc[toronto_merged['Cluster Labels'].isnull()]
144/255:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
    CLIENT_ID, 
    CLIENT_SECRET, 
    VERSION, 
    '43.834768', 
    '-79.204101', 
    500, 
    LIMIT)

print(requests.get(url).json())
144/256: toronto_merged = toronto_merged.loc[~toronto_merged['Cluster Labels'].isnull()]
144/257:
map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)

x = np.arange(kclusters)
ys = [i + x + (i*x)**2 for i in range(kclusters)]
colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))
rainbow = [colors.rgb2hex(i) for i in colors_array]

markers_colors = []
for lat, lon, poi, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'], toronto_merged['Neighborhood'], toronto_merged['Cluster Labels']):
    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=5,
        popup=label,
        color=rainbow[int(cluster)-1],
        fill=True,
        fill_color=rainbow[int(cluster)-1],
        fill_opacity=0.7).add_to(map_clusters)
       
map_clusters
144/258:
c0 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 0, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c0.head()
144/259: c0.iloc[:,1:].stack().value_counts()[:5]
144/260: c0.stack().value_counts()[:5]
144/261: c0.stack().value_counts()[:10]
144/262:
c0 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 0, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c0
144/263:
neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)

toronto_merged = df_postal_codes.iloc[:,[1:]]

toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')

toronto_merged.head()
144/264:
neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)

toronto_merged = df_postal_codes.iloc[:,1:]

toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')

toronto_merged.head()
144/265:
num_top_venues = 10

indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Neighborhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
neighborhoods_venues_sorted = pd.DataFrame(columns=columns)
neighborhoods_venues_sorted['Neighborhood'] = toronto_grouped['Neighborhood']

for ind in np.arange(toronto_grouped.shape[0]):
    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(toronto_grouped.iloc[ind, :], num_top_venues)

neighborhoods_venues_sorted.head()
144/266:
kclusters = 5

toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)

kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)

kmeans.labels_[0:10]
144/267:
neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)

toronto_merged = df_postal_codes.iloc[:,1:]

toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')

toronto_merged.head()
144/268: toronto_merged.isnull().sum()
144/269: toronto_merged.loc[toronto_merged['Cluster Labels'].isnull()]
144/270:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
    CLIENT_ID, 
    CLIENT_SECRET, 
    VERSION, 
    '43.834768', 
    '-79.204101', 
    500, 
    LIMIT)

print(requests.get(url).json())
144/271: toronto_merged = toronto_merged.loc[~toronto_merged['Cluster Labels'].isnull()]
144/272:
map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)

x = np.arange(kclusters)
ys = [i + x + (i*x)**2 for i in range(kclusters)]
colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))
rainbow = [colors.rgb2hex(i) for i in colors_array]

markers_colors = []
for lat, lon, poi, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'], toronto_merged['Neighborhood'], toronto_merged['Cluster Labels']):
    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=5,
        popup=label,
        color=rainbow[int(cluster)-1],
        fill=True,
        fill_color=rainbow[int(cluster)-1],
        fill_opacity=0.7).add_to(map_clusters)
       
map_clusters
144/273:
c0 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 0, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c0
144/274: df_postal_codes.iloc[26,39]
144/275: df_postal_codes.iloc[[26,39]]
144/276: c0.stack().value_counts()[:5]
144/277:
c1 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 1, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c1.head()
144/278: toronto_merged.iloc[[6,39]]
144/279: c1.iloc[:,1:].stack().value_counts()[:5]
144/280: c1.shape
144/281:
c1 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 1, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c1.head(10)
144/282: c1['1st Most Common Venue'].values_count()
144/283: c1.values_count()
144/284: c1.value_counts()
144/285: c1['1st Most Common Venue'].value_counts()
144/286: c1['1st Most Common Venue'].value_counts()[5]
144/287: c1['1st Most Common Venue'].value_counts()[:5]
144/288: c1['1st Most Common Venue'].value_counts()[:5]
144/289:
c1 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 1, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c1.head()
144/290: c1.shape
144/291: c1.iloc[:,1:].stack().value_counts()[:5]
144/292: c1['1st Most Common Venue'].value_counts()[:5]
144/293:
c2 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 2, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c2.head()
144/294:
c3 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 3, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c3.head()
144/295: c3.iloc[:,1:].stack().value_counts()[:5]
144/296: c3.shape()
144/297: c3.shape
144/298:
c4 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 4, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c4.head()
144/299: c4.shape
144/300:
c4 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 4, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c4.head()
144/301: c4.shape
144/302: c4.iloc[:,1:].stack().value_counts()[:5]
144/303: c4['1st Most Common Venue'].value_counts()[:5]
144/304:
c4 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 4, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c4.head()
144/305: c4.shape
144/306: c4.iloc[:,1:].stack().value_counts()[:5]
144/307: c4.iloc[:,1:].stack().value_counts()[:10]
144/308:
c4 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 4, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c4.head(10)
144/309:
c4 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 4, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]
c4.head()
145/1:
import pandas as pd
import pandas as pd
import numpy as np
import folium
import requests
145/2: poznan_coordinates = [52.2428.8, 16.56011]
145/3: poznan_coordinates = [52.24288, 16.56011]
145/4: poznan_old_market = folium.Map(location=poznan_coordinates)
145/5: poznan_old_market
145/6:
poznan_coordinates = [52.4069200, 16.9299300]
# zerokość: 52.4069200° 
# Długość: 16.9299300°
145/7: poznan_old_market = folium.Map(location=poznan_coordinates)
145/8: poznan_old_market
145/9: poznan_old_market = folium.Map(location=poznan_coordinates, zoom=6)
145/10: poznan_old_market
145/11: poznan_old_market = folium.Map(location=poznan_coordinates, zoom_start=6)
145/12: poznan_old_market
145/13: poznan_old_market = folium.Map(location=poznan_coordinates, zoom_start=12)
145/14: poznan_old_market
145/15: poznan_old_market = folium.Map(location=poznan_coordinates, zoom_start=13)
145/16: poznan_old_market
145/17: poznan_old_market = folium.Map(location=poznan_coordinates, zoom_start=16)
145/18: poznan_old_market
145/19: warsaw_geofile = 'geodata/warsaw_districts/dzielnice_Warszawy.shp'
145/20: w = open(warsaw_geofile)
145/21:
for x in range(10):
    w.read_line()
145/22: w.read()
145/23: w.read_line()
145/24: w.read_lines()
145/25: warsaw_geofile = 'geodata/warsaw_districts/warsaw_districts.geojson'
145/26: w = open(warsaw_geofile)
145/27: w.read_line()
145/28: w.read_lines()
145/29: help(w)
145/30: w.readline()
145/31: w.readline()
145/32: w = open(warsaw_geofile)
145/33:
for line in w:
    print(line)
145/34:
with open(warsaw_geofile) as w:
    warsaw_geojson = w.read()
145/35: warsaw_geojson
145/36: type(warsaw_geojson)
145/37:
import pandas as pd
import pandas as pd
import numpy as np
import folium
import json
import requests
145/38:
with open(warsaw_geofile) as w:
    file_content = w.read()
    warsaw_geojson = json.load(file_content)
145/39:
with open(warsaw_geofile) as w:
    #file_content = w.read()
    warsaw_geojson = json.load(w)
145/40: type(warsaw_geojson)
145/41: warsaw_geojson.keys()
145/42: warsaw_geojson['features']
145/43: len(warsaw_geojson['features'])
145/44:
for x in warsaw_geojson['features']:
    print(x['properties'])
145/45:
url = 'https://raw.githubusercontent.com/python-visualization/folium/master/examples/data'
state_geo = f'{url}/us-states.json'
state_unemployment = f'{url}/US_Unemployment_Oct2012.csv'
state_data = pd.read_csv(state_unemployment)

m = folium.Map(location=[48, -102], zoom_start=3)

folium.Choropleth(
    geo_data=state_geo,
    name='choropleth',
    data=state_data,
    columns=['State', 'Unemployment'],
    key_on='feature.id',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
).add_to(m)

folium.LayerControl().add_to(m)
145/46:
url = 'https://raw.githubusercontent.com/python-visualization/folium/master/examples/data'
state_geo = f'{url}/us-states.json'
state_unemployment = f'{url}/US_Unemployment_Oct2012.csv'
state_data = pd.read_csv(state_unemployment)

m = folium.Map(location=[48, -102], zoom_start=3)

# folium.Choropleth(
#     geo_data=state_geo,
#     name='choropleth',
#     data=state_data,
#     columns=['State', 'Unemployment'],
#     key_on='feature.id',
#     fill_color='YlGn',
#     fill_opacity=0.7,
#     line_opacity=0.2,
#     legend_name='Unemployment Rate (%)'
# ).add_to(m)

# folium.LayerControl().add_to(m)
145/47: state_data
145/48: state_data.head()
145/49: state_unemployment
145/50: state_geo
145/51: z = requests.get(state_geo)
145/52: z.response()
145/53: z
145/54: help(z)
145/55: z.text
145/56: z.json()
145/57:
for x in warsaw_geojson['features']:
    print(x['properties']['nazwa_dzie'])
145/58:
districts_names = []
for x in warsaw_geojson['features']:
    district_names.append(x['properties']['nazwa_dzie'])
145/59:
districts_names = []
for x in warsaw_geojson['features']:
    districts_names.append(x['properties']['nazwa_dzie'])
145/60: test_density = list(range(len(districts_names)))
145/61: df = pd.DataFrame([district_names, test_denisty])
145/62: df = pd.DataFrame([districts_names, test_denisty])
145/63: df = pd.DataFrame([districts_names, test_density])
145/64: df.head()
145/65: df = pd.DataFrame([districts_names, test_density]).T()
145/66: df = pd.DataFrame([districts_names, test_density]).T
145/67: df.head()
145/68:
df = pd.DataFrame([districts_names, test_density]).T
df.columns = ['District_name', 'Density']
145/69: df.head()
145/70:
folium.Choropleth(
    geo_data=state_geo,
    name='choropleth',
    data=state_data,
    columns=['State', 'Unemployment'],
    key_on='feature.id',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
).add_to(m)

folium.LayerControl().add_to(m)
145/71:
folium.choropleth(
    geo_data=state_geo,
    name='choropleth',
    data=state_data,
    columns=['State', 'Unemployment'],
    key_on='feature.id',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
).add_to(m)

folium.LayerControl().add_to(m)
145/72:
m.choropleth(
    geo_data=state_geo,
    name='choropleth',
    data=state_data,
    columns=['State', 'Unemployment'],
    key_on='feature.id',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
)

folium.LayerControl().add_to(m)
145/73: m
145/74: warsaw_coordinates = [52.2297700, 21.0117800]
145/75:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=15)
warsaw_map
145/76:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=12)
warsaw_map
145/77:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=10)
warsaw_map
145/78:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map
145/79:
for line in w[200]:
    print(line)
145/80:
for line in w:
    print(line)
145/81: warsaw_geojson
145/82:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'density'],
    key_on='feature.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
)
145/83:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
)
145/84: warsaw_map
145/85:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
)
145/86: warsaw_map
145/87:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
)
145/88:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='features.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
)
145/89: warsaw_map
145/90:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='Feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
)
145/91: warsaw_map
145/92:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
)
145/93: warsaw_map
145/94:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
)
145/95: warsaw_map
145/96:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
#warsaw_map
145/97:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
)
145/98: warsaw_map
145/99:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
#warsaw_map
145/100:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='Feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
)
145/101: warsaw_map
145/102:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
)
145/103: warsaw_map
145/104:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
#warsaw_map
145/105:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Unemployment Rate (%)'
)
145/106: warsaw_map
145/107:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
145/108: warsaw_map
145/109:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
#warsaw_map
145/110:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
145/111: warsaw_map
145/112: df.head()
145/113:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
145/114: warsaw_map
145/115:
df = pd.DataFrame([districts_names, test_density]).T
df.columns = ['District_name', 'Density']
145/116: df.head()
145/117:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
#warsaw_map
145/118:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
145/119: warsaw_map
145/120:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
#warsaw_map
145/121:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='Feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
145/122: warsaw_map
145/123:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
#warsaw_map
145/124:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='Feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
145/125: warsaw_map
145/126:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
#warsaw_map
145/127:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='features.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
145/128: warsaw_map
145/129:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='Features.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
145/130: warsaw_map
145/131:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
#warsaw_map
145/132:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
145/133: warsaw_map
145/134: df.sort_values(by='District_name', inplace=True)
145/135:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
#warsaw_map
145/136:
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
145/137: warsaw_map
145/138: warsaw_geojson
145/139:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
warsaw_map
145/140:
with open(warsaw_geofile) as w:
    warsaw_geojson = json.load(w)
145/141:
districts_names = []
for x in warsaw_geojson['features']:
    districts_names.append(x['properties']['nazwa_dzie'])
145/142: test_density = list(range(len(districts_names)))
145/143:
df = pd.DataFrame([districts_names, test_density]).T
df.columns = ['District_name', 'Density']
145/144: df.head()
145/145: df.sort_values(by='District_name', inplace=True)
145/146:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
warsaw_map
145/147: warsaw_geojson
145/148: df.iloc[0]
145/149: df.iloc[1]
145/150: df.iloc[16]
145/151: df.iloc[18]
145/152: df.iloc[17]
145/153:
df = pd.DataFrame([districts_names, test_density]).T
df.columns = ['Nazwa_dzie', 'Density']
145/154:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['District_name', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
warsaw_map
145/155:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['Nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
warsaw_map
145/156:
df = pd.DataFrame([districts_names, test_density]).T
df.columns = ['nazwa_dzie', 'Density']
145/157:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
warsaw_map
145/158: warsaw_geofile = 'geodata/warsaw_districts/warsaw_districts.geojson'
145/159:
with open(warsaw_geofile) as w:
    warsaw_geojson = json.load(w)
145/160:
districts_names = []
for x in warsaw_geojson['features']:
    districts_names.append(x['properties']['nazwa_dzie'])
145/161: test_density = list(range(len(districts_names)))
145/162:
df = pd.DataFrame([districts_names, test_density]).T
df.columns = ['nazwa_dzie', 'Density']
145/163:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
warsaw_map
145/164:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
folium.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
).add_to(warsaw_map)
warsaw_map
145/165:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
folium.Choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
).add_to(warsaw_map)
warsaw_map
145/166: warsaw_geofile = 'geodata/warsaw_districts/warsaw_districts.geojson'
145/167:
with open(warsaw_geofile) as w:
    warsaw_geojson = json.load(w)
145/168:
districts_names = []
for x in warsaw_geojson['features']:
    districts_names.append(x['properties']['nazwa_dzie'])
145/169: test_density = list(range(len(districts_names)))
145/170:
df = pd.DataFrame([districts_names, test_density]).T
df.columns = ['nazwa_dzie', 'Density']
145/171:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
folium.Choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
).add_to(warsaw_map)
warsaw_map
145/172:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.Choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
warsaw_map
145/173:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
warsaw_map
145/174: districts_names
145/175:
for x in warsaw_geojson['features']:
    print(x['properties']['nazwa_dzie'])
145/176:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=5)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
warsaw_map
145/177: warsaw_geojson
145/178:
folium.GeoJson(
    warsaw_geojson,
    name='geojson'
).add_to(warsaw_map)
warsaw_map
145/179:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=12)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
warsaw_map
145/180:
folium.GeoJson(
    warsaw_geojson,
    name='geojson'
).add_to(warsaw_map)
warsaw_map
145/181:
warsaw_geofile = 'geodata/warsaw_districts/warsaw_districts.geojson'
cracow_geofile = 'geodata/cracow_districts/cracow_districts'
145/182:
with open(cracow_geofile) as c:
    cracow_geojson = json.load(c)
145/183:
warsaw_geofile = 'geodata/Warsaw_districts/warsaw_districts.geojson'
cracow_geofile = 'geodata/Cracow_districts/cracow_districts.geojson'
145/184:
with open(warsaw_geofile) as w:
    warsaw_geojson = json.load(w)
145/185:
with open(cracow_geofile) as c:
    cracow_geojson = json.load(c)
145/186:
districts_names = []
for x in warsaw_geojson['features']:
    districts_names.append(x['properties']['nazwa_dzie'])
145/187: test_density = list(range(len(districts_names)))
145/188:
df = pd.DataFrame([districts_names, test_density]).T
df.columns = ['nazwa_dzie', 'Density']
145/189: districts_names
145/190:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=12)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
warsaw_map
145/191:
folium.GeoJson(
    warsaw_geojson,
    name='geojson'
).add_to(warsaw_map)
warsaw_map
145/192:
cracow_coordinates = [50.06143, 19.93658]
cracow_map = folium.Map(location=cracow_coordinates, zoom_start=12)

folium.GeoJson(
    cracow_geojson,
    name='geojson'
).add_to(cracow_map)
cracow_map
145/193:
with open(cracow_geofile) as c:
    cracow_geojson = json.load(c)
145/194:
with open(cracow_geofile) as c:
    cracow_geojson = json.load(c)
145/195:
cracow_coordinates = [50.06143, 19.93658]
cracow_map = folium.Map(location=cracow_coordinates, zoom_start=12)

folium.GeoJson(
    cracow_geojson,
    name='geojson'
).add_to(cracow_map)
cracow_map
145/196:
cracow_coordinates = [50.06143, 19.93658]
cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)

folium.GeoJson(
    cracow_geojson,
    name='geojson'
).add_to(cracow_map)
cracow_map
145/197:
warsaw_geofile = 'geodata/Warsaw_districts/warsaw_districts.geojson'
cracow_geofile = 'geodata/Cracow_districts/cracow_districts.geojson'
145/198:
with open(warsaw_geofile) as w:
    warsaw_geojson = json.load(w)
145/199:
with open(cracow_geofile) as c:
    cracow_geojson = json.load(c)
145/200:
districts_names = []
for x in warsaw_geojson['features']:
    districts_names.append(x['properties']['nazwa_dzie'])
145/201: test_density = list(range(len(districts_names)))
145/202:
df = pd.DataFrame([districts_names, test_density]).T
df.columns = ['nazwa_dzie', 'Density']
145/203: districts_names
145/204:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=12)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
warsaw_map
145/205:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=10)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
warsaw_map
145/206:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Density'
)
warsaw_map
145/207: poznan_old_market = folium.Map(location=poznan_coordinates, zoom_start=16)
145/208: poznan_old_market
145/209:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
warsaw_map
145/210:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20180605'
LIMIT = 200
145/211:
import pandas as pd
import pandas as pd
import numpy as np
import folium
import json
import requests
import os
145/212:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20180605'
LIMIT = 200
145/213: CLIENT_ID
145/214:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20180605'
LIMIT = 200
RADIUS=1000
145/215:
folium.Marker(warsaw_location).add_to(warsaw_map)
warsaw_map
145/216:
folium.Marker(warsaw_coordinates).add_to(warsaw_map)
warsaw_map
145/217: lat, lng = warsaw_coordinates
145/218: lat
145/219:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
145/220:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
145/221: result = requests.get(url).response
145/222: result = requests.get(url).json()
145/223: result
145/224: result.keys()
145/225: result['meta']
145/226: result['response']
145/227: result['response'].keys()
145/228: result['response']
145/229: result['response'].keys()
145/230: result['response']['groups']
145/231: type(result['response']['groups'])
145/232:
#type(result['response']['groups'])
len(result['response']['groups'])
145/233: result['response']['groups'][0]
145/234:
#type(result['response']['groups'])
#len(result['response']['groups'])
result['response']['groups'][0].keys()
145/235: result['response']['groups'][0]['items']
145/236:
type(result['response']['groups'][0]['items'])
#len(result['response']['groups'])
#result['response']['groups'][0].keys()
145/237:
#type(result['response']['groups'][0]['items'])
len(result['response']['groups'][0]['items'])
#result['response']['groups'][0].keys()
145/238:
#type(result['response']['groups'][0]['items'])
#len(result['response']['groups'][0]['items'])
result['response']['groups'][0].keys()
145/239:
#type(result['response']['groups'][0]['items'])
#len(result['response']['groups'][0]['items'])
result['response']['groups'][0]['items'][0].keys()
145/240: result['response']['groups'][0]['items'][0]
145/241: result['response']['groups'][0]['items'][0]['name']
145/242: result['response']['groups'][0]['items'][0]['venue']
145/243: result['response']['groups'][0]['items'][0]['venue']['name']
145/244: result['response']['groups'][0]['items'][0]['venue']['categories'][0]['name']
145/245:
for item in result['response']['groups'][0]['items']:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
145/246:
#type(result['response']['groups'][0]['items'])
len(result['response']['groups'][0]['items'])
#result['response']['groups'][0]['items'][0].keys()
145/247: len(result['response']['groups'])
145/248: len(result['response']['groups']['items'])
145/249: len(result['response']['groups'][0]['items'])
145/250: result
145/251: r = requests.get(url)
145/252: r
145/253: help(r)
145/254: r.text
145/255:
for item in result['response']['groups'][0]['items']:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['lat'], item['venue']['lng']]).add_to(warsaw_map)
145/256:
for item in result['response']['groups'][0]['items']:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
145/257: warsaw_map
145/258:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20180605'
LIMIT = 500
RADIUS=1000
145/259:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
145/260: result = requests.get(url).json()
145/261: len(result['response']['groups'][0]['items'])
145/262: result['response']
145/263:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20180605'
LIMIT = 500
RADIUS=1500
145/264:
folium.Marker(warsaw_coordinates).add_to(warsaw_map)
warsaw_map
145/265: lat, lng = warsaw_coordinates
145/266:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
145/267: result = requests.get(url).json()
145/268: #r = requests.get(url)
145/269: len(result['response']['groups'][0]['items'])
145/270: result['response']
145/271: result['response'].keys()
145/272:
#type(result['response']['groups'][0]['items'])
len(result['response']['groups'][0]['items'])
#result['response']['groups'][0]['items'][0].keys()
145/273: result['response']['groups'][0]['items'][0]['name']
145/274: result['response']['groups'][0]['items'][0]['venue']['name']
145/275: result['response']['groups'][0]['items'][0]['venue']['categories'][0]['name']
145/276:
for item in result['response']['groups'][0]['items']:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
145/277: warsaw_map
145/278: result
145/279: help(r)
145/280: r.next
145/281:
x = r.next
print(x)
145/282: r.text
145/283: r.headers
145/284:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20180605'
LIMIT = 50
RADIUS=1500
145/285:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
145/286: result = requests.get(url).json()
145/287: #r = requests.get(url)
145/288: r.headers
145/289: len(result['response']['groups'][0]['items'])
145/290: result
145/291:
OFFSET=50
url_2 = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
)
145/292: result_2 = requests.get(url).json()
145/293:
for item in result_2['response']['groups'][0]['items']:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
145/294: warsaw_map
145/295: warsaw_coordinates
145/296: warsaw_districts
145/297: warsaw_geojson
145/298: len(warsaw_geojson['features'])
145/299: warsaw_geojson['features'][0]['geometry']['coordinates']
145/300: type(warsaw_geojson['features'][0]['geometry']['coordinates'])
145/301: warsaw_geojson['features'][0]['geometry']['coordinates'][0]
145/302: warsaw_geojson['features'][0]['geometry']['coordinates'][0][0]
145/303: len(warsaw_geojson['features'][0]['geometry']['coordinates'][0][0])
145/304: t = pd.DataFrame(warsaw_geojson['features'][0]['geometry']['coordinates'][0][0]))
145/305: t = pd.DataFrame(warsaw_geojson['features'][0]['geometry']['coordinates'][0][0])
145/306: t
145/307: t.columns = ['latitude', 'longitude']
145/308: t.avg(axis=1)
145/309: t.latitude.average()
145/310: t.latitude.avg()
145/311: help(t.latitude)
145/312: t.latitude.mean()
145/313: zoliborz_center = [t.latitude.mean(), t.longitude.mean()]
145/314:
folium.Marker(zoliborz_center).add_to(warsaw_map)
warsaw_map
145/315: zoliborz_center
145/316: t.head()
145/317: t.columns = ['longitude', 'latitude']
145/318: zoliborz_center = [t.latitude.mean(), t.longitude.mean()]
145/319:
folium.Marker(zoliborz_center).add_to(warsaw_map)
warsaw_map
145/320: zoliborz_center
145/321:
def get_district_center(city_geojson):
    district_geometry = pd.DataFrame(
        city_geojson['features'][0]['geometry']['coordinates'][0][0],
        columns=['longitude', 'latitude']
    )
    district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]
    
    return district_center
145/322:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]
        district_centers[city_geojson['features']['properties']['nazwa_dzie']]=district_center
    
    district_centers = pd.DataFrame(district_centers, columns=['Distric_name', 'District_center'])
    
    return district_centers
145/323: print( get_district_centers(warsaw_geojson))
145/324:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]
        district_centers[district['properties']['nazwa_dzie']]=district_center
    
    district_centers = pd.DataFrame(district_centers, columns=['Distric_name', 'District_center'])
    
    return district_centers
145/325: print( get_district_centers(warsaw_geojson))
145/326:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]
        print(district_center)
        district_centers[district['properties']['nazwa_dzie']]=district_center
    
    district_centers = pd.DataFrame(district_centers, columns=['Distric_name', 'District_center'])
    
    return district_centers
145/327: print( get_district_centers(warsaw_geojson))
145/328:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]
        #print(district_center)
        district_centers[district['properties']['nazwa_dzie']]=district_center
        print(district_centers)
    
    district_centers = pd.DataFrame(district_centers, columns=['Distric_name', 'District_center'])
    
    return district_centers
145/329: print( get_district_centers(warsaw_geojson))
145/330:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]
        #print(district_center)
        district_centers[district['properties']['nazwa_dzie']]=district_center
        #print(district_centers)
    
    district_centers = pd.DataFrame(district_centers, columns=['Distric_name', 'District_center'])
    print(district_centers)
    
    return district_centers
145/331: print( get_district_centers(warsaw_geojson))
145/332:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]
        #print(district_center)
        district_centers[district['properties']['nazwa_dzie']]=district_center
        #print(district_centers)
    
    print(district_centers)
    district_centers = pd.DataFrame(district_centers, columns=['Distric_name', 'District_center'])
    #print(district_centers)
    
    return district_centers
145/333: print( get_district_centers(warsaw_geojson))
145/334:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20180605'
LIMIT = 50
RADIUS=15000
145/335:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
145/336: result = requests.get(url).json()
145/337: #r = requests.get(url)
145/338:
# number of items depends on LIMIT parameter from the URL
len(result['response']['groups'][0]['items'])
145/339: result
145/340:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20180605'
LIMIT = 100
RADIUS=15000
145/341:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
145/342: result = requests.get(url).json()
145/343: #r = requests.get(url)
145/344:
# number of items depends on LIMIT parameter from the URL
len(result['response']['groups'][0]['items'])
145/345: result
145/346:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20180605'
LIMIT = 100
RADIUS=150
145/347:
folium.Marker(warsaw_coordinates).add_to(warsaw_map)
warsaw_map
145/348:
#folium.Marker(warsaw_coordinates).add_to(warsaw_map)
#warsaw_map
145/349: lat, lng = warsaw_coordinates
145/350:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
145/351: result = requests.get(url).json()
145/352: #r = requests.get(url)
145/353:
# number of items depends on LIMIT parameter from the URL
len(result['response']['groups'][0]['items'])
145/354: result
145/355:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=150
145/356:
#folium.Marker(warsaw_coordinates).add_to(warsaw_map)
#warsaw_map
145/357: lat, lng = warsaw_coordinates
145/358:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=15000
145/359:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=15000
145/360:
#folium.Marker(warsaw_coordinates).add_to(warsaw_map)
#warsaw_map
145/361: lat, lng = warsaw_coordinates
145/362:
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
145/363: result = requests.get(url).json()
145/364: #r = requests.get(url)
145/365:
# number of items depends on LIMIT parameter from the URL
len(result['response']['groups'][0]['items'])
145/366: result
145/367: result['response'].keys()
145/368:
#type(result['response']['groups'][0]['items'])
len(result['response']['groups'][0]['items'])
#result['response']['groups'][0]['items'][0].keys()
145/369: result['response']['groups'][0]['items'][0]['venue']['name']
145/370: result['response']['groups'][0]['items'][0]['venue']['categories'][0]['name']
145/371:
OFFSET=100
url_2 = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
)
145/372: result_2 = requests.get(url).json()
145/373:
for item in result['response']['groups'][0]['items']:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
145/374:
for item in result_2['response']['groups'][0]['items']:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
145/375: warsaw_map
145/376: a,b = [1,2]
145/377: a
145/378: result['response']['totalResults']
145/379: 238//100
145/380:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=15000
OFFSET=LIMIT
145/381: 38//100
145/382: 238//100
145/383: ['a'].extend(['b','c'])
145/384: a = ['a'].extend(['b','c'])
145/385:
a = ['a'].extend(['b','c'])
a
145/386: a
145/387: print(a)
145/388: ['a'].extend(['b','c'])
145/389:
a = ['a']
a.extend(['b','c'])
145/390: print(a)
145/391:
a = ['a']
a.append(['b','c'])
145/392: print(a)
145/393: warsaw_district_centers = get_district_centers(warsaw_geojson)
145/394: zoliborz_venues = get_venues(warsaw_district_centers['Żoliborz'])
145/395:
def get_venues(distric_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = request.get(url)
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    
    requests_to_perform = total_results/100
    
    for request in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = request.get(url)
        venues.expand(result['response']['groups'][0]['items'])
        
    return venues
145/396: warsaw_district_centers = get_district_centers(warsaw_geojson)
145/397: zoliborz_venues = get_venues(warsaw_district_centers['Żoliborz'])
145/398: warsaw_district_centers
145/399: z = {'Żoliborz': [52.2688536216614, 20.985135391429377], 'Praga-Południe': [52.235168690871, 21.071138918572952], 'Mokotów': [52.18866196405862, 21.052814696946914], 'Wola': [52.22969762335711, 20.94634273758449], 'Wilanów': [52.15030830502675, 21.091139416474352], 'Wesoła': [52.225748918635986, 21.226622601358553], 'Wawer': [52.19528077282254, 21.172119282184973], 'Włochy': [52.18437620836855, 20.942760488079408], 'Ursynów': [52.1364151251951, 21.048950071266518], 'Śródmieście': [52.228730887415864, 21.036411009522975], 'Praga-Północ': [52.26643873898277, 21.02908592658115], 'Ursus': [52.192484448920005, 20.884160623792084], 'Targówek': [52.282296973303794, 21.063078989886197], 'Rembertów': [52.257272334893905, 21.141358877459673], 'Ochota': [52.21177552923926, 20.968494312669034], 'Bielany': [52.291264937036225, 20.935172056234144], 'Białołęka': [52.328042537247185, 21.019200173516968], 'Bemowo': [52.24189442018256, 20.90545046459841]}
145/400: type(z)
145/401: c = pd.DataFrame(z)
145/402: c
145/403: c = pd.DataFrame(z).T
145/404: c
145/405:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]
        #print(district_center)
        district_centers[district['properties']['nazwa_dzie']]=district_center
        #print(district_centers)
    
    print(district_centers)
    district_centers = pd.DataFrame.from_dict(district_centers,
                                              orient='index',
                                              columns=['Distric_name', 'District_center'])
    #print(district_centers)
    
    return district_centers
145/406: warsaw_district_centers = get_district_centers(warsaw_geojson)
145/407: zoliborz_venues = get_venues(warsaw_district_centers['Żoliborz'])
145/408: warsaw_district_centers
145/409: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])
145/410:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = request.get(url)
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    
    requests_to_perform = total_results/100
    
    for request in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = request.get(url)
        venues.expand(result['response']['groups'][0]['items'])
        
    return venues
145/411: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])
145/412:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url)
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    
    requests_to_perform = total_results/100
    
    for request in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = request.get(url)
        venues.expand(result['response']['groups'][0]['items'])
        
    return venues
145/413: warsaw_district_centers
145/414: warsaw_district_centers = get_district_centers(warsaw_geojson)
145/415: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])
145/416:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    
    requests_to_perform = total_results/100
    
    for request in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = request.get(url).json()
        venues.expand(result['response']['groups'][0]['items'])
        
    return venues
145/417: warsaw_district_centers = get_district_centers(warsaw_geojson)
145/418: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])
145/419: 238/100
145/420: 238//100
145/421:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    
    requests_to_perform = total_results//100
    
    for request in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = request.get(url).json()
        venues.expand(result['response']['groups'][0]['items'])
        
    return venues
145/422: warsaw_district_centers = get_district_centers(warsaw_geojson)
145/423: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])
145/424:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.expand(result['response']['groups'][0]['items'])
        
    return venues
145/425: warsaw_district_centers = get_district_centers(warsaw_geojson)
145/426: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])
145/427: [1].expand([2])
145/428: [1].expend([2])
145/429: [1].extand([2])
145/430: [1].extend([2])
145/431:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
145/432: warsaw_district_centers = get_district_centers(warsaw_geojson)
145/433: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])
145/434: zoliborz_venues
145/435:
for item in zoliborz_venues:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
145/436: warsaw_map
145/437:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
warsaw_map
145/438:
for item in zoliborz_venues:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
145/439: warsaw_map
145/440: len(zoliborz_venues)
145/441:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=3000
OFFSET=LIMIT
145/442:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
145/443: warsaw_district_centers = get_district_centers(warsaw_geojson)
145/444: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])
145/445: len(zoliborz_venues)
145/446:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
warsaw_map
145/447:
for item in zoliborz_venues:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
145/448: warsaw_map
145/449:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=10000
OFFSET=LIMIT
145/450:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
warsaw_map
145/451: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])
145/452: warsaw_map
145/453: len(zoliborz_venues)
145/454:
for item in zoliborz_venues:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
145/455: warsaw_map
145/456:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('Total results: ', total_results)
    
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
145/457: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])
145/458:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
warsaw_map
145/459:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=15000
OFFSET=LIMIT
145/460:
#folium.Marker(warsaw_coordinates).add_to(warsaw_map)
#warsaw_map
145/461: result = requests.get(url).json()
145/462: #r = requests.get(url)
145/463: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])
145/464: zoliborz_venues = get_venues(warsaw_district_centers.loc['Wola'])
145/465: zoliborz_venues = get_venues(warsaw_district_centers.loc['Włochy'])
145/466: zoliborz_venues = get_venues(warsaw_district_centers.loc['Bielany'])
145/467: warsaw_map
145/468:
for item in zoliborz_venues:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
145/469: warsaw_map
145/470:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
warsaw_map
145/471:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=2000
OFFSET=LIMIT
145/472: zoliborz_venues = get_venues(warsaw_district_centers.loc['Bielany'])
145/473: warsaw_map
145/474:
for item in zoliborz_venues:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
145/475: warsaw_map
145/476:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=3000
OFFSET=LIMIT
145/477: zoliborz_venues = get_venues(warsaw_district_centers.loc['Bielany'])
145/478:
for item in zoliborz_venues:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
145/479: warsaw_map
145/480: zoliborz_venues = get_venues(warsaw_district_centers.loc['Wawer'])
145/481:
for item in zoliborz_venues:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
145/482: warsaw_map
145/483: zoliborz_venues = get_venues(warsaw_district_centers.loc['Śródmieście'])
145/484:
for item in zoliborz_venues:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
145/485: warsaw_map
145/486: zoliborz_venues = get_venues(warsaw_district_centers.loc['Mokotów'])
145/487:
for item in zoliborz_venues:
    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
145/488: warsaw_map
147/1:
import pandas as pd
import pandas as pd
import numpy as np
import folium
import json
import requests
import os
147/2:
# geojson files exported from .shp with QGIS (EPSG:4326 WGS 84)
warsaw_geofile = 'geodata/Warsaw_districts/warsaw_districts.geojson'
cracow_geofile = 'geodata/Cracow_districts/cracow_districts.geojson'
147/3:
with open(warsaw_geofile) as w:
    warsaw_geojson = json.load(w)
147/4:
with open(cracow_geofile) as c:
    cracow_geojson = json.load(c)
147/5:
districts_names = []
for x in warsaw_geojson['features']:
    districts_names.append(x['properties']['nazwa_dzie'])
147/6: test_density = list(range(len(districts_names)))
147/7:
df = pd.DataFrame([districts_names, test_density]).T
df.columns = ['nazwa_dzie', 'Density']
147/8:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
warsaw_map
147/9:
cracow_coordinates = [50.06143, 19.93658]
cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)

folium.GeoJson(
    cracow_geojson,
    name='geojson'
).add_to(cracow_map)
cracow_map
147/10:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=3000
OFFSET=LIMIT
147/11:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]
        district_centers[district['properties']['nazwa_dzie']]=district_center
    
    district_centers = pd.DataFrame.from_dict(district_centers,
                                              orient='index',
                                              columns=['Distric_name', 'District_center'])
    
    return district_centers
147/12:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('Total results: ', total_results)
    
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
147/13: warsaw_district_centers = get_district_centers(warsaw_geojson)
147/14: warsaw_distric_centers
147/15: warsaw_district_centers
147/16: warsaw_district_centers.index
147/17:
warsaw_districts_venues = {}
for district in warsaw_district_centers.index:
    warsaw_district_venues[district] = get_venues(warsaw_district_centers.loc[district])
147/18:
warsaw_districts_venues = {}
for district in warsaw_district_centers.index:
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district])
147/19: warsaw_districts_venues
147/20: warsaw_map
147/21:
for key in warsaw_districts_venues.keys():    
    for item in warsaw_districts_venues[key]:
        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
147/22: warsaw_map
147/23:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
147/24: warsaw_map
147/25:
for center in warsaw_districts_centers:
    #folium.Marker()
    print(center)
147/26:
for center in warsaw_district_centers:
    #folium.Marker()
    print(center)
147/27: warsaw_district_centers.head()
147/28:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    #print('Total results: ', total_results)
    
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
147/29: warsaw_district_centers = get_district_centers(warsaw_geojson)
147/30: warsaw_district_centers.head()
147/31:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]
        district_centers[district['properties']['nazwa_dzie']]=district_center
    
    print(district_centers)
    district_centers = pd.DataFrame.from_dict(district_centers,
                                              orient='index',
                                              columns=['Distric_name', 'District_center'])
    
    return district_centers
147/32: warsaw_district_centers = get_district_centers(warsaw_geojson)
147/33:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]
        district_centers[district['properties']['nazwa_dzie']]=district_center
    
    print(district_centers)
    district_centers = pd.DataFrame.from_dict(district_centers,
                                              orient='index',
                                              columns=['Distric_name', 'District_center'])
    
    return district_centers
147/34: warsaw_district_centers = get_district_centers(warsaw_geojson)
147/35:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]
        district_centers[district['properties']['nazwa_dzie']]=district_center
    
    print(district_centers)
    district_centers = pd.DataFrame.from_dict(district_centers,
                                              orient='index',
                                              columns=['District_center'])
    
    return district_centers
147/36: warsaw_district_centers = get_district_centers(warsaw_geojson)
147/37: warsaw_district_centers
147/38: warsaw_district_centers.head()
147/39:
for center in warsaw_district_centers.index:
    folium.Marker(warsaw_district_centers.loc[center]).add_to(warsaw_map)
    #print(center)
147/40: warsaw_map
147/41:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
147/42: warsaw_map
147/43:
for center in warsaw_district_centers.index:
    folium.Marker(warsaw_district_centers.loc[center]).add_to(warsaw_map)
    #print(center)
147/44: warsaw_map
147/45: warsaw_district_centers.loc['Wola']
147/46: folium.Marker(warsaw_district_centers.loc['Wola']).add_to(warsaw_map)
147/47: warsaw_map
147/48: warsaw_map
147/49:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
147/50: warsaw_map
147/51: folium.Marker(warsaw_district_centers.loc['Wola']).add_to(warsaw_map)
147/52: warsaw_map
147/53: folium.Marker(warsaw_district_centers.loc['Wola']).add_to(warsaw_map)
147/54: warsaw_map
147/55: folium.Marker([warsaw_district_centers.loc['Wola'][1], warsaw_district_centers.loc['Wola'][0]]).add_to(warsaw_map)
147/56: [warsaw_district_centers.loc['Wola'][1], warsaw_district_centers.loc['Wola'][0]]
147/57: warsaw_district_centers.loc['Wola'][1], warsaw_district_centers.loc['Wola'][0]
147/58: warsaw_district_centers.loc['Wola'][1]
147/59: warsaw_district_centers.loc['Wola']
147/60: type(warsaw_district_centers.loc['Wola'])
147/61: type(warsaw_district_centers.loc['Wola'][0])
147/62: type(warsaw_district_centers.loc['Wola'][0][0])
147/63: folium.Marker([warsaw_district_centers.loc['Wola'][0][1], warsaw_district_centers.loc['Wola'][0][0]]).add_to(warsaw_map)
147/64: warsaw_map
147/65:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
147/66: warsaw_map
147/67: folium.Marker([warsaw_district_centers.loc['Wola'][0][1], warsaw_district_centers.loc['Wola'][0][0]]).add_to(warsaw_map)
147/68: warsaw_map
147/69:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
147/70: folium.Marker([warsaw_district_centers.loc['Wola'][0][0], warsaw_district_centers.loc['Wola'][0][1]]).add_to(warsaw_map)
147/71: warsaw_map
147/72: warsaw_district_centers.loc['Wola']
147/73: list(warsaw_district_centers.loc['Wola'])
147/74: warsaw_district_centers.loc['Wola']['Distric_center']
147/75: warsaw_district_centers.loc['Wola']['District_center']
147/76: warsaw_district_centers.loc['Wola','District_center']
147/77:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
147/78:
for center in warsaw_district_centers.index:
    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)
    #print(center)
147/79: warsaw_map
147/80:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('Total results: ', total_results)
    
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
147/81:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=4000
OFFSET=LIMIT
147/82:
warsaw_districts_venues = {}
for district in warsaw_district_centers.index:
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district])
147/83:
warsaw_districts_venues = {}
for district in warsaw_district_centers.index:
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
147/84:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=2000
OFFSET=LIMIT
147/85:
warsaw_districts_venues = {}
for district in warsaw_district_centers.index:
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
147/86: len([warsaw_district_venues[key] for key in warsaw_district_venues.keys()])
147/87: len([warsaw_districts_venues[key] for key in warsaw_district_venues.keys()])
147/88: len([warsaw_districts_venues[key] for key in warsaw_districts_venues.keys()])
147/89: warsaw_district_venues['Wola']
147/90: warsaw_districts_venues['Wola']
147/91: sum(len([warsaw_districts_venues[key]) for key in warsaw_districts_venues.keys()])
147/92:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=6000
OFFSET=LIMIT
147/93:
warsaw_districts_venues = {}
for district in warsaw_district_centers.index:
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
147/94:
for key in warsaw_districts_venues.keys():    
    for item in warsaw_districts_venues[key]:
        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
147/95: warsaw_map
148/1:
import pandas as pd
import pandas as pd
import numpy as np
import folium
import json
import requests
import os
from shapely.geometry import Point, Polygon
148/2: p1 = warsaw_district_centers.loc['Wola', 'Distric_Center']
148/3:
import pandas as pd
import pandas as pd
import numpy as np
import folium
import json
import requests
import os
from shapely.geometry import Point, Polygon
148/4:
# geojson files exported from .shp with QGIS (EPSG:4326 WGS 84)
warsaw_geofile = 'geodata/Warsaw_districts/warsaw_districts.geojson'
cracow_geofile = 'geodata/Cracow_districts/cracow_districts.geojson'
148/5:
with open(warsaw_geofile) as w:
    warsaw_geojson = json.load(w)
148/6:
with open(cracow_geofile) as c:
    cracow_geojson = json.load(c)
148/7:
districts_names = []
for x in warsaw_geojson['features']:
    districts_names.append(x['properties']['nazwa_dzie'])
148/8: test_density = list(range(len(districts_names)))
148/9:
df = pd.DataFrame([districts_names, test_density]).T
df.columns = ['nazwa_dzie', 'Density']
148/10:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
148/11:
cracow_coordinates = [50.06143, 19.93658]
cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)

folium.GeoJson(
    cracow_geojson,
    name='geojson'
).add_to(cracow_map)
cracow_map
148/12:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=6000
OFFSET=LIMIT
148/13:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=2000
OFFSET=LIMIT
148/14:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]
        district_centers[district['properties']['nazwa_dzie']]=district_center
    
    print(district_centers)
    district_centers = pd.DataFrame.from_dict(district_centers,
                                              orient='index',
                                              columns=['District_center'])
    
    return district_centers
148/15:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('Total results: ', total_results)
    
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
148/16: warsaw_district_centers = get_district_centers(warsaw_geojson)
148/17:
warsaw_districts_venues = {}
for district in warsaw_district_centers.index:
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
148/18: def check_if_inside_district(venue, district_shape):
148/19: warsaw_district_centers.head()
148/20:
for center in warsaw_district_centers.index:
    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)
    #print(center)
148/21:
for key in warsaw_districts_venues.keys():    
    for item in warsaw_districts_venues[key]:
        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
148/22: warsaw_map
148/23: warsaw_geojson
148/24: p1 = warsaw_district_centers.loc['Wola', 'Distric_Center']
148/25: p1 = warsaw_district_centers.loc['Wola', 'Distric_center']
148/26: p1 = warsaw_district_centers.loc['Wola', 'District_center']
148/27: p1
148/28: warsaw_geojson['features'][0]
148/29: warsaw_geojson['features'][0]['geometry']
148/30: p1 = [warsaw_district_centers.loc['Wola', 'District_center'][1], warsaw_district_centers.loc['Wola', 'District_center'][0]]
148/31: p1
148/32: warsaw_geojson['features'][0]['geometry']['coordinates'][0]
148/33: warsaw_geojson['features'][0]['geometry']['coordinates'][0][0]
148/34: coords = warsaw_geojson['features'][0]['geometry']['coordinates'][0][0]
148/35: poly = Polygon(coords)
148/36: poly
148/37: p1 = [warsaw_district_centers.loc['Żoliborz', 'District_center'][1], warsaw_district_centers.loc['Żoliborz', 'District_center'][0]]
148/38: p1
148/39: p1.within(poly)
148/40: p1 = Point(p1)
148/41: p1.within(poly)
148/42: p2 = [warsaw_district_centers.loc['Wola', 'District_center'][1], warsaw_district_centers.loc['Wola', 'District_center'][0]]
148/43: p2 = Point(p2)
148/44: p2.within(poly)
148/45:
def check_if_inside_district(venue_coords, district_shape):
    p = Point(venue_coords)
    poly = Polygon(district_shape)
    return p.within(poly)
148/46: warsaw_district_venues
148/47: warsaw_districts_venues
148/48: warsaw_districts_venues['Żoliborz']
148/49: x = pd.DataFrame(warsaw_districts_venues)
148/50: x = pd.DataFrame.from_dict(warsaw_districts_venues)
148/51: warsaw_districts_venues['Żoliborz'][0]
148/52: warsaw_districts_venues['Żoliborz'][1]
148/53: x = pd.DataFrame.from_dict(warsaw_districts_venues['Żoliborz'])
148/54: x
148/55: x = pd.DataFrame.from_dict(warsaw_districts_venues['Żoliborz']['venue'])
148/56: x
148/57: x = pd.DataFrame.from_dict(warsaw_districts_venues['Żoliborz'][0]['venue'])
148/58: x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][0]['venue'])
148/59: x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue'])
148/60: warsaw_districts_venues['Żoliborz'][1]['venue']
148/61: warsaw_districts_venues['Żoliborz'][1]#['venue']
148/62:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
zol_venues=[]
for item in warsaw_districts_venues['Żoliborz']:
    x={}
    x['name']=item['venue']['name']
    x['lat'] = item['venue']['location']['lat']
    x['lon'] = item['venue']['location']['lon']
    zol_venues.append(x)
148/63:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
zol_venues=[]
for item in warsaw_districts_venues['Żoliborz']:
    x={}
    x['name']=item['venue']['name']
    x['lat'] = item['venue']['location']['lat']
    x['lon'] = item['venue']['location']['lng']
    zol_venues.append(x)
148/64: zol_venues
148/65: zol_df = pd.DataFrame(zol_venues)
148/66: zol_df
148/67: zol_venues_2 = dict('Żoliborz': zol_venues)
148/68: zol_venues_2 = dict(Żoliborz: zol_venues)
148/69: zol_venues_2 = dict(Żoliborz=zol_venues)
148/70: zol_df = pd.DataFrame(zol_venues_2)
148/71: zol_df
148/72: zol_df = pd.DataFrame.from_dict(zol_venues_2, orient='index')
148/73: zol_df
148/74:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
zol_venues=[]
for item in warsaw_districts_venues['Żoliborz']:
    x={}
    x['district']='Żoliborz'
    x['name']=item['venue']['name']
    x['lat'] = item['venue']['location']['lat']
    x['lon'] = item['venue']['location']['lng']
    zol_venues.append(x)
148/75: zol_venues
148/76: zol_venues_2 = dict(Żoliborz=zol_venues)
148/77: zol_df = pd.DataFrame(zol_venues_2)
148/78: zol_df
148/79:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
zol_venues=[]
for item in warsaw_districts_venues['Żoliborz']:
    x={}
    x['district']='Żoliborz'
    x['name']=item['venue']['name']
    x['lat'] = item['venue']['location']['lat']
    x['lon'] = item['venue']['location']['lng']
    zol_venues.append(x)
148/80: zol_venues
148/81: zol_df = pd.DataFrame(zol_venues)
148/82: zol_df
148/83:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
def get_district_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']='district'
            x['Name']=item['venue']['name']
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
    all_city_venues.append(district_df)
148/84: h = get_districts_venues(warsaw_districts_venues)
148/85:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']='district'
            x['Name']=item['venue']['name']
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
    all_city_venues.append(district_df)
148/86: h = get_districts_venues(warsaw_districts_venues)
148/87: h.head()
148/88:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']='district'
            x['Name']=item['venue']['name']
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
    all_city_venues.append(district_df)
    return all_city_venues
148/89: h = get_districts_venues(warsaw_districts_venues)
148/90: h.head()
148/91:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        print(district)
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']='district'
            x['Name']=item['venue']['name']
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
    all_city_venues.append(district_df)
    return all_city_venues
148/92: h = get_districts_venues(warsaw_districts_venues)
148/93:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        print(district)
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']='district'
            x['Name']=item['venue']['name']
            print(x['Name'])
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
    all_city_venues.append(district_df)
    return all_city_venues
148/94: h = get_districts_venues(warsaw_districts_venues)
148/95:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        print(district)
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']='district'
            x['Name']=item['venue']['name']
            #print(x['Name'])
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
        print(district_df.head())
    all_city_venues.append(district_df)
    return all_city_venues
148/96: h = get_districts_venues(warsaw_districts_venues)
148/97:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        print(district)
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            #print(x['Name'])
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
        print(district_df.head())
    all_city_venues.append(district_df)
    return all_city_venues
148/98: h = get_districts_venues(warsaw_districts_venues)
148/99:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        print(district)
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            #print(x['Name'])
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
       # print(district_df.head())
    all_city_venues.append(district_df)
    print(all_city_venues.head())
    return all_city_venues
148/100: h = get_districts_venues(warsaw_districts_venues)
148/101:
h = pd.DataFrame()
c = pd.DataFrame({'a':1, 'b':2})
148/102:
h = pd.DataFrame()
c = pd.DataFrame({'a'=1, 'b'=2})
148/103:
h = pd.DataFrame()
c = pd.DataFrame([{'a':1, 'b':2}])
148/104: c
148/105: h
148/106: h.append(c)
148/107:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        print(district)
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            #print(x['Name'])
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
       # print(district_df.head())
        all_city_venues.append(district_df)
        print(all_city_venues.head())
    return all_city_venues
148/108: h = get_districts_venues(warsaw_districts_venues)
148/109: h
148/110:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        print(district)
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            #print(x['Name'])
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
        print(district_df.head())
        all_city_venues.append(district_df)
        print(all_city_venues.head())
    return all_city_venues
148/111: h = get_districts_venues(warsaw_districts_venues)
148/112:
h = pd.DataFrame()
c = pd.DataFrame([{'a':1, 'b':2}])
148/113: h.append(c)
148/114: h
148/115: h.append(c, inplace=True)
148/116: h.append(c)
148/117: h
148/118: h = h.append(c)
148/119: h
148/120:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        print(district)
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            #print(x['Name'])
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
        print(district_df.head())
        all_city_venues = all_city_venues.append(district_df)
        print(all_city_venues.head())
    return all_city_venues
148/121: h = get_districts_venues(warsaw_districts_venues)
148/122: h
148/123: warsaw_geojson
148/124: cracow_geojson
150/1:
import pandas as pd
import pandas as pd
import numpy as np
import folium
import json
import requests
import os
from shapely.geometry import Point, Polygon
150/2:
# geojson files exported from .shp with QGIS (EPSG:4326 WGS 84)
warsaw_geofile = 'geodata/Warsaw_districts/warsaw_districts.geojson'
cracow_geofile = 'geodata/Cracow_districts/cracow_districts.geojson'
150/3:
with open(warsaw_geofile) as w:
    warsaw_geojson = json.load(w)
150/4:
with open(cracow_geofile) as c:
    cracow_geojson = json.load(c)
150/5:
districts_names = []
for x in warsaw_geojson['features']:
    districts_names.append(x['properties']['nazwa_dzie'])
150/6:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
150/7: test_density = list(range(len(districts_names)))
150/8:
df = pd.DataFrame([districts_names, test_density]).T
df.columns = ['nazwa_dzie', 'Density']
150/9:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
150/10:
# cracow_coordinates = [50.06143, 19.93658]
# cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)

# folium.GeoJson(
#     cracow_geojson,
#     name='geojson'
# ).add_to(cracow_map)
# cracow_map
150/11:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=2000
OFFSET=LIMIT
150/12:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]
        district_centers[district['properties']['nazwa_dzie']]=district_center
    
    print(district_centers)
    district_centers = pd.DataFrame.from_dict(district_centers,
                                              orient='index',
                                              columns=['District_center'])
    
    return district_centers
150/13:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('Total results: ', total_results)
    
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/14: warsaw_district_centers = get_district_centers(warsaw_geojson)
150/15:
warsaw_districts_venues = {}
for district in warsaw_district_centers.index:
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/16:
def check_if_inside_district(venue_coords, district_shape):
    p = Point(venue_coords)
    poly = Polygon(district_shape)
    return p.within(poly)
150/17: warsaw_district_centers.head()
150/18:
for center in warsaw_district_centers.index:
    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)
    #print(center)
150/19:
for key in warsaw_districts_venues.keys():    
    for item in warsaw_districts_venues[key]:
        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
150/20: warsaw_map
150/21: coords = warsaw_geojson['features'][0]['geometry']['coordinates'][0][0]
150/22: warsaw_geojson
150/23:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        print(district)
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            #print(x['Name'])
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
        print(district_df.head())
        all_city_venues = all_city_venues.append(district_df)
        print(all_city_venues.head())
    return all_city_venues
150/24: h = get_districts_venues(warsaw_districts_venues)
150/25:
#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        print(district)
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            #print(x['Name'])
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
       # print(district_df.head())
        all_city_venues = all_city_venues.append(district_df)
        #print(all_city_venues.head())
    return all_city_venues
150/26: h = get_districts_venues(warsaw_districts_venues)
150/27: h
150/28:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=1500
OFFSET=LIMIT
150/29:
warsaw_districts_venues = {}
for district in warsaw_district_centers.index:
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/30:
for key in warsaw_districts_venues.keys():    
    for item in warsaw_districts_venues[key]:
        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
150/31: warsaw_map
150/32:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        print(district)
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            #print(x['Name'])
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
       # print(district_df.head())
        all_city_venues = all_city_venues.append(district_df)
        #print(all_city_venues.head())
    return all_city_venues
150/33: h = get_districts_venues(warsaw_districts_venues)
150/34: h
150/35: h.size()
150/36: h.size
150/37: h.shape
150/38: h
150/39: h.Name.unique()
150/40: len(h.Name.unique())
150/41:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
150/42:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=2000
OFFSET=LIMIT
150/43:
warsaw_districts_venues = {}
for district in warsaw_district_centers.index:
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/44:
def check_if_inside_district(venue_coords, district_shape):
    p = Point(venue_coords)
    poly = Polygon(district_shape)
    return p.within(poly)
150/45: warsaw_district_centers.head()
150/46:
for center in warsaw_district_centers.index:
    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)
    #print(center)
150/47:
for key in warsaw_districts_venues.keys():    
    for item in warsaw_districts_venues[key]:
        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
150/48: warsaw_map
150/49: h.loc['Wawer']
150/50: h = get_districts_venues(warsaw_districts_venues)
150/51: h.shape
150/52: len(h.Name.unique())
150/53: h.loc['Wawer']
150/54: h
150/55: h.loc[h.District=='Wawer']
150/56: h.loc[h.District=='Wawer'].Name.unique()
150/57:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=5000
OFFSET=LIMIT
150/58:
warsaw_districts_venues = {}
for district in warsaw_district_centers.index:
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/59:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
150/60: warsaw_map
150/61:
for key in warsaw_districts_venues.keys():    
    for item in warsaw_districts_venues[key]:
        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
150/62: warsaw_map
150/63: h = get_districts_venues(warsaw_districts_venues)
150/64: len(h.Name.unique())
150/65: h
150/66: len(h.Name.unique())
150/67: h.loc[h.District=='Wawer'].Name.unique()
150/68:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]
        district_centers[district['properties']['nazwa_dzie']]=district_center
    
    district_centers = pd.DataFrame.from_dict(district_centers,
                                              orient='index',
                                              columns=['District_center'])
    
    return district_centers
150/69: warsaw_district_centers = get_district_centers(warsaw_geojson)
150/70: warsaw_district_venues
150/71: warsaw_districts_venues
150/72: warsaw_districts_venues['Żoliborz']
150/73: warsaw_districts_venues['Żoliborz'][0]
150/74:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        print(district)
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            x['VenueId'] = item['venue']['id']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/75: h = get_districts_venues(warsaw_districts_venues)
150/76: h
150/77: len(h.VenueId.unique())
150/78: h.shape
150/79: h.drop_duplicates(subset='VenueId',inplace=True)
150/80: h.shape
150/81: warsaw_geojson
150/82:
def create_districts_polygons(geojson):
    polygons = {}
    for district in geojson['features']:
        polygons[district['nazwa_dzie']]=distric['geometry'][coordinates][0][0]
    return polygons
150/83: create_districts_polygons(warsaw_geojson)
150/84:
def create_districts_polygons(geojson):
    polygons = {}
    for district in geojson['features']:
        polygons[district['nazwa_dzie']]=district['geometry'][coordinates][0][0]
    return polygons
150/85: create_districts_polygons(warsaw_geojson)
150/86:
def create_districts_polygons(geojson):
    polygons = {}
    for district in geojson['features']:
        polygons[district['nazwa_dzie']]=district['geometry']['coordinates'][0][0]
    return polygons
150/87: create_districts_polygons(warsaw_geojson)
150/88:
def create_districts_polygons(geojson):
    polygons = {}
    for district in geojson['features']:
        polygons[district['properties']['nazwa_dzie']]=district['geometry']['coordinates'][0][0]
    return polygons
150/89: create_districts_polygons(warsaw_geojson)
150/90: test_density = 1 #list(range(len(districts_names)))
150/91:
df = pd.DataFrame([districts_names, test_density]).T
df.columns = ['nazwa_dzie', 'Density']
150/92: test_density = list(range(len(districts_names)))
150/93:
df = pd.DataFrame([districts_names, test_density]).T
df.columns = ['nazwa_dzie', 'Density']
150/94:
warsaw_districts_names = []
for x in warsaw_geojson['features']:
    districts_names.append(x['properties']['nazwa_dzie'])
150/95: warsaw_district_centers
150/96: warsaw_district_centers.head()
150/97:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
150/98:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
warsaw_map
150/99:
for center in warsaw_district_centers.index:
    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)
    warsaw_map
150/100:
for center in warsaw_district_centers.index:
    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)
warsaw_map
150/101:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('\tTotal results: ', total_results, '\n')
    
    #checking if there is more results -if true, next request with offset is send
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/102:
warsaw_districts_venues = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/103:
warsaw_districts_venues = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/104:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('\tTotal results: ', total_results, '\n')
    
    #checking if there is more results -if true, next request with offset is send
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/105:
warsaw_districts_venues = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/106:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]
        district_centers[district['properties']['nazwa_dzie']]=district_center
    
    district_centers = pd.DataFrame.from_dict(district_centers,
                                              orient='index',
                                              columns=['District_center'])
    
    return district_centers
150/107:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        print(district)
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            x['VenueId'] = item['venue']['id']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/108: warsaw_districts_venues = get_district_venues(warsaw_districts_venues)
150/109: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues)
150/110:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            x['VenueId'] = item['venue']['id']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/111: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues)
150/112:
warsaw_districts_venues_foursquare = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/113:
warsaw_districts_venues_foursquare = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/114:
warsaw_districts_venues_foursquare = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/115: warsaw_district_centers = get_district_centers(warsaw_geojson)
150/116: warsaw_district_centers.head()
150/117:
for center in warsaw_district_centers.index:
    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)
warsaw_map
150/118:
warsaw_districts_venues_foursquare = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/119:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]
        district_centers[district['properties']['nazwa_dzie']]=district_center
    
    district_centers = pd.DataFrame.from_dict(district_centers,
                                              orient='index',
                                              columns=['District_center'])
    
    return district_centers
150/120:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=5000
OFFSET=LIMIT
150/121:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('\tTotal results: ', total_results, '\n')
    
    #checking if there is more results -if true, next request with offset is send
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/122: warsaw_district_centers = get_district_centers(warsaw_geojson)
150/123: warsaw_district_centers.head()
150/124:
for center in warsaw_district_centers.index:
    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)
warsaw_map
150/125:
warsaw_districts_venues_foursquare = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/126:
warsaw_districts_venues_foursquare = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues_foursquare[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/127:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            x['VenueId'] = item['venue']['id']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/128: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues)
150/129: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/130: warsaw_district_venues.head()
150/131: warsaw_districts_venues.head()
150/132: warsaw_districts_venues.shape
150/133: warsaw_districts_venues = warsaw_districts_venues.drop_duplicates(subset='VenueId')
150/134: warsaw_districts_venues.shape
150/135:
def create_districts_polygons(geojson):
    polygons = {}
    for district in geojson['features']:
        polygons[district['properties']['nazwa_dzie']]=district['geometry']['coordinates'][0][0]
    return polygons
150/136: warsaw_districts_venues.head()
150/137:
def create_districts_polygons(geojson):
    polygons = {}
    for district in geojson['features']:
        polygons[district['properties']['nazwa_dzie']]=district['geometry']['coordinates'][0][0]
    return polygons
150/138: warsaw_districts_polygons = create_districts_polygons(warsaw_geojson)
150/139: warsaw_districts_polygons.head()
150/140: warsaw_districts_polygons
150/141:
def check_if_inside_district(venue_coords, district_shape):
    p = Point(venue_coords)
    poly = Polygon(district_shape)
    return p.within(poly)
150/142:
warsaw_districts_venues['inside'] = check_if_inside_district(
                                        [warsaw_districts_venues['Lat'], warsaw_districrs_venues['Lon']],
                                        warsaw_districts_polygons[warsaw_districts_venues['District']])
150/143:
warsaw_districts_venues['inside'] = check_if_inside_district(
                                        [warsaw_districts_venues['Lat'], warsaw_districts_venues['Lon']],
                                        warsaw_districts_polygons[warsaw_districts_venues['District']])
150/144: warsaw_districts_venues.index
150/145: warsaw_districts_venues.index[90]
150/146: warsaw_districts_venues.index[390]
150/147: warsaw_districts_venues
150/148: warsaw_districts_venues.head(200)
150/149: warsaw_districts_venues.reset_index(inplace=True)
150/150: warsaw_districts_venues.index[390]
150/151: warsaw_districts_venues.loc[0]
150/152: warsaw_districts_venues.loc[0, 'District']
150/153:
for i in warsaw_districts_venues.index:
    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [warsaw_districts_venues[i,'Lat'], warsaw_districts_venues[i,'Lon']],
                                        warsaw_districts_polygons[warsaw_districts_venues[i,'District']])
150/154: warsaw_districts_venues.loc[0, 'Lat']
150/155:
for i in warsaw_districts_venues.index:
    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [warsaw_districts_venues.loc[i,'Lat'], warsaw_districts_venues.loc[i,'Lon']],
                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])
150/156: warsaw_districts_venues.head()
150/157: warsaw_districts_venues.head(60)
150/158:
for i in warsaw_districts_venues.index:
    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],
                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])
150/159: warsaw_districts_venues.head(60)
150/160: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]
150/161:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=1000
OFFSET=LIMIT
150/162:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('\tTotal results: ', total_results, '\n')
    
    #checking if there is more results -if true, next request with offset is send
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/163: warsaw_district_centers = get_district_centers(warsaw_geojson)
150/164: warsaw_district_centers.head()
150/165:
for center in warsaw_district_centers.index:
    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)
warsaw_map
150/166:
warsaw_districts_venues_foursquare = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues_foursquare[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/167:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            x['VenueId'] = item['venue']['id']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/168: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/169: warsaw_districts_venues.shape
150/170: #warsaw_districts_venues = warsaw_districts_venues.drop_duplicates(subset='VenueId')
150/171: #warsaw_districts_venues.shape
150/172: #warsaw_districts_venues.head()
150/173:
def create_districts_polygons(geojson):
    polygons = {}
    for district in geojson['features']:
        polygons[district['properties']['nazwa_dzie']]=district['geometry']['coordinates'][0][0]
    return polygons
150/174: warsaw_districts_polygons = create_districts_polygons(warsaw_geojson)
150/175: warsaw_districts_polygons
150/176:
def check_if_inside_district(venue_coords, district_shape):
    p = Point(venue_coords)
    poly = Polygon(district_shape)
    return p.within(poly)
150/177:
for i in warsaw_districts_venues.index:
    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],
                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])
150/178: warsaw_districts_venues.reset_index(inplace=True)
150/179:
for i in warsaw_districts_venues.index:
    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],
                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])
150/180: warsaw_districts_venues.reset_index(inplace=True)
150/181: warsaw_districts_venues.index[390]
150/182: warsaw_districts_venues.loc[0, 'Lat']
150/183: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]
150/184:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=1500
OFFSET=LIMIT
150/185:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('\tTotal results: ', total_results, '\n')
    
    #checking if there is more results -if true, next request with offset is send
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/186: warsaw_district_centers = get_district_centers(warsaw_geojson)
150/187: warsaw_district_centers.head()
150/188:
for center in warsaw_district_centers.index:
    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)
warsaw_map
150/189:
warsaw_districts_venues_foursquare = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues_foursquare[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/190:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            x['VenueId'] = item['venue']['id']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/191: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/192: warsaw_districts_venues.shape
150/193: #warsaw_districts_venues = warsaw_districts_venues.drop_duplicates(subset='VenueId')
150/194: #warsaw_districts_venues.shape
150/195: #warsaw_districts_venues.head()
150/196:
def create_districts_polygons(geojson):
    polygons = {}
    for district in geojson['features']:
        polygons[district['properties']['nazwa_dzie']]=district['geometry']['coordinates'][0][0]
    return polygons
150/197: warsaw_districts_polygons = create_districts_polygons(warsaw_geojson)
150/198: warsaw_districts_polygons
150/199:
def check_if_inside_district(venue_coords, district_shape):
    p = Point(venue_coords)
    poly = Polygon(district_shape)
    return p.within(poly)
150/200: warsaw_districts_venues.reset_index(inplace=True)
150/201:
for i in warsaw_districts_venues.index:
    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],
                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])
150/202: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]
150/203:
for key in warsaw_districts_venues.keys():    
    for item in warsaw_districts_venues[key]:
        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])
        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)
150/204:
for i in warsaw_districts_venues.index:    
    folium.Marker(warsaw_districts_venues.loc[i,'Lat'], warsaw_districts_venues.loc[i,'Lon']).add_to(warsaw_map)
150/205: warsaw_districts_venues.loc[0,'Lat']
150/206: warsaw_districts_venues.loc[0,'Lon']
150/207:
for i in warsaw_districts_venues.index:    
    folium.Marker([warsaw_districts_venues.loc[i,'Lat'], warsaw_districts_venues.loc[i,'Lon']]).add_to(warsaw_map)
150/208: warsaw_map
150/209:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
150/210:
# cracow_coordinates = [50.06143, 19.93658]
# cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)

# folium.GeoJson(
#     cracow_geojson,
#     name='geojson'
# ).add_to(cracow_map)
# cracow_map
150/211:
def get_district_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]
        district_centers[district['properties']['nazwa_dzie']]=district_center
    
    district_centers = pd.DataFrame.from_dict(district_centers,
                                              orient='index',
                                              columns=['District_center'])
    
    return district_centers
150/212:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=1500
OFFSET=LIMIT
150/213:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('\tTotal results: ', total_results, '\n')
    
    #checking if there is more results -if true, next request with offset is send
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/214:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=2500
OFFSET=LIMIT
150/215:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=2500
OFFSET=LIMIT
150/216:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('\tTotal results: ', total_results, '\n')
    
    #checking if there is more results -if true, next request with offset is send
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/217: warsaw_district_centers = get_district_centers(warsaw_geojson)
150/218: warsaw_district_centers.head()
150/219:
for center in warsaw_district_centers.index:
    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)
warsaw_map
150/220:
warsaw_districts_venues_foursquare = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues_foursquare[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/221:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            x['VenueId'] = item['venue']['id']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/222: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/223: warsaw_districts_venues.shape
150/224: warsaw_districts_venues.reset_index(inplace=True)
150/225:
for i in warsaw_districts_venues.index:
    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],
                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])
150/226: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]
150/227:
for i in warsaw_districts_venues.index:    
    folium.Marker([warsaw_districts_venues.loc[i,'Lat'], warsaw_districts_venues.loc[i,'Lon']]).add_to(warsaw_map)
150/228: warsaw_map
150/229:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
150/230:
# cracow_coordinates = [50.06143, 19.93658]
# cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)

# folium.GeoJson(
#     cracow_geojson,
#     name='geojson'
# ).add_to(cracow_map)
# cracow_map
150/231:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=3500
OFFSET=LIMIT
150/232:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('\tTotal results: ', total_results, '\n')
    
    #checking if there is more results -if true, next request with offset is send
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/233:
warsaw_districts_venues_foursquare = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues_foursquare[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/234:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            x['VenueId'] = item['venue']['id']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/235: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/236: warsaw_districts_venues.shape
150/237: warsaw_districts_venues.reset_index(inplace=True)
150/238:
for i in warsaw_districts_venues.index:
    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],
                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])
150/239: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]
150/240:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
150/241:
# cracow_coordinates = [50.06143, 19.93658]
# cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)

# folium.GeoJson(
#     cracow_geojson,
#     name='geojson'
# ).add_to(cracow_map)
# cracow_map
150/242:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=3000
OFFSET=LIMIT
150/243:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('\tTotal results: ', total_results, '\n')
    
    #checking if there is more results -if true, next request with offset is send
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/244:
warsaw_districts_venues_foursquare = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues_foursquare[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/245: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/246: warsaw_districts_venues.shape
150/247: warsaw_districts_venues.reset_index(inplace=True)
150/248:
for i in warsaw_districts_venues.index:
    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],
                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])
150/249: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]
150/250:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=2500
OFFSET=LIMIT
150/251:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('\tTotal results: ', total_results, '\n')
    
    #checking if there is more results -if true, next request with offset is send
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/252: warsaw_district_centers = get_district_centers(warsaw_geojson)
150/253: warsaw_district_centers.head()
150/254:
warsaw_districts_venues_foursquare = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues_foursquare[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])
150/255:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            x['VenueId'] = item['venue']['id']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/256: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/257: warsaw_districts_venues.shape
150/258: warsaw_districts_venues.reset_index(inplace=True)
150/259:
for i in warsaw_districts_venues.index:
    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],
                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])
150/260: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]
150/261:
for i in warsaw_districts_venues.index:    
    folium.Marker([warsaw_districts_venues.loc[i,'Lat'], warsaw_districts_venues.loc[i,'Lon']]).add_to(warsaw_map)
150/262: warsaw_map
150/263: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer']
150/264: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer'].unique()
150/265: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer'].unique
150/266: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer']
150/267: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer'].VenueId.unique()
150/268: len(warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer'].VenueId.unique())
150/269: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer']
150/270: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer'].head()
150/271: warsaw_districts_centers
150/272: warsaw_district_centers
150/273: x = get_venues(warsaw_district_centers.loc['Wawer', 'District_center'])
150/274: x = get_venues(warsaw_district_centers.loc['Bielany', 'District_center'])
150/275:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=2700
OFFSET=LIMIT
150/276:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('\tTotal results: ', total_results, '\n')
    
    #checking if there is more results -if true, next request with offset is send
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/277: x = get_venues(warsaw_district_centers.loc['Bielany', 'District_center'])
150/278: x = get_venues(warsaw_district_centers.loc['Wawer', 'District_center'])
150/279: warsaw_districts_venues.groupby('Category').value_counts()
150/280: x = get_venues(warsaw_district_centers.loc['Wawer', 'District_center'])
150/281: warsaw_districts_venues.groupby('Category')['VenueId'].value_counts()
150/282: warsaw_districts_venues.groupby('Category')['VenueId'].value_counts(sort=True)
150/283: warsaw_districts_venues.groupby('Category').value_counts(sort=True)
150/284: warsaw_districts_venues.groupby('Category').count()
150/285: warsaw_districts_venues.groupby('Category').count().sort_values(by='VenueId')
150/286: warsaw_districts_venues.groupby('Category').count().sort_values(by='VenueId', ascending=False)
150/287: warsaw_districts_venues.groupby('Category').count().sort_values(by='VenueId', ascending=False).head(10)
150/288:
warsaw_coordinates = [52.2297700, 21.0117800]
warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
warsaw_map.choropleth(
    geo_data=warsaw_geojson,
    name='choropleth',
    data=df,
    columns=['nazwa_dzie', 'Density'],
    key_on='feature.properties.nazwa_dzie',
    fill_color='YlGn',
    fill_opacity=0.7,
    line_opacity=0.9,
    legend_name='Density'
)
#warsaw_map
150/289: warsaw_map
150/290: x
150/291: x = get_district_venues(x)
150/292: x = get_districts_venues(x)
150/293: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wesoła'].head()
150/294: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wesoła'].head(10)
150/295: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wesoła'].head(20)
150/296: x
150/297:
def extract_data(district_venues_foursquare):
    district_venues=[]
    for item in district_venues_foursquare:
        x={}
        #x['District']=district
        x['Name']=item['venue']['name']
        x['Category']=item['venue']['categories'][0]['name']
        x['Lat'] = item['venue']['location']['lat']
        x['Lon'] = item['venue']['location']['lng']
        x['VenueId'] = item['venue']['id']
        district_venues.append(x)
    district_df = pd.DataFrame(district_venues)
    return district_df
150/298: z = extract_data(x)
150/299: z
150/300: x = get_venues(warsaw_district_centers.loc['Wesoła', 'District_center'])
150/301: z = extract_data(x)
150/302: z
150/303:
def pin_venues(venues_df,city_map):
    for i in venues_df.index:
        folium.Marker([venues_df.loc[i,'Lat'], venues_df.loc[i,'Lon']]).add_to(city_map)
150/304: pin_venues(z, warsaw_map)
150/305: warsaw_map
150/306: x = get_venues(warsaw_district_centers.loc['Białołęka', 'District_center'])
150/307: z = extract_data(x)
150/308: z.head()
150/309: pin_venues(z, warsaw_map)
150/310: warsaw_map
150/311:
def create_warsaw_map():    
    warsaw_coordinates = [52.2297700, 21.0117800]
    warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
    warsaw_map.choropleth(
        geo_data=warsaw_geojson,
        name='choropleth',
        data=df,
        columns=['nazwa_dzie', 'Density'],
        key_on='feature.properties.nazwa_dzie',
        fill_color='YlGn',
        fill_opacity=0.7,
        line_opacity=0.9,
        legend_name='Density'
    )
#warsaw_map
150/312:
def create_warsaw_map():    
    warsaw_coordinates = [52.2297700, 21.0117800]
    warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
    warsaw_map.choropleth(
        geo_data=warsaw_geojson,
        name='choropleth',
        data=df,
        columns=['nazwa_dzie', 'Density'],
        key_on='feature.properties.nazwa_dzie',
        fill_color='YlGn',
        fill_opacity=0.7,
        line_opacity=0.9,
        legend_name='Density'
    )
    return warsaw_map
150/313: warsaw_map = create_warsaw_map
150/314: x = get_venues(warsaw_district_centers.loc['Wawer', 'District_center'])
150/315: warsaw_map = create_warsaw_map
150/316: z = extract_data(x)
150/317: z.head()
150/318: pin_venues(z, warsaw_map)
150/319: warsaw_map = create_warsaw_map()
150/320: z = extract_data(x)
150/321: z.head()
150/322:
def pin_venues(venues_df,city_map):
    for i in venues_df.index:
        folium.Marker([venues_df.loc[i,'Lat'], venues_df.loc[i,'Lon']]).add_to(city_map)
150/323: pin_venues(z, warsaw_map)
150/324: warsaw_map
150/325:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=3500
OFFSET=LIMIT
150/326:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('\tTotal results: ', total_results, '\n')
    
    #checking if there is more results -if true, next request with offset is send
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/327: warsaw_districts_venues.groupby('Category').count().sort_values(by='VenueId', ascending=False).head(5)
150/328: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wesoła'].head(5)
150/329: warsaw_map = create_warsaw_map()
150/330: z = extract_data(x)
150/331: z.head()
150/332:
def pin_venues(venues_df,city_map):
    for i in venues_df.index:
        folium.Marker([venues_df.loc[i,'Lat'], venues_df.loc[i,'Lon']]).add_to(city_map)
150/333: pin_venues(z, warsaw_map)
150/334: warsaw_map
150/335:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=4000
OFFSET=LIMIT
150/336:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('\tTotal results: ', total_results, '\n')
    
    #checking if there is more results -if true, next request with offset is send
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/337: x = get_venues(warsaw_district_centers.loc['Wawer', 'District_center'])
150/338: z = extract_data(x)
150/339: z.head()
150/340:
def pin_venues(venues_df,city_map):
    for i in venues_df.index:
        folium.Marker([venues_df.loc[i,'Lat'], venues_df.loc[i,'Lon']]).add_to(city_map)
150/341: pin_venues(z, warsaw_map)
150/342: warsaw_map
150/343:
CLIENT_ID = os.environ.get('FOURSQUAREID')
CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')
VERSION = '20200605'
LIMIT = 100
RADIUS=4500
OFFSET=LIMIT
150/344:
def get_venues(district_center):
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('\tTotal results: ', total_results, '\n')
    
    #checking if there is more results -if true, next request with offset is send
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/345: x = get_venues(warsaw_district_centers.loc['Wawer', 'District_center'])
150/346: warsaw_map = create_warsaw_map()
150/347:
def extract_data(district_venues_foursquare):
    district_venues=[]
    for item in district_venues_foursquare:
        x={}
        #x['District']=district
        x['Name']=item['venue']['name']
        x['Category']=item['venue']['categories'][0]['name']
        x['Lat'] = item['venue']['location']['lat']
        x['Lon'] = item['venue']['location']['lng']
        x['VenueId'] = item['venue']['id']
        district_venues.append(x)
    district_df = pd.DataFrame(district_venues)
    return district_df
150/348: z = extract_data(x)
150/349: z.head()
150/350:
def pin_venues(venues_df,city_map):
    for i in venues_df.index:
        folium.Marker([venues_df.loc[i,'Lat'], venues_df.loc[i,'Lon']]).add_to(city_map)
150/351: pin_venues(z, warsaw_map)
150/352: warsaw_map
150/353: x = get_venues(warsaw_district_centers.loc['Białołęka', 'District_center'])
150/354: z = extract_data(x)
150/355: pin_venues(z, warsaw_map)
150/356: warsaw_map
150/357: warsaw_districts_polygons
150/358: warsaw_districts_centers
150/359: warsaw_district_centers
150/360:
def calculate_radius(district_name):
    center_coords = warsaw_districts_centers(district_name)
    center_point = Point(center_coords)
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons(district_name):
        polygon_point = Point(point_coords)
        polygon_point.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    return max(distances)
150/361: max([0,1])
150/362: c = calculate_radius('Wawer')
150/363:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers(district_name)
    center_point = Point(center_coords)
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons(district_name):
        polygon_point = Point(point_coords)
        polygon_point.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    return max(distances)
150/364: max([0,1])
150/365: c = calculate_radius('Wawer')
150/366:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc(district_name)
    center_point = Point(center_coords)
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons(district_name):
        polygon_point = Point(point_coords)
        polygon_point.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    return max(distances)
150/367: max([0,1])
150/368: c = calculate_radius('Wawer')
150/369: warsaw_district_centers
150/370: warsaw_district_centers.loc['Wawer']
150/371:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name]
    center_point = Point(center_coords)
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons(district_name):
        polygon_point = Point(point_coords)
        polygon_point.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    return max(distances)
150/372: c = calculate_radius('Wawer')
150/373:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name]
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons(district_name):
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_point.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    return max(distances)
150/374: c = calculate_radius('Wawer')
150/375:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name]
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons(district_name):
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_point.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    return max(distances)
150/376: c = calculate_radius('Wawer')
150/377:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name]
    print(center_coords)
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons(district_name):
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_point.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    return max(distances)
150/378: c = calculate_radius('Wawer')
150/379: warsaw_district_centers.loc['Wawer'][1]
150/380: warsaw_district_centers.loc['Wawer']
150/381: warsaw_district_centers#.loc['Wawer']
150/382: warsaw_district_centers.loc['Wawer']
150/383: warsaw_district_centers.loc['Wawer'][0]
150/384: list(warsaw_district_centers.loc['Wawer'])
150/385: list(warsaw_district_centers.loc['Wawer', 'Distric_center'])
150/386: list(warsaw_district_centers.loc['Wawer', 'District_center'])
150/387:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name, 'District_center']
    print(center_coords)
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons(district_name):
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_point.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    return max(distances)
150/388: c = calculate_radius('Wawer')
150/389:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name, 'District_center']
    print(center_coords)
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons [district_name]:
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_point.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    return max(distances)
150/390: c = calculate_radius('Wawer')
150/391:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name, 'District_center']
    print(center_coords)
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons [district_name]:
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_points.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    return max(distances)
150/392: c = calculate_radius('Wawer')
150/393: c
150/394: k = 0.6157*111
150/395: c*k
150/396:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name, 'District_center']
    print(center_coords)
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons [district_name]:
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_points.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    max_distance = max(distances)    
    return max(distances)
150/397:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name, 'District_center']
    print(center_coords)
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons [district_name]:
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_points.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    max_distance = max(distances)
    factor = 0.6157*111/1000
    max_distance_meters = max_distance*factor
    
    return max_distance_meters
150/398: c = calculate_radius('Wawer')
150/399:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name, 'District_center']
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons [district_name]:
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_points.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    max_distance = max(distances)
    factor = 0.6157*111/1000
    max_distance_meters = max_distance*factor
    
    return max_distance_meters
150/400: c = calculate_radius('Wawer')
150/401: c
150/402:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name, 'District_center']
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons [district_name]:
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_points.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    max_distance = max(distances)
    factor = 0.6157*111
    max_distance_meters = max_distance*factor
    
    return max_distance_meters
150/403: c = calculate_radius('Wawer')
150/404: c
150/405:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name, 'District_center']
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons [district_name]:
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_points.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    max_distance = max(distances)
    factor = 0.6157*111*1000
    max_distance_meters = max_distance*factor
    
    return max_distance_meters
150/406: c = calculate_radius('Wawer')
150/407: c
150/408: c = calculate_radius('Wesoła')
150/409: c
150/410: DISTANCE
150/411: RADIUS
150/412:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name, 'District_center']
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons [district_name]:
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_points.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    max_distance = max(distances)
    factor = 0.6157*111,3*1000
    max_distance_meters = max_distance*factor
    
    return max_distance_meters
150/413: c = calculate_radius('Wesoła')
150/414:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name, 'District_center']
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons [district_name]:
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_points.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    max_distance = max(distances)
    factor = 0.6157*111.3*1000
    max_distance_meters = max_distance*factor
    
    return max_distance_meters
150/415: c = calculate_radius('Wesoła')
150/416: c
150/417: c = calculate_radius('Żoliborz')
150/418: c
150/419: c = calculate_radius('Wawer')
150/420: c
150/421: warsaw_district_centers
150/422:
warsaw_districts_radiuses = {}
for district in warsaw_districts_centers.index:
    warsaw_districts_radiuses[district] = calculate_radius(district)
150/423:
warsaw_districts_radiuses = {}
for district in warsaw_district_centers.index:
    warsaw_districts_radiuses[district] = calculate_radius(district)
150/424: warsaw_districts_radiuses
150/425:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name, 'District_center']
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons [district_name]:
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_points.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    max_distance = max(distances)
    factor = 0.6157*111.3*1000
    max_distance_meters = int(max_distance*factor)
    
    return max_distance_meters
150/426: c = calculate_radius('Wawer')
150/427:
warsaw_districts_radiuses = {}
for district in warsaw_district_centers.index:
    warsaw_districts_radiuses[district] = calculate_radius(district)
150/428: warsaw_districts_radiuses
150/429:
def calculate_radius(district_name):
    center_coords = warsaw_district_centers.loc[district_name, 'District_center']
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in warsaw_districts_polygons [district_name]:
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_points.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    max_distance = max(distances)
    factor = 0.6157*111.3*1000
    max_distance_meters = int(max_distance*factor)
    
    return max_distance_meters
150/430:
warsaw_districts_radiuses = {}
for district in warsaw_district_centers.index:
    warsaw_districts_radiuses[district] = calculate_radius(district)
150/431:
def get_venues(district_center, radius):
    RADIUS = radius
    lat, lng = district_center
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT)
    result = requests.get(url).json()
    
    venues = result['response']['groups'][0]['items']
    
    total_results = result['response']['totalResults']
    print('\tTotal results: ', total_results, '\n')
    
    #checking if there is more results -if true, next request with offset is send
    requests_to_perform = total_results//100
    
    for _ in range(requests_to_perform):
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            RADIUS, 
            LIMIT,
            OFFSET
        )
        result = requests.get(url).json()
        venues.extend(result['response']['groups'][0]['items'])
        
    return venues
150/432:
warsaw_districts_venues_foursquare = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues_foursquare[district] = get_venues(
        warsaw_district_centers.loc[district, 'District_center'],
        warsaw_districts_radiuses[district])
150/433:
warsaw_districts_venues_foursquare = {}
for district in warsaw_district_centers.index:
    print(district)
    warsaw_districts_venues_foursquare[district] = get_venues(
        warsaw_district_centers.loc[district, 'District_center'],
        warsaw_districts_radiuses[district])
150/434:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_venues=[]
        for item in city_districts_venues[district]:
            x={}
            x['District']=district
            x['Name']=item['venue']['name']
            x['Category']=item['venue']['categories'][0]['name']
            x['Lat'] = item['venue']['location']['lat']
            x['Lon'] = item['venue']['location']['lng']
            x['VenueId'] = item['venue']['id']
            district_venues.append(x)
        district_df = pd.DataFrame(district_venues)
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/435: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/436: warsaw_districts_venues.shape
150/437:
def create_districts_polygons(geojson):
    polygons = {}
    for district in geojson['features']:
        polygons[district['properties']['nazwa_dzie']]=district['geometry']['coordinates'][0][0]
    return polygons
150/438: warsaw_districts_polygons = create_districts_polygons(warsaw_geojson)
150/439:
def check_if_inside_district(venue_coords, district_shape):
    p = Point(venue_coords)
    poly = Polygon(district_shape)
    return p.within(poly)
150/440: warsaw_districts_venues.reset_index(inplace=True)
150/441:
for i in warsaw_districts_venues.index:
    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],
                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])
150/442: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]
150/443: warsaw_map = create_warsaw_map()
150/444:
for i in warsaw_districts_venues.index:    
    folium.Marker([warsaw_districts_venues.loc[i,'Lat'], warsaw_districts_venues.loc[i,'Lon']]).add_to(warsaw_map)
150/445: warsaw_map
150/446: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wesoła'].head(20)
150/447: warsaw_districts_venues = warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]
150/448: warsaw_map = create_warsaw_map()
150/449:
for i in warsaw_districts_venues.index:    
    folium.Marker([warsaw_districts_venues.loc[i,'Lat'], warsaw_districts_venues.loc[i,'Lon']]).add_to(warsaw_map)
150/450: warsaw_map
150/451:
def create_districts_polygons(geojson):
    polygons = {}
    for district in geojson['features']:
        polygons[district['properties']['nazwa_dzie']]=district['geometry']['coordinates'][0][0]
    return polygons
150/452: warsaw_districts_polygons = create_districts_polygons(warsaw_geojson)
150/453: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wesoła'].head()
150/454: warsaw_top5 = warsaw_districts_venues.groupby('Category').count().sort_values(by='VenueId', ascending=False).head()
150/455: warsaw_top5
150/456:
def extract_data(district_venues_foursquare):
    district_venues=[]
    for item in district_venues_foursquare:
        x={}
        x['Name']=item['venue']['name']
        x['Category']=item['venue']['categories'][0]['name']
        x['Lat'] = item['venue']['location']['lat']
        x['Lon'] = item['venue']['location']['lng']
        x['VenueId'] = item['venue']['id']
        district_venues.append(x)
    district_df = pd.DataFrame(district_venues)
    return district_df
150/457:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_df = extract_data(district)
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/458: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/459:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_df = extract_data[city_district_venues]
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/460:
# def get_districts_venues(city_districts_venues):
#     all_city_venues= pd.DataFrame()
#     for district in city_districts_venues.keys():
#         district_venues=[]
#         for item in city_districts_venues[district]:
#             x={}
#             x['District']=district
#             x['Name']=item['venue']['name']
#             x['Category']=item['venue']['categories'][0]['name']
#             x['Lat'] = item['venue']['location']['lat']
#             x['Lon'] = item['venue']['location']['lng']
#             x['VenueId'] = item['venue']['id']
#             district_venues.append(x)
#         district_df = pd.DataFrame(district_venues)
#         all_city_venues = all_city_venues.append(district_df)
#     return all_city_venues
150/461: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/462:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_df = extract_data[city_districts_venues]
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/463:
# def get_districts_venues(city_districts_venues):
#     all_city_venues= pd.DataFrame()
#     for district in city_districts_venues.keys():
#         district_venues=[]
#         for item in city_districts_venues[district]:
#             x={}
#             x['District']=district
#             x['Name']=item['venue']['name']
#             x['Category']=item['venue']['categories'][0]['name']
#             x['Lat'] = item['venue']['location']['lat']
#             x['Lon'] = item['venue']['location']['lng']
#             x['VenueId'] = item['venue']['id']
#             district_venues.append(x)
#         district_df = pd.DataFrame(district_venues)
#         all_city_venues = all_city_venues.append(district_df)
#     return all_city_venues
150/464: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/465:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_df = extract_data(city_districts_venues[district])
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/466:
# def get_districts_venues(city_districts_venues):
#     all_city_venues= pd.DataFrame()
#     for district in city_districts_venues.keys():
#         district_venues=[]
#         for item in city_districts_venues[district]:
#             x={}
#             x['District']=district
#             x['Name']=item['venue']['name']
#             x['Category']=item['venue']['categories'][0]['name']
#             x['Lat'] = item['venue']['location']['lat']
#             x['Lon'] = item['venue']['location']['lng']
#             x['VenueId'] = item['venue']['id']
#             district_venues.append(x)
#         district_df = pd.DataFrame(district_venues)
#         all_city_venues = all_city_venues.append(district_df)
#     return all_city_venues
150/467: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/468: warsaw_districts_venues.shape
150/469: warsaw_map
150/470: cracow_geojson
150/471: cracow_geojson['features']
150/472:
cracow_districts_names = []
for x in warsaw_geojson['features']:
    districts_names.append(x['properties']['nazwa'])
150/473:
cracow_districts_names = []
for x in cracow_geojson['features']:
    districts_names.append(x['properties']['nazwa'])
150/474: cracow_districts_names
150/475: cracow_geojson['features'][0]
150/476:
cracow_districts_names = []
for x in cracow_geojson['features']:
    districts_names.append(x['properties']['nazwa'])
150/477: cracow_geojson['features'][0]
150/478: cracow_districts_names
150/479:
cracow_districts_names = []
for x in cracow_geojson['features']:
    cracow_districts_names.append(x['properties']['nazwa'])
150/480: cracow_geojson['features'][0]
150/481: cracow_districts_names
150/482: cracow_test_density = list(range(len(districts_names)))
150/483:
df = pd.DataFrame([cracow_districts_names, test_density]).T
df.columns = ['nazwa', 'Density']
150/484:
def create_cracow_map():    
    cracow_coordinates = [50.06143, 19.93658]
    cracow_map = folium.Map(location=warsaw_coordinates, zoom_start=11)
    cracow_map.choropleth(
        geo_data=cracow_geojson,
        name='choropleth',
        data=df,
        columns=['nazwa', 'Density'],
        key_on='feature.properties.nazwa',
        fill_color='YlGn',
        fill_opacity=0.7,
        line_opacity=0.9,
        legend_name='Density'
    )
    return cracow_map
150/485:
cracow_map = create_cracow_map
cracow_map
150/486:
cracow_map = create_cracow_map()
cracow_map
150/487:
def create_cracow_map():    
    cracow_coordinates = [50.06143, 19.93658]
    cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)
    cracow_map.choropleth(
        geo_data=cracow_geojson,
        name='choropleth',
        data=df,
        columns=['nazwa', 'Density'],
        key_on='feature.properties.nazwa',
        fill_color='YlGn',
        fill_opacity=0.7,
        line_opacity=0.9,
        legend_name='Density'
    )
    return cracow_map
150/488:
cracow_map = create_cracow_map()
cracow_map
150/489:
def get_cracow_centers(city_geojson):
    district_centers = {}
    for district in city_geojson['features']:
        district_geometry = pd.DataFrame(
            district['geometry']['coordinates'][0][0],
            columns=['longitude', 'latitude']
        )
        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]
        district_centers[district['properties']['nazwa']]=district_center
    
    district_centers = pd.DataFrame.from_dict(district_centers,
                                              orient='index',
                                              columns=['District_center'])
    
    return district_centers
150/490: cracow_districts_centers = get_cracow_centers(cracow_geojson)
150/491: cracow_districts_centers
150/492: cracow_districts_centers.head()
150/493:
for center in cracow_districts_centers.index:
    folium.Marker(cracow_districts_centers.loc[center, 'District_center']).add_to(cracow_map)
cracow_map
150/494:
def create_cracow_polygons(geojson):
    polygons = {}
    for district in geojson['features']:
        polygons[district['properties']['nazwa']]=district['geometry']['coordinates'][0][0]
    return polygons
150/495: cracow_districts_polygons = cracow_districts_polygons(warsaw_geojson)
150/496: cracow_districts_polygons = create_cracow_polygons(cracow_geojson)
150/497:
def calculate_cracow_radius(district_name):
    center_coords = cracow_district_centers.loc[district_name, 'District_center']
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in cracow_districts_polygons [district_name]:
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_points.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    max_distance = max(distances)
    factor = 0.6157*111.3*1000
    max_distance_meters = int(max_distance*factor)
    
    return max_distance_meters
150/498:
cracow_districts_radiuses = {}
for district in cracow_district_centers.index:
    cracow_districts_radiuses[district] = calculate_radius(district)
150/499:
cracow_districts_radiuses = {}
for district in cracow_districts_centers.index:
    cracow_districts_radiuses[district] = calculate_radius(district)
150/500:
cracow_districts_radiuses = {}
for district in cracow_districts_centers.index:
    cracow_districts_radiuses[district] = calculate_radius(district)
150/501:
cracow_districts_radiuses = {}
for district in cracow_districts_centers.index:
    cracow_districts_radiuses[district] = calculate_cracow_radius(district)
150/502:
def calculate_cracow_radius(district_name):
    center_coords = cracow_districts_centers.loc[district_name, 'District_center']
    center_point = Point(center_coords[1], center_coords[0])
    
    polygon_points = []
    for point_coords in cracow_districts_polygons [district_name]:
        polygon_point = Point(point_coords[0], point_coords[1])
        polygon_points.append(polygon_point)
    
    distances = []
    for point in polygon_points:
        distance_to_center = center_point.distance(point)
        distances.append(distance_to_center)
        
    max_distance = max(distances)
    factor = 0.6157*111.3*1000
    max_distance_meters = int(max_distance*factor)
    
    return max_distance_meters
150/503:
cracow_districts_radiuses = {}
for district in cracow_districts_centers.index:
    cracow_districts_radiuses[district] = calculate_cracow_radius(district)
150/504:
cracow_districts_venues_foursquare = {}
for district in cracow_district_centers.index:
    print(district)
    cracow_districts_venues_foursquare[district] = get_venues(
        cracow_district_centers.loc[district, 'District_center'],
        cracow_districts_radiuses[district])
150/505:
cracow_districts_venues_foursquare = {}
for district in cracow_districts_centers.index:
    print(district)
    cracow_districts_venues_foursquare[district] = get_venues(
        cracow_districts_centers.loc[district, 'District_center'],
        cracow_districts_radiuses[district])
150/506:
for i in cracow_districts_venues.index:    
    folium.Marker([cracow_districts_venues.loc[i,'Lat'], cracow_districts_venues.loc[i,'Lon']]).add_to(cracow_map)
150/507: cracow_districts_venues = get_districts_venues(cracow_districts_venues_foursquare)
150/508:
for i in cracow_districts_venues.index:    
    folium.Marker([cracow_districts_venues.loc[i,'Lat'], cracow_districts_venues.loc[i,'Lon']]).add_to(cracow_map)
150/509: cracow_districts_venues
150/510: cracow_districts_venues.loc[cracow_districts_venues.Lat is False]
150/511: cracow_districts_venues.loc[cracow_districts_venues.Lon is False]
150/512: cracow_districts_venues.loc[cracow_districts_venues.isna().any(axis=1)]
150/513: cracow_districts_venues[cracow_districts_venues.isna().any(axis=1)]
150/514: cracow_districts_venues[cracow_districts_venues.isna().any()]
150/515: cracow_districts_venues.isna().any()
150/516: cracow_districts_venues = get_districts_venues(cracow_districts_venues_foursquare)
150/517: cracow_districts_venues.isna().any()
150/518:
for i in cracow_districts_venues.index:    
    folium.Marker([cracow_districts_venues.loc[i,'Lat'], cracow_districts_venues.loc[i,'Lon']]).add_to(cracow_map)
150/519: cracow_districts_venues#.isna().any()
150/520: [cracow_districts_venues.loc[0,'Lat'], cracow_districts_venues.loc[0,'Lon']]
150/521: [cracow_districts_venues.loc[0,'Lat']#, cracow_districts_venues.loc[0,'Lon']]
150/522: cracow_districts_venues.loc[0,'Lat'], cracow_districts_venues.loc[0,'Lon']
150/523: cracow_districts_venues.loc[1,'Lat'], cracow_districts_venues.loc[0,'Lon']
150/524: cracow_districts_venues.loc[1,'Lat']#, cracow_districts_venues.loc[0,'Lon']
150/525: cracow_districts_venues.iloc[1,'Lat']#, cracow_districts_venues.loc[0,'Lon']
150/526: cracow_districts_venues[1,'Lat']#, cracow_districts_venues.loc[0,'Lon']
150/527: cracow_districts_venues.loc[1,'Lat']#, cracow_districts_venues.loc[0,'Lon']
150/528: cracow_districts_venues.loc[[1,'Lat']]#, cracow_districts_venues.loc[0,'Lon']
150/529: cracow_districts_venues.loc[1,'Lat']#, cracow_districts_venues.loc[0,'Lon']
150/530: cracow_districts_venues.at[1,'Lat']#, cracow_districts_venues.loc[0,'Lon']
150/531: cracow_districts_venues.at[0,'Lat']#, cracow_districts_venues.loc[0,'Lon']
150/532: cracow_districts_venues.at[0,'Lon']#, cracow_districts_venues.loc[0,'Lon']
150/533: cracow_districts_venues.loc[0]['Lon']#, cracow_districts_venues.loc[0,'Lon']
150/534: cracow_districts_venues.iloc[0]['Lon']#, cracow_districts_venues.loc[0,'Lon']
150/535:
for i in cracow_districts_venues.index:    
    folium.Marker([cracow_districts_venues.iloc[i]['Lat'], cracow_districts_venues.iloc[i]['Lon']]).add_to(cracow_map)
150/536: cracow_map
150/537: cracow_districts_venues.head()
150/538: warsaw_districts_venues.reset_index(inplace=True)
150/539:
for i in cracow_districts_venues.index:    
    folium.Marker([cracow_districts_venues.iloc[i]['Lat'], cracow_districts_venues.iloc[i]['Lon']]).add_to(cracow_map)
150/540: cracow_map
150/541: cracow_districts_venues.head(5-)
150/542: cracow_districts_venues.head(50)
150/543: cracow_districts_venues.tail(50)
150/544: cracow_districts_venues.reset_index(inplace=True)
150/545: cracow_districts_venues.tail(50)
150/546:
for i in cracow_districts_venues.index:    
    folium.Marker([cracow_districts_venues.iloc[i]['Lat'], cracow_districts_venues.iloc[i]['Lon']]).add_to(cracow_map)
150/547: cracow_map
150/548:
for i in cracow_districts_venues.index:
    cracow_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [cracow_districts_venues.loc[i,'Lon'], cracow_districts_venues.loc[i,'Lat']],
                                        cracow_districts_polygons[cracow_districts_venues.loc[i,'District']])
150/549: cracow_district_venues
150/550: cracow_districts_venues
150/551:
for i in cracow_districts_venues.index:
    cracow_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [cracow_districts_venues.iloc[i]['Lon'], cracow_districts_venues.loc[i]['Lat']],
                                        cracow_districts_polygons[cracow_districts_venues.iloc[i]['District']])
150/552: warsaw_districts_venues.head()
150/553:
def check_if_inside_district(venue_coords, district_shape):
    p = Point(venue_coords)
    poly = Polygon(district_shape)
    return p.within(poly)
150/554: warsaw_districts_venues.reset_index(inplace=True)
150/555:
for i in warsaw_districts_venues.index:
    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],
                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])
150/556: warsaw_districts_venues_foursquare
150/557:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_df = extract_data(city_districts_venues[district])
        all_city_venues = all_city_venues.append({district: district_df})
    return all_city_venues
150/558: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/559:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_df = pd.DataFrame({district:extract_data(city_districts_venues[district])})
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/560: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/561:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_df = extract_data(city_districts_venues[district])
        district_df['District'] = district
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/562: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/563: warsaw_districts_venues.head()
150/564:
def extract_data(district,district_venues_foursquare):
    district_venues=[]
    for item in district_venues_foursquare:
        x={}
        x['District']=district
        x['Name']=item['venue']['name']
        x['Category']=item['venue']['categories'][0]['name']
        x['Lat'] = item['venue']['location']['lat']
        x['Lon'] = item['venue']['location']['lng']
        x['VenueId'] = item['venue']['id']
        district_venues.append(x)
    district_df = pd.DataFrame(district_venues)
    return district_df
150/565:
def get_districts_venues(city_districts_venues):
    all_city_venues= pd.DataFrame()
    for district in city_districts_venues.keys():
        district_df = extract_data(district, city_districts_venues[district])
        district_df['District'] = district
        all_city_venues = all_city_venues.append(district_df)
    return all_city_venues
150/566: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)
150/567: warsaw_districts_venues.head()
150/568:
def check_if_inside_district(venue_coords, district_shape):
    p = Point(venue_coords)
    poly = Polygon(district_shape)
    return p.within(poly)
150/569: warsaw_districts_venues.reset_index(inplace=True)
150/570:
for i in warsaw_districts_venues.index:
    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],
                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])
150/571: cracow_districts_venues = get_districts_venues(cracow_districts_venues_foursquare)
150/572: cracow_districts_venues.reset_index(inplace=True)
150/573:
for i in cracow_districts_venues.index:
    cracow_districts_venues.loc[i, 'Inside'] = check_if_inside_district(
                                        [cracow_districts_venues.iloc[i]['Lon'], cracow_districts_venues.loc[i]['Lat']],
                                        cracow_districts_polygons[cracow_districts_venues.iloc[i]['District']])
150/574:
for i in cracow_districts_venues.index:    
    folium.Marker([cracow_districts_venues.iloc[i]['Lat'], cracow_districts_venues.iloc[i]['Lon']]).add_to(cracow_map)
150/575: cracow_districts_venues
150/576: cracow_districts_venues = cracow_districts_venues.loc[cracow_districts_venues['Inside']==True]
150/577: cracow_top5 = cracow_districts_venues.groupby('Category').count().sort_values(by='VenueId', ascending=False).head()
150/578: cracow_top5
150/579: cracow_districts_venues
150/580:
for i in cracow_districts_venues.index:    
    folium.Marker([cracow_districts_venues.loc[i,'Lat'], cracow_districts_venues.loc[i,'Lon']]).add_to(cracow_map)
150/581: cracow_map
150/582:
cracow_map = create_cracow_map()
cracow_map
150/583:
for i in cracow_districts_venues.index:    
    folium.Marker([cracow_districts_venues.loc[i,'Lat'], cracow_districts_venues.loc[i,'Lon']]).add_to(cracow_map)
150/584: cracow_map
150/585: top_categories = warsaw_top5.add(cracow_top5, fill_value=0)
150/586: top_categories
150/587: warsaw_top5
150/588: top_5_categories = top_categories.head()
150/589: top_5_categories = top_categories.head().index
150/590: top_5_categories
150/591: warsaw_districts_wiki = 'https://pl.wikipedia.org/wiki/Podział_administracyjny_Warszawy'#Dzielnice_Warszawy
150/592: warsaw_districts_wiki = 'https://pl.wikipedia.org/wiki/Podział_administracyjny_Warszawy'
150/593: cracow_districts_wiki = 'https://pl.wikipedia.org/wiki/Podział_administracyjny_Krakowa'
150/594: warsaw_districts_df = pd.read_html(warsaw_districts_wiki)
150/595: warsaw_districts_df = pd.read_html(warsaw_districts_wiki)
150/596: warsaw_districts_df = pd.read_html(warsaw_districts_wiki)
153/1: %history -g
   1: %history -g -f the_battle_of_neighborhoods.ipynb
   2: %history -g -f the_battle_of_neighborhoods_xxx.ipynb
   3: %history -g
   4: %history
   5: %history -g -f history_file.json
