{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geojson files exported from .shp with QGIS (EPSG:4326 WGS 84)\n",
    "warsaw_geofile = 'geodata/Warsaw_districts/warsaw_districts.geojson'\n",
    "cracow_geofile = 'geodata/Cracow_districts/cracow_districts.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(warsaw_geofile) as w:\n",
    "    warsaw_geojson = json.load(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cracow_geofile) as c:\n",
    "    cracow_geojson = json.load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_districts_names = []\n",
    "for x in warsaw_geojson['features']:\n",
    "    warsaw_districts_names.append(x['properties']['nazwa_dzie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_density = list(range(len(warsaw_districts_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([warsaw_districts_names, test_density]).T\n",
    "df.columns = ['nazwa_dzie', 'Density']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_coordinates = [52.2297700, 21.0117800]\n",
    "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
    "warsaw_map.choropleth(\n",
    "    geo_data=warsaw_geojson,\n",
    "    name='choropleth',\n",
    "    data=df,\n",
    "    columns=['nazwa_dzie', 'Density'],\n",
    "    key_on='feature.properties.nazwa_dzie',\n",
    "    fill_color='YlGn',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.9,\n",
    "    legend_name='Density'\n",
    ")\n",
    "#warsaw_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cracow_coordinates = [50.06143, 19.93658]\n",
    "# cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)\n",
    "\n",
    "# folium.GeoJson(\n",
    "#     cracow_geojson,\n",
    "#     name='geojson'\n",
    "# ).add_to(cracow_map)\n",
    "# cracow_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
    "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
    "VERSION = '20200605'\n",
    "LIMIT = 100\n",
    "OFFSET=LIMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_district_centers(city_geojson):\n",
    "    district_centers = {}\n",
    "    for district in city_geojson['features']:\n",
    "        district_geometry = pd.DataFrame(\n",
    "            district['geometry']['coordinates'][0][0],\n",
    "            columns=['longitude', 'latitude']\n",
    "        )\n",
    "        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]\n",
    "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
    "    \n",
    "    district_centers = pd.DataFrame.from_dict(district_centers,\n",
    "                                              orient='index',\n",
    "                                              columns=['District_center'])\n",
    "    \n",
    "    return district_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_district_centers = get_district_centers(warsaw_geojson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for center in warsaw_district_centers.index:\n",
    "    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)\n",
    "    #print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_venues(district_center, radius):\n",
    "    RADIUS = radius\n",
    "    lat, lng = district_center\n",
    "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            RADIUS, \n",
    "            LIMIT)\n",
    "    result = requests.get(url).json()\n",
    "    \n",
    "    venues = result['response']['groups'][0]['items']\n",
    "    \n",
    "    total_results = result['response']['totalResults']\n",
    "    print('\\tTotal results: ', total_results, '\\n')\n",
    "    \n",
    "    #checking if there is more results -if true, next request with offset is send\n",
    "    requests_to_perform = total_results//100\n",
    "    \n",
    "    for _ in range(requests_to_perform):\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            RADIUS, \n",
    "            LIMIT,\n",
    "            OFFSET\n",
    "        )\n",
    "        result = requests.get(url).json()\n",
    "        venues.extend(result['response']['groups'][0]['items'])\n",
    "        \n",
    "    return venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_radius(district_name):\n",
    "    center_coords = warsaw_district_centers.loc[district_name, 'District_center']\n",
    "    center_point = Point(center_coords[1], center_coords[0])\n",
    "    \n",
    "    polygon_points = []\n",
    "    for point_coords in warsaw_districts_polygons [district_name]:\n",
    "        polygon_point = Point(point_coords[0], point_coords[1])\n",
    "        polygon_points.append(polygon_point)\n",
    "    \n",
    "    distances = []\n",
    "    for point in polygon_points:\n",
    "        distance_to_center = center_point.distance(point)\n",
    "        distances.append(distance_to_center)\n",
    "        \n",
    "    max_distance = max(distances)\n",
    "    factor = 0.6157*111.3*1000\n",
    "    max_distance_meters = int(max_distance*factor)\n",
    "    \n",
    "    return max_distance_meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_districts_polygons(geojson):\n",
    "    polygons = {}\n",
    "    for district in geojson['features']:\n",
    "        polygons[district['properties']['nazwa_dzie']]=district['geometry']['coordinates'][0][0]\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_districts_polygons = create_districts_polygons(warsaw_geojson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_districts_radiuses = {}\n",
    "for district in warsaw_district_centers.index:\n",
    "    warsaw_districts_radiuses[district] = calculate_radius(district)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Żoliborz\n",
      "\tTotal results:  154 \n",
      "\n",
      "Praga-Południe\n",
      "\tTotal results:  238 \n",
      "\n",
      "Mokotów\n",
      "\tTotal results:  234 \n",
      "\n",
      "Wola\n",
      "\tTotal results:  209 \n",
      "\n",
      "Wilanów\n",
      "\tTotal results:  156 \n",
      "\n",
      "Wesoła\n",
      "\tTotal results:  17 \n",
      "\n",
      "Wawer\n",
      "\tTotal results:  99 \n",
      "\n",
      "Włochy\n",
      "\tTotal results:  225 \n",
      "\n",
      "Ursynów\n",
      "\tTotal results:  143 \n",
      "\n",
      "Śródmieście\n",
      "\tTotal results:  234 \n",
      "\n",
      "Praga-Północ\n",
      "\tTotal results:  155 \n",
      "\n",
      "Ursus\n",
      "\tTotal results:  48 \n",
      "\n",
      "Targówek\n",
      "\tTotal results:  146 \n",
      "\n",
      "Rembertów\n",
      "\tTotal results:  37 \n",
      "\n",
      "Ochota\n",
      "\tTotal results:  167 \n",
      "\n",
      "Bielany\n",
      "\tTotal results:  100 \n",
      "\n",
      "Białołęka\n",
      "\tTotal results:  127 \n",
      "\n",
      "Bemowo\n",
      "\tTotal results:  60 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "warsaw_districts_venues_foursquare = {}\n",
    "for district in warsaw_district_centers.index:\n",
    "    print(district)\n",
    "    warsaw_districts_venues_foursquare[district] = get_venues(\n",
    "        warsaw_district_centers.loc[district, 'District_center'],\n",
    "        warsaw_districts_radiuses[district])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_inside_district(venue_coords, district_shape):\n",
    "    p = Point(venue_coords)\n",
    "    poly = Polygon(district_shape)\n",
    "    return p.within(poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(district,district_venues_foursquare):\n",
    "    district_venues=[]\n",
    "    for item in district_venues_foursquare:\n",
    "        x={}\n",
    "        x['District']=district\n",
    "        x['Name']=item['venue']['name']\n",
    "        x['Category']=item['venue']['categories'][0]['name']\n",
    "        x['Lat'] = item['venue']['location']['lat']\n",
    "        x['Lon'] = item['venue']['location']['lng']\n",
    "        x['VenueId'] = item['venue']['id']\n",
    "        district_venues.append(x)\n",
    "    district_df = pd.DataFrame(district_venues)\n",
    "    return district_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_districts_venues(city_districts_venues):\n",
    "    all_city_venues= pd.DataFrame()\n",
    "    for district in city_districts_venues.keys():\n",
    "        district_df = extract_data(district, city_districts_venues[district])\n",
    "        district_df['District'] = district\n",
    "        all_city_venues = all_city_venues.append(district_df)\n",
    "    return all_city_venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in warsaw_districts_venues.keys():    \n",
    "    for item in warsaw_districts_venues[key]:\n",
    "        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
    "        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/1:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    chanel_list = []\n",
      "    for f in fiel_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                chanel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arrange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_directory\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      " 2/2:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import fiona as fio\n",
      "import matplotlib as plt\n",
      " 2/3:\n",
      "# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.\n",
      "# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.\n",
      "%matplotlib notebook\n",
      " 2/4:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    chanel_list = []\n",
      "    for f in fiel_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                chanel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arrange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_directory\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      " 2/5:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import fiona as fio\n",
      "import matplotlib as plt\n",
      " 2/6:\n",
      "# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.\n",
      "# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.\n",
      "%matplotlib notebook\n",
      " 3/1:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import fiona as fio\n",
      "import matplotlib as plt\n",
      " 3/2:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    chanel_list = []\n",
      "    for f in fiel_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                chanel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arrange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_directory\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      " 3/3:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    chanel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                chanel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arrange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_directory\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      " 3/4:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                chanel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arrange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_directory\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      " 3/5:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                chanel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arrange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_directory\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      " 3/6:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                chanel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arrange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_directory\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      " 3/7:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arrange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_directory\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      " 3/8:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_directory\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      " 3/9:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_dictionary\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      "3/10:\n",
      "# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.\n",
      "# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.\n",
      "%matplotlib notebook\n",
      "3/11:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "    test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "    show_band(test, color_map = 'winter')\n",
      "3/12:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "    test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "    show_band(test, color_map = 'winter')\n",
      "3/13:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      "3/14:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import fiona as fio\n",
      "import matplotlib.pyplot as plt\n",
      "3/15:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "    test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "    show_band(test, color_map = 'winter')\n",
      "3/16:\n",
      "# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.\n",
      "# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.\n",
      "%matplotlib notebook\n",
      "3/17:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import fiona as fio\n",
      "import matplotlib.pyplot as plt\n",
      "3/18:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_dictionary\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      "3/19:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "    test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "    show_band(test, color_map = 'winter')\n",
      "3/20:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      "3/21:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import fiona as fio\n",
      "import matplotlib.pyplot as plt\n",
      " 4/1:\n",
      "# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.\n",
      "# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.\n",
      "%matplotlib notebook\n",
      " 4/2:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import fiona as fio\n",
      "import matplotlib.pyplot as plt\n",
      " 4/3:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import fiona as fio\n",
      "import matplotlib.pyplot as plt\n",
      " 4/4:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_dictionary\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      " 4/5:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "    test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "    show_band(test, color_map = 'winter')\n",
      " 4/6:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      " 4/7:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona as fio\n",
      " 4/8:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_dictionary\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      " 4/9:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "    test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "    show_band(test, color_map = 'winter')\n",
      "4/10:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      " 5/1:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona as fio\n",
      " 6/1:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona as fio\n",
      " 6/2:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "    test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "    show_band(test, color_map = 'winter')\n",
      " 6/3:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      " 6/4:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_dictionary\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      " 6/5:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "    test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "    show_band(test, color_map = 'winter')\n",
      " 6/6:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      " 6/7:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "show_band(test, color_map = 'winter')\n",
      " 6/8:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      " 6/9:\n",
      "def show_band(band, color_map = 'gray', remove_negative =True):\n",
      "    matrix = band.astype(float)\n",
      "    if remove_negative:\n",
      "        matrix[matrix<=0] = np.nan\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(matrix)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "6/10:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      "6/11:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      " 7/1:\n",
      "def clip_area(vactor_file, raster_file, save_image_to):\n",
      "    with fio.open(vector_file, 'r') as clipper:\n",
      "        geometry = [feature[\"geometry\"] for feature in clipper]\n",
      "        \n",
      "    with rio.open(raster_file, 'r') as raster_source\n",
      "        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)\n",
      "        metadata = raster._source.meta.copy()\n",
      "        \n",
      "    metadata.update({\"driver\": \"GTiff\",\n",
      "                     \"height\": clipped_image.shape[1],\n",
      "                     \"width\": clipped_image.shape[2],\n",
      "                     \"transform\":transform})\n",
      "    with rio.open(save_image_to, \"w\", **metadata) as g_tiff:\n",
      "        g_tiff.write(clipped_image)\n",
      " 7/2:\n",
      "def clip_area(vactor_file, raster_file, save_image_to):\n",
      "    with fio.open(vector_file, 'r') as clipper:\n",
      "        geometry = [feature[\"geometry\"] for feature in clipper]\n",
      "        \n",
      "    with rio.open(raster_file, 'r') as raster_source:\n",
      "        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)\n",
      "        metadata = raster._source.meta.copy()\n",
      "        \n",
      "    metadata.update({\"driver\": \"GTiff\",\n",
      "                     \"height\": clipped_image.shape[1],\n",
      "                     \"width\": clipped_image.shape[2],\n",
      "                     \"transform\":transform})\n",
      "    with rio.open(save_image_to, \"w\", **metadata) as g_tiff:\n",
      "        g_tiff.write(clipped_image)\n",
      " 7/3:\n",
      "with rio.open(clipped_bands[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "show_band(band_matrix)\n",
      " 7/4:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona as fio\n",
      " 7/5:\n",
      "with rio.open(clipped_bands[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "show_band(band_matrix)\n",
      " 7/6: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      " 7/7:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_dictionary\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      " 7/8: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      " 7/9:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "7/10:\n",
      "clipped_bands = read_landsat_images('clipped/')\n",
      "for band in clipped_bands:\n",
      "    print(clipped_bands[band])\n",
      "7/11:\n",
      "with rio.open(clipped_bands[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "show_band(band_matrix)\n",
      "7/12:\n",
      "def clip_area(vactor_file, raster_file, save_image_to):\n",
      "    with fio.open(vector_file, 'r') as clipper:\n",
      "        geometry = [feature[\"geometry\"] for feature in clipper]\n",
      "        \n",
      "    with rio.open(raster_file, 'r') as raster_source:\n",
      "        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)\n",
      "        metadata = raster._source.meta.copy()\n",
      "        \n",
      "    metadata.update({\"driver\": \"GTiff\",\n",
      "                     \"height\": clipped_image.shape[1],\n",
      "                     \"width\": clipped_image.shape[2],\n",
      "                     \"transform\":transform})\n",
      "    with rio.open(save_image_to, \"w\", **metadata) as g_tiff:\n",
      "        g_tiff.write(clipped_image)\n",
      "7/13: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "7/14:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "7/15:\n",
      "clipped_bands = read_landsat_images('clipped/')\n",
      "for band in clipped_bands:\n",
      "    print(clipped_bands[band])\n",
      "7/16:\n",
      "with rio.open(clipped_bands[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "show_band(band_matrix)\n",
      "7/17:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      "7/18:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "show_band(test, color_map = 'winter')\n",
      "7/19:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      "7/20:\n",
      "def show_band(band, color_map = 'gray', remove_negative =True):\n",
      "    matrix = band.astype(float)\n",
      "    if remove_negative:\n",
      "        matrix[matrix<=0] = np.nan\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(matrix)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "7/21:\n",
      "def clip_area(vactor_file, raster_file, save_image_to):\n",
      "    with fio.open(vector_file, 'r') as clipper:\n",
      "        geometry = [feature[\"geometry\"] for feature in clipper]\n",
      "        \n",
      "    with rio.open(raster_file, 'r') as raster_source:\n",
      "        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)\n",
      "        metadata = raster._source.meta.copy()\n",
      "        \n",
      "    metadata.update({\"driver\": \"GTiff\",\n",
      "                     \"height\": clipped_image.shape[1],\n",
      "                     \"width\": clipped_image.shape[2],\n",
      "                     \"transform\":transform})\n",
      "    with rio.open(save_image_to, \"w\", **metadata) as g_tiff:\n",
      "        g_tiff.write(clipped_image)\n",
      "7/22: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "7/23:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "7/24:\n",
      "clipped_bands = read_landsat_images('clipped/')\n",
      "for band in clipped_bands:\n",
      "    print(clipped_bands[band])\n",
      "7/25:\n",
      "with rio.open(clipped_bands[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "show_band(band_matrix)\n",
      "7/26:\n",
      "# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.\n",
      "# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.\n",
      "%matplotlib notebook\n",
      " 8/1:\n",
      "# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.\n",
      "# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.\n",
      "%matplotlib notebook\n",
      " 8/2:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona as fio\n",
      " 8/3:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_dictionary\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      " 8/4:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "show_band(test, color_map = 'winter')\n",
      " 8/5:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      " 8/6:\n",
      "def show_band(band, color_map = 'gray', remove_negative =True):\n",
      "    matrix = band.astype(float)\n",
      "    if remove_negative:\n",
      "        matrix[matrix<=0] = np.nan\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(matrix)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      " 8/7:\n",
      "def clip_area(vactor_file, raster_file, save_image_to):\n",
      "    with fio.open(vector_file, 'r') as clipper:\n",
      "        geometry = [feature[\"geometry\"] for feature in clipper]\n",
      "        \n",
      "    with rio.open(raster_file, 'r') as raster_source:\n",
      "        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)\n",
      "        metadata = raster._source.meta.copy()\n",
      "        \n",
      "    metadata.update({\"driver\": \"GTiff\",\n",
      "                     \"height\": clipped_image.shape[1],\n",
      "                     \"width\": clipped_image.shape[2],\n",
      "                     \"transform\":transform})\n",
      "    with rio.open(save_image_to, \"w\", **metadata) as g_tiff:\n",
      "        g_tiff.write(clipped_image)\n",
      " 8/8: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      " 8/9:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "8/10:\n",
      "clipped_bands = read_landsat_images('clipped/')\n",
      "for band in clipped_bands:\n",
      "    print(clipped_bands[band])\n",
      "8/11:\n",
      "with rio.open(clipped_bands[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "show_band(band_matrix)\n",
      "8/12:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona as fio\n",
      "8/13:\n",
      "def clip_area(vactor_file, raster_file, save_image_to):\n",
      "    with fio.open(vector_file, 'r') as clipper:\n",
      "        geometry = [feature[\"geometry\"] for feature in clipper]\n",
      "        \n",
      "    with rio.open(raster_file, 'r') as raster_source:\n",
      "        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)\n",
      "        metadata = raster._source.meta.copy()\n",
      "        \n",
      "    metadata.update({\"driver\": \"GTiff\",\n",
      "                     \"height\": clipped_image.shape[1],\n",
      "                     \"width\": clipped_image.shape[2],\n",
      "                     \"transform\":transform})\n",
      "    with rio.open(save_image_to, \"w\", **metadata) as g_tiff:\n",
      "        g_tiff.write(clipped_image)\n",
      "8/14:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "8/15:\n",
      "with rio.open(clipped_bands[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "show_band(band_matrix)\n",
      "8/16:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "8/17:\n",
      "clipped_bands = read_landsat_images('clipped/')\n",
      "for band in clipped_bands:\n",
      "    print(clipped_bands[band])\n",
      "8/18:\n",
      "with rio.open(clipped_bands[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "show_band(band_matrix)\n",
      "8/19:\n",
      "def clip_area(vactor_file, raster_file, save_image_to):\n",
      "    with fiona.open(vector_file, 'r') as clipper:\n",
      "        geometry = [feature[\"geometry\"] for feature in clipper]\n",
      "        \n",
      "    with rio.open(raster_file, 'r') as raster_source:\n",
      "        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)\n",
      "        metadata = raster._source.meta.copy()\n",
      "        \n",
      "    metadata.update({\"driver\": \"GTiff\",\n",
      "                     \"height\": clipped_image.shape[1],\n",
      "                     \"width\": clipped_image.shape[2],\n",
      "                     \"transform\":transform})\n",
      "    with rio.open(save_image_to, \"w\", **metadata) as g_tiff:\n",
      "        g_tiff.write(clipped_image)\n",
      "8/20: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "8/21:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "8/22:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "8/23:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_dictionary\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      "8/24:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "show_band(test, color_map = 'winter')\n",
      "8/25:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      "8/26:\n",
      "def show_band(band, color_map = 'gray', remove_negative =True):\n",
      "    matrix = band.astype(float)\n",
      "    if remove_negative:\n",
      "        matrix[matrix<=0] = np.nan\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(matrix)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "8/27:\n",
      "def clip_area(vactor_file, raster_file, save_image_to):\n",
      "    with fiona.open(vector_file, 'r') as clipper:\n",
      "        geometry = [feature[\"geometry\"] for feature in clipper]\n",
      "        \n",
      "    with rio.open(raster_file, 'r') as raster_source:\n",
      "        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)\n",
      "        metadata = raster._source.meta.copy()\n",
      "        \n",
      "    metadata.update({\"driver\": \"GTiff\",\n",
      "                     \"height\": clipped_image.shape[1],\n",
      "                     \"width\": clipped_image.shape[2],\n",
      "                     \"transform\":transform})\n",
      "    with rio.open(save_image_to, \"w\", **metadata) as g_tiff:\n",
      "        g_tiff.write(clipped_image)\n",
      "8/28: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "8/29:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "8/30:\n",
      "clipped_bands = read_landsat_images('clipped/')\n",
      "for band in clipped_bands:\n",
      "    print(clipped_bands[band])\n",
      "8/31:\n",
      "with rio.open(clipped_bands[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "show_band(band_matrix)\n",
      "8/32:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "8/33:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "8/34:\n",
      "clipped_bands = read_landsat_images('clipped/')\n",
      "for band in clipped_bands:\n",
      "    print(clipped_bands[band])\n",
      "8/35:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "8/36:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "8/37:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "8/38:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "8/39:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "8/40:\n",
      "clipped_bands = read_landsat_images('clipped/')\n",
      "for band in clipped_bands:\n",
      "    print(clipped_bands[band])\n",
      "8/41:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "8/42:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "8/43:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "8/44:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "8/45:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "8/46:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "8/47:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "8/48:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "8/49:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "8/50:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_dictionary\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      "8/51:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "show_band(test, color_map = 'winter')\n",
      "8/52:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      "8/53:\n",
      "def show_band(band, color_map = 'gray', remove_negative =True):\n",
      "    matrix = band.astype(float)\n",
      "    if remove_negative:\n",
      "        matrix[matrix<=0] = np.nan\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(matrix)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "8/54:\n",
      "def clip_area(vactor_file, raster_file, save_image_to):\n",
      "    with fiona.open(vector_file, 'r') as clipper:\n",
      "        geometry = [feature[\"geometry\"] for feature in clipper]\n",
      "        \n",
      "    with rio.open(raster_file, 'r') as raster_source:\n",
      "        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)\n",
      "        metadata = raster._source.meta.copy()\n",
      "        \n",
      "    metadata.update({\"driver\": \"GTiff\",\n",
      "                     \"height\": clipped_image.shape[1],\n",
      "                     \"width\": clipped_image.shape[2],\n",
      "                     \"transform\":transform})\n",
      "    with rio.open(save_image_to, \"w\", **metadata) as g_tiff:\n",
      "        g_tiff.write(clipped_image)\n",
      "8/55: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "8/56:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "8/57:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "8/58:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      " 9/1:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      " 9/2:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      " 9/3:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_dictionary\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      " 9/4:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      " 9/5:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "10/1:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "10/2:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "11/1:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "11/2:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "11/3:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "11/4:\n",
      "# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.\n",
      "# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.\n",
      "%matplotlib notebook\n",
      "12/1:\n",
      "# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.\n",
      "# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.\n",
      "%matplotlib notebook\n",
      "12/2:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "12/3:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "12/4:\n",
      "# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.\n",
      "# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.\n",
      "%matplotlib notebook\n",
      "12/5:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "12/6:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = np.arange(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_dictionary\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      "12/7:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "show_band(test, color_map = 'winter')\n",
      "12/8:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      "12/9:\n",
      "def show_band(band, color_map = 'gray', remove_negative =True):\n",
      "    matrix = band.astype(float)\n",
      "    if remove_negative:\n",
      "        matrix[matrix<=0] = np.nan\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(matrix)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "12/10:\n",
      "def clip_area(vactor_file, raster_file, save_image_to):\n",
      "    with fiona.open(vector_file, 'r') as clipper:\n",
      "        geometry = [feature[\"geometry\"] for feature in clipper]\n",
      "        \n",
      "    with rio.open(raster_file, 'r') as raster_source:\n",
      "        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)\n",
      "        metadata = raster._source.meta.copy()\n",
      "        \n",
      "    metadata.update({\"driver\": \"GTiff\",\n",
      "                     \"height\": clipped_image.shape[1],\n",
      "                     \"width\": clipped_image.shape[2],\n",
      "                     \"transform\":transform})\n",
      "    with rio.open(save_image_to, \"w\", **metadata) as g_tiff:\n",
      "        g_tiff.write(clipped_image)\n",
      "12/11: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "12/12:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "12/13:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "12/14:\n",
      "def clip_area(vector_file, raster_file, save_image_to):\n",
      "    with fiona.open(vector_file, 'r') as clipper:\n",
      "        geometry = [feature[\"geometry\"] for feature in clipper]\n",
      "        \n",
      "    with rio.open(raster_file, 'r') as raster_source:\n",
      "        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)\n",
      "        metadata = raster._source.meta.copy()\n",
      "        \n",
      "    metadata.update({\"driver\": \"GTiff\",\n",
      "                     \"height\": clipped_image.shape[1],\n",
      "                     \"width\": clipped_image.shape[2],\n",
      "                     \"transform\":transform})\n",
      "    with rio.open(save_image_to, \"w\", **metadata) as g_tiff:\n",
      "        g_tiff.write(clipped_image)\n",
      "12/15: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "12/16:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "12/17:\n",
      "def clip_area(vector_file, raster_file, save_image_to):\n",
      "    with fiona.open(vector_file, 'r') as clipper:\n",
      "        geometry = [feature[\"geometry\"] for feature in clipper]\n",
      "        \n",
      "    with rio.open(raster_file, 'r') as raster_source:\n",
      "        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)\n",
      "        metadata = raster_source.meta.copy()\n",
      "        \n",
      "    metadata.update({\"driver\": \"GTiff\",\n",
      "                     \"height\": clipped_image.shape[1],\n",
      "                     \"width\": clipped_image.shape[2],\n",
      "                     \"transform\":transform})\n",
      "    with rio.open(save_image_to, \"w\", **metadata) as g_tiff:\n",
      "        g_tiff.write(clipped_image)\n",
      "12/18: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "12/19:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "12/20:\n",
      "clipped_bands = read_landsat_images('clipped/')\n",
      "for band in clipped_bands:\n",
      "    print(clipped_bands[band])\n",
      "12/21:\n",
      "with rio.open(clipped_bands[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "show_band(band_matrix)\n",
      "12/22:\n",
      "def calculate_index(index_name, landsat_8_bands):\n",
      "    indexes = {\n",
      "        'ndvi': (5,4),\n",
      "        'ndbi': (6,5),\n",
      "        'ndwi': (3,6)\n",
      "    }\n",
      "    \n",
      "    if index_name in indexes:\n",
      "        bands = indexes[index_name]\n",
      "        \n",
      "        with rio.open(landsat_8_bands[bands[0]]) as a:\n",
      "            band_a = (a.read()[0]/1000)astype(np.float)\n",
      "        with rio.open(landsat_8_bands[bands[1]]) ab b:\n",
      "            band_b = (b.read()[1]/1000)astype(np.float)\n",
      "            \n",
      "        numerator = band_a - band_b\n",
      "        denominator = band_a + band_b\n",
      "        \n",
      "        idx = numerator/denominator\n",
      "        idx[idx >1] = 1\n",
      "        idx[idx <-1] = -1\n",
      "        return idx\n",
      "    else:\n",
      "        raise ValueError('Brak wskaźnika do wyboru, dostępne wskaźniki to: ndbi, ndvi i ndwi')\n",
      "12/23:\n",
      "def calculate_index(index_name, landsat_8_bands):\n",
      "    indexes = {\n",
      "        'ndvi': (5,4),\n",
      "        'ndbi': (6,5),\n",
      "        'ndwi': (3,6)\n",
      "    }\n",
      "    \n",
      "    if index_name in indexes:\n",
      "        bands = indexes[index_name]\n",
      "        \n",
      "        with rio.open(landsat_8_bands[bands[0]]) as a:\n",
      "            band_a = (a.read()[0]/1000).astype(np.float)\n",
      "        with rio.open(landsat_8_bands[bands[1]]) ab b:\n",
      "            band_b = (b.read()[1]/1000).astype(np.float)\n",
      "            \n",
      "        numerator = band_a - band_b\n",
      "        denominator = band_a + band_b\n",
      "        \n",
      "        idx = numerator/denominator\n",
      "        idx[idx >1] = 1\n",
      "        idx[idx <-1] = -1\n",
      "        return idx\n",
      "    else:\n",
      "        raise ValueError('Brak wskaźnika do wyboru, dostępne wskaźniki to: ndbi, ndvi i ndwi')\n",
      "12/24:\n",
      "def calculate_index(index_name, landsat_8_bands):\n",
      "    indexes = {\n",
      "        'ndvi': (5,4),\n",
      "        'ndbi': (6,5),\n",
      "        'ndwi': (3,6)\n",
      "    }\n",
      "    \n",
      "    if index_name in indexes:\n",
      "        bands = indexes[index_name]\n",
      "        \n",
      "        with rio.open(landsat_8_bands[bands[0]]) as a:\n",
      "            band_a = (a.read()[0]/1000).astype(np.float)\n",
      "        with rio.open(landsat_8_bands[bands[1]]) as b:\n",
      "            band_b = (b.read()[1]/1000).astype(np.float)\n",
      "            \n",
      "        numerator = band_a - band_b\n",
      "        denominator = band_a + band_b\n",
      "        \n",
      "        idx = numerator/denominator\n",
      "        idx[idx >1] = 1\n",
      "        idx[idx <-1] = -1\n",
      "        return idx\n",
      "    else:\n",
      "        raise ValueError('Brak wskaźnika do wyboru, dostępne wskaźniki to: ndbi, ndvi i ndwi')\n",
      "12/25:\n",
      "ndvi = calculate_index('ndvi', clipped_bands)\n",
      "ndvi [ndvi ==0] =-1\n",
      "show_band(ndvi, color_map ='virdis', remove_negative =True)\n",
      "12/26:\n",
      "ndwi = calculate_index('ndwi', clipped_bands)\n",
      "ndwi [ndwi ==0] =np.nan\n",
      "shwow_band(ndwi, color_map = 'virdis', remove_negative =False)\n",
      "12/27:\n",
      "def calculate_index(index_name, landsat_8_bands):\n",
      "    indexes = {\n",
      "        'ndvi': (5,4),\n",
      "        'ndbi': (6,5),\n",
      "        'ndwi': (3,6)\n",
      "    }\n",
      "    \n",
      "    if index_name in indexes:\n",
      "        bands = indexes[index_name]\n",
      "        \n",
      "        with rio.open(landsat_8_bands[bands[0]]) as a:\n",
      "            band_a = (a.read()[0]/1000).astype(np.float)\n",
      "        with rio.open(landsat_8_bands[bands[1]]) as b:\n",
      "            band_b = (b.read()[0]/1000).astype(np.float)\n",
      "            \n",
      "        numerator = band_a - band_b\n",
      "        denominator = band_a + band_b\n",
      "        \n",
      "        idx = numerator/denominator\n",
      "        idx[idx >1] = 1\n",
      "        idx[idx <-1] = -1\n",
      "        return idx\n",
      "    else:\n",
      "        raise ValueError('Brak wskaźnika do wyboru, dostępne wskaźniki to: ndbi, ndvi i ndwi')\n",
      "12/28:\n",
      "ndvi = calculate_index('ndvi', clipped_bands)\n",
      "ndvi [ndvi ==0] =-1\n",
      "show_band(ndvi, color_map ='virdis', remove_negative =True)\n",
      "12/29:\n",
      "ndvi = calculate_index('ndvi', clipped_bands)\n",
      "ndvi [ndvi ==0] =-1\n",
      "show_band(ndvi, color_map ='viridis', remove_negative =True)\n",
      "12/30:\n",
      "ndwi = calculate_index('ndwi', clipped_bands)\n",
      "ndwi [ndwi ==0] =np.nan\n",
      "shwow_band(ndwi, color_map = 'virdis', remove_negative =False)\n",
      "12/31:\n",
      "ndwi = calculate_index('ndwi', clipped_bands)\n",
      "ndwi [ndwi ==0] =np.nan\n",
      "show_band(ndwi, color_map = 'viridis', remove_negative =False)\n",
      "12/32:\n",
      "ndbi = calculate_index('ndbi', clipped_bands)\n",
      "ndbi [ndbi ==0] =np.nan\n",
      "shwow_band(ndbi, color_map = 'virdis', remove_negative =False)\n",
      "12/33:\n",
      "ndbi = calculate_index('ndbi', clipped_bands)\n",
      "ndbi [ndbi ==0] =np.nan\n",
      "shwow_band(ndbi, color_map = 'viridis', remove_negative =False)\n",
      "12/34:\n",
      "ndbi = calculate_index('ndbi', clipped_bands)\n",
      "ndbi [ndbi ==0] =np.nan\n",
      "show_band(ndbi, color_map = 'viridis', remove_negative =False)\n",
      "12/35: show_band(ndbi -ndvi, color_map ='virdis', remove_negative = False)\n",
      "12/36: show_band(ndbi -ndvi, color_map ='viridis', remove_negative = False)\n",
      "13/1:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = range(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_dictionary\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      "13/2:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "13/3:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = range(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_dictionary\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      "13/4:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "show_band(test, color_map = 'winter')\n",
      "13/5:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      "13/6:\n",
      "def show_band(band, color_map = 'gray', remove_negative =True):\n",
      "    matrix = band.astype(float)\n",
      "    if remove_negative:\n",
      "        matrix[matrix<=0] = np.nan\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(matrix)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "13/7:\n",
      "def clip_area(vector_file, raster_file, save_image_to):\n",
      "    with fiona.open(vector_file, 'r') as clipper:\n",
      "        geometry = [feature[\"geometry\"] for feature in clipper]\n",
      "        \n",
      "    with rio.open(raster_file, 'r') as raster_source:\n",
      "        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)\n",
      "        metadata = raster_source.meta.copy()\n",
      "        \n",
      "    metadata.update({\"driver\": \"GTiff\",\n",
      "                     \"height\": clipped_image.shape[1],\n",
      "                     \"width\": clipped_image.shape[2],\n",
      "                     \"transform\":transform})\n",
      "    with rio.open(save_image_to, \"w\", **metadata) as g_tiff:\n",
      "        g_tiff.write(clipped_image)\n",
      "13/8: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "13/9:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "13/10:\n",
      "clipped_bands = read_landsat_images('clipped/')\n",
      "for band in clipped_bands:\n",
      "    print(clipped_bands[band])\n",
      "13/11:\n",
      "with rio.open(clipped_bands[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "show_band(band_matrix)\n",
      "13/12:\n",
      "def calculate_index(index_name, landsat_8_bands):\n",
      "    indexes = {\n",
      "        'ndvi': (5,4),\n",
      "        'ndbi': (6,5),\n",
      "        'ndwi': (3,6)\n",
      "    }\n",
      "    \n",
      "    if index_name in indexes:\n",
      "        bands = indexes[index_name]\n",
      "        \n",
      "        with rio.open(landsat_8_bands[bands[0]]) as a:\n",
      "            band_a = (a.read()[0]/1000).astype(np.float)\n",
      "        with rio.open(landsat_8_bands[bands[1]]) as b:\n",
      "            band_b = (b.read()[0]/1000).astype(np.float)\n",
      "            \n",
      "        numerator = band_a - band_b\n",
      "        denominator = band_a + band_b\n",
      "        \n",
      "        idx = numerator/denominator\n",
      "        idx[idx >1] = 1\n",
      "        idx[idx <-1] = -1\n",
      "        return idx\n",
      "    else:\n",
      "        raise ValueError('Brak wskaźnika do wyboru, dostępne wskaźniki to: ndbi, ndvi i ndwi')\n",
      "13/13:\n",
      "ndvi = calculate_index('ndvi', clipped_bands)\n",
      "ndvi [ndvi ==0] =-1\n",
      "show_band(ndvi, color_map ='viridis', remove_negative =True)\n",
      "13/14:\n",
      "ndwi = calculate_index('ndwi', clipped_bands)\n",
      "ndwi [ndwi ==0] =np.nan\n",
      "show_band(ndwi, color_map = 'viridis', remove_negative =False)\n",
      "13/15:\n",
      "ndbi = calculate_index('ndbi', clipped_bands)\n",
      "ndbi [ndbi ==0] =np.nan\n",
      "show_band(ndbi, color_map = 'viridis', remove_negative =False)\n",
      "13/16: show_band(ndbi -ndvi, color_map ='viridis', remove_negative = False)\n",
      "13/17:\n",
      "# Kod wpisujemy normalnie w komórkach. Shift + Enter realizuje kod w danej komórce.\n",
      "# Komenda poniżej obowiązkowa w celu poprawnego wyświetlania obrazów.\n",
      "%matplotlib notebook\n",
      "13/18:\n",
      "import os\n",
      "import numpy as np\n",
      "import rasterio as rio\n",
      "import rasterio.mask as rmask\n",
      "import matplotlib.pyplot as plt\n",
      "import fiona\n",
      "13/19:\n",
      "def read_landsat_images (folder_name):\n",
      "    file_list = os.listdir(folder_name)\n",
      "    channel_list = []\n",
      "    for f in file_list:\n",
      "        if (f.startswith('LC') and f.endswith('.tif')):\n",
      "            if  'band' in f:\n",
      "                channel_list.append(folder_name +f)\n",
      "    channel_list.sort()\n",
      "    channel_numbers = range(1,8)\n",
      "    bands_dictionary = dict(zip(channel_numbers, channel_list))\n",
      "    return bands_dictionary\n",
      "\n",
      "#test\n",
      "satelite_images = read_landsat_images('LC081900232018080301T1-SC20181218151707')\n",
      "for band in satelite_images:\n",
      "    print(band, satelite_images[band])\n",
      "13/20:\n",
      "def show_band(band, color_map = 'gray'):\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(band)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "    \n",
      "    #test\n",
      "test = np.random.randint(low =0, high = 255, size =(200,200))\n",
      "show_band(test, color_map = 'winter')\n",
      "13/21:\n",
      "band_list = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "with rio.open(band_list[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "\n",
      "show_band(band_matrix)\n",
      "13/22:\n",
      "def show_band(band, color_map = 'gray', remove_negative =True):\n",
      "    matrix = band.astype(float)\n",
      "    if remove_negative:\n",
      "        matrix[matrix<=0] = np.nan\n",
      "    fig = plt.figure(figsize=(8,8))\n",
      "    image_layer = plt.imshow(matrix)\n",
      "    image_layer.set_cmap(color_map)\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "13/23:\n",
      "def clip_area(vector_file, raster_file, save_image_to):\n",
      "    with fiona.open(vector_file, 'r') as clipper:\n",
      "        geometry = [feature[\"geometry\"] for feature in clipper]\n",
      "        \n",
      "    with rio.open(raster_file, 'r') as raster_source:\n",
      "        clipped_image, transform = rmask.mask(raster_source, geometry, crop =True)\n",
      "        metadata = raster_source.meta.copy()\n",
      "        \n",
      "    metadata.update({\"driver\": \"GTiff\",\n",
      "                     \"height\": clipped_image.shape[1],\n",
      "                     \"width\": clipped_image.shape[2],\n",
      "                     \"transform\":transform})\n",
      "    with rio.open(save_image_to, \"w\", **metadata) as g_tiff:\n",
      "        g_tiff.write(clipped_image)\n",
      "13/24: bands = read_landsat_images('LC081900232018080301T1-SC20181218151707/')\n",
      "13/25:\n",
      "vector = 'vector/znin_powiat.shp'\n",
      "clipped_folder = 'clipped/'\n",
      "for band in bands:\n",
      "    destination = clipped_folder + 'LC_clipped_band' + str(band) + '.tif'\n",
      "    clip_area(vector, bands[band], destination)\n",
      "13/26:\n",
      "clipped_bands = read_landsat_images('clipped/')\n",
      "for band in clipped_bands:\n",
      "    print(clipped_bands[band])\n",
      "13/27:\n",
      "with rio.open(clipped_bands[5], 'r') as src:\n",
      "    band_matrix = src.read(1)\n",
      "show_band(band_matrix)\n",
      "13/28:\n",
      "def calculate_index(index_name, landsat_8_bands):\n",
      "    indexes = {\n",
      "        'ndvi': (5,4),\n",
      "        'ndbi': (6,5),\n",
      "        'ndwi': (3,6)\n",
      "    }\n",
      "    \n",
      "    if index_name in indexes:\n",
      "        bands = indexes[index_name]\n",
      "        \n",
      "        with rio.open(landsat_8_bands[bands[0]]) as a:\n",
      "            band_a = (a.read()[0]/1000).astype(np.float)\n",
      "        with rio.open(landsat_8_bands[bands[1]]) as b:\n",
      "            band_b = (b.read()[0]/1000).astype(np.float)\n",
      "            \n",
      "        numerator = band_a - band_b\n",
      "        denominator = band_a + band_b\n",
      "        \n",
      "        idx = numerator/denominator\n",
      "        idx[idx >1] = 1\n",
      "        idx[idx <-1] = -1\n",
      "        return idx\n",
      "    else:\n",
      "        raise ValueError('Brak wskaźnika do wyboru, dostępne wskaźniki to: ndbi, ndvi i ndwi')\n",
      "13/29:\n",
      "ndvi = calculate_index('ndvi', clipped_bands)\n",
      "ndvi [ndvi ==0] =-1\n",
      "show_band(ndvi, color_map ='viridis', remove_negative =True)\n",
      "13/30:\n",
      "ndwi = calculate_index('ndwi', clipped_bands)\n",
      "ndwi [ndwi ==0] =np.nan\n",
      "show_band(ndwi, color_map = 'viridis', remove_negative =False)\n",
      "13/31:\n",
      "ndbi = calculate_index('ndbi', clipped_bands)\n",
      "ndbi [ndbi ==0] =np.nan\n",
      "show_band(ndbi, color_map = 'viridis', remove_negative =False)\n",
      "13/32: show_band(ndbi -ndvi, color_map ='viridis', remove_negative = False)\n",
      "14/1: import numpy as np\n",
      "14/2:\n",
      "a = np.array([1,2,3])\n",
      "a+a\n",
      "14/3:\n",
      "def print_array_details(a):\n",
      "    print('Dimensions: %d, shape: %s, dtype: %s' % (a.ndim, a.shape, a.dtype))\n",
      "14/4:\n",
      "def print_array_details(a):\n",
      "    print('Dimensions: %d, shape: %s, dtype: %s' % (a.ndim, a.shape, a.dtype))\n",
      "14/5: print_array_details(a)\n",
      "14/6: a = np.array([1,2,3,4,5,6,7,8])\n",
      "14/7: a = np.array([1,2,3,4,5,6,7,8])\n",
      "14/8: a\n",
      "14/9: print_array_details(a)\n",
      "14/10: print_array_details(a)\n",
      "14/11: print_array_details(a)\n",
      "14/12: a = a.reshape([2,4])\n",
      "14/13: a\n",
      "14/14: print_array_details(a)\n",
      "14/15: a=a.reshape([2,2,2])\n",
      "14/16: a\n",
      "14/17: print_array_details(a)\n",
      "14/18: x = np.array([[1,2,3] , [4,5,6]], np.int32)\n",
      "14/19: x.shape\n",
      "14/20: x.shape=(6,)\n",
      "14/21: x\n",
      "14/22: x = x.astype('int64')\n",
      "14/23: x\n",
      "14/24: x.dtype\n",
      "14/25: a = np.zeros([2,3])\n",
      "14/26: a\n",
      "14/27: a.dtype\n",
      "14/28: np.ones([2,3])\n",
      "14/29: empty_array = np.empty((2,3))\n",
      "14/30: empty_array\n",
      "14/31: np.random.random((2,3))\n",
      "14/32: np.linspace(2, 10, 5)\n",
      "14/33: np.arange(2,10,2)\n",
      "14/34: a = np.array([1,2,3,4,5,6])\n",
      "14/35: a[2]\n",
      "14/36: a[3:5]\n",
      "14/37: a[:4:2]\n",
      "14/38: a[:4:2] = 0\n",
      "14/39: a\n",
      "14/40: a[::-1]\n",
      "14/41: a = np.arange(8)\n",
      "14/42: a\n",
      "14/43: a.shape = (2,2,2)\n",
      "14/44: a\n",
      "14/45: a[1]\n",
      "14/46: a*2\n",
      "14/47: a-2\n",
      "14/48: a/2.0\n",
      "14/49: a = np.array([45, 65, 76, 32, 99, 22])\n",
      "14/50: a<50\n",
      "14/51: a = np.arange(8).reshape((2,4))\n",
      "14/52: a\n",
      "14/53: a.min(axis=1)\n",
      "14/54: a.min(axis=2)\n",
      "14/55: a.min\n",
      "14/56: a.min()\n",
      "14/57: a.min(axis=0)\n",
      "14/58: a.sum(axis=0)\n",
      "14/59: a.sum(axis=1)\n",
      "14/60: a.mean(axis=1)\n",
      "14/61: a.mean(axis=0)\n",
      "14/62: a.std(axis=1)\n",
      "15/1: ufo_path='/users/wioletanytko/documents/worskpace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'\n",
      "15/2: ufo_path='/users/wioletanytko/documents/worskpace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'\n",
      "15/3: ufo_df = pd.read_csv(ufo_path, delimiter='\\t')\n",
      "15/4: import pandas an pd\n",
      "15/5: import pandas as pd\n",
      "15/6: ufo_df = pd.read_csv(ufo_path, delimiter='\\t')\n",
      "15/7: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'\n",
      "15/8: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'\n",
      "15/9: ufo_df = pd.read_csv(ufo_path, delimiter='\\t')\n",
      "18/1: ufo_df = pd.read_csv(ufo_path, delimiter='\\t', nrows=5)\n",
      "18/2: import pandas as pd\n",
      "18/3: ufo_df = pd.read_csv(ufo_path, delimiter='\\t', nrows=5)\n",
      "18/4: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'\n",
      "18/5: ufo_df = pd.read_csv(ufo_path, delimiter='\\t', nrows=5)\n",
      "18/6: ufo_df\n",
      "18/7: ufo_df = pd.read_csv(ufo_path, delimiter='\\t', nrows=5, header=None)\n",
      "18/8: ufo_df\n",
      "18/9: ufo_df = pd.read_csv(ufo_path, delimiter='\\t', nrows=5, header=None, names=['date1', 'date2','2','3','4','5'])\n",
      "18/10: ufo_df\n",
      "18/11: ufo_df = pd.read_csv(ufo_path, delimiter='\\t', nrows=5, header=None, names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'])\n",
      "18/12: ufo_df\n",
      "18/13: ufo_df = pd.read_csv(ufo_path, delimiter='\\t', nrows=10, header=None, names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'])\n",
      "18/14: ufo_df\n",
      "18/15:\n",
      "ufo_df = pd.read_csv(\n",
      "    ufo_path, delimiter='\\t',\n",
      "    nrows=100,\n",
      "    header=None,\n",
      "    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],\n",
      "    error_bad_lines=False)\n",
      "18/16: ufo_df\n",
      "18/17: head(ufo_df)\n",
      "18/18: pd.head(ufo_df)\n",
      "18/19: ufo_df.head\n",
      "18/20: ufo_df.head()\n",
      "18/21: ufo_df.head(10)\n",
      "18/22:\n",
      "ufo_df = pd.read_csv(\n",
      "    ufo_path, delimiter='\\t',\n",
      "    nrows=1000,\n",
      "    header=None,\n",
      "    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],\n",
      "    error_bad_lines=False)\n",
      "18/23: ufo_df\n",
      "18/24: ufo_df[900]\n",
      "18/25: ufo_df.columns\n",
      "18/26: ufo_df.iloc(2)\n",
      "18/27: ufo_df.index\n",
      "18/28: ufo_df.loc(1)\n",
      "18/29: ufo_df.loc[1]\n",
      "18/30: ufo_df.loc[0]\n",
      "18/31: ufo_df.set_index('Date_occured')\n",
      "18/32: df.loc['19951009']\n",
      "18/33: ufo_df.loc['19951009']\n",
      "18/34: ufo_df.loc[19951009]\n",
      "18/35: ufo_df.loc['19951009']\n",
      "18/36: ufo_df.iloc['19951009']\n",
      "18/37: ufo_df.loc['19951010']\n",
      "18/38: ufo_df.loc[19951010]\n",
      "18/39: ufo_df.loc[19930701]\n",
      "18/40: ufo_df.loc['19930701']\n",
      "18/41: ufo_df.columns\n",
      "18/42: ufo_df.loc(19951009)\n",
      "18/43: ufo_df.loc('19951009')\n",
      "18/44: ufo_df.loc[19951009]\n",
      "18/45: ufo_df.loc['19951009']\n",
      "18/46: ufo_df.loc[19951009]\n",
      "18/47: ufo_df.loc['19951009']\n",
      "18/48: ufo_df.set_index('Date_occured')\n",
      "18/49: ufo_df.loc['19951009']\n",
      "18/50: ufo_df.loc[19951009]\n",
      "18/51: ufo_df.reset_index()\n",
      "18/52: ufo_df.loc[999]\n",
      "18/53: ufo_df.set_index('Location')\n",
      "18/54: ufo_df.loc['Iowa City, IA']\n",
      "18/55: ufo_df.iloc['Iowa City, IA']\n",
      "18/56: ufo_df.reset_index\n",
      "18/57: ufo_df.set_index('Short_description')\n",
      "18/58: ufo_df.iloc['triangle']\n",
      "18/59: ufo_df.dtypes()\n",
      "18/60: ufo_df.dtypes\n",
      "18/61: ufo_df.loc['light']\n",
      "18/62: ufo_df.reset_index()\n",
      "18/63: ufo_df.set_index('Date_occured')\n",
      "18/64: ufo_df.loc[19960305]\n",
      "18/65: ufo_df.reset_index()\n",
      "18/66: ufo_df.loc[0]\n",
      "18/67: ufo_df.loc[0]['Long_description']\n",
      "18/68: ufo_df.loc[0]['Date_occured']\n",
      "18/69: ufo_df.set_index('Date_occured')\n",
      "18/70: ufo_df.loc[19951009]\n",
      "18/71: ufo_df.loc[\"19951009\"]\n",
      "18/72: ufo_df.loc[\"19951009\"]['Short_description']\n",
      "18/73: ufo_df.loc['19951009']['Short_description']\n",
      "18/74: ufo_df.loc[19951009]['Short_description']\n",
      "18/75: ufo_df.index\n",
      "18/76: ufo_df.set_index('Date_occured')\n",
      "18/77: ufo_df.index\n",
      "18/78: ufo_df.loc[0]\n",
      "18/79: ufo_df.set_index('Date_occured', inplace=True)\n",
      "18/80: ufo_df.loc[19951009]\n",
      "18/81: ufo_df.loc[19951009:19951010]\n",
      "18/82: ufo_df.loc[19951009:19951011]\n",
      "18/83: ufo_df.index\n",
      "18/84: ufo_df[19951009:19951010]\n",
      "18/85: ufo_df.loc[19951009:19951010]\n",
      "18/86: ufo_df.loc[19951010]\n",
      "18/87: ufo_df.reset_index()\n",
      "18/88: ufo_df.loc[0]\n",
      "18/89: ufo_df.groupby('Date_occured')\n",
      "18/90: df.groups.keys()\n",
      "18/91: ufo_df.groups.keys()\n",
      "18/92: ufo_df.group.keys()\n",
      "18/93: ufo_df.groups.keys()\n",
      "18/94: ufo_df = ufo_df.groupby('Date_occured')\n",
      "18/95: ufo_df.groups.keys()\n",
      "18/96: ufo_df.groups.keys().len()\n",
      "18/97: len(ufo_df.groups.keys())\n",
      "18/98: ufo_df.groups.keys()\n",
      "18/99: 19970404_group = ufo_df.get_group(19970404)\n",
      "18/100: april_group = ufo_df.get_group(19970404)\n",
      "18/101: april_group.head()\n",
      "18/102: ufo_df\n",
      "18/103: ufo_df.head()\n",
      "18/104: ufo_df.reset_index()\n",
      "17/1: import pandas as pd\n",
      "17/2:\n",
      "ufo_df = pd.read_csv(\n",
      "    ufo_path, delimiter='\\t',\n",
      "    header=None,\n",
      "    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],\n",
      "    error_bad_lines=False)\n",
      "17/3: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'\n",
      "17/4:\n",
      "ufo_df = pd.read_csv(\n",
      "    ufo_path, delimiter='\\t',\n",
      "    header=None,\n",
      "    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],\n",
      "    error_bad_lines=False)\n",
      "17/5: ufo_df.head()\n",
      "19/1: ufo_df.reset_index()\n",
      "20/1: import pandas as pd\n",
      "20/2: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'\n",
      "20/3:\n",
      "ufo_df = pd.read_csv(\n",
      "    ufo_path, delimiter='\\t',\n",
      "    nrows=5,\n",
      "    header=None,\n",
      "    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],\n",
      "    error_bad_lines=False)\n",
      "20/4: ufo_df.head()\n",
      "20/5: ufo_df.loc[0]['Date_occured']\n",
      "20/6: pd.to_datetime(ufo_df.loc[0]['Date_occured'])\n",
      "20/7: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')\n",
      "20/8: ufo_df['Date_occured']\n",
      "20/9: ufo_df.head()\n",
      "20/10: ufo_df['Date_occured']=pd.ufo_df['Date_occured', format='%Y%m%d']\n",
      "20/11: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')\n",
      "20/12: ufo_df.head()\n",
      "20/13: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'],ufo_df['Date_reported'], format='%Y%m%d')\n",
      "20/14: ufo_df.head()\n",
      "20/15: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')\n",
      "20/16: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')\n",
      "20/17: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')\n",
      "20/18: ufo_df.head()\n",
      "20/19: ufo_df['Date_reported']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')\n",
      "20/20: import pandas as pd\n",
      "20/21: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'\n",
      "20/22:\n",
      "ufo_df = pd.read_csv(\n",
      "    ufo_path, delimiter='\\t',\n",
      "    nrows=5,\n",
      "    header=None,\n",
      "    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],\n",
      "    error_bad_lines=False)\n",
      "20/23: ufo_df.head()\n",
      "20/24: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')\n",
      "20/25: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')\n",
      "20/26: ufo_df['Date_reported']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')\n",
      "20/27: ufo_df.head()\n",
      "21/1: import pandas as pd\n",
      "21/2: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'\n",
      "21/3:\n",
      "ufo_df = pd.read_csv(\n",
      "    ufo_path, delimiter='\\t',\n",
      "    nrows=5,\n",
      "    header=None,\n",
      "    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],\n",
      "    error_bad_lines=False)\n",
      "21/4: ufo_df.head()\n",
      "21/5: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')\n",
      "21/6: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')\n",
      "21/7: ufo_df['Date_reported']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')\n",
      "21/8: ufo_df.head()\n",
      "21/9: import pandas as pd\n",
      "21/10: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'\n",
      "21/11:\n",
      "ufo_df = pd.read_csv(\n",
      "    ufo_path, delimiter='\\t',\n",
      "    header=None,\n",
      "    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],\n",
      "    error_bad_lines=False)\n",
      "21/12: ufo_df.head()\n",
      "21/13: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')\n",
      "21/14: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')\n",
      "21/15: ufo_df.loc[pd.str.len(ufo_df['Date_occured'])>8]\n",
      "21/16: ufo_df.loc[str.len(ufo_df['Date_occured'])>8]\n",
      "21/17: ufo_df.loc[ufo_df['Date_occured'].str.len()>8]\n",
      "21/18: ufo_df.loc[ufo_df['Date_occured'].str.len()>8]\n",
      "21/19: import pandas as pd\n",
      "21/20: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'\n",
      "21/21:\n",
      "ufo_df = pd.read_csv(\n",
      "    ufo_path, delimiter='\\t',\n",
      "    header=None,\n",
      "    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],\n",
      "    error_bad_lines=False)\n",
      "21/22: ufo_df.head()\n",
      "21/23: ufo_df.loc[ufo_df['Date_occured'].str.len()>8]\n",
      "21/24: ufo_df.index()\n",
      "21/25: ufo_df.columns\n",
      "21/26: ufo_df.dtypes\n",
      "21/27: ufo_df.loc[ufo_df['Date_occured']>99999999]\n",
      "21/28: bad_date = ufo_df.loc[ufo_df['Date_occured']>99999999]\n",
      "21/29: bad_date.head()\n",
      "21/30: bad_date = ufo_df.loc[ufo_df['Date_occured']>100]\n",
      "21/31: bad_date = ufo_df.loc[ufo_df['Date_occured']>100]\n",
      "21/32: bad_date.head()\n",
      "21/33: bad_date = ufo_df.loc[ufo_df['Date_occured']>19951009]\n",
      "21/34: bad_date.head()\n",
      "21/35: ufo_df['Date_occured_len']=ufo_df.loc['Date_occured'].astype(str).len()\n",
      "21/36: ufo_df['Date_occured_len']=0#ufo_df.loc['Date_occured'].astype(str).len()\n",
      "21/37: ufo_df.head\n",
      "21/38: ufo_df.head()\n",
      "21/39: ufo_df['Date_occured_len']=ufo_df.loc['Date_occured'].astype(str).map(len)\n",
      "21/40: ufo_df['Date_occured_len']=ufo_df['Date_occured'].astype(str).map(len)\n",
      "21/41: ufo_df.head()\n",
      "21/42: ufo_df['Date_occured_len']=ufo_df['Date_occured'].astype(str).len()\n",
      "21/43: ufo_df['Date_occured_len']=ufo_df['Date_occured'].astype(str).str.len()\n",
      "21/44: ufo_df.head()\n",
      "21/45: ufo_df.head()\n",
      "21/46: bad_date_df = ufo_df.loc[ufo_df['Date_occured_len'] != 8]\n",
      "21/47: bad_date.head()\n",
      "21/48: bad_date_df.head()\n",
      "21/49: bad_date_df\n",
      "21/50: bad_date_df.info\n",
      "21/51: bad_date_df.info()\n",
      "21/52: (ufo_df['Date_occured_len'] != 8).count()\n",
      "21/53: (ufo_df['Date_occured_len'] == 8).count()\n",
      "21/54: [ufo_df['Date_occured_len'] == 8].count()\n",
      "21/55: ufo_df[ufo_df['Date_occured_len'] == 8].count()\n",
      "21/56: ufo_df[ufo_df['Date_occured_len'] == 8].count()['Date_occured_len']\n",
      "21/57: ufo_df[ufo_df['Date_occured_len'] != 8].count()['Date_occured_len']\n",
      "21/58: ufo_df['Date_reported_len']=ufo_df['Date_reported'].astype(str).str.len()\n",
      "21/59: bad_date_df = ufo_df.loc[(ufo_df['Date_occured_len'] != 8) | (udo_df['Date_reported_len'] != 8)]\n",
      "21/60: bad_date_df = ufo_df.loc[(ufo_df['Date_occured_len'] != 8) | (ufo_df['Date_reported_len'] != 8)]\n",
      "21/61: bad_date_df\n",
      "21/62: bad_date_df.info()\n",
      "21/63: ufo_df[ufo_df['Date_occured_len'] != 8].count()['Date_occured_len']\n",
      "21/64: ufo_df[(ufo_df['Date_reported_len'] != 8) | (ufo_df[''])].count()['Date_reported_len']\n",
      "21/65: ufo_df[ufo_df['Date_reported_len'] != 8].count()['Date_reported_len']\n",
      "21/66: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')\n",
      "21/67: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')\n",
      "21/68: ufo_df.drop(ufo_df[ufo_df.Date_occured_len !=8].index, inplace=True)\n",
      "21/69: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')\n",
      "21/70: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')\n",
      "21/71: ufo_df['Date_occured'].astype(str).str.contains('00')\n",
      "21/72: zerodate_df = ufo_df['Date_occured'].astype(str).str.contains('00')\n",
      "21/73: zerodate\n",
      "21/74: zerodate_df\n",
      "21/75: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.contains('00')\n",
      "21/76: ufo_df\n",
      "21/77: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.contains('0000')\n",
      "21/78: ufo_df\n",
      "21/79: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/80: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.contains(r'....00..')\n",
      "21/81: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/82: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.contains(r'....00..')\n",
      "21/83: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/84: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.contains(r'.00..')\n",
      "21/85: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/86: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'00..')\n",
      "21/87: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/88: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00..')\n",
      "21/89: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/90: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00.')\n",
      "21/91: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/92: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/93: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'00')\n",
      "21/94: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/95: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'00..')\n",
      "21/96: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'..00..')\n",
      "21/97: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/98: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00')\n",
      "21/99: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/100: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'..00')\n",
      "21/101: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/102: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00..')\n",
      "21/103: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/104: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00')\n",
      "21/105: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/106: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/107: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/108: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'...00')\n",
      "21/109: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/110: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00')\n",
      "21/111: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/112: ufo_df.drop(ufo_df.loc['zerodate'] == True)\n",
      "21/113: ufo_df.drop(ufo_df[ufo_df.loc['zerodate'] == True])\n",
      "21/114: ufo_df.drop(ufo_df.loc[ufo_df['zerodate'] == True])\n",
      "21/115: ufo_df.drop(ufo_df.loc[ufo_df['zerodate'].index == True])\n",
      "21/116: ufo_df.drop(ufo_df.loc[ufo_df['zerodate'] == True].index, inplace=True)\n",
      "21/117: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')\n",
      "21/118: ufo_df.Date_occured\n",
      "21/119: ufo_df.sort_values(by=['Date_occured'])\n",
      "21/120: ufo_df.sort_values(by=['Date_occured']).head()\n",
      "21/121: ufo_df.drop(40901, inplace=True)\n",
      "21/122: ufo_df.sort_values(by=['Date_occured']).head()\n",
      "21/123: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')\n",
      "21/124: ufo_df['Date_reported']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')\n",
      "21/125: ufo_df.head()\n",
      "21/126: bad_date_df.head()\n",
      "21/127: mask = ufo_df.Date_occured>1999-01-01\n",
      "21/128: mask = ufo_df.Date_occured>(1999-01-01)\n",
      "21/129: mask = ufo_df.Date_occured>('1999-01-01')\n",
      "21/130: mask\n",
      "21/131: since_1999 = ufo_df[mask]\n",
      "21/132: since_1999.head()\n",
      "21/133: ufo_df.describe()\n",
      "21/134: ufo_df.describe(include=['object'])\n",
      "21/135: ufo_df.colums\n",
      "21/136: ufo_df.columns\n",
      "21/137: ufo_df.dtype\n",
      "21/138: ufo_df.dtype()\n",
      "21/139: ufo_df.dtypes()\n",
      "21/140: ufo_df.dtypes\n",
      "21/141: ufo_df.describe(include=['datetime64'])\n",
      "21/142: import pandas as pd\n",
      "21/143: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'\n",
      "21/144:\n",
      "ufo_df = pd.read_csv(\n",
      "    ufo_path, delimiter='\\t',\n",
      "    header=None,\n",
      "    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],\n",
      "    error_bad_lines=False)\n",
      "21/145: ufo_df.head()\n",
      "21/146: ufo_df.dtypes\n",
      "21/147: bad_date = ufo_df.loc[ufo_df['Date_occured']>19951009]\n",
      "21/148: bad_date.head()\n",
      "21/149: ufo_df['Date_occured_len']=ufo_df['Date_occured'].astype(str).str.len()\n",
      "21/150: ufo_df['Date_reported_len']=ufo_df['Date_reported'].astype(str).str.len()\n",
      "21/151: ufo_df.head()\n",
      "21/152: bad_date_df.head()\n",
      "21/153: bad_date_df = ufo_df.loc[(ufo_df['Date_occured_len'] != 8) | (ufo_df['Date_reported_len'] != 8)]\n",
      "21/154: bad_date_df.head()\n",
      "21/155: bad_date_df.tail()\n",
      "21/156: bad_date_df.sort_values(by=['Date_occured_len'])\n",
      "21/157: bad_date_df.info()\n",
      "21/158: ufo_df[ufo_df['Date_occured_len'] != 8].count()['Date_occured_len']\n",
      "21/159: ufo_df[ufo_df['Date_reported_len'] != 8].count()['Date_reported_len']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/160: ufo_df.drop(ufo_df[ufo_df.Date_occured_len !=8].index, inplace=True)\n",
      "21/161: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')\n",
      "21/162: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00')\n",
      "21/163: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "21/164: ufo_df.drop(ufo_df.loc[ufo_df['zerodate'] == True].index, inplace=True)\n",
      "21/165: ufo_df.sort_values(by=['Date_occured']).head()\n",
      "21/166: ufo_df.drop(40901, inplace=True)\n",
      "21/167: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')\n",
      "21/168: ufo_df['Date_reported']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')\n",
      "21/169: ufo_df.head()\n",
      "21/170: mask = ufo_df.Date_occured>('1999-01-01')\n",
      "21/171: since_1999 = ufo_df[mask]\n",
      "21/172: since_1999.head()\n",
      "21/173: ufo_df.describe(include=['datetime64'])\n",
      "21/174: ufo_df.dtypes\n",
      "21/175: new=ufo_df['Location'].str.split(',')\n",
      "21/176: new.head()\n",
      "21/177: new.head(20)\n",
      "21/178: new['len'] = new.str.len()\n",
      "21/179: new.head()\n",
      "21/180: newest = new.str.len()\n",
      "21/181: newest.head()\n",
      "21/182: newest.describe\n",
      "21/183: newest.describe()\n",
      "21/184: new=ufo_df['Location'].str.split(',', n=1)\n",
      "21/185: newest = new.str.len()\n",
      "21/186: newest.describe()\n",
      "21/187: newest.head()\n",
      "21/188: new.head()\n",
      "21/189: new.sort_values()\n",
      "21/190: new.loc[3394]\n",
      "21/191: newest.sort_values()\n",
      "21/192: newest.loc(19201)\n",
      "21/193: new.loc(19201)\n",
      "21/194: new.loc(61392)\n",
      "21/195: new.head()\n",
      "21/196: new.loc(20583)\n",
      "21/197: new(20583)\n",
      "21/198: new.loc(2)\n",
      "21/199: new.head(2)\n",
      "21/200: new.loc[2]\n",
      "21/201: new.loc[19201]\n",
      "21/202: new.loc[6295]\n",
      "21/203: ufo_df.info\n",
      "21/204: ufo_df.info()\n",
      "21/205: new.head()\n",
      "21/206: new.loc[0][0]\n",
      "21/207: new.loc[0][1]\n",
      "21/208: new.loc[0][0][1]\n",
      "21/209: new.loc[0][0],new.loc[0][1]\n",
      "21/210: ufo_df['City'] = new[0]\n",
      "21/211: new.info()\n",
      "21/212: new.len()\n",
      "21/213: len(new)\n",
      "21/214: ufo_df['City'] = new[0]\n",
      "21/215: new[0]\n",
      "21/216: ufo_df['City'][0] = 'Test'\n",
      "21/217: ufo_df['City'] = new\n",
      "21/218: ufo_df[['City', 'State']] = new([0,1])\n",
      "21/219: ufo_df[['City', 'State']] = new[0,1]\n",
      "21/220: ufo_df['City', 'State'] = new[0,1]\n",
      "21/221: new.columns\n",
      "21/222: new.head()\n",
      "21/223: ufo_df['City'] = new[0]\n",
      "21/224: ufo_df['City'] = new.str[0]\n",
      "21/225: ufo_df.head()\n",
      "21/226: ufo_df['State'] = new.str[1]\n",
      "21/227: ufo_df.head()\n",
      "21/228: ufo_df.drop(['Date_occured_len', 'Date_reported_len', 'zerodate'])\n",
      "21/229: ufo_df.drop(['Date_occured_len', 'Date_reported_len', 'zerodate'], axis=1)\n",
      "21/230: states_abbreviations = [ 'AL', 'AK', 'AS', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FM', 'FL', 'GA', 'GU', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MH', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'MP', 'OH', 'OK', 'OR', 'PW', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VI', 'VA', 'WA', 'WV', 'WI', 'WY' ]\n",
      "21/231: states_mask = df.State in states_abbreaviations\n",
      "21/232: states_mask = ufo_df.State in states_abbreaviations\n",
      "21/233: states_mask = ufo_df.State in states_abbreviations\n",
      "21/234: states_mask = ufo_df.State.isin(states_abbreviations)\n",
      "21/235: states_mask.head()\n",
      "21/236: ufo_df.States[0]\n",
      "21/237: ufo_df.States[0, axis=1]\n",
      "21/238: ufo_df.State[0]\n",
      "21/239: ufo_df.str.strip()\n",
      "21/240: ufo_df.State.str.strip()\n",
      "21/241: states_mask = ufo_df.State.isin(states_abbreviations)\n",
      "21/242: states_mask.head()\n",
      "21/243: ufo_df.State[0]\n",
      "21/244: ufo_df.State.str.strip()\n",
      "21/245: states_abbreviations = [ 'AL', 'AK', 'AS', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FM', 'FL', 'GA', 'GU', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MH', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'MP', 'OH', 'OK', 'OR', 'PW', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VI', 'VA', 'WA', 'WV', 'WI', 'WY' ]\n",
      "21/246: states_mask = ufo_df.State.isin(states_abbreviations)\n",
      "21/247: states_mask.head()\n",
      "21/248: ufo_df.State[0]\n",
      "21/249: ufo_df.State.str.strip(inplace=True)\n",
      "21/250: ufo_df.dtypes\n",
      "21/251: ufo_df.State = ufo_df.State.astype(str)\n",
      "21/252: ufo_df.dtypes\n",
      "21/253: ufo_df.State.str.strip()\n",
      "21/254: states_mask = ufo_df.State.isin(states_abbreviations)\n",
      "21/255: states_mask.head()\n",
      "21/256: ufo_df.State[0]\n",
      "21/257: ufo_df.State = ufo_df.State.str.strip()\n",
      "21/258: states_abbreviations = [ 'AL', 'AK', 'AS', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FM', 'FL', 'GA', 'GU', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MH', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'MP', 'OH', 'OK', 'OR', 'PW', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VI', 'VA', 'WA', 'WV', 'WI', 'WY' ]\n",
      "21/259: states_mask = ufo_df.State.isin(states_abbreviations)\n",
      "21/260: states_mask.head()\n",
      "21/261: ufo_df.State[0]\n",
      "21/262: ufo_us = ufo_df[states_mask]\n",
      "21/263: ufo_us.head()\n",
      "21/264: ufo_df.drop(['Date_occured_len', 'Date_reported_len', 'zerodate'], axis=1, inplace=True)\n",
      "21/265: ufo_us = ufo_df[states_mask]\n",
      "21/266: ufo_us.head()\n",
      "21/267: ufo_us.info\n",
      "21/268: ufo_us.info()\n",
      "21/269: ufo_us.describe(inclue=['datetime64'])\n",
      "21/270: ufo_us.describe(include=['datetime64'])\n",
      "21/271: ufo_us.quantile(columns=['Date_occured'])\n",
      "21/272: ufo_us.quantile()\n",
      "21/273: ufo_us.quantile(axis=1)\n",
      "21/274: ufo_us.quantile(axis=1, numeric_only = False)\n",
      "21/275: ufo_us.quantile(axis=0, numeric_only = False)\n",
      "21/276: ufo_us.quantile(axis='columns', numeric_only = False)\n",
      "21/277: ufo_us.Date_occured.quantile(numeric_only = False)\n",
      "21/278: ufo_us.Date_occured.quantile()\n",
      "21/279: ufo_us.Date_occured.quantile([.25, .5, .75])\n",
      "21/280: ufo_us.describe[include='datetime64']\n",
      "21/281: ufo_us.describe(include=['datetime64'])\n",
      "21/282: ufo_us.Date_occured.max()\n",
      "21/283: ufo_us.Date_occured.min()\n",
      "21/284: hist = ufo_us.hist(column='Date_occured')\n",
      "21/285: hist = ufo_us.hist(column='Date_occured', bins=20)\n",
      "21/286: time_mask = (ufo_us.Date_occured > '1990-01-01')\n",
      "21/287: ufo_us = ufo_us[time_mask]\n",
      "21/288: ufo_us.info()\n",
      "21/289: hist = ufo_us.hist(column='Date_occured', bins=20)\n",
      "21/290: ufo_us.['Date_occured'].plot.hist()\n",
      "21/291: ufo_us['Date_occured'].plot.hist()\n",
      "21/292: ufo_us['Duration'].plot.hist()\n",
      "21/293: ufo_us.dtypes()\n",
      "21/294: ufo_us.dtypes\n",
      "21/295: ufo_us['Duration'].hist()\n",
      "19/2: z=2\n",
      "22/1: import pandas as pd\n",
      "22/2: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'\n",
      "22/3:\n",
      "ufo_df = pd.read_csv(\n",
      "    ufo_path, delimiter='\\t',\n",
      "    header=None,\n",
      "    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],\n",
      "    error_bad_lines=False)\n",
      "22/4: ufo_df.head()\n",
      "22/5: ufo_df.dtypes\n",
      "22/6: bad_date = ufo_df.loc[ufo_df['Date_occured']>19951009]\n",
      "22/7: bad_date.head()\n",
      "22/8: ufo_df['Date_occured_len']=ufo_df['Date_occured'].astype(str).str.len()\n",
      "22/9: ufo_df['Date_reported_len']=ufo_df['Date_reported'].astype(str).str.len()\n",
      "22/10: ufo_df.head()\n",
      "22/11: bad_date_df = ufo_df.loc[(ufo_df['Date_occured_len'] != 8) | (ufo_df['Date_reported_len'] != 8)]\n",
      "22/12: bad_date_df.sort_values(by=['Date_occured_len'])\n",
      "22/13: bad_date_df.info()\n",
      "22/14: ufo_df[ufo_df['Date_occured_len'] != 8].count()['Date_occured_len']\n",
      "22/15: ufo_df[ufo_df['Date_reported_len'] != 8].count()['Date_reported_len']\n",
      "22/16: ufo_df.drop(ufo_df[ufo_df.Date_occured_len !=8].index, inplace=True)\n",
      "22/17: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')\n",
      "22/18: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00')\n",
      "22/19: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "22/20: ufo_df.drop(ufo_df.loc[ufo_df['zerodate'] == True].index, inplace=True)\n",
      "22/21: ufo_df.sort_values(by=['Date_occured']).head()\n",
      "22/22: ufo_df.drop(40901, inplace=True)\n",
      "22/23: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')\n",
      "22/24: ufo_df['Date_reported']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')\n",
      "22/25: ufo_df.head()\n",
      "22/26: mask = ufo_df.Date_occured>('1999-01-01')\n",
      "22/27: since_1999 = ufo_df[mask]\n",
      "22/28: since_1999.head()\n",
      "22/29: ufo_df.describe(include=['datetime64'])\n",
      "22/30: ufo_df.info()\n",
      "22/31: new=ufo_df['Location'].str.split(',', n=1)\n",
      "22/32: len(new)\n",
      "22/33: newest = new.str.len()\n",
      "22/34: new.loc[0][0],new.loc[0][1]\n",
      "22/35: new.head()\n",
      "22/36: ufo_df['City'] = new.str[0]\n",
      "22/37: ufo_df['State'] = new.str[1]\n",
      "22/38: ufo_df.head()\n",
      "22/39: ufo_df.drop(['Date_occured_len', 'Date_reported_len', 'zerodate'], axis=1, inplace=True)\n",
      "22/40: ufo_df.State = ufo_df.State.str.strip()\n",
      "22/41: ufo_df.dtypes\n",
      "22/42: states_abbreviations = [ 'AL', 'AK', 'AS', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FM', 'FL', 'GA', 'GU', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MH', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'MP', 'OH', 'OK', 'OR', 'PW', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VI', 'VA', 'WA', 'WV', 'WI', 'WY' ]\n",
      "22/43: states_mask = ufo_df.State.isin(states_abbreviations)\n",
      "22/44: states_mask.head()\n",
      "22/45: ufo_us = ufo_df[states_mask]\n",
      "22/46: ufo_us.head()\n",
      "22/47: ufo_us.Date_occured.quantile([.25, .5, .75])\n",
      "22/48: ufo_us.Date_occured.max()\n",
      "22/49: ufo_us.Date_occured.min()\n",
      "22/50: time_mask = (ufo_us.Date_occured > '1990-01-01')\n",
      "22/51: ufo_us = ufo_us[time_mask]\n",
      "22/52: import matplotlib.pyplot as pp\n",
      "22/53: ufo_us.dtypes\n",
      "22/54: z=2\n",
      "22/55: z\n",
      "22/56: pp.hist(ufo_us['Date_occured'])\n",
      "22/57: pp.hist(ufo_us['Date_occured'], bins=20)\n",
      "22/58: pp.hist(ufo_us['Date_occured'], bins=20, density = True)\n",
      "22/59: import matplotlib.pyplot as plt\n",
      "22/60: plt.hist(ufo_us['Date_occured'], bins=20, density = True)\n",
      "22/61: plt.hist(ufo_us['Date_occured'], bins=20)\n",
      "22/62: histogram = ufo_us['Date_occured'].hist()\n",
      "22/63: histogram = ufo_us['Date_occured'].hist(bins=20)\n",
      "22/64: histogram.set_label('ufo')\n",
      "22/65: histogram.plot()\n",
      "22/66: histogram.showt()\n",
      "22/67: histogram.show()\n",
      "22/68: histogram = ufo_us['Date_occured'].hist(bins=20, labels=('year', 'number'))\n",
      "22/69: histogram = ufo_us['Date_occured'].hist(bins=20, label=('year', 'number'))\n",
      "22/70: histogram = ufo_us['Date_occured'].hist(bins=20, label=['year', 'number'])\n",
      "22/71: %matplotlib inline\n",
      "22/72: histogram = ufo_us['Date_occured'].hist(bins=20, label=['year', 'number'])\n",
      "19/3: x = pd.period_range(pd.datetime.now(), periods=200, freq='d')\n",
      "19/4: import pandas as pd\n",
      "19/5: x = pd.period_range(pd.datetime.now(), periods=200, freq='d')\n",
      "19/6: import matplotlib.pyplot as plt\n",
      "19/7: x = x.to_timestamp().to_pydatetime()\n",
      "19/8: plt.plot(x,y)\n",
      "19/9: y = np.random.randn(200, 3).cumsum(0)\n",
      "19/10: import numpy as np\n",
      "19/11: y = np.random.randn(200, 3).cumsum(0)\n",
      "19/12: plt.plot(x,y)\n",
      "19/13: import matplotlib as mpl\n",
      "19/14: plt.rcParams['figure.figsize'] = (8,4)\n",
      "19/15: plt.gcf().set_size_inches(8,4)\n",
      "19/16:\n",
      "plt.plot(x,y)\n",
      "plt.gcf().set_size_inches(8,4)\n",
      "19/17: y = np.random.randn(200)\n",
      "19/18:\n",
      "plt.plot(x,y)\n",
      "plt.gcf().set_size_inches(8,4)\n",
      "19/19: y = np.random.randn(200, 3)\n",
      "19/20:\n",
      "plt.plot(x,y)\n",
      "plt.gcf().set_size_inches(8,4)\n",
      "19/21: y = np.random.randn(200, 3).cumsum(0)\n",
      "19/22:\n",
      "plt.plot(x,y)\n",
      "plt.gcf().set_size_inches(8,4)\n",
      "19/23:\n",
      "plots = plt.plot(x,y)\n",
      "plt.gcf().set_size_inches(8,4)\n",
      "19/24: plots\n",
      "19/25: plt.legend(plots, ('foo', 'bar', 'baz'), loc='best')\n",
      "19/26:\n",
      "plt.legend(plots, ('foo', 'bar', 'baz'), loc='best')\n",
      "framealpha=0.5\n",
      "prop={'size': 'small', 'familu': 'monospace'}\n",
      "19/27:\n",
      "plots = plt.plot(x,y)\n",
      "plt.gcf().set_size_inches(8,4)\n",
      "19/28:\n",
      "plots = plt.plot(x,y)\n",
      "plots\n",
      "19/29:\n",
      "plots = plt.plot(x,y)\n",
      "plt.legend(plots, ('foo', 'bar', 'baz'), loc='best')\n",
      "19/30:\n",
      "plots = plt.plot(x,y)\n",
      "plt.legend(plots, ('foo', 'bar', 'baz'), loc='best')\n",
      "framealpha=0.5\n",
      "prop={'size': 'small', 'familu': 'monospace'}\n",
      "22/73:\n",
      "histogram = ufo_us['Date_occured'].hist(bins=20, label=['year', 'number'])\n",
      "plt.title('Ufo observation')\n",
      "22/74:\n",
      "histogram = ufo_us['Date_occured'].hist(bins=20, label=['year', 'number'])\n",
      "plt.title('Ufo observation')\n",
      "plt.xlabel('Year')\n",
      "plt.ylabel('Observations')\n",
      "22/75: ufo_us['year'] = ufo_us['Date_occured'].year\n",
      "22/76: ufo_us['Date_occured']\n",
      "22/77: ufo_us['Date_occured'][0]\n",
      "22/78: ufo_us['Date_occured'][0].year\n",
      "22/79: ufo_us = ufo_us['Date_occured'].year\n",
      "22/80: ufo_us['Year'] = ufo_us['Date_occured'].year\n",
      "22/81: ufo_us['Year'] = ufo_us['Date_occured'].dt.year\n",
      "22/82: ufo_us['Year'] = ufo_us['Date_occured'].dt.year\n",
      "22/83: ufo_us.head()\n",
      "22/84: ufo_us['Month'] = ufo_us['Date_occured'].dt.month\n",
      "22/85: ufo_us.head()\n",
      "22/86: ufo_grouped = ufo_us.groupby('State').count()\n",
      "22/87: ufo_grouped.head()\n",
      "22/88: ufo_grouped = ufo_us.groupby('State', 'Year', 'Month').count()\n",
      "22/89: ufo_grouped = ufo_us.groupby(['State', 'Year', 'Month']).count()\n",
      "22/90: ufo_grouped.head()\n",
      "22/91: ufo_grouped = ufo_us.groupby(['State', 'Year', 'Month']).['Date_occured']count()\n",
      "22/92: ufo_grouped = ufo_us.groupby(['State', 'Year', 'Month'])['Date_occured']count()\n",
      "22/93: ufo_grouped = ufo_us.groupby(['State', 'Year', 'Month'])['Date_occured'].count()\n",
      "22/94: ufo_grouped.head()\n",
      "22/95: ufo_grouped.info()\n",
      "22/96: ufo_grouped.info\n",
      "22/97: ufo_grouped\n",
      "22/98: ufo_grouped.describe()\n",
      "22/99: ufo_grouped.head()\n",
      "22/100: ufo_grouped.columns\n",
      "22/101: ufo_grouped.sort_values(by=['Year'])\n",
      "22/102: ufo_us.sort_values(by=['Year'])\n",
      "22/103: ufo_us.sort_values(by=['Year', 'Month'])\n",
      "22/104: period = pd.df({'Year': range(1990, 2010)})\n",
      "22/105: period = pd.DataFrame({'Year': range(1990, 2010)})\n",
      "22/106: period.head()\n",
      "22/107: period_years = pd.DataFrame({'Year': range(1990, 2010)})\n",
      "22/108: period\n",
      "22/109: period_months = pd.DataFrame({'Month': range(1, 13)})\n",
      "22/110: period_months\n",
      "22/111: period = pd.merge(period_years, period_months, how='outer')\n",
      "22/112: period = period_years.assign(foo=1).merge(period_months.assign(foo=1))\n",
      "22/113: period\n",
      "22/114: period = period_years.assign(foo=1).merge(period_months.assign(foo=1)).drop('foo',1)\n",
      "22/115: period\n",
      "22/116: period_years = pd.DataFrame({'Year': range(1990, 2011)})\n",
      "22/117: period_months = pd.DataFrame({'Month': range(1, 13)})\n",
      "22/118: period = period_years.assign(foo=1).merge(period_months.assign(foo=1)).drop('foo',1)\n",
      "22/119: period\n",
      "22/120: ufo_grouped_2 = ufo_us.groupby(ufo_us.Date_occured.dy.year)\n",
      "22/121: ufo_grouped_2 = ufo_us.groupby(ufo_us.Date_occured.dt.year)\n",
      "22/122: ufo_grouped_2.head()\n",
      "22/123: ufo_grouped_2 = ufo_us.groupby(ufo_us.Date_occured.dt.year)['Date_occured'].count()\n",
      "22/124: ufo_grouped_2.head()\n",
      "22/125: ufo_grouped_2[0]\n",
      "22/126: ufo_grouped_2\n",
      "22/127: ufo_grouped_2.columns()\n",
      "22/128: ufo_grouped_2.columns\n",
      "22/129: ufo_grouped_2.index\n",
      "22/130:\n",
      "ufo_grouped_2 = ufo_us.groupby([\n",
      "    ufo_us.Date_occured.dt.year,\n",
      "    ufo_us.Date_occured.dt.month,\n",
      "    ufo_us.State])\n",
      "    ['Date_occured'].count()\n",
      "22/131:\n",
      "ufo_grouped_2 = ufo_us.groupby([\n",
      "    ufo_us.Date_occured.dt.year,\n",
      "    ufo_us.Date_occured.dt.month,\n",
      "    ufo_us.State])['Date_occured'].count()\n",
      "22/132: ufo_grouped_2.\n",
      "22/133: ufo_grouped_2\n",
      "22/134:\n",
      "ufo_grouped_2 = ufo_us.groupby([\n",
      "    ufo_us.State,\n",
      "    ufo_us.Date_occured.dt.year,\n",
      "    ufo_us.Date_occured.dt.month,\n",
      "    ])['Date_occured'].count()\n",
      "22/135: ufo_grouped_2\n",
      "22/136: ufo_grouped_2.columns\n",
      "22/137: ufo_grouped_2.index\n",
      "22/138: ufo_grouped_2\n",
      "22/139: ufo_grouped_2.head()\n",
      "22/140: ufo_grouped_2.index[0]\n",
      "22/141: state, year, month = ufo_grouped_2.index[0]\n",
      "22/142: state\n",
      "22/143: ufo_grouped_2.index\n",
      "22/144: ufo_grouped_2.index.levels\n",
      "22/145: ufo_grouped_2.index.levels[0]\n",
      "22/146: ufo_grouped_2[0]\n",
      "22/147: ufo_grouped_2[1]\n",
      "22/148: x=2\n",
      "22/149:\n",
      "for x in range(2):\n",
      "    print(x)\n",
      "22/150:\n",
      "for month in range(1, 13):\n",
      "    for year in range(10):\n",
      "        print(month, year)\n",
      "22/151:\n",
      "for month in range(1, 13):\n",
      "    for year in range(10):\n",
      "        print(year, month)\n",
      "22/152:\n",
      "for month in range(1, 13):\n",
      "    for year in range(1990, 2011):\n",
      "        print(year, month)\n",
      "22/153:\n",
      "for year in range(1990, 2011):\n",
      "    for month in range(1, 13):\n",
      "        print(year, month)\n",
      "22/154: ufo_grouped_2.index.levels[1]\n",
      "22/155: state, year, month = ufo_grouped_2.index\n",
      "22/156: ufo_grouped_2.index\n",
      "22/157: state, year, month = ufo_grouped_2.index.levels\n",
      "22/158: state\n",
      "22/159: state[0]\n",
      "22/160: ufo_grouped_2\n",
      "22/161: ufo_grouped_2.index[0]\n",
      "22/162: ufo_grouped_2.loc['Ak', 1990, 1]\n",
      "22/163: ufo_grouped_2.loc['AK', 1990, 1]\n",
      "22/164: state\n",
      "22/165: year\n",
      "22/166: observations = pd.DataFrame(columns=['State', 'Year', 'Month'])\n",
      "22/167: observations.loc[0]='a','b', 'c'\n",
      "22/168: observations\n",
      "22/169: states, years, months = ufo_grouped_2.index.levels\n",
      "22/170: i=0\n",
      "22/171:\n",
      "for state in states:\n",
      "    for year in years:\n",
      "        for month in months:\n",
      "            observations.loc[i]=state, year, month\n",
      "            i+=1\n",
      "22/172: observations\n",
      "22/173: observations.loc['AK', 1990, 1]\n",
      "22/174: observations['counted'] = 0\n",
      "22/175: observations\n",
      "22/176: observations[0]\n",
      "22/177: observations.loc[0]\n",
      "22/178: observations.loc[0]['State']\n",
      "22/179: observations.index\n",
      "22/180:\n",
      "for x in observations.index:\n",
      "    print(x)\n",
      "22/181:\n",
      "for x in observations.index:\n",
      "    print(observations.loc[x]['State'], observations.loc[x]['Year'])\n",
      "22/182: my_indexes = (observations.loc[x]['State'], observations.loc[x]['Year'], observations.loc[x]['Month'] for x in observations.index)\n",
      "22/183: my_indexes = ((observations.loc[x]['State'], observations.loc[x]['Year'], observations.loc[x]['Month']) for x in observations.index)\n",
      "22/184: my_indexes.head()\n",
      "22/185: ufo_grouped_2.loc[('AK', 1990, 1)]\n",
      "22/186: all_observations = [ufo_grouped_2.loc[element] for element in my_indexes]\n",
      "22/187:\n",
      "for element in my_indexes:\n",
      "    print element\n",
      "22/188:\n",
      "for element in my_indexes:\n",
      "    print(element)\n",
      "22/189:\n",
      "for element in my_indexes:\n",
      "    print(observations.loc[element])\n",
      "22/190:\n",
      "for element in my_indexes:\n",
      "    print(observations.loc[element])\n",
      "22/191:\n",
      "for element in my_indexes:\n",
      "    print(ufo_grouped_2.loc[element])\n",
      "22/192:\n",
      "for element in my_indexes:\n",
      "    print(ufo_grouped_2.loc[element])\n",
      "22/193: my_indexes = ((observations.loc[x]['State'], observations.loc[x]['Year'], observations.loc[x]['Month']) for x in observations.index)\n",
      "22/194:\n",
      "for element in my_indexes:\n",
      "    print(ufo_grouped_2.loc[element])\n",
      "22/195:\n",
      "for element in my_indexes:\n",
      "    print(ufo_grouped_2.loc[*element])\n",
      "22/196:\n",
      "for element in my_indexes:\n",
      "    print(ufo_grouped_2.loc[element])\n",
      "22/197:\n",
      "for element in my_indexes:\n",
      "    print(element)\n",
      "22/198: my_indexes = [(observations.loc[x]['State'], observations.loc[x]['Year'], observations.loc[x]['Month']) for x in observations.index]\n",
      "22/199: my_indexes[0]\n",
      "22/200: ufo_grouped_2.loc[my_indexes[0]]\n",
      "22/201:\n",
      "for single_index in observations.index:\n",
      "    observations[single_index]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)\n",
      "22/202: observations[0]['counted'] = ufo_grouped_2.get(my_indexes[0], default=0)\n",
      "22/203: observations.loc[0]['counted'] = ufo_grouped_2.get(my_indexes[0], default=0)\n",
      "22/204:\n",
      "for single_index in observations.index\n",
      "    observations.loc[0]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)\n",
      "22/205:\n",
      "for single_index in observations.index:\n",
      "    observations.loc[0]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)\n",
      "22/206: observations.loc[100]\n",
      "22/207: observations.head(100)\n",
      "22/208: observations.loc[300]\n",
      "22/209: observations.loc[400]\n",
      "22/210: observations.info()\n",
      "22/211: observations.describe()\n",
      "22/212: observations.loc[900]\n",
      "22/213: observations.loc[800]\n",
      "22/214:\n",
      "for single_index in observations.index:\n",
      "    observations.loc[0]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)\n",
      "22/215:\n",
      "for single_index in observations.index:\n",
      "    observations.loc[single_index]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)\n",
      "22/216: ufo_grouped_2.get(my_indexes[0])\n",
      "22/217: ufo_grouped_2.get(my_indexes[0], default=0)\n",
      "22/218: ufo_grouped_2.get(my_indexes[900], default=0)\n",
      "22/219: observations.index\n",
      "22/220:\n",
      "for single_index in observations.index:\n",
      "    observations.loc[single_index]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)\n",
      "22/221:\n",
      "for single_index in range(100):\n",
      "    observations.loc[single_index]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)\n",
      "22/222:\n",
      "for single_index in range(100):\n",
      "    observations.loc[single_index]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)\n",
      "22/223:\n",
      "for single_index in range(100):\n",
      "    observations.loc[single_index]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)\n",
      "22/224:\n",
      "for single_index in range(100):\n",
      "    observations.loc[single_index]['counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)\n",
      "22/225: observations.head(100)\n",
      "22/226: observations.loc[0]['counted'] = 1\n",
      "22/227: observations.loc[0]['counted'] = 1\n",
      "22/228: observations.head()\n",
      "22/229: observations[0]['counted'] = 1\n",
      "22/230: observations[0, 'counted'] = 1\n",
      "22/231: observations.head()\n",
      "22/232: observations.loc[0, 'counted'] = 2\n",
      "22/233: observations.head()\n",
      "22/234: observations.loc[0, 'counted'] = 2\n",
      "22/235: observations.head()\n",
      "22/236: observations.drop(0, 'counted')\n",
      "22/237: observations.columns\n",
      "22/238: observations.drop((0, 'counted'))\n",
      "22/239: observations.drop((0, 'counted'), axis=1)\n",
      "22/240: observations.head()\n",
      "22/241:\n",
      "for single_index in range(100):\n",
      "    observations.loc[single_index, 'counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)\n",
      "22/242: observations.head()\n",
      "22/243: observations.drop((0, 'counted'), axis=1)\n",
      "22/244: observations.columns\n",
      "22/245: observations.drop((0, 'counted'), axis=1, inplace=True)\n",
      "22/246: observations.head()\n",
      "22/247: observations.columns\n",
      "22/248: observations.head(100)\n",
      "22/249: observations.describe()\n",
      "22/250: observations.head(300)\n",
      "22/251:\n",
      "for single_index in observations.index:\n",
      "    observations.loc[single_index, 'counted'] = ufo_grouped_2.get(my_indexes[single_index], default=0)\n",
      "22/252: observations.head(300)\n",
      "22/253: observations.describe()\n",
      "22/254: states_years_months = ((state, year, month) for state in states for year in years for month in months)\n",
      "22/255: ex = dict(state='AK', year= 2000, month= 1)\n",
      "22/256: observations.append(ex)\n",
      "22/257: observations.append(pd.DataFrame(ex))\n",
      "19/31: df = pd.DateFrame(columns=['Year', 'Month'])\n",
      "19/32: df = pd.DataFrame(columns=['Year', 'Month'])\n",
      "19/33: d1 = dict(year=1990, month=10)\n",
      "19/34: df2 = pd.DataFrame(d1)\n",
      "19/35: l1=[d1]\n",
      "19/36: df2 = pd.DataFrame(l1)\n",
      "22/258: states_years_months = [(state, year, month) for state in states for year in years for month in months]\n",
      "22/259: states_years_months\n",
      "22/260:\n",
      "for element in states_years_months:\n",
      "    state, year, month = element\n",
      "    d1=dict(state=state, year = year, month = month)\n",
      "22/261: list_of_dics = []\n",
      "22/262:\n",
      "for element in states_years_months:\n",
      "    state, year, month = element\n",
      "    d1=dict(state=state, year = year, month = month)\n",
      "22/263:\n",
      "for element in states_years_months:\n",
      "    state, year, month = element\n",
      "    d1=dict(state=state, year = year, month = month)\n",
      "    list_of_dics.append(d1)\n",
      "22/264: list_of_dics\n",
      "22/265: df2 = pd.DataFrame(list_of_dics)\n",
      "22/266: df2\n",
      "22/267:\n",
      "for element in states_years_months:\n",
      "    state, year, month = element\n",
      "    d1=dict(State=state, Year = year, Month = month)\n",
      "    list_of_dics.append(d1)\n",
      "22/268: df2 = pd.DataFrame(list_of_dics)\n",
      "22/269: df2\n",
      "22/270: observations = pd.DataFrame(columns=['State', 'Year', 'Month'])\n",
      "22/271: states_years_months = [(state, year, month) for state in states for year in years for month in months]\n",
      "22/272: list_of_dics = []\n",
      "22/273:\n",
      "for element in states_years_months:\n",
      "    state, year, month = element\n",
      "    d1=dict(State=state, Year = year, Month = month)\n",
      "    list_of_dics.append(d1)\n",
      "22/274: df2 = pd.DataFrame(list_of_dics)\n",
      "22/275: df2\n",
      "22/276: states_years_months[0]\n",
      "22/277:\n",
      "for single_index in df2:\n",
      "    df2.loc[single_index, 'counted'] = ufo_grouped_2.get(states_years_months[single_index], default=0)\n",
      "22/278: df2['Observations']=0\n",
      "22/279:\n",
      "for single_index in df2:\n",
      "    df2.loc[single_index, 'Observations'] = ufo_grouped_2.get(states_years_months[single_index], default=0)\n",
      "22/280:\n",
      "for single_index in df2.index:\n",
      "    df2.loc[single_index, 'Observations'] = ufo_grouped_2.get(states_years_months[single_index], default=0)\n",
      "22/281: df2.head(50)\n",
      "22/282: df2.head(150)\n",
      "22/283: df2.describe()\n",
      "22/284: x=[1,2,3]\n",
      "22/285: y= x**2\n",
      "22/286: y= [1,4,9]\n",
      "22/287: plt.plot(x,y)\n",
      "22/288: x=['a', 'b','c']\n",
      "22/289: y= [1,4,9]\n",
      "22/290: plt.plot(x,y)\n",
      "22/291: plt.plot(df2.State, df2.Observations)\n",
      "22/292:\n",
      "plt.plot(df2.State, df2.Observations)\n",
      "plt.rcParams['figure.figsize']=(8,4)\n",
      "22/293:\n",
      "plt.plot(df2.State, df2.Observations)\n",
      "plt.rcParams['figure.figsize']=(12,6)\n",
      "22/294:\n",
      "plt.plot(df2.State, df2.Observations)\n",
      "plt.rcParams['figure.figsize']=(16,8)\n",
      "22/295:\n",
      "plt.plot(df2.State, df2.Observations)\n",
      "plt.rcParams['figure.figsize']=(32,16)\n",
      "22/296: z=[1,2,3]\n",
      "22/297: plt.plot(x,y,z)\n",
      "22/298: plt.plot((x,y),z)\n",
      "22/299: df2.YearMonth = 'x'\n",
      "22/300: df2.YearMonth = df2.Year.as_type(str)\n",
      "22/301: df2.YearMonth = df2.Year.astype(str)\n",
      "22/302: df2.head()\n",
      "22/303: df2.head(50)\n",
      "22/304: df2.YearMonth = df2.Year.astype(str)\n",
      "22/305: df2.YearMonth = 'x' #df2.Year.astype(str)\n",
      "22/306: df2.head(50)\n",
      "22/307: df2['YearMonth'] = 'x' #df2.Year.astype(str)\n",
      "22/308: df2.head(50)\n",
      "22/309: df2['YearMonth'] = df2.Year.astype(str)\n",
      "22/310: df2.head(50)\n",
      "22/311: df2['YearMonth'] = df2.Year.astype(str) + df2.Month.astype(str)\n",
      "22/312: df2.head(50)\n",
      "22/313: df2.head(20)\n",
      "22/314:\n",
      "for state in pd2.State:\n",
      "    plt.plot(pd2.YearMonth, pd2.Observations)\n",
      "22/315:\n",
      "for state in df2.State:\n",
      "    plt.plot(df2.YearMonth, df2.Observations)\n",
      "22/316: df2.head()\n",
      "22/317: akdf = df2[df2.State == 'AK']\n",
      "22/318: akdf.head()\n",
      "22/319: plt.plot(akdf.YearMonth, akdf.Observations)\n",
      "22/320:\n",
      "plt.plot(df2.State, df2.Observations)\n",
      "plt.rcParams['figure.figsize']=(32,16)\n",
      "plt.xlabel('YearMonth')\n",
      "22/321:\n",
      "plt.plot(df2.State, df2.Observations)\n",
      "plt.rcParams['figure.figsize']=(32,16)\n",
      "plt.xlabel('YearMonth')\n",
      "22/322:\n",
      "plt.plot(df2.State, df2.Observations)\n",
      "#plt.rcParams['figure.figsize']=(32,16)\n",
      "plt.xlabel('YearMonth')\n",
      "22/323:\n",
      "plt.plot(df2.State, df2.Observations)\n",
      "#plt.rcParams['figure.figsize']=(32,16)\n",
      "plt.ylabel('YearMonth')\n",
      "22/324:\n",
      "plt.plot(df2.State, df2.Observations)\n",
      "#plt.rcParams['figure.figsize']=(32,16)\n",
      "plt.xlabel('YearMonth')\n",
      "22/325:\n",
      "plt.plot(df2.State, df2.Observations)\n",
      "plt.rcParams['figure.figsize']=(16,8)\n",
      "plt.xlabel('YearMonth')\n",
      "22/326:\n",
      "plt.plot(df2.State, df2.Observations)\n",
      "plt.rcParams['figure.figsize']=(16,8)\n",
      "plt.xlabel('YearMonth')\n",
      "plt.ylabel('Ufo observations')\n",
      "22/327:\n",
      "plt.plot(akdf.YearMonth, akdf.Observations)\n",
      "plt.rcParams['figure.figsize']=(16,8)\n",
      "plt.xlabel('YearMonth')\n",
      "plt.ylabel('Ufo observations')\n",
      "22/328:\n",
      "plt.plot(akdf.YearMonth, akdf.Observations)\n",
      "plt.rcParams['figure.figsize']=(16,8)\n",
      "plt.xlabel('YearMonth')\n",
      "plt.ylabel('Ufo observations')\n",
      "plt.grid(True)\n",
      "22/329: akdf.head(20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/330: akdf.describe()\n",
      "22/331:\n",
      "all_dfs=[]\n",
      "for state in states_abbreviations:\n",
      "    df = df2[df2.State==state]\n",
      "    all_dfs.append(df)\n",
      "22/332: all_dfs\n",
      "22/333: all_dfs[0]\n",
      "22/334: all_dfs[49]\n",
      "22/335: len(all_dfs)\n",
      "22/336: len(states_abbreviations)\n",
      "22/337: len(states_abbreviations)\n",
      "22/338: len(states_abbreviations)\n",
      "22/339: all_dfs_2 = [df2.State==state for state in states_abbreviations]\n",
      "22/340: all_dfs_2[0]\n",
      "22/341: all_dfs_2 = [df2[df2.State==state] for state in states_abbreviations]\n",
      "22/342: all_dfs_2[0]\n",
      "22/343: all_dfs = [df2[df2.State==state] for state in states_abbreviations]\n",
      "22/344:\n",
      "for df in all_dfs:\n",
      "    plt.plot(df.YearMonth, df.Observations)\n",
      "22/345:\n",
      "for df in all_dfs:\n",
      "    plt.figure()\n",
      "    plt.plot(df.YearMonth, df.Observations)\n",
      "23/1:\n",
      "for df in all_dfs[:2]:\n",
      "    plt.figure()\n",
      "    plt.plot(df.YearMonth, df.Observations)\n",
      "23/2: import pandas as pd\n",
      "23/3: import pandas as pd\n",
      "23/4: ufo_path='/users/wioletanytko/documents/workspace/r/umapro/01-Introduction/data/ufo/ufo_awesome.tsv'\n",
      "23/5:\n",
      "ufo_df = pd.read_csv(\n",
      "    ufo_path, delimiter='\\t',\n",
      "    header=None,\n",
      "    names=['Date_occured', 'Date_reported','Location','Short_description','Duration','Long_description'],\n",
      "    error_bad_lines=False)\n",
      "23/6: ufo_df.head()\n",
      "23/7: ufo_df.dtypes\n",
      "23/8: bad_date = ufo_df.loc[ufo_df['Date_occured']>19951009]\n",
      "23/9: bad_date.head()\n",
      "23/10: ufo_df['Date_occured_len']=ufo_df['Date_occured'].astype(str).str.len()\n",
      "23/11: ufo_df['Date_reported_len']=ufo_df['Date_reported'].astype(str).str.len()\n",
      "23/12: ufo_df.head()\n",
      "23/13: bad_date_df = ufo_df.loc[(ufo_df['Date_occured_len'] != 8) | (ufo_df['Date_reported_len'] != 8)]\n",
      "23/14: bad_date_df.sort_values(by=['Date_occured_len'])\n",
      "23/15: bad_date_df.info()\n",
      "23/16: ufo_df[ufo_df['Date_occured_len'] != 8].count()['Date_occured_len']\n",
      "23/17: ufo_df[ufo_df['Date_reported_len'] != 8].count()['Date_reported_len']\n",
      "23/18: ufo_df.drop(ufo_df[ufo_df.Date_occured_len !=8].index, inplace=True)\n",
      "23/19: pd.to_datetime(ufo_df.loc[0]['Date_occured'],format='%Y%m%d')\n",
      "23/20: ufo_df['zerodate'] = ufo_df['Date_occured'].astype(str).str.match(r'....00')\n",
      "23/21: ufo_df.loc[ufo_df['zerodate'] == True]\n",
      "23/22: ufo_df.drop(ufo_df.loc[ufo_df['zerodate'] == True].index, inplace=True)\n",
      "23/23: ufo_df.sort_values(by=['Date_occured']).head()\n",
      "23/24: ufo_df.drop(40901, inplace=True)\n",
      "23/25: ufo_df['Date_occured']=pd.to_datetime(ufo_df['Date_occured'], format='%Y%m%d')\n",
      "23/26: ufo_df['Date_reported']=pd.to_datetime(ufo_df['Date_reported'], format='%Y%m%d')\n",
      "23/27: ufo_df.head()\n",
      "23/28: mask = ufo_df.Date_occured>('1999-01-01')\n",
      "23/29: since_1999 = ufo_df[mask]\n",
      "23/30: since_1999.head()\n",
      "23/31: ufo_df.describe(include=['datetime64'])\n",
      "23/32: ufo_df.info()\n",
      "23/33: new=ufo_df['Location'].str.split(',', n=1)\n",
      "23/34: len(new)\n",
      "23/35: newest = new.str.len()\n",
      "23/36: new.loc[0][0],new.loc[0][1]\n",
      "23/37: new.head()\n",
      "23/38: ufo_df['City'] = new.str[0]\n",
      "23/39: ufo_df['State'] = new.str[1]\n",
      "23/40: ufo_df.head()\n",
      "23/41: ufo_df.drop(['Date_occured_len', 'Date_reported_len', 'zerodate'], axis=1, inplace=True)\n",
      "23/42: ufo_df.State = ufo_df.State.str.strip()\n",
      "23/43: ufo_df.dtypes\n",
      "23/44: states_abbreviations = [ 'AL', 'AK', 'AS', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FM', 'FL', 'GA', 'GU', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MH', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'MP', 'OH', 'OK', 'OR', 'PW', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VI', 'VA', 'WA', 'WV', 'WI', 'WY' ]\n",
      "23/45: states_mask = ufo_df.State.isin(states_abbreviations)\n",
      "23/46: states_mask.head()\n",
      "23/47: ufo_us = ufo_df[states_mask]\n",
      "23/48: ufo_us.head()\n",
      "23/49: ufo_us.Date_occured.quantile([.25, .5, .75])\n",
      "23/50: ufo_us.Date_occured.max()\n",
      "23/51: ufo_us.Date_occured.min()\n",
      "23/52: time_mask = (ufo_us.Date_occured > '1990-01-01')\n",
      "23/53: ufo_us = ufo_us[time_mask]\n",
      "23/54: import matplotlib.pyplot as plt\n",
      "23/55: %matplotlib inline\n",
      "23/56:\n",
      "histogram = ufo_us['Date_occured'].hist(bins=20, label=['year', 'number'])\n",
      "plt.title('Ufo observation')\n",
      "plt.xlabel('Year')\n",
      "plt.ylabel('Observations')\n",
      "23/57: ufo_us['Year'] = ufo_us['Date_occured'].dt.year\n",
      "23/58: ufo_us['Month'] = ufo_us['Date_occured'].dt.month\n",
      "23/59: ufo_us.head()\n",
      "23/60:\n",
      "ufo_grouped_2 = ufo_us.groupby([\n",
      "    ufo_us.State,\n",
      "    ufo_us.Date_occured.dt.year,\n",
      "    ufo_us.Date_occured.dt.month,\n",
      "    ])['Date_occured'].count()\n",
      "23/61: states, years, months = ufo_grouped_2.index.levels\n",
      "23/62: ufo_grouped_2.index[0]\n",
      "23/63: observations = pd.DataFrame(columns=['State', 'Year', 'Month'])\n",
      "23/64: states_years_months = [(state, year, month) for state in states for year in years for month in months]\n",
      "23/65: list_of_dics = []\n",
      "23/66:\n",
      "for element in states_years_months:\n",
      "    state, year, month = element\n",
      "    d1=dict(State=state, Year = year, Month = month)\n",
      "    list_of_dics.append(d1)\n",
      "23/67: df2 = pd.DataFrame(list_of_dics)\n",
      "23/68: df2['Observations']=0\n",
      "23/69: states_years_months[0]\n",
      "23/70: my_indexes = [(observations.loc[x]['State'], observations.loc[x]['Year'], observations.loc[x]['Month']) for x in observations.index]\n",
      "23/71:\n",
      "for single_index in df2.index:\n",
      "    df2.loc[single_index, 'Observations'] = ufo_grouped_2.get(states_years_months[single_index], default=0)\n",
      "23/72: df2.head(150)\n",
      "23/73: df2.describe()\n",
      "23/74: x=['a', 'b','c']\n",
      "23/75: y= [1,4,9]\n",
      "23/76: z=[1,2,3]\n",
      "23/77: df2['YearMonth'] = df2.Year.astype(str) + df2.Month.astype(str)\n",
      "23/78: df2.head()\n",
      "23/79: akdf = df2[df2.State == 'AK']\n",
      "23/80: akdf.describe()\n",
      "23/81:\n",
      "plt.plot(akdf.YearMonth, akdf.Observations)\n",
      "plt.rcParams['figure.figsize']=(16,8)\n",
      "plt.xlabel('YearMonth')\n",
      "plt.ylabel('Ufo observations')\n",
      "plt.grid(True)\n",
      "23/82: all_dfs = [df2[df2.State==state] for state in states_abbreviations]\n",
      "23/83:\n",
      "for df in all_dfs[:2]:\n",
      "    plt.figure()\n",
      "    plt.plot(df.YearMonth, df.Observations)\n",
      "23/84:\n",
      "for df in all_dfs[:4]:\n",
      "    plt.figure()\n",
      "    plt.plot(df.YearMonth, df.Observations)\n",
      "23/85:\n",
      "plt.figure\n",
      "for df in all_dfs[:4]:\n",
      "    plt.plot(df.YearMonth, df.Observations)\n",
      "23/86:\n",
      "plt.figure\n",
      "for df in all_dfs[:4]:\n",
      "    plt.subplot(2,2)\n",
      "    plt.plot(df.YearMonth, df.Observations)\n",
      "23/87:\n",
      "plt.figure\n",
      "plt.subplot(2,2)\n",
      "for df in all_dfs[:4]:\n",
      "    plt.plot(df.YearMonth, df.Observations)\n",
      "23/88:\n",
      "plt.figure\n",
      "plt.subplots(2,2)\n",
      "for df in all_dfs[:4]:\n",
      "    plt.plot(df.YearMonth, df.Observations)\n",
      "23/89:\n",
      "plt.figure\n",
      "plt.subplots(4)\n",
      "for df in all_dfs[:4]:\n",
      "    plt.plot(df.YearMonth, df.Observations)\n",
      "23/90:\n",
      "plt.figure\n",
      "plt.subplots(4)\n",
      "for x in range(4):\n",
      "    plt.plot(all_dfs[x].YearMonth, all_dfs[x].Observations)\n",
      "23/91:\n",
      "plt.figure\n",
      "fig, axes = plt.subplots(4)\n",
      "for x in range(4):\n",
      "    axes[x].plot(all_dfs[x].YearMonth, all_dfs[x].Observations)\n",
      "23/92:\n",
      "plt.figure\n",
      "fig, axes = plt.subplots(10)\n",
      "for x in range(10):\n",
      "    axes[x].plot(all_dfs[x].YearMonth, all_dfs[x].Observations)\n",
      "24/1: import pandas as pd\n",
      "24/2: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'\n",
      "24/3:\n",
      "hwg_df = pd.read_csv(\n",
      "    heights_weights_gender_data)\n",
      "24/4: hwg_df.head()\n",
      "24/5: hwg_df.describe()\n",
      "24/6: hwg_df.Height =hwg_df.Height * 2.54\n",
      "24/7: hwg_df.describe()\n",
      "24/8: hwg_df.Weight = hwg_df.Weight * 0.454\n",
      "24/9: hwg_df.describe()\n",
      "26/1: hwg_df.guantile(q=range(0,100,10))\n",
      "26/2: import pandas as pd\n",
      "26/3: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'\n",
      "26/4:\n",
      "hwg_df = pd.read_csv(\n",
      "    heights_weights_gender_data)\n",
      "26/5: hwg_df.head()\n",
      "26/6: hwg_df.describe()\n",
      "26/7: hwg_df.Height =hwg_df.Height * 2.54\n",
      "26/8: hwg_df.Weight = hwg_df.Weight * 0.454\n",
      "26/9: hwg_df.describe()\n",
      "26/10: hwg_df.guantile(q=range(0,100,10))\n",
      "26/11: hwg_df.quantile(q=range(0,100,10))\n",
      "26/12: hwg_df.quantile(q=range(0,1,0.01))\n",
      "26/13: hwg_df.quantile(q=0.1)\n",
      "26/14: hwg_df.quantile(q=[0.1, 0.2])\n",
      "26/15: hwg_df.quantile(q=range(0,1,0.1))\n",
      "26/16: hwg_df.quantile(q=[x*0.1 for x in range(0,10)])\n",
      "26/17: help(quantile)\n",
      "26/18: help(str)\n",
      "26/19: help(pd.quantile)\n",
      "26/20: help(pd.DataFrame.quantile)\n",
      "26/21: pd.__dic__\n",
      "26/22: pd.__dic__()\n",
      "26/23: dic(pd)\n",
      "26/24: pd.__dict__\n",
      "26/25: DataFrame in pd.__dict__\n",
      "26/26: 'DataFrame' in pd.__dict__\n",
      "26/27: pd.__dict__('DataFrame')\n",
      "26/28: pd.__dict__['DataFrame']\n",
      "26/29: pd.__dict__['DataFrame'].__dict__\n",
      "26/30: dir(pd.DataFrame)\n",
      "26/31: hwg_df.quantile(q=[x*0.2 for x in range(0,11)])\n",
      "26/32: import pandas as pd\n",
      "26/33: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'\n",
      "26/34:\n",
      "hwg_df = pd.read_csv(\n",
      "    heights_weights_gender_data)\n",
      "26/35: hwg_df.head()\n",
      "26/36: hwg_df.Height =hwg_df.Height * 2.54\n",
      "26/37: hwg_df.Weight = hwg_df.Weight * 0.454\n",
      "26/38: hwg_df.describe()\n",
      "26/39: hwg_df.quantile(q=[x*0.2 for x in range(0,11)])\n",
      "26/40: 'DataFrame' in pd.__dict__\n",
      "26/41: hwg_df.describe()\n",
      "26/42: hwg_df.quantile(q=[x*0.2 for x in range(0,11)])\n",
      "26/43: hwg_df.quantile(q=[x*0.2 for x in range(0,6)])\n",
      "26/44: hwg_df.quantile(q=0.25), hwg_df.quantile(q=0.75)\n",
      "26/45: hwg_df.Height.quantile(q=0.25), hwg_df.Height.quantile(q=0.75)\n",
      "26/46: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)\n",
      "26/47: help(avg)\n",
      "26/48:\n",
      "def my_var(x):\n",
      "    m = sum(x)/len(x)\n",
      "    return sum((x-m)**2)/len(x)\n",
      "26/49: my_var([1,2,3,4,5])\n",
      "26/50:\n",
      "def my_var(x):\n",
      "    m = sum(x)/len(x)\n",
      "    return sum([(element-m)**2 for element in x])/len(x)\n",
      "26/51: my_var([1,2,3,4,5])\n",
      "26/52:\n",
      "def my_var(x):\n",
      "    m = sum(x)/len(x)\n",
      "    return sum([(element-m)**2 for element in x])/(len(x)-1)\n",
      "26/53: my_var([1,2,3,4,5])\n",
      "26/54: my_var(hwg_df.Height)\n",
      "26/55: hwg_df.Height.mean() -hwg_df.Height.variance()\n",
      "26/56: hwg_df.Height.mean() -hwg_df.Height.var()\n",
      "26/57: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() -hwg_df.Height.var()\n",
      "26/58: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()\n",
      "26/59:\n",
      "def my_sd(x):\n",
      "    return sqrt(my_var(x))\n",
      "26/60: my_std(hwg_df.Height)\n",
      "26/61: my_sd(hwg_df.Height)\n",
      "26/62:\n",
      "def my_sd(x):\n",
      "    return my_var(x)**(0.5)\n",
      "26/63: my_sd(hwg_df.Height)\n",
      "26/64: my_sd(hwg_df.Height) - hwg_df.Height.std()\n",
      "26/65: df = hwg_df\n",
      "26/66: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std\n",
      "26/67: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()\n",
      "26/68: from scipy import stats\n",
      "26/69: stats.percentileofscore(df.Height, 158.8)\n",
      "26/70: stats.percentileofscore(df.Height, 178.34)\n",
      "26/71: 82.5-17.31\n",
      "26/72: histogram = df.Height.hist(bins=100)\n",
      "26/73: import pandas as pd\n",
      "26/74: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'\n",
      "26/75:\n",
      "hwg_df = pd.read_csv(\n",
      "    heights_weights_gender_data)\n",
      "26/76: hwg_df.head()\n",
      "26/77: hwg_df.Height =hwg_df.Height * 2.54\n",
      "26/78: hwg_df.Weight = hwg_df.Weight * 0.454\n",
      "26/79: hwg_df.describe()\n",
      "26/80: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)\n",
      "26/81:\n",
      "def my_var(x):\n",
      "    m = sum(x)/len(x)\n",
      "    return sum([(element-m)**2 for element in x])/(len(x)-1)\n",
      "26/82:\n",
      "def my_sd(x):\n",
      "    return my_var(x)**(0.5)\n",
      "26/83: my_var([1,2,3,4,5])\n",
      "26/84: my_var(hwg_df.Height)\n",
      "26/85: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()\n",
      "26/86: my_sd(hwg_df.Height) - hwg_df.Height.std()\n",
      "26/87: df = hwg_df\n",
      "26/88: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()\n",
      "26/89: from scipy import stats\n",
      "26/90: stats.percentileofscore(df.Height, 158.8)\n",
      "26/91: stats.percentileofscore(df.Height, 178.34)\n",
      "26/92: 82.5-17.31\n",
      "26/93: histogram = df.Height.hist(bins=100)\n",
      "26/94: df.describe()\n",
      "26/95: histogram = df.Height.hist(bins=64)\n",
      "26/96:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "plt.tilte('Height')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Number')\n",
      "26/97:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "plt.title('Height')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Number')\n",
      "26/98:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "26/99:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "plt.title('Height')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Number')\n",
      "26/100:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "plt.title('Height distribution')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Number')\n",
      "26/101:\n",
      "histogram = df.Height.hist(bins=64, density=True)\n",
      "plt.title('Height distribution')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Number')\n",
      "26/102:\n",
      "histogram = df.Height.hist(bins=13, density=True)\n",
      "plt.title('Height distribution')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Number')\n",
      "26/103:\n",
      "histogram = df.Height.hist(bins=1000, density=True)\n",
      "plt.title('Height distribution')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Number')\n",
      "26/104: df.Height.plot.kde()\n",
      "26/105:\n",
      "df.Height.plot.kde()\n",
      "plt.xlabel('Height')\n",
      "26/106:\n",
      "df.Height.plot.kde()\n",
      "plt.xlabel('Height')\n",
      "26/107:\n",
      "histogram = df.Height.hist(bins=1000, density=True)\n",
      "plt.title('Height distribution')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Number')\n",
      "26/108:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.xlabel('Height')\n",
      "26/109: kde = df.Height.plot.kde()\n",
      "26/110:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.xlabel='Height'\n",
      "26/111:\n",
      "histogram = df.Height.hist(bins=1000, density=True)\n",
      "plt.title('Height distribution')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Number')\n",
      "26/112:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.xlabel='Height'\n",
      "26/113:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.xlabel = 'Height'\n",
      "26/114:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.ylabel = 'Height'\n",
      "26/115:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.xlabel = 'Height'\n",
      "26/116:\n",
      "histogram = df.Height.hist(bins=1000, density=True)\n",
      "plt.title('Height distribution')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Number')\n",
      "26/117:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "26/118:\n",
      "histogram = df.Height.hist(bins=64, density=True)\n",
      "plt.title('Height distribution')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Number')\n",
      "26/119:\n",
      "histogram = df.Height.hist(bins=64, density=True)\n",
      "plt.title('Height distribution')\n",
      "26/120:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "plt.title('Height distribution')\n",
      "26/121:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.xlabel = 'Height'\n",
      "26/122:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "plt.title('Height distribution')\n",
      "plt.xlabel('Height')\n",
      "26/123:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "26/124:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "26/125:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "plt.title('Height distribution')\n",
      "plt.xlabel('Height')\n",
      "26/126:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "plt.title('Height distribution')\n",
      "plt.xlabel('x')\n",
      "26/127:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "plt.title('Height distribution')\n",
      "plt.ylabel('x')\n",
      "26/128:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "plt.ylabel('x')\n",
      "26/129: histogram = df.Height.hist(bins=64)\n",
      "26/130:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "histogram.xlabel = ('x')\n",
      "26/131:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "plt.xlabel = ('x')\n",
      "26/132:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "plt.xlabel = ('Height')\n",
      "26/133:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.xlabel = 'Height'\n",
      "26/134:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.xlabel('Height')\n",
      "26/135:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "plt.xlabel = ('Height')\n",
      "26/136:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "plt.xlabel('Height')\n",
      "26/137:\n",
      "histogram = df.Height.hist(bins=64)\n",
      "plt.title('Height')\n",
      "26/138:\n",
      "histogram = df.Height.hist(bins=64, label=['Height', 'Number'])\n",
      "plt.title('Height')\n",
      "26/139: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])\n",
      "26/140: kde = df.Height.plot.kde(label=['Height', 'Probability'])\n",
      "26/141: kde = df.Height.plot.kde()\n",
      "26/142:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.title('kde')\n",
      "26/143:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.title('kde!!!')\n",
      "26/144:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.title('kde!!!')\n",
      "plt.xlabel('x')\n",
      "28/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "28/2: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'\n",
      "28/3:\n",
      "hwg_df = pd.read_csv(\n",
      "    heights_weights_gender_data)\n",
      "28/4: hwg_df.head()\n",
      "28/5: hwg_df.Height =hwg_df.Height * 2.54\n",
      "28/6: hwg_df.Weight = hwg_df.Weight * 0.454\n",
      "28/7: hwg_df.describe()\n",
      "28/8: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)\n",
      "28/9:\n",
      "def my_var(x):\n",
      "    m = sum(x)/len(x)\n",
      "    return sum([(element-m)**2 for element in x])/(len(x)-1)\n",
      "28/10:\n",
      "def my_sd(x):\n",
      "    return my_var(x)**(0.5)\n",
      "28/11: my_var([1,2,3,4,5])\n",
      "28/12: my_var(hwg_df.Height)\n",
      "28/13: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()\n",
      "28/14: my_sd(hwg_df.Height) - hwg_df.Height.std()\n",
      "28/15: df = hwg_df\n",
      "28/16: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()\n",
      "28/17: from scipy import stats\n",
      "28/18: stats.percentileofscore(df.Height, 158.8)\n",
      "28/19: stats.percentileofscore(df.Height, 178.34)\n",
      "28/20: 82.5-17.31\n",
      "28/21: df.describe()\n",
      "28/22: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])\n",
      "28/23:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.title('kde!!!')\n",
      "plt.xlabel('x')\n",
      "28/24: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])\n",
      "28/25:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.title('kde!!!')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Probability')\n",
      "28/26:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.title('KDE)\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Probability')\n",
      "28/27:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.title('KDE')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Probability')\n",
      "28/28: df.head()\n",
      "28/29: df.Height[df.Gender='Male']\n",
      "28/30: df.Height[df.Gender=='Male']\n",
      "28/31: df.Height[df.Gender=='Male'].plot.kde()\n",
      "28/32:\n",
      "df.Height[df.Gender=='Male'].plot.kde()\n",
      "df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/1:\n",
      "f1, male = df.Height[df.Gender=='Male'].plot.kde()\n",
      "df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "29/3: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'\n",
      "29/4:\n",
      "hwg_df = pd.read_csv(\n",
      "    heights_weights_gender_data)\n",
      "29/5: hwg_df.head()\n",
      "29/6: hwg_df.Height =hwg_df.Height * 2.54\n",
      "29/7: hwg_df.Weight = hwg_df.Weight * 0.454\n",
      "29/8: hwg_df.describe()\n",
      "29/9: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)\n",
      "29/10:\n",
      "def my_var(x):\n",
      "    m = sum(x)/len(x)\n",
      "    return sum([(element-m)**2 for element in x])/(len(x)-1)\n",
      "29/11:\n",
      "def my_sd(x):\n",
      "    return my_var(x)**(0.5)\n",
      "29/12: my_var([1,2,3,4,5])\n",
      "29/13: my_var(hwg_df.Height)\n",
      "29/14: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()\n",
      "29/15: my_sd(hwg_df.Height) - hwg_df.Height.std()\n",
      "29/16: df = hwg_df\n",
      "29/17: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()\n",
      "29/18: from scipy import stats\n",
      "29/19: stats.percentileofscore(df.Height, 158.8)\n",
      "29/20: stats.percentileofscore(df.Height, 178.34)\n",
      "29/21: 82.5-17.31\n",
      "29/22: df.describe()\n",
      "29/23: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])\n",
      "29/24:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.title('KDE')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Probability')\n",
      "29/25: df.head()\n",
      "29/26:\n",
      "f1, male = df.Height[df.Gender=='Male'].plot.kde()\n",
      "df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/27:\n",
      "f1 = df.Height[df.Gender=='Male'].plot.kde()\n",
      "df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/28:\n",
      "f1 = df.Height[df.Gender=='Male'].plot.kde()\n",
      "f1.set_title('male')\n",
      "df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/29:\n",
      "f1 = df.Height[df.Gender=='Male', df.Gender=='Female'].plot.kde()\n",
      "f1.set_title('male')\n",
      "#df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/30:\n",
      "f1 = df.Height[df.Gender=='Male'].plot.kde()\n",
      "f1.set_title('male')\n",
      "#df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/31:\n",
      "f1 = df.Height[df.Gender=='Male'].plot.kde()\n",
      "plt.legend(f1, ('Male'))\n",
      "#df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/32:\n",
      "f1 = df.Height[df.Gender=='Male'].plot.kde()\n",
      "plt.legend(f1, ('Male',))\n",
      "#df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/33:\n",
      "f1 = df.Height[df.Gender=='Male'].plot.kde()\n",
      "plt.legend(f1, ['Male'])\n",
      "#df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/34:\n",
      "f1 = df.Height[df.Gender=='Male'].plot.kde(label='Male')\n",
      "#plt.legend(f1, ['Male'])\n",
      "#df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/35:\n",
      "f1 = df.Height[df.Gender=='Male'].plot.kde(label='Male!!!')\n",
      "#plt.legend(f1, ['Male'])\n",
      "#df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/36:\n",
      "f1 = df.Height[df.Gender=='Male'].plot.kde(label='Male!!!')\n",
      "df.legend()\n",
      "#plt.legend(f1, ['Male'])\n",
      "#df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/37:\n",
      "f1, = df.Height[df.Gender=='Male'].plot.kde(label='Male!!!')\n",
      "df.legend()\n",
      "#plt.legend(f1, ['Male'])\n",
      "#df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/38:\n",
      "f1, = df.Height[df.Gender=='Male'].plot.kde()\n",
      "f1.set_label('Male!!!')\n",
      "#plt.legend(f1, ['Male'])\n",
      "#df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/39:\n",
      "f1, = df.Height[df.Gender=='Male'].plot.kde()\n",
      "f1.set_label('Male!!!')\n",
      "plt.legend()\n",
      "#plt.legend(f1, ['Male'])\n",
      "#df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/40:\n",
      "f1, = df.Height[df.Gender=='Male'].plot.kde()\n",
      "#df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/41:\n",
      "f1 = df.Height[df.Gender=='Male'].plot.kde(label='male!')\n",
      "#df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/42:\n",
      "f1 = df.Height[df.Gender=='Male'].plot.kde(label='male!')\n",
      "plt.legend()\n",
      "#df.Height[df.Gender=='Female'].plot.kde()\n",
      "29/43:\n",
      "f1 = df.Height[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Height[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.legend()\n",
      "29/44:\n",
      "df.Height[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Height[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.legend()\n",
      "29/45:\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.legend()\n",
      "29/46:\n",
      "df.Height[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Height[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Height')\n",
      "29/47:\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Weight')\n",
      "29/48:\n",
      "plt.figure\n",
      "fig, axes = plt.subplots(2)\n",
      "axes[1].df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "29/49:\n",
      "plt.figure\n",
      "fig, axes = plt.subplots(2)\n",
      "axes[1].plot(Weight[df.Gender=='Male'].plot.kde(label='Male'))\n",
      "29/50:\n",
      "plt.figure\n",
      "fig, axes = plt.subplots(2)\n",
      "axes[1].plot(df.Weight[df.Gender=='Male'].plot.kde(label='Male'))\n",
      "29/51:\n",
      "plt.figure\n",
      "fig, axes = plt.subplots(2)\n",
      "axes[1].plot(df.Weight[df.Gender=='Male'].kde(label='Male'))\n",
      "29/52:\n",
      "plt.figure\n",
      "fig, axes = plt.subplots(2)\n",
      "axes[1].plot(df.Weight[df.Gender=='Male'].plot.kde(label='Male'))\n",
      "29/53:\n",
      "plt.figure\n",
      "fig, axes = plt.subplots(2)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "29/54:\n",
      "plt.figure\n",
      "fig, axes = plt.subplots(2)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/55:\n",
      "plt.figure\n",
      "fig, axes = plt.subplot()\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/56:\n",
      "plt.figure\n",
      "plt.subplot()\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/57:\n",
      "plt.figure\n",
      "plt.subplot()\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.subplot()\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/58:\n",
      "plt.figure\n",
      "plt.subplot()\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.subplot()\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/59:\n",
      "plt.figure\n",
      "plt.subplot(1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.subplot(1)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/60:\n",
      "plt.figure\n",
      "plt.subplot(1,1,0)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.subplot(1,1,0)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/61:\n",
      "plt.figure\n",
      "plt.subplot(1,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.subplot(1,1,1)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/62:\n",
      "plt.figure\n",
      "plt.subplot(1,1,1)\n",
      "plt.subplot(1,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/63:\n",
      "plt.figure\n",
      "plt.subplot()\n",
      "plt.subplot()\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/64:\n",
      "plt.figure\n",
      "plt.subplot()\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.subplot()\n",
      "29/65:\n",
      "plt.figure\n",
      "plt.subplot(2,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.subplot(2,1,2)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/66:\n",
      "plt.figure\n",
      "plt.subplot(2,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.subplot(2,1,2)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/67:\n",
      "plt.figure\n",
      "plt.subplot(2,1,1, sharex=True)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.subplot(2,1,2, sharex=True)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/68:\n",
      "plt.figure\n",
      "ax1 = plt.subplot(2,1,1, sharex=True)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.subplot(2,1,2)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/69:\n",
      "plt.figure\n",
      "ax1 = plt.subplot(2,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.subplot(2,1,2)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/70:\n",
      "plt.figure\n",
      "ax1 = plt.subplot(2,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "ax2 = plt.subplot(2,1,2, sharex = ax1)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "29/71:\n",
      "plt.figure\n",
      "ax1 = plt.subplot(2,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "ax2 = plt.subplot(2,1,2, sharex = ax1)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.grid(True)\n",
      "29/72:\n",
      "plt.figure\n",
      "ax1 = plt.subplot(2,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.grid(True)\n",
      "ax2 = plt.subplot(2,1,2, sharex = ax1)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.grid(True)\n",
      "29/73:\n",
      "plt.figure\n",
      "ax1 = plt.subplot(2,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.grid(True)\n",
      "ax2 = plt.subplot(2,1,2, sharex = ax1)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.grid(True)\n",
      "plt.xlabel('Weight')\n",
      "29/74:\n",
      "plt.figure\n",
      "ax1 = plt.subplot(2,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.grid(True)\n",
      "plt.legend\n",
      "ax2 = plt.subplot(2,1,2, sharex = ax1)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.grid(True)\n",
      "plt.xlabel('Weight')\n",
      "plt.legend()\n",
      "29/75:\n",
      "plt.figure\n",
      "ax1 = plt.subplot(2,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.grid(True)\n",
      "plt.legend()\n",
      "ax2 = plt.subplot(2,1,2, sharex = ax1)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.grid(True)\n",
      "plt.xlabel('Weight')\n",
      "plt.legend()\n",
      "29/76:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as stats\n",
      "%matplotlib inline\n",
      "29/77:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as stats\n",
      "import numpy as np\n",
      "%matplotlib inline\n",
      "29/78:\n",
      "mu = 0\n",
      "variance = 5\n",
      "sigma = variance**0.5\n",
      "x = np.linspace(mu-3*sigma, mu+3*sigma, 100)\n",
      "plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
      "29/79:\n",
      "mu = 0\n",
      "variance = 1\n",
      "sigma = variance**0.5\n",
      "x = np.linspace(mu-3*sigma, mu+3*sigma, 100)\n",
      "plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
      "29/80:\n",
      "mu = 0\n",
      "variance = 10\n",
      "sigma = variance**0.5\n",
      "x = np.linspace(mu-3*sigma, mu+3*sigma, 100)\n",
      "plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
      "29/81: sigma\n",
      "29/82:\n",
      "mu = 0\n",
      "variance = 1\n",
      "sigma = variance**0.5\n",
      "x = np.linspace(mu-3*sigma, mu+3*sigma, 100)\n",
      "plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
      "29/83: sigma\n",
      "29/84:\n",
      "mu = 0\n",
      "variance = 5\n",
      "sigma = variance**0.5\n",
      "x = np.linspace(mu-3*sigma, mu+3*sigma, 100)\n",
      "plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
      "29/85: sigma\n",
      "29/86:\n",
      "mu = 10\n",
      "variance = 5\n",
      "sigma = variance**0.5\n",
      "x = np.linspace(mu-3*sigma, mu+3*sigma, 100)\n",
      "plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
      "29/87: sigma\n",
      "29/88:\n",
      "mu = 10\n",
      "variance = 5\n",
      "sigma = variance**0.5\n",
      "x = np.linspace(mu-3*sigma, mu+3*sigma, 100)\n",
      "plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
      "plt.plot(x, stats.cauchy.pdf(x, mu, sigma))\n",
      "29/89:\n",
      "mu = 10\n",
      "variance = 5\n",
      "sigma = variance**0.5\n",
      "x = np.linspace(mu-3*sigma, mu+3*sigma, 100)\n",
      "plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Gauss')\n",
      "plt.plot(x, stats.cauchy.pdf(x, mu, sigma))\n",
      "29/90:\n",
      "mu = 10\n",
      "variance = 5\n",
      "sigma = variance**0.5\n",
      "x = np.linspace(mu-3*sigma, mu+3*sigma, 100)\n",
      "plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Gauss')\n",
      "plt.plot(x, stats.cauchy.pdf(x, mu, sigma), label='Cauchy')\n",
      "plt.legend()\n",
      "29/91: plt.scatter(df.Height, df.Weight)\n",
      "29/92:\n",
      "plt.figure(figsize=(10,10))\n",
      "ax1 = plt.subplot(2,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.grid(True)\n",
      "plt.legend()\n",
      "ax2 = plt.subplot(2,1,2, sharex = ax1)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.grid(True)\n",
      "plt.xlabel('Weight')\n",
      "plt.legend()\n",
      "29/93:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "plt.geom_smooth()\n",
      "29/94:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "plt.plot(regression_line)\n",
      "29/95:\n",
      "def f1(x,a,b):\n",
      "    return a*x + b\n",
      "\n",
      "a,b = scipy.optimize.curve_fit(f1, df.Height, df.Weight )\n",
      "29/96:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as stats\n",
      "from scipy.optimize import curve_fit\n",
      "import numpy as np\n",
      "%matplotlib inline\n",
      "29/97:\n",
      "def f1(x,a,b):\n",
      "    return a*x + b\n",
      "\n",
      "a,b = curve_fit(f1, df.Height, df.Weight )\n",
      "29/98: plt.scatter(df.Height, df.Weight)\n",
      "29/99:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "def f1(x,a,b):\n",
      "    return a*x + b\n",
      "\n",
      "popt,pcov = curve_fit(f1, df.Height, df.Weight )\n",
      "29/100:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "def f1(x,a,b):\n",
      "    return a*x + b\n",
      "\n",
      "popt,pcov = curve_fit(f1, df.Height, df.Weight )\n",
      "plt.plot(df.Height, f1(df.Height, *popt))\n",
      "29/101:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "def f1(x,a,b):\n",
      "    return a*x + b\n",
      "\n",
      "popt,pcov = curve_fit(f1, df.Height, df.Weight )\n",
      "plt.plot(df.Height, f1(df.Height, *popt), color = 'red')\n",
      "29/102:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "def f1(x,a,b):\n",
      "    return a*x**2 + c*x + b\n",
      "\n",
      "popt,pcov = curve_fit(f1, df.Height, df.Weight )\n",
      "plt.plot(df.Height, f1(df.Height, *popt), color = 'red')\n",
      "29/103:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "def f1(x,a,c,b):\n",
      "    return a*x**2 + c*x + b\n",
      "\n",
      "popt,pcov = curve_fit(f1, df.Height, df.Weight )\n",
      "plt.plot(df.Height, f1(df.Height, *popt), color = 'red')\n",
      "29/104:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "def f1(x,a,b):\n",
      "    return a*x + b\n",
      "\n",
      "popt,pcov = curve_fit(f1, df.Height, df.Weight )\n",
      "plt.plot(df.Height, f1(df.Height, *popt), color = 'red')\n",
      "29/105: popt\n",
      "29/106:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "def f1(x,a,b):\n",
      "    return a*x + b\n",
      "\n",
      "popt,pcov = curve_fit(f1, df.Height, df.Weight )\n",
      "plt.plot(df.Height, f1(df.Height, *popt), color = 'red')\n",
      "xlabel('Height')\n",
      "ylabel('Weight')\n",
      "29/107:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "def f1(x,a,b):\n",
      "    return a*x + b\n",
      "\n",
      "popt,pcov = curve_fit(f1, df.Height, df.Weight )\n",
      "plt.plot(df.Height, f1(df.Height, *popt), color = 'red')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Weight')\n",
      "29/108:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "def f1(x,a,b):\n",
      "    return a*x + b\n",
      "\n",
      "popt,pcov = curve_fit(f1, df.Height, df.Weight )\n",
      "plt.plot(df.Height, f1(df.Height, *popt), color = 'red')\n",
      "plt.xlabel('Height [cm]')\n",
      "plt.ylabel('Weight [kg]')\n",
      "29/109: 1.38*160-159\n",
      "29/110: colors = dict(Male='red', Woman='blue')\n",
      "29/111: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x])\n",
      "29/112: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x]))\n",
      "29/113: colors = dict(Male='red', Female='blue')\n",
      "29/114: colors = dict(Male='red', Female='blue')\n",
      "29/115: colors\n",
      "29/116: df.head(15)\n",
      "29/117: df.head(100)\n",
      "29/118: df.head()\n",
      "29/119: df.columns\n",
      "29/120: df.dtypes()\n",
      "29/121: df.dtypes\n",
      "29/122: df.Gender.loc[0]\n",
      "29/123: df.Gender.loc[0] == 'Male'\n",
      "29/124: df.Gender\n",
      "29/125: df.Gender.loc[9970]\n",
      "29/126: plt.scatter(df.Height, df.Weight, c=df['Gender'].apply(lambda x: colors[x]))\n",
      "29/127: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x]))\n",
      "29/128: plt.scatter(df[df.Gender='Male']['Height'], df.Weight)\n",
      "29/129: plt.scatter(df[df.Gender=='Male']['Height'], df.Weight)\n",
      "29/130: plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'])\n",
      "29/131:\n",
      "plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'])\n",
      "plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'])\n",
      "29/132:\n",
      "plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')\n",
      "plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')\n",
      "29/133:\n",
      "plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')\n",
      "plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')\n",
      "plt.legend()\n",
      "29/134:\n",
      "plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')\n",
      "plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Weight')\n",
      "30/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "30/2:\n",
      "spam_path = 'data/spam'\n",
      "spam2_path = 'data/spam2'\n",
      "easyham_path = 'data/easy_ham'\n",
      "easyham2_path = 'data/easy_ham2'\n",
      "hardham_path = 'data/hard_ham'\n",
      "hardham2_path = 'data/har_ham_2'\n",
      "30/3:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "30/4: os.listdir(spam_path)\n",
      "30/5:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "30/6: os.listdir(spam_path)\n",
      "32/1: all_emails = os.listdir(spam_path)\n",
      "32/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "32/3:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "32/4:\n",
      "def get_messages(path_to_file):\n",
      "    for email in os.listdir(path_to_file):\n",
      "32/5: email =all_emails[0]\n",
      "32/6: all_emails = os.listdir(spam_path)\n",
      "32/7: email =all_emails[0]\n",
      "32/8: print(email.open())\n",
      "32/9: email =spam_path + all_emails[0]\n",
      "32/10: print(email.open())\n",
      "32/11: print(open(email))\n",
      "32/12: email =open(spam_path + all_emails[0])\n",
      "32/13: print(email.read())\n",
      "32/14: all_spam_emails = [spam_path + email_name for email in os.listdir(spam_path)]\n",
      "32/15: all_spam_emails = [spam_path + email_name for email_name in os.listdir(spam_path)]\n",
      "32/16: email =open(all_spam_emails[0])\n",
      "32/17: print(email.read())\n",
      "32/18: email =open(all_spam_emails[0]).read()\n",
      "32/19: print(email)\n",
      "32/20: email\n",
      "32/21: email.split('\\n')\n",
      "32/22: splited_email = email.split('\\n')\n",
      "32/23: print(splited_email[0])\n",
      "32/24: print(email)\n",
      "32/25: splited_email = email.split('\\n\\n')\n",
      "32/26: print(splited_email[0])\n",
      "32/27: len(splitted_email)\n",
      "32/28: len(splited_email)\n",
      "32/29: test = 'aaabbbcccnnaabbcc'\n",
      "32/30: test.find('nn')\n",
      "32/31: pos = test.find('nn')\n",
      "32/32: test2 = test[pos:]\n",
      "32/33: print(test2)\n",
      "32/34: test = 'aaabbbccc\\n\\naabbcc'\n",
      "32/35: pos = test.find('nn')\n",
      "32/36: test2 = test[pos:]\n",
      "32/37: print(test2)\n",
      "32/38: pos = test.find('\\n\\n')\n",
      "32/39: test2 = test[pos:]\n",
      "32/40: print(test2)\n",
      "32/41: print(test2.rstrip())\n",
      "32/42: print(test2.rstrip())\n",
      "32/43: print(test2.rstrip())\n",
      "32/44: print(test2.rstrip('\\n'))\n",
      "32/45: print(test2.strip('\\n'))\n",
      "32/46: test = 'aaabbbccc\\n\\naa\\n\\nbbcc'\n",
      "32/47: pos = test.find('\\n\\n')\n",
      "32/48: test2 = test[pos:]\n",
      "32/49: print(test2.strip('\\n'))\n",
      "32/50: test = 'aaabbbccc\\n\\naa\\n\\nbbcc\\n\\n'\n",
      "32/51: pos = test.find('\\n\\n')\n",
      "32/52: test2 = test[pos:]\n",
      "32/53: print(test2.strip('\\n'))\n",
      "32/54:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names]\n",
      "    all_email_messages = []\n",
      "    for file in path_to_all_files:\n",
      "        email = open(file).read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        email.close()\n",
      "32/55: all_spam_emails = [spam_path + email_name for email_name in os.listdir(spam_path)]\n",
      "32/56: email =open(all_spam_emails[0]).read()\n",
      "32/57: print(email)\n",
      "32/58: splited_email = email.split('\\n\\n')\n",
      "32/59: print(splited_email[0])\n",
      "32/60: all_spam = get_messages(spam_path)\n",
      "32/61:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names]\n",
      "    all_email_messages = []\n",
      "    for file in path_to_all_files:\n",
      "        email = open(file).read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "32/62: all_spam = get_messages(spam_path)\n",
      "32/63:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names]\n",
      "    all_email_messages = []\n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "32/64: all_spam = get_messages(spam_path)\n",
      "32/65: all_spam[0]\n",
      "32/66: all_spam\n",
      "32/67:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names]\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    return all_email_messages\n",
      "32/68: all_spam = get_messages(spam_path)\n",
      "32/69: all_spam\n",
      "32/70: all_spam[0]\n",
      "32/71: print(all_spam[0])\n",
      "32/72: print(all_spam[99])\n",
      "32/73: print(all_spam[199])\n",
      "33/1:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "33/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "33/3:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "33/4:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "33/5: all_spam = get_messages(spam_path)\n",
      "33/6: import re\n",
      "33/7: test = ['Testing for emails \\n searching']\n",
      "33/8: test = 'Testing for emails \\n searching'\n",
      "33/9: result = re.findall(r'\\w+', test)\n",
      "33/10: result\n",
      "33/11:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "33/12: test = 'Testing for emails \\n searching for'\n",
      "33/13: result = re.findall(r'\\w+', test)\n",
      "33/14: result\n",
      "33/15: set(result)\n",
      "33/16: result_set = set(result)\n",
      "33/17: result_set + set('lol')\n",
      "33/18: result_set = result_set | set('lol')\n",
      "33/19: result_set\n",
      "33/20: result_set = set(result)\n",
      "33/21: result_set = result_set | set(['lol'])\n",
      "33/22: result_set\n",
      "33/23:\n",
      "def build_corpus(messages_vector):\n",
      "    corpus = set()\n",
      "    for message in messages vector:\n",
      "        message_words = re.findall(r'\\w+', message)\n",
      "        corpus = corpus | set(message_words)\n",
      "    return corpus\n",
      "33/24:\n",
      "def build_corpus(messages_vector):\n",
      "    corpus = set()\n",
      "    for message in messages_vector:\n",
      "        message_words = re.findall(r'\\w+', message)\n",
      "        corpus = corpus | set(message_words)\n",
      "    return corpus\n",
      "33/25: test_corpus = build_corpus(all_spam[0])\n",
      "33/26: print(test_corpus)\n",
      "33/27:\n",
      "def build_corpus(messages_vector):\n",
      "    corpus = set()\n",
      "    for message in messages_vector:\n",
      "        message_words = re.findall(r'\\w+', message)\n",
      "        print(message_words)\n",
      "        corpus = corpus | set(message_words)\n",
      "    return corpus\n",
      "33/28: test_corpus = build_corpus(all_spam[0])\n",
      "33/29: test_corpus = build_corpus([all_spam[0]])\n",
      "33/30: print(test_corpus)\n",
      "33/31:\n",
      "def build_corpus(messages_vector):\n",
      "    corpus = set()\n",
      "    for message in messages_vector:\n",
      "        message_words = re.findall(r'\\w+', message)\n",
      "        #print(message_words)\n",
      "        corpus = corpus | set(message_words)\n",
      "    return corpus\n",
      "33/32: test_corpus = build_corpus([all_spam[0]])\n",
      "33/33: print(test_corpus)\n",
      "33/34: test.count('for')\n",
      "33/35:\n",
      "def build_corpus(messages_vector):\n",
      "    corpus = set()\n",
      "    for message in messages_vector:\n",
      "        message_words = re.findall(r'\\w+', message)\n",
      "        #print(message_words)\n",
      "        corpus = corpus | set(message_words)\n",
      "    return list(corpus)\n",
      "33/36: test_corpus = build_corpus([all_spam[0]])\n",
      "33/37: print(test_corpus)\n",
      "33/38: occurances = {word: all_spam[0].count(word) for word in test_corpus}\n",
      "33/39: print(occurances)\n",
      "33/40: test_corpus = build_corpus(all_spam)\n",
      "33/41: print(test_corpus)\n",
      "33/42: occurances = {word: all_spam[0].count(word) for word in test_corpus}\n",
      "33/43: print(occurances)\n",
      "33/44:\n",
      "def count_words(message, corpus):\n",
      "    occurances = {word: message.count(word) for word in corpus}\n",
      "33/45:\n",
      "def count_words(message, corpus):\n",
      "    occurances = {word: message.count(word) for word in corpus}\n",
      "    return occurances\n",
      "33/46:\n",
      "def count_words(message, corpus):\n",
      "    occurances = {word: message.count(word) for word in corpus}\n",
      "    return occurances\n",
      "34/1:\n",
      "def count_words(message, corpus):\n",
      "    occurances = {word: message.count(word) for word in corpus if message.count(word)>1}\n",
      "    return occurances\n",
      "34/2: occurances = {word: all_spam[0].count(word) for word in test_corpus}\n",
      "34/3:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "34/4:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "34/5:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "34/6: all_spam = get_messages(spam_path)\n",
      "34/7:\n",
      "def build_corpus(messages_vector):\n",
      "    corpus = set()\n",
      "    for message in messages_vector:\n",
      "        message_words = re.findall(r'\\w+', message)\n",
      "        #print(message_words)\n",
      "        corpus = corpus | set(message_words)\n",
      "    return list(corpus)\n",
      "34/8: test_corpus = build_corpus(all_spam)\n",
      "34/9: print(test_corpus)\n",
      "34/10:\n",
      "def count_words(message, corpus):\n",
      "    occurances = {word: message.count(word) for word in corpus if message.count(word)>1}\n",
      "    return occurances\n",
      "34/11: occurances = {word: all_spam[0].count(word) for word in test_corpus}\n",
      "34/12: test = count_words(all_spam[0], corpus)\n",
      "34/13: test = count_words(all_spam[0], test_corpus)\n",
      "34/14: test\n",
      "34/15: test = count_words([all_spam[0]], test_corpus)\n",
      "34/16: test\n",
      "34/17: test = count_words(all_spam[0], test_corpus)\n",
      "34/18: test\n",
      "34/19: all_spam[0]\n",
      "34/20:\n",
      "def count_words(message, corpus):\n",
      "    occurances = {word: message.count(word) for word in corpus}\n",
      "    return occurances\n",
      "34/21: test = count_words(all_spam[0], test_corpus)\n",
      "34/22: test\n",
      "34/23: test.sorted(key=get())\n",
      "34/24: test.sort(key=get())\n",
      "34/25: sorted(test, key=test.get())\n",
      "34/26: sorted(test, key=test.getitem())\n",
      "34/27: sorted(test, key=test.get())\n",
      "34/28: sorted(test, key=test.__getitem__())\n",
      "34/29: sorted(test, key=test.__getitem__\n",
      "34/30: sorted(test, key=test.__getitem__)\n",
      "34/31: sorted(test, key=test.get)\n",
      "34/32: sorted(test, key=test.get, reverse=True)\n",
      "34/33: [(key, test[key]) for key in sorted(test, key=test.get) ]\n",
      "34/34: [(key, test[key]) for key in sorted(test, key=test.get, reverse=True) ]\n",
      "34/35: all_spam[0].count('e')\n",
      "34/36: words_in_message =  re.findall(r'\\w+', all_spam[0])\n",
      "34/37: print(words_in_message)\n",
      "34/38: from collections import Counter\n",
      "34/39: print(Counter(words_in_message))\n",
      "34/40: Counter(words_in_message)\n",
      "34/41:\n",
      "def count_words(message, corpus):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "34/42: words_in_message =  re.findall(r'\\w+', all_spam[0])\n",
      "34/43: print(words_in_message)\n",
      "34/44: from collections import Counter\n",
      "34/45: Counter(words_in_message)\n",
      "34/46: test = count_words(all_spam[0], test_corpus)\n",
      "34/47: all_spam[0].count('e')\n",
      "34/48:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "34/49:\n",
      "def count_words(message, corpus):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "34/50: matrix = pd.DataFrame()\n",
      "34/51: test_occurance = count_words(all_spam[0])\n",
      "34/52: test_occurance = count_words(all_spam[0], test_corpus)\n",
      "34/53: test_occurance\n",
      "34/54: matrix = pd.DataFrame(columns=[test_corpus])\n",
      "34/55: matrix.head()\n",
      "34/56: matrix = pd.DataFrame(data=test_corpus)\n",
      "34/57: matrix.head()\n",
      "34/58: matrix = pd.DataFrame(index=test_corpus)\n",
      "34/59: matrix.head()\n",
      "34/60: matrix['test'] = 0\n",
      "34/61: matrix.head()\n",
      "34/62: matrix[1] = 0\n",
      "34/63: matrix.head()\n",
      "34/64: matrix.drop(['test', 1], axis = 1)\n",
      "34/65: matrix.head()\n",
      "34/66: matrix.head()\n",
      "34/67: matrix.drop(['test', 1], axis = 1, inplace=True)\n",
      "34/68: matrix.head()\n",
      "34/69: matrix.at['Minister', 'test'] = 10\n",
      "34/70: matrix.head()\n",
      "34/71: matrix.drop(['test'], axis = 1, inplace=True)\n",
      "34/72: matrix.head()\n",
      "34/73:\n",
      "for key in test_occurance.keys():\n",
      "    matrix.at[key, 1] = test_occurance[key]\n",
      "34/74: matrix.head()\n",
      "34/75: matrix.head(50)\n",
      "34/76: matrix.loc['help']\n",
      "34/77: matrix.head()\n",
      "34/78: matrix.loc['home']\n",
      "34/79:\n",
      "for i, message in enumerate(all_spam):\n",
      "    occurances = count_words(message, test_corpus)\n",
      "    for key in occurances.keys():\n",
      "        matrix[key,i] = occurances[key]\n",
      "34/80:\n",
      "for i, message in enumerate(all_spam[:10]):\n",
      "    occurances = count_words(message, test_corpus)\n",
      "    for key in occurances.keys():\n",
      "        matrix[key,i] = occurances[key]\n",
      "34/81: matrix.head()\n",
      "34/82: matrix = pd.DataFrame(index=test_corpus)\n",
      "34/83: matrix.head()\n",
      "34/84:\n",
      "for i, message in enumerate(all_spam[:10]):\n",
      "    occurances = count_words(message, test_corpus)\n",
      "    for key in occurances.keys():\n",
      "        matrix.at[key,i] = occurances[key]\n",
      "34/85: matrix.head()\n",
      "34/86: matrix.head(25)\n",
      "34/87: matrix.head(50)\n",
      "34/88: len(all_spam)\n",
      "34/89: matrix = pd.DataFrame(index=test_corpus)\n",
      "34/90: matrix.head(50)\n",
      "34/91:\n",
      "for i, message in enumerate(all_spam[400:]):\n",
      "    occurances = count_words(message, test_corpus)\n",
      "    for key in occurances.keys():\n",
      "        matrix.at[key,i] = occurances[key]\n",
      "34/92: matrix.head(50)\n",
      "34/93: sorted(all_spam, key=str.len())\n",
      "34/94: sorted(all_spam, key=len(str))\n",
      "34/95: sorted(all_spam, key=lambda x: len(x))\n",
      "34/96: sorted(test_corpus, key=lambda x: len(x))\n",
      "34/97: sorted(test_corpus, key=lambda x: len(x), reverse=True)\n",
      "34/98: [x in test_corpus if len(x)>20]\n",
      "34/99: [x for x in test_corpus if len(x)>20]\n",
      "34/100: len([x for x in test_corpus if len(x)>20])\n",
      "34/101: len(test_corpus)\n",
      "34/102:\n",
      "def build_corpus(messages_vector):\n",
      "    corpus = set()\n",
      "    for message in messages_vector:\n",
      "        message_words = [x for x in re.findall(r'\\w+', message) if len(x)<20 and not any(i.isdigit() for i in x)] \n",
      "        #print(message_words)\n",
      "        corpus = corpus | set(message_words)\n",
      "    return list(corpus)\n",
      "34/103: test_corpus = build_corpus(all_spam)\n",
      "34/104: print(test_corpus)\n",
      "34/105: len(test_corpus)\n",
      "34/106:\n",
      "for i, message in enumerate(all_spam):\n",
      "    occurances = count_words(message, test_corpus)\n",
      "    for key in occurances.keys():\n",
      "        matrix.at[key,i] = occurances[key]\n",
      "34/107: matrix.head(50)\n",
      "34/108: matrix = pd.DataFrame(index=test_corpus)\n",
      "34/109:\n",
      "for i, message in enumerate(all_spam):\n",
      "    occurances = count_words(message, test_corpus)\n",
      "    for key in occurances.keys():\n",
      "        matrix.at[key,i] = occurances[key]\n",
      "34/110: matrix.head(50)\n",
      "34/111:\n",
      "def build_corpus(messages_vector):\n",
      "    corpus = set()\n",
      "    for message in messages_vector:\n",
      "        message_words = [x for x in re.findall(r'\\w+', message) if len(x)<20 and x.isalpha()] \n",
      "        #print(message_words)\n",
      "        corpus = corpus | set(message_words)\n",
      "    return list(corpus)\n",
      "34/112: test_corpus = build_corpus(all_spam)\n",
      "34/113: print(test_corpus)\n",
      "34/114:\n",
      "def count_words(message, corpus):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "34/115: test_occurance = count_words(all_spam[0], test_corpus)\n",
      "34/116: matrix = pd.DataFrame(index=test_corpus)\n",
      "34/117: matrix.head(50)\n",
      "34/118:\n",
      "for i, message in enumerate(all_spam):\n",
      "    occurances = count_words(message, test_corpus)\n",
      "    for key in occurances.keys():\n",
      "        matrix.at[key,i] = occurances[key]\n",
      "34/119:\n",
      "for i, message in enumerate(all_spam[:100]):\n",
      "    occurances = count_words(message, test_corpus)\n",
      "    for key in occurances.keys():\n",
      "        matrix.at[key,i] = occurances[key]\n",
      "34/120:\n",
      "for i, message in enumerate(all_spam[:200]):\n",
      "    occurances = count_words(message, test_corpus)\n",
      "    for key in occurances.keys():\n",
      "        matrix.at[key,i] = occurances[key]\n",
      "34/121:\n",
      "for i, message in enumerate(all_spam[300]):\n",
      "    occurances = count_words(message, test_corpus)\n",
      "    for key in occurances.keys():\n",
      "        matrix.at[key,i] = occurances[key]\n",
      "34/122: matrix.head(50)\n",
      "34/123: matrix = pd.DataFrame(index=test_corpus)\n",
      "34/124: matrix.head(50)\n",
      "34/125:\n",
      "for i, message in enumerate(all_spam[:300]):\n",
      "    occurances = count_words(message, test_corpus)\n",
      "    for key in occurances.keys():\n",
      "        matrix.at[key,i] = occurances[key]\n",
      "36/1:\n",
      "a=dict(a=1)\n",
      "b=dict(a=2)\n",
      "a.update(b)\n",
      "print(a)\n",
      "36/2:\n",
      "def build_corpus(messages_vector):\n",
      "    corpus = []\n",
      "    for message in messages_vector:\n",
      "        message_words = [x for x in re.findall(r'\\w+', message) if len(x)<20 and x.isalpha()] \n",
      "        #print(message_words)\n",
      "        corpus.append(message_words)\n",
      "    return [x for x in corpus if Counter(corpus)[x] > 1]\n",
      "36/3: test_corpus = build_corpus(all_spam)\n",
      "36/4:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "36/5:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "36/6:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "36/7: all_spam = get_messages(spam_path)\n",
      "36/8:\n",
      "def build_corpus(messages_vector):\n",
      "    corpus = []\n",
      "    for message in messages_vector:\n",
      "        message_words = [x for x in re.findall(r'\\w+', message) if len(x)<20 and x.isalpha()] \n",
      "        #print(message_words)\n",
      "        corpus.append(message_words)\n",
      "    return [x for x in corpus if Counter(corpus)[x] > 1]\n",
      "36/9: test_corpus = build_corpus(all_spam)\n",
      "37/1:\n",
      "a=dict(a=1)\n",
      "b=dict(a=2)\n",
      "37/2: l=[a, b]\n",
      "37/3:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "37/4: pd.DataFrame(l)\n",
      "37/5:\n",
      "a=dict(a=1, b=2)\n",
      "b=dict(a=2, c=3)\n",
      "37/6: l=[a, b]\n",
      "37/7: pd.DataFrame(l)\n",
      "37/8: pd.DataFrame(l, orientation=index)\n",
      "37/9: pd.DataFrame(l, orient='index')\n",
      "37/10: pd.DataFrame.from_dict(l, orient='index')\n",
      "37/11: pd.DataFrame.from_dict(l)\n",
      "37/12: pd.DataFrame(l)\n",
      "37/13: pd.DataFrame.from_dict(l, orient='index')\n",
      "37/14: pd.DataFrame.from_dict(l, orient='columns')\n",
      "37/15: pd.DataFrame.from_dict(l, orient='index')\n",
      "37/16: z=pd.DataFrame(index=['a', 'b', 'c'])\n",
      "37/17: zpd.DataFrame.from_dict(l, orient='index')\n",
      "37/18: pd.DataFrame.from_dict(l, orient='index')\n",
      "37/19: z.DataFrame.from_dict(l, orient='index')\n",
      "37/20: z.from_dict(l, orient='index')\n",
      "37/21: z=pd.DataFrame(l)\n",
      "37/22: z\n",
      "37/23: z = pd.DataFrame(l)\n",
      "37/24: z\n",
      "37/25: z.transpose()\n",
      "37/26: z\n",
      "37/27: z.transpose(inplace=True)\n",
      "37/28: z=z.transpose(inplace=True)\n",
      "37/29: z=z.transpose()\n",
      "37/30: z\n",
      "37/31: matrix.head()\n",
      "37/32:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "37/33:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "37/34:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "37/35: all_spam = get_messages(spam_path)\n",
      "37/36: spam_dicts = [Counter(message) for message in all_spam]\n",
      "37/37: matrix = pd.DataFrame(spam_dicts)\n",
      "37/38: matrix\n",
      "37/39: all_spam\n",
      "37/40: spam_dicts[0]\n",
      "37/41:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "37/42: spam_dicts = [count_words(message) for message in all_spam]\n",
      "37/43: matrix = pd.DataFrame(spam_dicts)\n",
      "37/44: spam_dicts[0]\n",
      "37/45: matrix\n",
      "37/46:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "37/47: matrix = matrix.transpose()\n",
      "37/48: matrix\n",
      "37/49:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word for word in words_in_message if word.isalpha() and len(word)<20]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "37/50:\n",
      "def build_corpus(messages_vector):\n",
      "    corpus = []\n",
      "    for message in messages_vector:\n",
      "        message_words = [x for x in re.findall(r'\\w+', message) if len(x)<20 and x.isalpha()] \n",
      "        #print(message_words)\n",
      "        corpus.append(message_words)\n",
      "    return [x for x in corpus if Counter(corpus)[x] > 1]\n",
      "37/51: spam_dicts = [count_words(message) for message in all_spam]\n",
      "37/52: matrix = pd.DataFrame(spam_dicts)\n",
      "37/53: matrix = matrix.transpose()\n",
      "37/54: matrix\n",
      "37/55: matrix.loc('html')\n",
      "37/56: matrix.loc('dear')\n",
      "37/57: matrix.iloc('dear')\n",
      "37/58: matrix\n",
      "37/59: matrix.loc('A')\n",
      "37/60: matrix\n",
      "37/61: matrix.index\n",
      "37/62: matrix.loc('AAD')\n",
      "37/63: matrix.loc['A']\n",
      "37/64: matrix.loc['html']\n",
      "37/65: matrix['sum']=matrix.sum(axis=1)\n",
      "37/66: matrix\n",
      "37/67: matrix=matrix.loc[matrix.sum>1]\n",
      "37/68: matrix=matrix.loc[matrix['sum']>1]\n",
      "37/69: matrix\n",
      "37/70:\n",
      "def build_corpus(messages_vector):\n",
      "    corpus = []\n",
      "    for message in messages_vector:\n",
      "        message_words = [x for x in re.findall(r'\\w+', message) if <1len(x)<20 and x.isalpha()] \n",
      "        #print(message_words)\n",
      "        corpus.append(message_words)\n",
      "    return [x for x in corpus if Counter(corpus)[x] > 1]\n",
      "37/71:\n",
      "def build_corpus(messages_vector):\n",
      "    corpus = []\n",
      "    for message in messages_vector:\n",
      "        message_words = [x for x in re.findall(r'\\w+', message) if 1<len(x)<20 and x.isalpha()] \n",
      "        #print(message_words)\n",
      "        corpus.append(message_words)\n",
      "    return [x for x in corpus if Counter(corpus)[x] > 1]\n",
      "37/72:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word for word in words_in_message if word.isalpha() and 1<len(word)<20]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "37/73: spam_dicts = [count_words(message) for message in all_spam]\n",
      "37/74: matrix = pd.DataFrame(spam_dicts)\n",
      "37/75: matrix = matrix.transpose()\n",
      "37/76: matrix['sum']=matrix.sum(axis=1)\n",
      "37/77: matrix=matrix.loc[matrix['sum']>1 ]\n",
      "37/78: matrix\n",
      "37/79: matrix.loc['html']\n",
      "37/80: matrix\n",
      "37/81: matrix['number_of_documents']=matrix.count(axis=1)\n",
      "37/82: matrix\n",
      "37/83: matrix.loc['AAD']\n",
      "37/84:  z = [matrix.loc['AAD']>0]\n",
      "37/85: z\n",
      "37/86: matrix.loc['AAD'][z]\n",
      "37/87:  z = [x for x in matrix.loc['AAD'] if x>0]\n",
      "37/88: z\n",
      "37/89: matrix['number_of_documents']=matrix.count(axis=1)-1\n",
      "37/90: matrix\n",
      "37/91: matrix['number_of_documents']=matrix.count(axis=1)-1\n",
      "37/92:  z = [x for x in matrix.loc['AAD'] if x>0]\n",
      "37/93: z\n",
      "37/94: matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "37/95:  z = [x for x in matrix.loc['AAD'] if x>0]\n",
      "37/96: z\n",
      "37/97: matrix\n",
      "37/98: matrix.drop(columns=['number_of_documents'])\n",
      "37/99: matrix\n",
      "37/100: matrix\n",
      "37/101: matrix=matrix.drop(columns=['number_of_documents'])\n",
      "37/102: matrix\n",
      "37/103: matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "37/104: matrix\n",
      "37/105: matrix.loc['kiedy']\n",
      "37/106: matrix.loc['ów']\n",
      "37/107: all_spam[6]\n",
      "37/108: print(all_spam[6])\n",
      "37/109: matrix.loc['jak']\n",
      "37/110: matrix.loc['gdy']\n",
      "37/111: matrix.loc['do']\n",
      "37/112: matrix.loc['pl']\n",
      "37/113: print(all_spam[8])\n",
      "37/114: print(all_spam[99])\n",
      "37/115: matrix.loc['/div']\n",
      "37/116: matrix.loc['div']\n",
      "37/117: matrix.loc['<']\n",
      "37/118: matrix\n",
      "37/119: matrix.loc['porn']\n",
      "37/120: matrix.loc['www']\n",
      "37/121: matrix.loc['viagra']\n",
      "37/122: matrix.sort_values(by=['sum'])\n",
      "37/123: matrix.sort_values(by=['sum'],ascending=False)\n",
      "37/124: matrix.sort_values(by=['number_of_documents'],ascending=False)\n",
      "38/1: matrix=matrix.drop(columns=['number_of_documents'])/len(all_spam)\n",
      "38/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "38/3:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "38/4:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "38/5: all_spam = get_messages(spam_path)\n",
      "38/6:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word for word in words_in_message if word.isalpha() and 1<len(word)<20]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "38/7: spam_dicts = [count_words(message) for message in all_spam]\n",
      "38/8: matrix = pd.DataFrame(spam_dicts)\n",
      "38/9: matrix = matrix.transpose()\n",
      "38/10: matrix['sum']=matrix.sum(axis=1)\n",
      "38/11: matrix=matrix.loc[matrix['sum']>1 ]\n",
      "38/12: matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "38/13: matrix=matrix.drop(columns=['number_of_documents'])/len(all_spam)\n",
      "38/14: matrix['documents_percentage']=matrix['number_of_documents']/\n",
      "38/15:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "38/16:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "38/17:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "38/18: all_spam = get_messages(spam_path)\n",
      "38/19:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word for word in words_in_message if word.isalpha() and 1<len(word)<20]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "38/20: spam_dicts = [count_words(message) for message in all_spam]\n",
      "38/21: matrix = pd.DataFrame(spam_dicts)\n",
      "38/22: matrix = matrix.transpose()\n",
      "38/23: matrix['sum']=matrix.sum(axis=1)\n",
      "38/24: matrix=matrix.loc[matrix['sum']>1 ]\n",
      "38/25: matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "38/26: matrix['documents_percentage']=matrix['number_of_documents']/len(all_spam)\n",
      "38/27: matrix.sort_values(by=['number_of_documents'],ascending=False)\n",
      "38/28: stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
      "38/29:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word for word in words_in_message if word.isalpha() and 1<len(word)<20 and word is not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "38/30:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word for word in words_in_message if word.isalpha() and 1<len(word)<20 and word not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "38/31: spam_dicts = [count_words(message) for message in all_spam]\n",
      "38/32: matrix = pd.DataFrame(spam_dicts)\n",
      "38/33: matrix = matrix.transpose()\n",
      "38/34: matrix['sum']=matrix.sum(axis=1)\n",
      "38/35: matrix=matrix.loc[matrix['sum']>1 ]\n",
      "38/36: matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "38/37: matrix['documents_percentage']=matrix['number_of_documents']/len(all_spam)\n",
      "38/38: matrix.sort_values(by=['number_of_documents'],ascending=False)\n",
      "38/39:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if word.isalpha() and 1<len(word)<20 and word not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "38/40: spam_dicts = [count_words(message) for message in all_spam]\n",
      "38/41: matrix = pd.DataFrame(spam_dicts)\n",
      "38/42: matrix = matrix.transpose()\n",
      "38/43: matrix['sum']=matrix.sum(axis=1)\n",
      "38/44: matrix=matrix.loc[matrix['sum']>1 ]\n",
      "38/45: matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "38/46: matrix['documents_percentage']=matrix['number_of_documents']/len(all_spam)\n",
      "38/47: matrix.sort_values(by=['number_of_documents'],ascending=False)\n",
      "38/48:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "38/49:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "38/50: stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
      "38/51:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "38/52: all_spam = get_messages(spam_path)\n",
      "38/53:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if word.isalpha() and 1<len(word)<20 and word not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "38/54: spam_dicts = [count_words(message) for message in all_spam]\n",
      "38/55: matrix = pd.DataFrame(spam_dicts)\n",
      "38/56: matrix = matrix.transpose()\n",
      "38/57: matrix['sum']=matrix.sum(axis=1)\n",
      "38/58: matrix=matrix.loc[matrix['sum']>1 ]\n",
      "38/59: matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "38/60: matrix['documents_percentage']=matrix['number_of_documents']/len(all_spam)\n",
      "38/61: matrix.sort_values(by=['number_of_documents'],ascending=False)\n",
      "38/62:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if word.isalpha() and 1<len(word)<20 and word.lower() not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "38/63: spam_dicts = [count_words(message) for message in all_spam]\n",
      "38/64: matrix = pd.DataFrame(spam_dicts)\n",
      "38/65: matrix = matrix.transpose()\n",
      "38/66: matrix['sum']=matrix.sum(axis=1)\n",
      "38/67: matrix=matrix.loc[matrix['sum']>1 ]\n",
      "38/68: matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "38/69: matrix['documents_percentage']=matrix['number_of_documents']/len(all_spam)\n",
      "38/70: matrix.sort_values(by=['number_of_documents'],ascending=False)\n",
      "38/71:\n",
      "i = 0\n",
      "for message in all_spam:\n",
      "    if 'html' in message:\n",
      "        i+=1\n",
      "        \n",
      "i/500\n",
      "38/72: len(all_spam)\n",
      "38/73:\n",
      "i = 0\n",
      "for message in all_spam:\n",
      "    if 'html' in message:\n",
      "        i+=1\n",
      "\n",
      "print('i: ', i)\n",
      "i/500\n",
      "38/74:\n",
      "x = 0\n",
      "for message in all_dicts:\n",
      "    if 'html' in message.keys():\n",
      "        x+=1\n",
      "\n",
      "print('x: ', x)\n",
      "x/500\n",
      "38/75:\n",
      "x = 0\n",
      "for message in spam_dicts:\n",
      "    if 'html' in message.keys():\n",
      "        x+=1\n",
      "\n",
      "print('x: ', x)\n",
      "x/500\n",
      "38/76: len(spam_dicts)\n",
      "39/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "39/2:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "39/3: stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
      "39/4:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "39/5: all_spam = get_messages(spam_path)\n",
      "39/6:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "39/7: spam_dicts = [count_words(message) for message in all_spam]\n",
      "39/8: matrix = pd.DataFrame(spam_dicts)\n",
      "39/9: matrix = matrix.transpose()\n",
      "39/10: matrix['sum']=matrix.sum(axis=1)\n",
      "39/11: matrix=matrix.loc[matrix['sum']>1 ]\n",
      "39/12: matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "39/13: matrix['documents_percentage']=matrix['number_of_documents']/len(all_spam)\n",
      "39/14: matrix.sort_values(by=['number_of_documents'],ascending=False)\n",
      "39/15:\n",
      "i = 0\n",
      "for message in all_spam:\n",
      "    if 'html' in message:\n",
      "        i+=1\n",
      "\n",
      "print('i: ', i)\n",
      "i/500\n",
      "39/16: len(all_spam)\n",
      "39/17:\n",
      "x = 0\n",
      "for message in spam_dicts:\n",
      "    if 'html' in message.keys():\n",
      "        x+=1\n",
      "\n",
      "print('x: ', x)\n",
      "x/500\n",
      "39/18: len(spam_dicts)\n",
      "40/1: matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "40/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "40/3:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "40/4: stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
      "40/5:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "40/6: all_spam = get_messages(spam_path)\n",
      "40/7:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "40/8: spam_dicts = [count_words(message) for message in all_spam]\n",
      "40/9: matrix = pd.DataFrame(spam_dicts)\n",
      "40/10: matrix = matrix.transpose()\n",
      "40/11: matrix['sum']=matrix.sum(axis=1)\n",
      "40/12: matrix=matrix.loc[matrix['sum']>1 ]\n",
      "40/13: matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "40/14: matrix['documents_percentage']=matrix['number_of_documents']/len(all_spam)\n",
      "40/15: matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "40/16: matrix.sort_values(by=['number_of_documents'],ascending=False)\n",
      "40/17:\n",
      "i = 0\n",
      "for message in all_spam:\n",
      "    if 'html' in message:\n",
      "        i+=1\n",
      "\n",
      "print('i: ', i)\n",
      "i/500\n",
      "40/18: len(all_spam)\n",
      "40/19:\n",
      "x = 0\n",
      "for message in spam_dicts:\n",
      "    if 'html' in message.keys():\n",
      "        x+=1\n",
      "\n",
      "print('x: ', x)\n",
      "x/500\n",
      "40/20: len(spam_dicts)\n",
      "40/21: matrix.sort_values(by=['sum'],ascending=False)\n",
      "40/22: all_spam[496]\n",
      "40/23:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "40/24:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "40/25: stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
      "40/26:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "40/27: all_spam = get_messages(spam_path)\n",
      "40/28:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "40/29: spam_dicts = [count_words(message) for message in all_spam]\n",
      "40/30: matrix = pd.DataFrame(spam_dicts)\n",
      "40/31: matrix = matrix.transpose()\n",
      "40/32: matrix['sum']=matrix.sum(axis=1)\n",
      "40/33: matrix=matrix.loc[matrix['sum']>1 ]\n",
      "40/34: matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "40/35: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)\n",
      "40/36:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "40/37:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "40/38: stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
      "40/39:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "40/40: all_spam = get_messages(spam_path)\n",
      "40/41:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "40/42: spam_dicts = [count_words(message) for message in all_spam]\n",
      "40/43: matrix_spam = pd.DataFrame(spam_dicts)\n",
      "40/44: matrix_spam = matrix_spam.transpose()\n",
      "40/45: matrix_spam['sum']=matrix_spam.sum(axis=1)\n",
      "40/46: matrix_spam=matrix_spam.loc[matrix_spam['sum']>1 ]\n",
      "40/47: matrix_spam['number_of_documents']=(matrix_spam.count(axis=1)-1)\n",
      "40/48: matrix_spam_spam['documents_percentage']=matrix_spam_spam['number_of_documents']/len(all_spam)\n",
      "40/49: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)\n",
      "40/50: matrix_spam['word_density'] = matrix_spam['sum']/matrix_spam['sum'].sum()\n",
      "40/51: matrix_spam.sort_values(by=['sum'],ascending=False)\n",
      "40/52:\n",
      "i = 0\n",
      "for message in all_spam:\n",
      "    if 'html' in message:\n",
      "        i+=1\n",
      "\n",
      "print('i: ', i)\n",
      "i/500\n",
      "40/53: len(all_spam)\n",
      "40/54:\n",
      "x = 0\n",
      "for message in spam_dicts:\n",
      "    if 'html' in message.keys():\n",
      "        x+=1\n",
      "\n",
      "print('x: ', x)\n",
      "x/500\n",
      "40/55: len(spam_dicts)\n",
      "40/56: all_spam[496]\n",
      "40/57:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "40/58:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "40/59: stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
      "40/60:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "40/61: all_spam = get_messages(spam_path)\n",
      "40/62:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "40/63: spam_dicts = [count_words(message) for message in all_spam]\n",
      "40/64: matrix_spam = pd.DataFrame(spam_dicts)\n",
      "40/65: matrix_spam = matrix_spam.transpose()\n",
      "40/66: matrix_spam['sum']=matrix_spam.sum(axis=1)\n",
      "40/67: matrix_spam=matrix_spam.loc[matrix_spam['sum']>1 ]\n",
      "40/68: matrix_spam['number_of_documents']=(matrix_spam.count(axis=1)-1)\n",
      "40/69: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)\n",
      "40/70: matrix_spam['word_density'] = matrix_spam['sum']/matrix_spam['sum'].sum()\n",
      "40/71: matrix_spam.sort_values(by=['sum'],ascending=False)\n",
      "40/72:\n",
      "i = 0\n",
      "for message in all_spam:\n",
      "    if 'html' in message:\n",
      "        i+=1\n",
      "\n",
      "print('i: ', i)\n",
      "i/500\n",
      "40/73: len(all_spam)\n",
      "40/74:\n",
      "x = 0\n",
      "for message in spam_dicts:\n",
      "    if 'html' in message.keys():\n",
      "        x+=1\n",
      "\n",
      "print('x: ', x)\n",
      "x/500\n",
      "40/75: len(spam_dicts)\n",
      "40/76: all_spam[496]\n",
      "40/77: matrix_spam.sort_values(by=['sum'],ascending=False).head()\n",
      "40/78: all_ham = get_messages(spam_path)[:500]\n",
      "40/79: ham_dicts = [count_words(message) for message in all_ham]\n",
      "40/80:\n",
      "matrix_ham = pd.DataFrame(ham_dicts)\n",
      "matrix_ham = matrix_spam.transpose()\n",
      "40/81:\n",
      "matrix_spam['sum']=matrix_spam.sum(axis=1)\n",
      "matrix_spam=matrix_spam.loc[matrix_spam['sum']>1 ]\n",
      "40/82: matrix_spam['number_of_documents']=(matrix_spam.count(axis=1)-1)\n",
      "40/83: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)\n",
      "40/84:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "40/85:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "40/86: stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
      "40/87:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "40/88: all_spam = get_messages(spam_path)\n",
      "40/89:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "40/90: spam_dicts = [count_words(message) for message in all_spam]\n",
      "40/91:\n",
      "matrix_spam = pd.DataFrame(spam_dicts)\n",
      "matrix_spam = matrix_spam.transpose()\n",
      "40/92:\n",
      "matrix_spam['sum']=matrix_spam.sum(axis=1)\n",
      "matrix_spam=matrix_spam.loc[matrix_spam['sum']>1 ]\n",
      "40/93: matrix_spam['number_of_documents']=(matrix_spam.count(axis=1)-1)\n",
      "40/94: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)\n",
      "40/95: matrix_spam['word_density'] = matrix_spam['sum']/matrix_spam['sum'].sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/96: matrix_spam.sort_values(by=['sum'],ascending=False).head()\n",
      "40/97: all_ham = get_messages(spam_path)[:500]\n",
      "40/98: ham_dicts = [count_words(message) for message in all_ham]\n",
      "40/99:\n",
      "matrix_ham = pd.DataFrame(ham_dicts)\n",
      "matrix_ham = matrix_ham.transpose()\n",
      "40/100:\n",
      "matrix_ham['sum']=matrix_ham.sum(axis=1)\n",
      "matrix_ham=matrix_ham.loc[matrix_ham['sum']>1 ]\n",
      "40/101: matrix_ham['number_of_documents']=(matrix_ham.count(axis=1)-1)\n",
      "40/102: matrix_ham['documents_percentage']=matrix_ham['number_of_documents']/len(all_ham)\n",
      "40/103: matrix_ham['word_density'] = matrix_ham['sum']/matrix_ham['sum'].sum()\n",
      "40/104: matrix_ham.sort_values(by=['sum'],ascending=False).head()\n",
      "40/105:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "40/106:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "40/107: stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
      "40/108:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "40/109: all_spam = get_messages(spam_path)\n",
      "40/110:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "40/111: spam_dicts = [count_words(message) for message in all_spam]\n",
      "40/112:\n",
      "matrix_spam = pd.DataFrame(spam_dicts)\n",
      "matrix_spam = matrix_spam.transpose()\n",
      "40/113:\n",
      "matrix_spam['sum']=matrix_spam.sum(axis=1)\n",
      "matrix_spam=matrix_spam.loc[matrix_spam['sum']>1 ]\n",
      "40/114: matrix_spam['number_of_documents']=(matrix_spam.count(axis=1)-1)\n",
      "40/115: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)\n",
      "40/116: matrix_spam['word_density'] = matrix_spam['sum']/matrix_spam['sum'].sum()\n",
      "40/117: matrix_spam.sort_values(by=['sum'],ascending=False).head()\n",
      "40/118: all_ham = get_messages(ham_path)[:500]\n",
      "40/119: all_ham = get_messages(easyham_path)[:500]\n",
      "40/120: ham_dicts = [count_words(message) for message in all_ham]\n",
      "40/121:\n",
      "matrix_ham = pd.DataFrame(ham_dicts)\n",
      "matrix_ham = matrix_ham.transpose()\n",
      "40/122:\n",
      "matrix_ham['sum']=matrix_ham.sum(axis=1)\n",
      "matrix_ham=matrix_ham.loc[matrix_ham['sum']>1 ]\n",
      "40/123: matrix_ham['number_of_documents']=(matrix_ham.count(axis=1)-1)\n",
      "40/124: matrix_ham['documents_percentage']=matrix_ham['number_of_documents']/len(all_ham)\n",
      "40/125: matrix_ham['word_density'] = matrix_ham['sum']/matrix_ham['sum'].sum()\n",
      "40/126: matrix_ham.sort_values(by=['sum'],ascending=False).head()\n",
      "40/127: matrix_ham.sort_values(by=['sum'],ascending=False).head(10)\n",
      "40/128: matrix_ham.sort_values(by=['documents_percentage'],ascending=False).head(10)\n",
      "40/129: matrix_ham['yahoo']\n",
      "40/130: matrix_ham['linux']\n",
      "40/131: matrix_ham['list']\n",
      "40/132:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "40/133:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "40/134: stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
      "40/135:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "40/136: all_spam = get_messages(spam_path)\n",
      "40/137:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "40/138: spam_dicts = [count_words(message) for message in all_spam]\n",
      "40/139:\n",
      "matrix_spam = pd.DataFrame(spam_dicts)\n",
      "matrix_spam = matrix_spam.transpose()\n",
      "40/140:\n",
      "matrix_spam['sum']=matrix_spam.sum(axis=1)\n",
      "matrix_spam=matrix_spam.loc[matrix_spam['sum']>1 ]\n",
      "40/141: matrix_spam['number_of_documents']=(matrix_spam.count(axis=1)-1)\n",
      "40/142: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)\n",
      "40/143: matrix_spam['word_density'] = matrix_spam['sum']/matrix_spam['sum'].sum()\n",
      "40/144: matrix_spam.sort_values(by=['sum'],ascending=False).head()\n",
      "40/145: all_ham = get_messages(easyham_path)[:500]\n",
      "40/146: ham_dicts = [count_words(message) for message in all_ham]\n",
      "40/147:\n",
      "matrix_ham = pd.DataFrame(ham_dicts)\n",
      "matrix_ham = matrix_ham.transpose()\n",
      "40/148:\n",
      "matrix_ham['sum']=matrix_ham.sum(axis=1)\n",
      "matrix_ham=matrix_ham.loc[matrix_ham['sum']>1 ]\n",
      "40/149: matrix_ham['number_of_documents']=(matrix_ham.count(axis=1)-1)\n",
      "40/150: matrix_ham['documents_percentage']=matrix_ham['number_of_documents']/len(all_ham)\n",
      "40/151: matrix_ham['word_density'] = matrix_ham['sum']/matrix_ham['sum'].sum()\n",
      "40/152: matrix_ham.sort_values(by=['documents_percentage'],ascending=False).head(10)\n",
      "40/153: matrix_ham['list']\n",
      "40/154: matrix_ham.loc['list']\n",
      "40/155: matrix_ham.loc['yahoo']\n",
      "40/156: matrix_ham.loc['linux']\n",
      "40/157:\n",
      "def classify_email(email, prior=0.5, c=10**-6):\n",
      "    pass\n",
      "40/158:\n",
      "def create_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix.transpose()\n",
      "    matrix['sum']=matrix_spam.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "40/159: spam_matrix.index\n",
      "40/160: spam_matrix.index()\n",
      "40/161: matrix_spam.index()\n",
      "40/162: matrix_spam.index\n",
      "40/163:\n",
      "def classify_emails(emails, prior=0.5, c=10**-6):\n",
      "    messages = get_messages(emails)\n",
      "    matrix = create_matrix(messages)\n",
      "    common_words = set(spam_matrix.index).intersection(set(matrix.index))\n",
      "40/164:\n",
      "def classify_email(email, prior=0.5, c=10**-6):\n",
      "    matrix = create_matrix(email)\n",
      "    common_words = set(spam_matrix.index).intersection(set(matrix.index))\n",
      "40/165:\n",
      "def classify_email(email, prior=0.5, c=10**-6):\n",
      "    matrix = create_matrix(email)\n",
      "    common_words = set(spam_matrix.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "40/166: classify_email(all_spam[0])\n",
      "40/167:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix_spam.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "40/168: classify_email(all_spam[0])\n",
      "40/169:\n",
      "def classify_email(email, prior=0.5, c=10**-6):\n",
      "    matrix = create_matrix(email)\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "40/170: classify_email(all_spam[0])\n",
      "40/171:\n",
      "def classify_email(email, prior=0.5, c=10**-6):\n",
      "    matrix = create_matrix(email)\n",
      "    matrix.head(10)\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "40/172: classify_email(all_spam[0])\n",
      "40/173:\n",
      "def classify_email(email, prior=0.5, c=10**-6):\n",
      "    matrix = create_matrix(email)\n",
      "    print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "40/174: classify_email(all_spam[0])\n",
      "40/175: classify_email([all_spam[0]])\n",
      "40/176:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix_spam.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "40/177: classify_email([all_spam[0]])\n",
      "40/178:\n",
      "def classify_email(email, prior=0.5, c=10**-6):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "40/179: classify_email([all_spam[0]])\n",
      "40/180: matrix_spam.at['list', 'documents_percentage']\n",
      "41/1: matrix_ham.sort_values(by=['documents_percentage'],ascending=False).head()\n",
      "41/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "41/3:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "41/4: stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
      "41/5:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "41/6: all_spam = get_messages(spam_path)\n",
      "41/7:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "41/8:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix_spam.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "41/9: spam_dicts = [count_words(message) for message in all_spam]\n",
      "41/10:\n",
      "matrix_spam = pd.DataFrame(spam_dicts)\n",
      "matrix_spam = matrix_spam.transpose()\n",
      "41/11:\n",
      "matrix_spam['sum']=matrix_spam.sum(axis=1)\n",
      "matrix_spam=matrix_spam.loc[matrix_spam['sum']>1 ]\n",
      "41/12: matrix_spam['number_of_documents']=(matrix_spam.count(axis=1)-1)\n",
      "41/13: matrix_spam['documents_percentage']=matrix_spam['number_of_documents']/len(all_spam)\n",
      "41/14: matrix_spam['word_density'] = matrix_spam['sum']/matrix_spam['sum'].sum()\n",
      "41/15: matrix_spam.sort_values(by=['sum'],ascending=False).head()\n",
      "41/16: all_ham = get_messages(easyham_path)[:500]\n",
      "41/17: ham_dicts = [count_words(message) for message in all_ham]\n",
      "41/18:\n",
      "matrix_ham = pd.DataFrame(ham_dicts)\n",
      "matrix_ham = matrix_ham.transpose()\n",
      "41/19:\n",
      "matrix_ham['sum']=matrix_ham.sum(axis=1)\n",
      "matrix_ham=matrix_ham.loc[matrix_ham['sum']>1 ]\n",
      "41/20: matrix_ham['number_of_documents']=(matrix_ham.count(axis=1)-1)\n",
      "41/21: matrix_ham['documents_percentage']=matrix_ham['number_of_documents']/len(all_ham)\n",
      "41/22: matrix_ham['word_density'] = matrix_ham['sum']/matrix_ham['sum'].sum()\n",
      "41/23: matrix_ham.sort_values(by=['documents_percentage'],ascending=False).head()\n",
      "41/24:\n",
      "def classify_email(email, prior=0.5, c=10**-6):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = re.findall(r'\\w+', email[0])\n",
      "    if common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probabilities = [matrix_spam.at[word,'documents_percentage'] for word in common_words]\n",
      "        \n",
      "        return(prior)\n",
      "41/25: classify_email([all_spam[0]])\n",
      "41/26:\n",
      "def classify_email(email, prior=0.5, c=10**-6):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = re.findall(r'\\w+', email[0])\n",
      "    if len(common_words) < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probabilities = [matrix_spam.at[word,'documents_percentage'] for word in common_words]\n",
      "        \n",
      "        return(prior)\n",
      "41/27: classify_email([all_spam[0]])\n",
      "41/28: classify_email([all_spam[1]])\n",
      "41/29:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "41/30:\n",
      "def classify_email(email, prior=0.5, c=10**-6):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = re.findall(r'\\w+', email[0])\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probabilities_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        probability_uncommon = c**number_of_uncommon\n",
      "        return(prior)\n",
      "41/31:\n",
      "def classify_email(email, prior=0.5, c=10**-6):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = re.findall(r'\\w+', email[0])\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probabilities_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        probability_uncommon = c**number_of_uncommon\n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "41/32: classify_email([all_spam[1]])\n",
      "41/33:\n",
      "def classify_email(email, prior=0.5, c=10**-6):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = len(re.findall(r'\\w+', email[0]))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probabilities_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        probability_uncommon = c**number_of_uncommon\n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "41/34: classify_email([all_spam[1]])\n",
      "41/35:\n",
      "def classify_email(email, prior=0.5, c=10**-6):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = len(re.findall(r'\\w+', email[0]))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probabilities_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "41/36: classify_email([all_spam[1]])\n",
      "41/37:\n",
      "def classify_email(email, prior=0.5, c=10**-6):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = len(re.findall(r'\\w+', email[0]))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "41/38: classify_email([all_spam[1]])\n",
      "41/39:\n",
      "def classify_email(email, prior=0.5, c=10**-6):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = len(re.findall(r'\\w+', email[0]))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        print('probability_common: ', probability_common)\n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        print('probability_uncommon: ', probability_uncommon)\n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "41/40: classify_email([all_spam[1]])\n",
      "41/41: classify_email([all_spam[0]])\n",
      "41/42: classify_email([all_spam[0y7]])\n",
      "41/43: classify_email([all_spam[7]])\n",
      "41/44:\n",
      "def classify_email(email, prior=0.5, c=10**-3):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = len(re.findall(r'\\w+', email[0]))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        print('probability_common: ', probability_common)\n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        print('probability_uncommon: ', probability_uncommon)\n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "41/45: classify_email([all_spam[7]])\n",
      "41/46:\n",
      "def classify_email(email, prior=0.5, c=10**-1):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = len(re.findall(r'\\w+', email[0]))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        print('probability_common: ', probability_common)\n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        print('probability_uncommon: ', probability_uncommon)\n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "41/47: classify_email([all_spam[7]])\n",
      "41/48:\n",
      "def classify_email(email, prior=0.5, c=0.2):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = len(re.findall(r'\\w+', email[0]))\n",
      "    number_of_common_words = len(common_words)\n",
      "    print()\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        print('probability_common: ', probability_common)\n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        print('probability_uncommon: ', probability_uncommon)\n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "41/49: classify_email([all_spam[7]])\n",
      "41/50: classify_email([all_spam[70]])\n",
      "41/51:\n",
      "def classify_email(email, prior=0.5, c=0.3):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = len(re.findall(r'\\w+', email[0]))\n",
      "    number_of_common_words = len(common_words)\n",
      "    print()\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        print('probability_common: ', probability_common)\n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        print('probability_uncommon: ', probability_uncommon)\n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "41/52: all_spam[70]\n",
      "41/53: all_spam[7]\n",
      "41/54: count_words(all_spam[7])\n",
      "41/55: re.findall(r'\\w+', all_spam[7])\n",
      "41/56: re.findall(r'\\w+', all_spam[7]).sorted()\n",
      "41/57: sorted(re.findall(r'\\w+', all_spam[7]))\n",
      "41/58:\n",
      "def classify_email(email, prior=0.5, c=0.):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0]))\n",
      "    number_of_common_words = len(common_words)\n",
      "    print()\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        print('probability_common: ', probability_common)\n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        print('probability_uncommon: ', probability_uncommon)\n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "41/59:\n",
      "def classify_email(email, prior=0.5, c=0.):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    print()\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        print('probability_common: ', probability_common)\n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        print('probability_uncommon: ', probability_uncommon)\n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "41/60: classify_email([all_spam[70]])\n",
      "41/61: classify_email([all_spam[7]])\n",
      "41/62:\n",
      "def classify_email(email, prior=0.5, c=0.1):\n",
      "    matrix = create_matrix(email)\n",
      "    #print(matrix.head(10))\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    print()\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        print('probability_common: ', probability_common)\n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        print('probability_uncommon: ', probability_uncommon)\n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "41/63: classify_email([all_spam[7]])\n",
      "41/64: classify_email([all_spam[70]])\n",
      "41/65: all_spam[70]\n",
      "41/66: all_spam[170]\n",
      "43/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "43/2:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "43/3: stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
      "43/4:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "43/5:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "43/6:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix_spam.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "43/7: all_spam = get_messages(spam_path)\n",
      "43/8: matrix_spam = create_matrix(all_spam)\n",
      "43/9:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "43/10: all_spam = get_messages(spam_path)\n",
      "43/11: matrix_spam = create_matrix(all_spam)\n",
      "43/12: matrix_ham = create_matrix(all_ham)\n",
      "43/13: all_ham = get_messages(easyham_path)[:500]\n",
      "43/14: matrix_ham = create_matrix(all_ham)\n",
      "43/15:\n",
      "def classify_email_as(email, prior=0.5, c=0.1, corpus):\n",
      "    matrix = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    print()\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "43/16: classify_email([all_spam[70]])\n",
      "43/17:\n",
      "def classify_email(email, prior=0.5, c=0.1, matrix):\n",
      "    matrix = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    print()\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "43/18:\n",
      "def classify_email(email,matrix, prior=0.5, c=0.1, ):\n",
      "    matrix = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_spam.index).intersection(set(matrix.index))\n",
      "    print(common_words)\n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    print()\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_spam.at[word,'documents_percentage'] for word in common_words])\n",
      "        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "43/19:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.1, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "43/20: classify_email([all_spam[70]], matrix_spam)\n",
      "43/21: classify_email([all_spam[70]], matrix_ham)\n",
      "43/22:\n",
      "spam_or_ham(email):\n",
      "    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)\n",
      "    if result >0:\n",
      "        print('Spam!')\n",
      "    else:\n",
      "        print('Ham!')\n",
      "43/23:\n",
      "def spam_or_ham(email):\n",
      "    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)\n",
      "    if result >0:\n",
      "        print('Spam!')\n",
      "    else:\n",
      "        print('Ham!')\n",
      "43/24: spam_or_ham(all_spam[70])\n",
      "43/25:\n",
      "def spam_or_ham(email):\n",
      "    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)\n",
      "    print(result)\n",
      "    if result >0:\n",
      "        print('Spam!')\n",
      "    else:\n",
      "        print('Ham!')\n",
      "43/26: spam_or_ham(all_spam[70])\n",
      "43/27: spam_or_ham([all_spam[70]])\n",
      "43/28:\n",
      "for message in all_spam[:10]:\n",
      "    spam_or_ham([message])\n",
      "43/29:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.1, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "   # print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "43/30: classify_email([all_spam[70]], matrix_spam)\n",
      "43/31: classify_email([all_spam[70]], matrix_ham)\n",
      "43/32:\n",
      "def spam_or_ham(email):\n",
      "    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)\n",
      "    #print(result)\n",
      "    if result >0:\n",
      "        print('Spam!')\n",
      "    else:\n",
      "        print('Ham!')\n",
      "43/33: spam_or_ham([all_spam[70]])\n",
      "43/34:\n",
      "for message in all_spam[:10]:\n",
      "    spam_or_ham([message])\n",
      "43/35:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.1, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "   # print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "        #print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        #print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "43/36: classify_email([all_spam[70]], matrix_spam)\n",
      "43/37: classify_email([all_spam[70]], matrix_ham)\n",
      "43/38:\n",
      "def spam_or_ham(email):\n",
      "    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)\n",
      "    #print(result)\n",
      "    if result >0:\n",
      "        print('Spam!')\n",
      "    else:\n",
      "        print('Ham!')\n",
      "43/39: spam_or_ham([all_spam[70]])\n",
      "43/40:\n",
      "for message in all_spam[:10]:\n",
      "    spam_or_ham([message])\n",
      "43/41:\n",
      "for message in all_spam[:50]:\n",
      "    spam_or_ham([message])\n",
      "43/42:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "   # print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "        #print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        #print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "43/43: classify_email([all_spam[70]], matrix_spam)\n",
      "43/44: classify_email([all_spam[70]], matrix_ham)\n",
      "43/45:\n",
      "def spam_or_ham(email):\n",
      "    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)\n",
      "    #print(result)\n",
      "    if result >0:\n",
      "        print('Spam!')\n",
      "    else:\n",
      "        print('Ham!')\n",
      "43/46: spam_or_ham([all_spam[70]])\n",
      "43/47:\n",
      "for message in all_spam[:50]:\n",
      "    spam_or_ham([message])\n",
      "43/48: [spam_or_ham([message]) for message in all_spam]\n",
      "43/49:\n",
      "def spam_or_ham(email):\n",
      "    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)\n",
      "    #print(result)\n",
      "    if result >0:\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        print 'Ham!'\n",
      "43/50:\n",
      "def spam_or_ham(email):\n",
      "    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)\n",
      "    #print(result)\n",
      "    if result >0:\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "43/51: [spam_or_ham([message]) for message in all_spam]\n",
      "43/52: spam_test_result = [spam_or_ham([message]) for message in all_spam]\n",
      "43/53: Counter(spam_test_result)\n",
      "43/54:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "   # print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "        #print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        #print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "43/55: classify_email([all_spam[70]], matrix_spam)\n",
      "43/56: classify_email([all_spam[70]], matrix_ham)\n",
      "43/57:\n",
      "def spam_or_ham(email):\n",
      "    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)\n",
      "    #print(result)\n",
      "    if result >0:\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "43/58: spam_test_result = [spam_or_ham([message]) for message in all_spam]\n",
      "43/59: Counter(spam_test_result)\n",
      "43/60:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.1, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "   # print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "        #print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        #print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "43/61: classify_email([all_spam[70]], matrix_spam)\n",
      "43/62: classify_email([all_spam[70]], matrix_ham)\n",
      "43/63:\n",
      "def spam_or_ham(email):\n",
      "    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)\n",
      "    #print(result)\n",
      "    if result >0:\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "43/64: spam_test_result = [spam_or_ham([message]) for message in all_spam]\n",
      "43/65: Counter(spam_test_result)\n",
      "43/66:\n",
      "def classify_email(email,matrix_corpus, prior=0.2, c=0.1, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "   # print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "        #print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        #print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "43/67: classify_email([all_spam[70]], matrix_spam)\n",
      "43/68: classify_email([all_spam[70]], matrix_ham)\n",
      "43/69:\n",
      "def spam_or_ham(email):\n",
      "    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)\n",
      "    #print(result)\n",
      "    if result >0:\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "43/70: spam_test_result = [spam_or_ham([message]) for message in all_spam]\n",
      "43/71: Counter(spam_test_result)\n",
      "43/72:\n",
      "def classify_email(email,matrix_corpus, prior=0.2, c=0.1, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "   # print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "        #print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        #print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior-probability_common*probability_uncommon)\n",
      "43/73: classify_email([all_spam[70]], matrix_spam)\n",
      "43/74: classify_email([all_spam[70]], matrix_ham)\n",
      "43/75:\n",
      "def spam_or_ham(email):\n",
      "    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)\n",
      "    #print(result)\n",
      "    if result >0:\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "43/76: spam_test_result = [spam_or_ham([message]) for message in all_spam]\n",
      "43/77: Counter(spam_test_result)\n",
      "43/78:\n",
      "def classify_email(email,matrix_corpus, prior=0.2, c=0.1, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "   # print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "        #print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        #print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "43/79: classify_email([all_spam[70]], matrix_spam)\n",
      "43/80: classify_email([all_spam[70]], matrix_ham)\n",
      "43/81:\n",
      "def spam_or_ham(email):\n",
      "    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)\n",
      "    #print(result)\n",
      "    if result >0:\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "43/82: spam_test_result = [spam_or_ham([message]) for message in all_spam]\n",
      "43/83: Counter(spam_test_result)\n",
      "43/84: hard_ham = get_messages(hardham_path)\n",
      "43/85: matrix_hardham = create_matrix(hard_ham)\n",
      "43/86: len(hard_ham)\n",
      "43/87: hardham_result = [spam_or_ham([message]) for message in hard_ham]\n",
      "43/88: Counter(hardham_result)\n",
      "43/89: hardham_correct = hardham_counter['Ham!']/len(hard_ham)\n",
      "43/90: hardham_counter = Counter(hardham_result)\n",
      "43/91: hardham_counter = Counter(hardham_result)\n",
      "43/92: hardham_correct = hardham_counter['Ham!']/len(hard_ham)\n",
      "43/93: hardham_correct\n",
      "44/1:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham[message] for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    print(classification_counter)\n",
      "44/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "44/3:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "44/4: stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
      "44/5:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "44/6:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "44/7:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "44/8: all_spam = get_messages(spam_path)\n",
      "44/9: matrix_spam = create_matrix(all_spam)\n",
      "44/10: all_ham = get_messages(easyham_path)[:500]\n",
      "44/11: matrix_ham = create_matrix(all_ham)\n",
      "44/12:\n",
      "def classify_email(email,matrix_corpus, prior=0.2, c=0.1, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "   # print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "        #print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        #print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "44/13: classify_email([all_spam[70]], matrix_spam)\n",
      "44/14: classify_email([all_spam[70]], matrix_ham)\n",
      "44/15:\n",
      "def spam_or_ham(email):\n",
      "    result = classify_email(email, matrix_spam) - classify_email(email, matrix_ham)\n",
      "    #print(result)\n",
      "    if result >0:\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "44/16: hard_ham = get_messages(hardham_path)\n",
      "44/17: matrix_hardham = create_matrix(hard_ham)\n",
      "44/18:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham[message] for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    print(classification_counter)\n",
      "44/19: evaluate_results(hard_ham)\n",
      "44/20: evaluate_results(hard_ham, _)\n",
      "44/21:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham[message] for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    #print(classification_counter)\n",
      "44/22: evaluate_results(hard_ham, _)\n",
      "44/23:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    #print(classification_counter)\n",
      "44/24:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    print(classification_counter)\n",
      "44/25: evaluate_results(hard_ham, _)\n",
      "44/26: evaluate_results(all_spam, _)\n",
      "44/27: evaluate_results(all_ham, _)\n",
      "45/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "45/2:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "45/3: stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
      "45/4:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "45/5:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "45/6:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "45/7: all_spam = get_messages(spam_path)\n",
      "45/8: matrix_spam = create_matrix(all_spam)\n",
      "45/9: all_ham = get_messages(easyham_path)[:500]\n",
      "45/10: matrix_ham = create_matrix(all_ham)\n",
      "45/11:\n",
      "def classify_email(email,matrix_corpus, prior=0.2, c=0.1, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "   # print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "        #print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        #print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "45/12:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "45/13: hard_ham = get_messages(hardham_path)\n",
      "45/14: matrix_hardham = create_matrix(hard_ham)\n",
      "45/15:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "45/16: evaluate_results(hard_ham, _)\n",
      "45/17: evaluate_results(all_spam, _)\n",
      "45/18: evaluate_results(all_ham, _)\n",
      "45/19: print(all_ham[0])\n",
      "45/20: count_words([all_ham[0]])\n",
      "45/21: count_words(all_ham[0])\n",
      "45/22:\n",
      "def classify_email(email,matrix_corpus, prior=0.2, c=0.1, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "45/23: classify_email(all_ham[0], matrix_spam)\n",
      "45/24: classify_email([all_ham[0]], matrix_spam)\n",
      "45/25: classify_email([all_ham[0]], matrix_ham)\n",
      "45/26: matrix_spam.at(['rules', documents_percentage])\n",
      "45/27: matrix_spam.at(['rules', 'documents_percentage'])\n",
      "45/28: matrix_spam.at['rules', 'documents_percentage']\n",
      "45/29: matrix_ham.at['rules', 'documents_percentage']\n",
      "45/30: matrix_spam.at['want', 'documents_percentage']\n",
      "45/31: matrix_ham.at['want', 'documents_percentage']\n",
      "45/32: classify_email([all_ham[0]], matrix_ham)-classify_email([all_ham[0]], matrix_spam)\n",
      "45/33: classify_email([all_ham[1]], matrix_ham)-classify_email([all_ham[1]], matrix_spam)\n",
      "45/34:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "45/35: evaluate_results(all_ham, _)\n",
      "45/36:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "45/37: evaluate_results(hard_ham, _)\n",
      "45/38: evaluate_results(all_spam, _)\n",
      "45/39: evaluate_results(all_ham, _)\n",
      "45/40:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "45/41: evaluate_results(all_spam, _)\n",
      "45/42: matrix_spam.at['html', 'documents_percentage']\n",
      "45/43:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.0001, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "45/44:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "45/45: hard_ham = get_messages(hardham_path)\n",
      "45/46: matrix_hardham = create_matrix(hard_ham)\n",
      "45/47:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "45/48: evaluate_results(hard_ham, _)\n",
      "45/49: evaluate_results(all_spam, _)\n",
      "45/50: evaluate_results(all_ham, _)\n",
      "45/51:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "45/52: stop_words = stopwords.words('english')\n",
      "45/53:\n",
      "import nltk\n",
      "nltk.download()\n",
      "45/54: stop_words = stopwords.words('english')\n",
      "45/55:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "45/56: stop_words = stopwords.words('english')\n",
      "45/57: stop_words = stopwords.words('english')\n",
      "45/58:\n",
      "import nltk\n",
      "nltk.download()\n",
      "45/59: stop_words = stopwords.words('english')\n",
      "45/60: print(stopwords)\n",
      "45/61: print(stop_words)\n",
      "45/62:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "45/63:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "45/64:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "45/65: all_spam = get_messages(spam_path)\n",
      "45/66: matrix_spam = create_matrix(all_spam)\n",
      "45/67: all_ham = get_messages(easyham_path)[:500]\n",
      "45/68: matrix_ham = create_matrix(all_ham)\n",
      "45/69:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and word.lower() not in stop_words]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "45/70:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "45/71: all_spam = get_messages(spam_path)\n",
      "45/72: matrix_spam = create_matrix(all_spam)\n",
      "45/73: all_ham = get_messages(easyham_path)[:500]\n",
      "45/74: matrix_ham = create_matrix(all_ham)\n",
      "45/75:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.0001, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "45/76:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "45/77: hard_ham = get_messages(hardham_path)\n",
      "45/78: matrix_hardham = create_matrix(hard_ham)\n",
      "45/79:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "45/80: evaluate_results(hard_ham, _)\n",
      "45/81: evaluate_results(all_spam, _)\n",
      "45/82: evaluate_results(all_ham, _)\n",
      "45/83:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "45/84:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "45/85: hard_ham = get_messages(hardham_path)\n",
      "45/86: matrix_hardham = create_matrix(hard_ham)\n",
      "45/87:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "45/88: evaluate_results(hard_ham, _)\n",
      "45/89: evaluate_results(all_spam, _)\n",
      "45/90: evaluate_results(all_ham, _)\n",
      "45/91:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "45/92: evaluate_results(all_spam, _)\n",
      "45/93:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "45/94: help(nltk.FreqDist)\n",
      "45/95:\n",
      "import nltk\n",
      "nltk.download()\n",
      "47/1:\n",
      "import nltk\n",
      "nltk.download()\n",
      "47/2:\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "47/3: fdist = FreqDist()\n",
      "47/4:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "47/5:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "47/6: stop_words = stopwords.words('english')\n",
      "47/7:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "47/8:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and word.lower() not in stop_words]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "47/9:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "47/10: all_spam = get_messages(spam_path)\n",
      "47/11: matrix_spam = create_matrix(all_spam)\n",
      "47/12: all_ham = get_messages(easyham_path)[:500]\n",
      "47/13: matrix_ham = create_matrix(all_ham)\n",
      "47/14:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "47/15:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "47/16: hard_ham = get_messages(hardham_path)\n",
      "47/17: matrix_hardham = create_matrix(hard_ham)\n",
      "47/18:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "47/19: fdist = FreqDist(words.lower() for word in word_tokenize(all_spam[0]))\n",
      "47/20: fdist = FreqDist(word.lower() for word in word_tokenize(all_spam[0]))\n",
      "47/21: fdist\n",
      "47/22: word_tokenize(all_spam[0])\n",
      "47/23: print(fdist)\n",
      "47/24: word_tokenize(all_spam[0])\n",
      "47/25: cout_words(all_spam[0])\n",
      "47/26: count_words(all_spam[0])\n",
      "47/27: len(count_words(all_spam[0]).keys())\n",
      "47/28: x=set(fdist)\n",
      "47/29: print(x)\n",
      "47/30: print(len(x)\n",
      "47/31: print(len(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/32: fdist = FreqDist(word.lower() for word in word_tokenize(all_spam[0]))\n",
      "47/33: fdist\n",
      "47/34: fdist['home']\n",
      "47/35: fdist.keys()\n",
      "47/36: print(len(fdist.keys()))\n",
      "47/37: len(word_tokenize(all_spam[0]))\n",
      "47/38: fdist.plot()\n",
      "47/39: help(nltk.tokenize)\n",
      "47/40: len(fdist.keys())\n",
      "47/41: word_tokenize(all_spam[0])\n",
      "47/42: len(word_tokenize(all_spam[0]))\n",
      "47/43: fdist = FreqDist(word for word in word_tokenize(all_spam[0]))\n",
      "47/44: fdist.plot()\n",
      "47/45: fdist['home']\n",
      "47/46: fdist.keys()\n",
      "47/47: len(fdist.keys())\n",
      "47/48: len(word_tokenize(all_spam[0]))\n",
      "47/49: len(count_words(all_spam[0]).keys())\n",
      "47/50: help(nltk.tokenize)\n",
      "47/51: len(word_tokenize(all_spam[0]))\n",
      "47/52:\n",
      "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "47/53: wordpunt_tokenize(all_spam[0])\n",
      "47/54: wordpunct_tokenize(all_spam[0])\n",
      "47/55: len(wordpunct_tokenize(all_spam[0]))\n",
      "48/1: stowp_words_pl = stopwords.words('polish')\n",
      "48/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "48/3:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "48/4: stop_words = stopwords.words('english')\n",
      "48/5: stowp_words_pl = stopwords.words('polish')\n",
      "48/6:\n",
      "import nltk\n",
      "nltk.download()\n",
      "48/7: help(stopwords.words)\n",
      "48/8: stowp_words_pl = stopwords.words('polish')\n",
      "48/9: stop_words_pl\n",
      "48/10: stop_words_pl = stopwords.words('polish')\n",
      "48/11: stop_words_pl\n",
      "48/12: stopwords_pl = open('/Użytkownicy/wioletanytko/nltk_data/corpora/stopwords/polish')\n",
      "48/13: stopwords_pl = open(r'/Użytkownicy/wioletanytko/nltk_data/corpora/stopwords/polish')\n",
      "48/14: stopwords_pl = open(r'/Użytkownicy/wioletanytko/nltk_data/corpora/stopwords/german')\n",
      "48/15: stopwords_pl = open('/Użytkownicy/wioletanytko//nltk_data/corpora/stopwords/german')\n",
      "48/16: stopwords_pl = open('/Użytkownicy/wioletanytko/\\nltk_data/corpora/stopwords/german')\n",
      "48/17: stopwords_pl = open(r\"/Użytkownicy/wioletanytko/nltk_data/corpora/stopwords/german\")\n",
      "48/18: stopwords_pl = open(r\"/users/wioletanytko/nltk_data/corpora/stopwords/german\")\n",
      "48/19: stopwords_pl = open(r\"/users/wioletanytko/nltk_data/corpora/stopwords/polish\")\n",
      "48/20: stopwords_txt = stopwords.read()\n",
      "48/21: stopwords_pl = open(r\"/users/wioletanytko/nltk_data/corpora/stopwords/polish\", w)\n",
      "48/22: help(open)\n",
      "48/23: stopwords_pl = open(r\"/users/wioletanytko/nltk_data/corpora/stopwords/polish\", 'w')\n",
      "48/24: stopwords_txt = stopwords_pl.read()\n",
      "48/25: stopwords_pl = open(r\"/users/wioletanytko/nltk_data/corpora/stopwords/polish\", 'w', encoding='UTF-8')\n",
      "48/26: stopwords_txt = stopwords_pl.read()\n",
      "48/27: stopwords_pl = open(r\"/users/wioletanytko/nltk_data/corpora/stopwords/polish\", 'r+w', encoding='UTF-8')\n",
      "48/28: stopwords_pl = open(r\"/users/wioletanytko/nltk_data/corpora/stopwords/polish\", 'r', encoding='UTF-8')\n",
      "48/29: stopwords_txt = stopwords_pl.read()\n",
      "48/30: stopwords_txt = stopwords_pl.read()\n",
      "48/31: stopwords_pl.close()\n",
      "48/32: stopwords_txt = stopwords_txt.replace('\\n', '\\s')\n",
      "48/33: stopwords_pl = open(r\"/users/wioletanytko/nltk_data/corpora/stopwords/polish\", 'w', encoding='UTF-8')\n",
      "48/34: stopwords_pl.write(stopwords_txt)\n",
      "48/35: stopwords_pl.close()\n",
      "48/36: stop_words_pl = stopwords.words('polish')\n",
      "48/37: stop_words_pl\n",
      "48/38: stopwords_txt\n",
      "48/39: x='a\\nb\\nc'\n",
      "48/40: print(x)\n",
      "48/41: x\n",
      "48/42: x = x.replace('\\n','\\s')\n",
      "48/43: x\n",
      "48/44: print(x)\n",
      "48/45: a = ['a','b','c']\n",
      "48/46: a\n",
      "48/47: a = ['a\\n','b\\n','c\\n']\n",
      "48/48: a\n",
      "48/49: print(a)\n",
      "48/50: stopwords_pl = open(r\"/users/wioletanytko/nltk_data/corpora/stopwords/polish\", 'w+', encoding='UTF-8')\n",
      "48/51: stopwords_txt = stopwords_pl.read()\n",
      "49/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "49/2:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "49/3: stop_words = stopwords.words('english')\n",
      "49/4: stop_words_pl = stopwords.words('polish')\n",
      "49/5:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "49/6:\n",
      "def count_words(message):\n",
      "    words_in_message =  set(re.findall(r'\\w+', message))\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and word.lower() not in stop_words]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "49/7:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "49/8: all_spam = get_messages(spam_path)\n",
      "49/9: matrix_spam = create_matrix(all_spam)\n",
      "49/10: all_ham = get_messages(easyham_path)[:500]\n",
      "49/11: matrix_ham = create_matrix(all_ham)\n",
      "49/12:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "49/13:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "49/14: hard_ham = get_messages(hardham_path)\n",
      "49/15: matrix_hardham = create_matrix(hard_ham)\n",
      "49/16:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "49/17: evaluate_results(hard_ham, _)\n",
      "49/18: evaluate_results(all_spam, _)\n",
      "50/1:\n",
      "def count_words(message):\n",
      "    words_in_message =  set(re.findall(r'\\w+', message))\n",
      "    words_in_message = [word.lower() for word in words_in_message if len(word)>2 not word.isnumeric() and word.lower() not in stop_words]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "50/2:\n",
      "def count_words(message):\n",
      "    words_in_message =  set(re.findall(r'\\w+', message))\n",
      "    words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "50/3:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "50/4:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "50/5: stop_words = stopwords.words('english')\n",
      "50/6: stop_words_pl = stopwords.words('polish')\n",
      "50/7:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "50/8:\n",
      "def count_words(message):\n",
      "    words_in_message =  set(re.findall(r'\\w+', message))\n",
      "    words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "50/9:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "50/10:\n",
      "def count_words(message):\n",
      "    #words_in_message =  set(re.findall(r'\\w+', message))\n",
      "    #words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]\n",
      "    #occurances = Counter(words_in_message)\n",
      "    occurances = FreqDist(word_tokenize(message))\n",
      "    return occurances\n",
      "50/11:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "50/12: all_spam = get_messages(spam_path)\n",
      "50/13: matrix_spam = create_matrix(all_spam)\n",
      "50/14: all_ham = get_messages(easyham_path)[:500]\n",
      "50/15: matrix_ham = create_matrix(all_ham)\n",
      "50/16:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "50/17:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "50/18: hard_ham = get_messages(hardham_path)\n",
      "50/19: matrix_hardham = create_matrix(hard_ham)\n",
      "50/20:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "50/21: evaluate_results(all_spam, _)\n",
      "50/22:\n",
      "def count_words(message):\n",
      "    #words_in_message =  set(re.findall(r'\\w+', message))\n",
      "    #words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]\n",
      "    #occurances = Counter(words_in_message)\n",
      "    occurances = set(word_tokenize(message))\n",
      "    return dict.fromkeys(occurances, 1)\n",
      "50/23:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "50/24: all_spam = get_messages(spam_path)\n",
      "50/25: matrix_spam = create_matrix(all_spam)\n",
      "50/26: all_ham = get_messages(easyham_path)[:500]\n",
      "50/27: matrix_ham = create_matrix(all_ham)\n",
      "50/28:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "50/29:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "50/30: hard_ham = get_messages(hardham_path)\n",
      "50/31: matrix_hardham = create_matrix(hard_ham)\n",
      "50/32:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "50/33: evaluate_results(all_spam, _)\n",
      "50/34: matrix_spam.head()\n",
      "50/35: matrix_spam.head(20)\n",
      "50/36:\n",
      "def count_words(message):\n",
      "    #words_in_message =  set(re.findall(r'\\w+', message))\n",
      "    #words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]\n",
      "    #occurances = Counter(words_in_message)\n",
      "    occurances = set([word.lower for word in word_tokenize(message) if len(word)>2 and not word.isnumeric and word.lower() not in stop_words])\n",
      "    return dict.fromkeys(occurances, 1)\n",
      "50/37: matrix_spam = create_matrix(all_spam)\n",
      "50/38: matrix_ham = create_matrix(all_ham)\n",
      "50/39:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "50/40:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "50/41: hard_ham = get_messages(hardham_path)\n",
      "50/42: matrix_hardham = create_matrix(hard_ham)\n",
      "50/43:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "50/44: evaluate_results(all_spam, _)\n",
      "50/45: matrix_spam.head(20)\n",
      "50/46: word_tokenize(all_spam[0])\n",
      "50/47: set(word_tokenize(all_spam[0]))\n",
      "50/48: dict.fromkeys(set(word_tokenize(all_spam[0])), 1)\n",
      "50/49: all_spam = get_messages(spam_path)\n",
      "50/50: matrix_spam = create_matrix(all_spam)\n",
      "50/51: matrix_spam.head(20)\n",
      "50/52:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "50/53: all_spam = get_messages(spam_path)\n",
      "50/54: matrix_spam = create_matrix(all_spam)\n",
      "50/55:\n",
      "def count_words(message):\n",
      "    #words_in_message =  set(re.findall(r'\\w+', message))\n",
      "    #words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]\n",
      "    #occurances = Counter(words_in_message)\n",
      "    occurances = set([word.lower for word in word_tokenize(message) if len(word)>2 and not word.isnumeric and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    print('result_of_count_words:', result)\n",
      "    return result\n",
      "50/56:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "50/57: all_spam = get_messages(spam_path)\n",
      "50/58: matrix_spam = create_matrix(all_spam)\n",
      "50/59:\n",
      "def count_words(message):\n",
      "    #words_in_message =  set(re.findall(r'\\w+', message))\n",
      "    #words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]\n",
      "    #occurances = Counter(words_in_message)\n",
      "    occurances = set([word.lower for word in word_tokenize(message) if len(word)>2 and not word.isnumeric and word.lower() not in stop_words])\n",
      "    print('occurances: ', occurances)\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    print('result_of_count_words:', result)\n",
      "    return result\n",
      "50/60: matrix_spam = create_matrix(all_spam)\n",
      "50/61:\n",
      "def count_words(message):\n",
      "    #words_in_message =  set(re.findall(r'\\w+', message))\n",
      "    #words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]\n",
      "    #occurances = Counter(words_in_message)\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words])\n",
      "    print('occurances: ', occurances)\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    print('result_of_count_words:', result)\n",
      "    return result\n",
      "50/62: matrix_spam = create_matrix(all_spam)\n",
      "50/63:\n",
      "def count_words(message):\n",
      "    #words_in_message =  set(re.findall(r'\\w+', message))\n",
      "    #words_in_message = [word.lower() for word in words_in_message if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words]\n",
      "    #occurances = Counter(words_in_message)\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words])\n",
      "  #  print('occurances: ', occurances)\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "  #  print('result_of_count_words:', result)\n",
      "    return result\n",
      "50/64: evaluate_results(all_spam, _)\n",
      "50/65: all_ham = get_messages(easyham_path)[:500]\n",
      "50/66: matrix_ham = create_matrix(all_ham)\n",
      "50/67:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "50/68:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "50/69: hard_ham = get_messages(hardham_path)\n",
      "50/70: matrix_hardham = create_matrix(hard_ham)\n",
      "50/71:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "50/72: evaluate_results(all_spam, _)\n",
      "50/73: %time evaluate_results(all_spam, _)\n",
      "50/74: matrix_spam.head(20)\n",
      "50/75: all_ham = get_messages(easyham_path)[:500]\n",
      "50/76: matrix_ham.head()\n",
      "50/77:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "50/78:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "50/79: hard_ham = get_messages(hardham_path)\n",
      "50/80: matrix_hardham = create_matrix(hard_ham)\n",
      "50/81:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "50/82: %time evaluate_results(all_spam, _)\n",
      "50/83: evaluate_results(hard_ham, _)\n",
      "50/84: create_matrix(all_spam[0])\n",
      "50/85: create_matrix([all_spam[0]])\n",
      "50/86: cre\n",
      "50/87: create_matrix([all_spam[0]])\n",
      "50/88: cre\n",
      "50/89: create_matrix([all_spam[1]])\n",
      "50/90: all_spam\n",
      "50/91: create_matrix([all_spam[9]])\n",
      "50/92: all_spam\n",
      "50/93: create_matrix([all_spam[9]])\n",
      "50/94: create_matrix([all_spam[9]])\n",
      "50/95:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "50/96: create_matrix([all_spam[9]])\n",
      "51/1: 'a b c d'.split(' ')\n",
      "51/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "51/3: ps = PorterStemmer()\n",
      "51/4:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "51/5:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "51/6: dict.fromkeys(set(word_tokenize(all_spam[0])), 1)\n",
      "51/7:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "51/8:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "51/9: ps = PorterStemmer()\n",
      "51/10: stop_words = stopwords.words('english')\n",
      "51/11: stop_words_pl = stopwords.words('polish')\n",
      "51/12:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "51/13:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and not word.isnumeric() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "51/14:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/15: all_spam = get_messages(spam_path)\n",
      "51/16: matrix_spam = create_matrix(all_spam)\n",
      "51/17: matrix_spam.head(20)\n",
      "51/18: all_ham = get_messages(easyham_path)[:500]\n",
      "51/19: matrix_ham = create_matrix(all_ham)\n",
      "51/20:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and word.asalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "51/21:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/22: all_spam = get_messages(spam_path)\n",
      "51/23: matrix_spam = create_matrix(all_spam)\n",
      "51/24:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "51/25: dict.fromkeys(set(word_tokenize(all_spam[0])), 1)\n",
      "51/26:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/27: all_spam = get_messages(spam_path)\n",
      "51/28: matrix_spam = create_matrix(all_spam)\n",
      "51/29: matrix_spam.head(20)\n",
      "51/30: all_ham = get_messages(easyham_path)[:500]\n",
      "51/31: all_ham = get_messages(easyham_path)[:500]\n",
      "51/32: matrix_ham = create_matrix(all_ham)\n",
      "51/33: matrix_ham.head()\n",
      "51/34:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "51/35:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "51/36: hard_ham = get_messages(hardham_path)\n",
      "51/37: matrix_hardham = create_matrix(hard_ham)\n",
      "51/38:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "51/39: evaluate_results(hard_ham, _)\n",
      "51/40: %time evaluate_results(all_spam, _)\n",
      "51/41: count_words(all_spam[0])\n",
      "51/42: matrix_ham\n",
      "51/43: create_matrix([all_spam[9]])\n",
      "51/44: create_matrix([all_spam[0]])\n",
      "51/45:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/46: create_matrix([all_spam[0]])\n",
      "51/47: matrix_spam.head()\n",
      "51/48:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix.head()\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/49: create_matrix([all_spam[0]])\n",
      "51/50:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    print(dicts)\n",
      "    matrix = pd.DataFrame.from_dict(dicts)\n",
      "    matrix.head()\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/51: create_matrix([all_spam[0]])\n",
      "51/52: d1 = dict(a=1, b=2)\n",
      "51/53: df = pd.DataFrame([d1])\n",
      "51/54: df\n",
      "51/55:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix.head()\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/56: create_matrix([all_spam[0]])\n",
      "51/57: d2 = dict(a=3, b=4)\n",
      "51/58: df = pd.DataFrame([d1, d2])\n",
      "51/59: df\n",
      "51/60:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    print(matrix)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/61: all_spam = get_messages(spam_path)\n",
      "51/62: create_matrix([all_spam[0]])\n",
      "51/63:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix.head()\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/64: create_matrix([all_spam[0]])\n",
      "51/65:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix.head()\n",
      "    matrix = matrix.transpose()\n",
      "    matrix.head()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/66: create_matrix([all_spam[0]])\n",
      "51/67:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix.head()\n",
      "    matrix = matrix.transpose()\n",
      "    matrix.head\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/68: create_matrix([all_spam[0]])\n",
      "51/69: xxx = create_matrix([all_spam[0]])\n",
      "51/70: xxx\n",
      "51/71: xxx.head()\n",
      "51/72:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    print(matrix)\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/73: xxx = create_matrix([all_spam[0]])\n",
      "51/74: xxx.head()\n",
      "51/75:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "51/76:\n",
      "import nltk\n",
      "nltk.download()\n",
      "51/77:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "51/78: ps = PorterStemmer()\n",
      "51/79: stop_words = stopwords.words('english')\n",
      "51/80: stop_words_pl = stopwords.words('polish')\n",
      "51/81:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "51/82:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "51/83:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    print(matrix)\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/84: all_spam = get_messages(spam_path)\n",
      "51/85: matrix_spam = create_matrix(all_spam)\n",
      "51/86: all_ham = get_messages(easyham_path)[:500]\n",
      "51/87: matrix_ham = create_matrix(all_ham)\n",
      "51/88:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "51/89:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "51/90: hard_ham = get_messages(hardham_path)\n",
      "51/91: matrix_hardham = create_matrix(hard_ham)\n",
      "51/92:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "51/93: #evaluate_results(hard_ham, _)\n",
      "51/94: %time evaluate_results(all_spam, _)\n",
      "51/95: #evaluate_results(all_ham, _)\n",
      "51/96: xxx = create_matrix([all_spam[0]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/97: xxx.head()\n",
      "51/98: xxx.head(10)\n",
      "51/99: xxx = create_matrix([all_spam[0]])\n",
      "51/100: xxx = create_matrix([all_spam[1]])\n",
      "51/101: xxx.head(10)\n",
      "51/102: xxx = create_matrix([all_spam[1]])\n",
      "51/103: xxx.head(10)\n",
      "51/104: xxx\n",
      "51/105:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "51/106:\n",
      "#import nltk\n",
      "#nltk.download()\n",
      "51/107:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "51/108: ps = PorterStemmer()\n",
      "51/109: stop_words = stopwords.words('english')\n",
      "51/110: stop_words_pl = stopwords.words('polish')\n",
      "51/111:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "51/112:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "51/113:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/114: all_spam = get_messages(spam_path)\n",
      "51/115: matrix_spam = create_matrix(all_spam)\n",
      "51/116: all_ham = get_messages(easyham_path)[:500]\n",
      "51/117: matrix_ham = create_matrix(all_ham)\n",
      "51/118:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "51/119:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "51/120: hard_ham = get_messages(hardham_path)\n",
      "51/121: matrix_hardham = create_matrix(hard_ham)\n",
      "51/122:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "51/123: #evaluate_results(hard_ham, _)\n",
      "51/124: %time evaluate_results(all_spam, _)\n",
      "51/125:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/126:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "51/127:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "51/128: %time evaluate_results(all_spam, _)\n",
      "51/129: xxx = create_matrix([all_spam[1]])\n",
      "51/130: xxx.\n",
      "51/131: xxx\n",
      "51/132: xxx = create_message_matrix([all_spam[1]])\n",
      "51/133: xxx\n",
      "51/134: evaluate_results(hard_ham, _)\n",
      "52/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "52/2:\n",
      "#import nltk\n",
      "#nltk.download()\n",
      "52/3:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "52/4: ps = PorterStemmer()\n",
      "52/5: stop_words = stopwords.words('english')\n",
      "52/6: stop_words_pl = stopwords.words('polish')\n",
      "52/7:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "52/8:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "52/9:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/10:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/11: all_spam = get_messages(spam_path)\n",
      "52/12:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=0)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=0)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/13: all_spam = get_messages(spam_path)\n",
      "52/14: matrix_spam = create_matrix(all_spam)\n",
      "52/15:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=0)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=0)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/16:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=0)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=0)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/17: all_spam = get_messages(spam_path)\n",
      "52/18: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/19:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=0)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=0)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/20:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=0)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=0)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/21: all_spam = get_messages(spam_path)\n",
      "52/22: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/23: %time evaluate_results(all_spam, _)\n",
      "52/24:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "52/25:\n",
      "#import nltk\n",
      "#nltk.download()\n",
      "52/26:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "52/27: ps = PorterStemmer()\n",
      "52/28: stop_words = stopwords.words('english')\n",
      "52/29: stop_words_pl = stopwords.words('polish')\n",
      "52/30:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "52/31:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "52/32:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=0)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=0)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/33:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=0)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=0)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/34: all_spam = get_messages(spam_path)\n",
      "52/35: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/36: all_ham = get_messages(easyham_path)[:500]\n",
      "52/37: matrix_ham = create_matrix(all_ham)\n",
      "52/38: matrix_ham = create_corpus_matrix(all_ham)\n",
      "52/39:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "52/40:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "52/41: hard_ham = get_messages(hardham_path)\n",
      "52/42: matrix_hardham = create_matrix(hard_ham)\n",
      "52/43: matrix_hardham = create_corpus_matrix(hard_ham)\n",
      "52/44:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "52/45: #evaluate_results(hard_ham, _)\n",
      "52/46: %time evaluate_results(all_spam, _)\n",
      "52/47: #evaluate_results(all_ham, _)\n",
      "52/48: xxx = create_message_matrix([all_spam[1]])\n",
      "52/49: xxx\n",
      "52/50: evaluate_results(hard_ham, _)\n",
      "52/51: matrix_all_spam.head()\n",
      "52/52: matrix_spam.head()\n",
      "52/53:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/54:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/55: all_spam = get_messages(spam_path)\n",
      "52/56: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/57: matrix_spam.head()\n",
      "52/58: matrix_spam.head(10)\n",
      "52/59:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame.from_dict(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/60: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/61: matrix_spam.head(10)\n",
      "52/62:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame.from_dict(dicts, orient='index')\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/63: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/64: d1 = dict(a=1, b=2)\n",
      "52/65: d2=dict(b=3, c=4)\n",
      "52/66: pd1 = pd.DataFrame(d1)\n",
      "52/67: pd1 = pd.DataFrame([d1])\n",
      "52/68: pd1.head()\n",
      "52/69: pd1 = pd.DataFrame(d1, orient='index')\n",
      "52/70: pd1 = pd.DataFrame.from_dict(d1, orient='index')\n",
      "52/71: pd1.head()\n",
      "52/72: pd2.DataFrame.from_dict(d2, orient='index')\n",
      "52/73: pd2 = pd.DataFrame.from_dict(d2, orient='index')\n",
      "52/74: pd2\n",
      "52/75: pd1.join(pd2)\n",
      "52/76: pd1.merge(pd2)\n",
      "52/77: pd1.join(pd2, how='outer')\n",
      "52/78: pd1.join(pd2, lsuffix='_caller', rsuffix='_other')\n",
      "52/79: x=pd.DataFrame.join([pd1, pd2])\n",
      "52/80: x=pd.DataFrame.join(pd1, pd2)\n",
      "52/81: pd1 = pd.DataFrame.from_dict(d1)\n",
      "52/82: pd1 = pd.DataFrame(d1)\n",
      "52/83: pd = pd.DataFrame([d1,d2])\n",
      "52/84: pd\n",
      "52/85:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "52/86: pd3 = pd.DataFrame([d1,d2])\n",
      "52/87: pd3\n",
      "52/88: pd3.loc['summa']= pd3.count()\n",
      "52/89: pd3\n",
      "52/90:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix.loc['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/91: all_spam = get_messages(spam_path)\n",
      "52/92: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/93:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/94: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/95:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    #matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/96: all_spam = get_messages(spam_path)\n",
      "52/97: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/98:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    #matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/99: all_spam = get_messages(spam_path)\n",
      "52/100: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/101:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/102:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/103: all_spam = get_messages(spam_path)\n",
      "52/104: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/105: %time evaluate_results(all_spam, _)\n",
      "52/106: matrix_spam.head(10)\n",
      "52/107:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    matrix=matrix[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/108: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/109: %time evaluate_results(all_spam, _)\n",
      "52/110: matrix_spam.head(10)\n",
      "52/111: pd3.loc['number_of_documents']=pd3.count()-1\n",
      "52/112: pd3\n",
      "52/113:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "   # matrix.loc['summa']=matrix.sum()\n",
      "   # matrix=matrix[matrix['sum']>1 ]\n",
      "    \n",
      "   # matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/114: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/115: matrix_spam.head()\n",
      "52/116: matrix_spam.head(10)\n",
      "52/117:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "52/118:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "52/119: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/120: matrix_spam.head(10)\n",
      "52/121:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "   # matrix=matrix[matrix['sum']>1 ]\n",
      "    \n",
      "   # matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/122: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/123: matrix_spam.head(10)\n",
      "52/124: matrix_spam.tail(10)\n",
      "52/125:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    matrix=matrix[matrix['sum']>1 ]\n",
      "    \n",
      "   # matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/126: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/127: matrix_spam.tail(10)\n",
      "52/128:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    matrix=matrix[matrix.loc['sum']>1 ]\n",
      "    \n",
      "   # matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/129:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/130: all_spam = get_messages(spam_path)\n",
      "52/131: matrix_spam = create_corpus_matrix(all_spam)\n",
      "52/132:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    matrix=matrix[matrix.loc['summa']>1 ]\n",
      "    \n",
      "   # matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/133:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "52/134: all_spam = get_messages(spam_path)\n",
      "52/135: matrix_spam = create_corpus_matrix(all_spam)\n",
      "53/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "53/2:\n",
      "#import nltk\n",
      "#nltk.download()\n",
      "53/3:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "53/4: ps = PorterStemmer()\n",
      "53/5: stop_words = stopwords.words('english')\n",
      "53/6: stop_words_pl = stopwords.words('polish')\n",
      "53/7:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "53/8:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "53/9:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    matrix=matrix[matrix.loc['summa']>1 ]\n",
      "    \n",
      "   # matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "53/10:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "53/11: all_spam = get_messages(spam_path)\n",
      "53/12: matrix_spam = create_corpus_matrix(all_spam)\n",
      "53/13:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    matrix=matrix.loc[matrix.loc['summa']>1 ]\n",
      "    \n",
      "   # matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "53/14: matrix_spam = create_corpus_matrix(all_spam)\n",
      "53/15:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "   # matrix=matrix.loc[matrix.loc['summa']>1 ]\n",
      "    \n",
      "   # matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "53/16: matrix_spam = create_corpus_matrix(all_spam)\n",
      "53/17: %time matrix_spam = create_corpus_matrix(all_spam)\n",
      "53/18: all_ham = get_messages(easyham_path)[:500]\n",
      "53/19: matrix_ham = create_corpus_matrix(all_ham)\n",
      "53/20:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "53/21:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "53/22:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "53/23: %time evaluate_results(all_spam, _)\n",
      "53/24: matrix_spam.tail(10)\n",
      "53/25:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "   # matrix=matrix.loc[matrix.loc['summa']>1 ]\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "53/26: %time matrix_spam = create_corpus_matrix(all_spam)\n",
      "53/27: matrix_spam.tail(10)\n",
      "53/28: %time evaluate_results(all_spam, _)\n",
      "53/29:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "53/30:\n",
      "#import nltk\n",
      "#nltk.download()\n",
      "53/31:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "53/32: ps = PorterStemmer()\n",
      "53/33:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "53/34:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha()])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "53/35:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    matrix=matrix.loc[matrix.loc['summa']>1 ]\n",
      "    \n",
      "   # matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "53/36:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "53/37: all_spam = get_messages(spam_path)\n",
      "53/38: matrix_spam = create_corpus_matrix(all_spam)\n",
      "54/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "54/2:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "54/3: stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
      "54/4:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "54/5:\n",
      "def count_words(message):\n",
      "    words_in_message =  re.findall(r'\\w+', message)\n",
      "    words_in_message = [word.lower() for word in words_in_message if not word.isnumeric() and 1<len(word)<20 and word.lower() not in stopwords]\n",
      "    occurances = Counter(words_in_message)\n",
      "    return occurances\n",
      "54/6:\n",
      "def create_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    matrix=matrix.loc[matrix['sum']>1 ]\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "54/7: all_spam = get_messages(spam_path)\n",
      "54/8: matrix_spam = create_matrix(all_spam)\n",
      "54/9: all_ham = get_messages(easyham_path)[:500]\n",
      "54/10: matrix_ham = create_matrix(all_ham)\n",
      "54/11:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.0001, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "54/12:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "54/13: hard_ham = get_messages(hardham_path)\n",
      "54/14: matrix_hardham = create_matrix(hard_ham)\n",
      "54/15:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "54/16: %time evaluate_results(all_spam, _)\n",
      "54/17:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "54/18: %time evaluate_results(all_spam, _)\n",
      "55/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "55/2:\n",
      "#import nltk\n",
      "#nltk.download()\n",
      "55/3:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "55/4: ps = PorterStemmer()\n",
      "55/5:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "55/6:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "55/7:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    matrix=matrix.loc[matrix.loc['summa']>1 ]\n",
      "    \n",
      "   # matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "55/8:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "55/9: all_spam = get_messages(spam_path)\n",
      "55/10: matrix_spam = create_corpus_matrix(all_spam)\n",
      "55/11: stop_words = stopwords('english')\n",
      "55/12: stop_words = stopwords.stopwords('english')\n",
      "55/13: stop_words = stopwords.words('english')\n",
      "55/14:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "55/15:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "55/16:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    matrix=matrix.loc[matrix.loc['summa']>1 ]\n",
      "    \n",
      "   # matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "   # matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    #matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "55/17:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "55/18: all_spam = get_messages(spam_path)\n",
      "55/19: matrix_spam = create_corpus_matrix(all_spam)\n",
      "55/20:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    #matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "   # matrix=matrix.loc[matrix.loc['summa']>1 ]\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "55/21: matrix_spam = create_corpus_matrix(all_spam)\n",
      "55/22:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    #matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "   # matrix=matrix.loc[matrix.loc['summa']>1 ]\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "   # matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "55/23: matrix_spam = create_corpus_matrix(all_spam)\n",
      "55/24: matrix_spam.head()\n",
      "55/25: matrix_spam.tail()\n",
      "55/26:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):\n",
      "    matrix_email = create_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/27:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "55/28:\n",
      "def evaluate_results(messages, correct_group):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter['Spam!']/len(messages), classification_counter['Ham!']/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "55/29: %time evaluate_results(all_spam, _)\n",
      "55/30:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/31: %time evaluate_results(all_spam, _)\n",
      "55/32: all_ham = get_messages(easyham_path)[:500]\n",
      "55/33: matrix_ham = create_corpus_matrix(all_ham)\n",
      "55/34: %time evaluate_results(all_spam, _)\n",
      "55/35: matrix_spam.index\n",
      "55/36: matrix_spam.columns\n",
      "55/37:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.column).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/38: %time evaluate_results(all_spam, _)\n",
      "55/39:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.columns).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at[word,'documents_percentage'] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/40: %time evaluate_results(all_spam, _)\n",
      "55/41:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.001, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.columns).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at['documents_percentage', word] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/42:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 'Spam!'\n",
      "    else:\n",
      "        return 'Ham!'\n",
      "55/43: %time evaluate_results(all_spam, _)\n",
      "55/44:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.01, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.columns).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at['documents_percentage', word] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/45: %time evaluate_results(all_spam, _)\n",
      "55/46: matrix_spam.tail()\n",
      "55/47:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.columns).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at['documents_percentage', word] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/48: unnecessary_columns = matrix_spam.columns\n",
      "55/49: unnecessry_columns\n",
      "55/50: unnecessary_columns\n",
      "55/51: unnecessary_indexes = matrix_spam.index - ['documents_percentage']\n",
      "55/52: unnecessary_indexes = matrix_spam.index\n",
      "55/53: unnecessary_indexes\n",
      "55/54: matrix_spam = matrix_spam.drop(columns = list(range(500)))\n",
      "55/55: spam_percentages = matrix_spam.loc['spam_percentages']\n",
      "55/56: spam_percentages = matrix_spam.loc['documents_percentage']\n",
      "55/57: spam_percentages\n",
      "55/58: spam_percentages.min()\n",
      "55/59: matrix_spam = matrix_spam.loc['documents_percentage']\n",
      "55/60: matrix_spam.tail()\n",
      "55/61: %time evaluate_results(all_spam, _)\n",
      "55/62:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.at['documents_percentage', word] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/63: %time evaluate_results(all_spam, _)\n",
      "55/64: matrix_spam.index\n",
      "55/65: matrix_spam.loc('house')\n",
      "55/66: matrix_spam.loc['house']\n",
      "55/67: matrix_spam['house']\n",
      "55/68: matrix_spam.head\n",
      "55/69: matrix_spam.loc['abu']\n",
      "55/70:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.loc[ word] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/71: %time evaluate_results(all_spam, _)\n",
      "55/72:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "55/73: x = (x for x in 'abcs')\n",
      "55/74: x = (x*2 for x in 'abcd')\n",
      "55/75: Counter(x)\n",
      "55/76:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "55/77: ps = PorterStemmer()\n",
      "55/78: stop_words = stopwords.words('english')\n",
      "55/79:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "55/80:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "55/81:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    #matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "   # matrix=matrix.loc[matrix.loc['summa']>1 ]\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "   # matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "55/82:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "55/83: all_spam = get_messages(spam_path)\n",
      "55/84:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "55/85:\n",
      "#import nltk\n",
      "#nltk.download()\n",
      "55/86:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "55/87: ps = PorterStemmer()\n",
      "55/88: stop_words = stopwords.words('english')\n",
      "55/89:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "55/90:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "55/91:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    #matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "   # matrix=matrix.loc[matrix.loc['summa']>1 ]\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "   # matrix.loc['word_density'] = matrix.loc['sum']/matrix.loc['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "55/92:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    matrix['word_density'] = matrix['sum']/matrix['sum'].sum()\n",
      "    \n",
      "    return matrix\n",
      "55/93: all_spam = get_messages(spam_path)\n",
      "55/94: matrix_spam = create_corpus_matrix(all_spam)\n",
      "55/95: matrix_spam = matrix_spam.loc['documents_percentage']\n",
      "55/96: matrix_spam.loc['abu']\n",
      "55/97: all_ham = get_messages(easyham_path)[:500]\n",
      "55/98: matrix_ham = create_corpus_matrix(all_ham)\n",
      "55/99:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))#\n",
      "#    print(common_words)\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.loc[ word] for word in common_words])\n",
      "#        print('probability_common: ', probability_common)\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "#        print('probability_uncommon: ', probability_uncommon)\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/100:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 0\n",
      "    else:\n",
      "        return 1\n",
      "55/101: hard_ham = get_messages(hardham_path)\n",
      "55/102: matrix_hardham = create_corpus_matrix(hard_ham)\n",
      "55/103:\n",
      "def evaluate_results(messages):\n",
      "    classification = [spam_or_ham([message]) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter[0]/len(messages), classification_counter[1]/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "55/104: #evaluate_results(hard_ham)\n",
      "55/105: %time evaluate_results(all_spam)\n",
      "55/106:\n",
      "def evaluate_results(messages):\n",
      "    classification = (spam_or_ham([message]) for message in messages)\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter[0]/len(messages), classification_counter[1]/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "55/107: #evaluate_results(hard_ham)\n",
      "55/108: %time evaluate_results(all_spam)\n",
      "55/109:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    #matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "   # matrix=matrix.loc[matrix.loc['summa']>1 ]\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "55/110:\n",
      "def create_message_matrix(messages):\n",
      "    dicts = count_words(message)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "55/111:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "    \n",
      "    number_of_words = len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.loc[ word] for word in common_words])\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/112:\n",
      "def evaluate_results(messages):\n",
      "    classification = (spam_or_ham(message) for message in messages)\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter[0]/len(messages), classification_counter[1]/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "55/113: %time evaluate_results(all_spam)\n",
      "55/114:\n",
      "def create_message_matrix(message):\n",
      "    dicts = count_words(message)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "55/115: %time evaluate_results(all_spam)\n",
      "55/116:\n",
      "def create_message_matrix(message):\n",
      "    dicts = count_words(message)\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "    matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "55/117: %time evaluate_results(all_spam)\n",
      "55/118:\n",
      "def create_message_matrix(message):\n",
      "    dicts = count_words(message)\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['sum']=matrix.sum(axis=1)\n",
      "    \n",
      "    matrix['number_of_documents']=(matrix.count(axis=1)-1)\n",
      "   # matrix['documents_percentage']=matrix['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "55/119: %time evaluate_results(all_spam)\n",
      "55/120:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "    \n",
      "    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = np.prod([matrix_corpus.loc[ word] for word in common_words])\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/121: %time evaluate_results(all_spam)\n",
      "55/122: evaluate_results(hard_ham)\n",
      "55/123:\n",
      "def evaluate_results(messages):\n",
      "    classification = [spam_or_ham(message) for message in messages]\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter[0]/len(messages), classification_counter[1]/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "55/124: %time evaluate_results(all_spam)\n",
      "55/125:\n",
      "def evaluate_results(messages):\n",
      "    classification = (spam_or_ham(message) for message in messages)\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter[0]/len(messages), classification_counter[1]/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "55/126: %time evaluate_results(all_spam)\n",
      "55/127: %time evaluate_results(all_spam)\n",
      "55/128:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = pd.Series(set(matrix_corpus.index).intersection(set(matrix_email.index)))\n",
      "    \n",
      "    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/129: %time evaluate_results(all_spam)\n",
      "55/130:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = pd.Series([set(matrix_corpus.index).intersection(set(matrix_email.index))])\n",
      "    \n",
      "    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/131: %time evaluate_results(all_spam)\n",
      "55/132:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = pd.Series([set(matrix_corpus.index).intersection(set(matrix_email.index))])\n",
      "    print('common_words: 'common_words)\n",
      "    \n",
      "    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/133:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = pd.Series([set(matrix_corpus.index).intersection(set(matrix_email.index))])\n",
      "    print('common_words: ', common_words)\n",
      "    \n",
      "    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/134:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = pd.Series([set(matrix_corpus.index).intersection(set(matrix_email.index))])\n",
      "    print('common_words_prod: ', common_words.prod())\n",
      "    \n",
      "    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/135: %time evaluate_results(all_spam)\n",
      "55/136:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = pd.Series(list(set(matrix_corpus.index).intersection(set(matrix_email.index))))\n",
      "    print('common_words_prod: ', common_words.prod())\n",
      "    \n",
      "    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/137: %time evaluate_results(all_spam)\n",
      "55/138:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = pd.Series(list(set(matrix_corpus.index).intersection(set(matrix_email.index))))\n",
      "    print('common_words: ', common_words)\n",
      "    print('common_words_prod: ', common_words.prod())\n",
      "    \n",
      "    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/139: %time evaluate_results(all_spam)\n",
      "55/140: matrix_spam.head()\n",
      "55/141:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "    common_words_frequencies = matrix_corpus[matrix_corpus.index in common_words]\n",
      "   # print('common_words: ', common_words)\n",
      "   # print('common_words_prod: ', common_words.prod())\n",
      "    \n",
      "    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/142: %time evaluate_results(all_spam)\n",
      "55/143:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "    common_words_frequencies = matrix_corpus.filter(item=list(common_words))\n",
      "   # print('common_words: ', common_words)\n",
      "   # print('common_words_prod: ', common_words.prod())\n",
      "    \n",
      "    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/144: %time evaluate_results(all_spam)\n",
      "55/145:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "    common_words_frequencies = matrix_corpus.filter(items=list(common_words))\n",
      "   # print('common_words: ', common_words)\n",
      "   # print('common_words_prod: ', common_words.prod())\n",
      "    \n",
      "    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = common_words.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/146: %time evaluate_results(all_spam)\n",
      "55/147:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "    common_words_frequencies = matrix_corpus.filter(items=list(common_words))\n",
      "   # print('common_words: ', common_words)\n",
      "   # print('common_words_prod: ', common_words.prod())\n",
      "    \n",
      "    number_of_words = len(matrix_email.index)     #len(set(re.findall(r'\\w+', email[0])))\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = common_words_frequencies.prod()    #np.prod([matrix_corpus.loc[ word] for word in common_words])\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "55/148: %time evaluate_results(all_spam)\n",
      "55/149: %time evaluate_results(all_spam)\n",
      "56/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "58/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "58/2:\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "60/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "60/2:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "60/3: stop_words = stopwords.words('english')\n",
      "60/4:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "60/5:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "60/6:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "60/7:\n",
      "def create_message_matrix(message):\n",
      "    dicts = count_words(message)\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    return matrix\n",
      "60/8: all_spam = get_messages(spam_path)\n",
      "60/9: matrix_spam = create_corpus_matrix(all_spam)\n",
      "60/10: ps = PorterStemmer\n",
      "60/11:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "60/12:\n",
      "def count_words(message):\n",
      "    occurances = set([ps.stem(word.lower()) for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "60/13:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "60/14:\n",
      "def create_message_matrix(message):\n",
      "    dicts = count_words(message)\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    return matrix\n",
      "60/15: all_spam = get_messages(spam_path)\n",
      "60/16: matrix_spam = create_corpus_matrix(all_spam)\n",
      "60/17: ps = PorterStemmer('english')\n",
      "60/18: help(PorterStemmer)\n",
      "60/19:\n",
      "def count_words(message):\n",
      "    occurances = set(word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "60/20:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "60/21:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "60/22:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "60/23:\n",
      "def create_message_matrix(message):\n",
      "    dicts = count_words(message)\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    return matrix\n",
      "60/24: all_spam = get_messages(spam_path)\n",
      "60/25: matrix_spam = create_corpus_matrix(all_spam)\n",
      "60/26: matrix_spam.head(10)\n",
      "60/27: matrix_spam = create_corpus_matrix(all_spam)\n",
      "60/28: matrix_spam = matrix_spam.loc['documents_percentage']\n",
      "60/29: matrix_spam.head(10)\n",
      "60/30: x = count_words('Testowy tekst do zliczenia słów')\n",
      "60/31: print(x)\n",
      "60/32: x = count_words('Testowy tekst do zliczenia słów słów')\n",
      "60/33: print(x)\n",
      "60/34: x = count_words('Testowy tekst do zliczenia słów słów tekst')\n",
      "60/35: x = word_tokenize('Testowy tekst do zliczenia słów słów tekst')\n",
      "60/36: print(x)\n",
      "60/37: all_ham = get_messages(easyham_path)[:500]\n",
      "60/38: matrix_ham = create_corpus_matrix(all_ham)\n",
      "60/39:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.002, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "    common_words_frequencies = matrix_corpus.filter(items=list(common_words))\n",
      "    \n",
      "    number_of_words = len(matrix_email.index)\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = common_words_frequencies.prod()\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "60/40:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 0\n",
      "    else:\n",
      "        return 1\n",
      "60/41: hard_ham = get_messages(hardham_path)\n",
      "60/42: matrix_hardham = create_corpus_matrix(hard_ham)\n",
      "60/43:\n",
      "def evaluate_results(messages):\n",
      "    classification = (spam_or_ham(message) for message in messages)\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter[0]/len(messages), classification_counter[1]/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "60/44: %time evaluate_results(all_spam)\n",
      "60/45:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.0001, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "    common_words_frequencies = matrix_corpus.filter(items=list(common_words))\n",
      "    \n",
      "    number_of_words = len(matrix_email.index)\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = common_words_frequencies.prod()\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "60/46: %time evaluate_results(all_spam)\n",
      "60/47: evaluate_results(hard_ham)\n",
      "61/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "61/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "61/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "61/4: headers, messages = get_emails(easyham_path)\n",
      "61/5:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "61/6:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "61/7: headers, messages = get_emails(easyham_path)\n",
      "61/8: headers[:5]\n",
      "61/9:\n",
      "for header in headers[:5]:\n",
      "    print(header)\n",
      "61/10: headers_lines = [header.split('\\n') for header in headers]\n",
      "61/11: headers_lines[:2]\n",
      "61/12: headers_lines[0]\n",
      "61/13:\n",
      "for header in headers[:5]:\n",
      "    print(header,'\\n\\n')\n",
      "61/14: len(headers_lines)\n",
      "61/15: headers_lines[0]\n",
      "61/16: header_lines[0]\n",
      "61/17: headers_lines[0]\n",
      "61/18:\n",
      "for header_lines in headers_lines[:5]:\n",
      "    for line in header_lines:\n",
      "        if 'From:' in line:\n",
      "            print(header_,'\\n\\n')\n",
      "61/19:\n",
      "for header_lines in headers_lines[:5]:\n",
      "    for line in header_lines:\n",
      "        if 'From:' in line:\n",
      "            print(line,'\\n\\n')\n",
      "61/20:\n",
      "for header_lines,x in enumerate(headers_lines[:5]):\n",
      "    for line in header_lines:\n",
      "        if 'From:' in line:\n",
      "            print(x, ': ',line,'\\n\\n')\n",
      "61/21:\n",
      "for x,header_lines in enumerate(headers_lines[:5]):\n",
      "    for line in header_lines:\n",
      "        if 'From:' in line:\n",
      "            print(x, ': ',line,'\\n\\n')\n",
      "61/22:\n",
      "for x,header_lines in enumerate(headers_lines[:10]):\n",
      "    for line in header_lines:\n",
      "        if 'From:' in line:\n",
      "            print(x, ': ',line,'\\n\\n')\n",
      "61/23:\n",
      "from_lines = []\n",
      "for x,header_lines in enumerate(headers_lines[:10]):\n",
      "    for line in header_lines:\n",
      "        if 'From:' in line:\n",
      "            from_lines.append(x, line)\n",
      "61/24:\n",
      "from_lines = []\n",
      "for x,header_lines in enumerate(headers_lines[:10]):\n",
      "    for line in header_lines:\n",
      "        if 'From:' in line:\n",
      "            from_lines.append((x,line))\n",
      "61/25: len(from_lines)\n",
      "61/26:\n",
      "from_lines = []\n",
      "for x,header_lines in enumerate(headers_lines):\n",
      "    for line in header_lines:\n",
      "        if 'From:' in line:\n",
      "            from_lines.append((x,line))\n",
      "61/27: len(from_lines)\n",
      "61/28:\n",
      "from_lines = []\n",
      "for x,header_lines in enumerate(headers_lines):\n",
      "    for line in header_lines:\n",
      "        if line startswith('From:'):\n",
      "            from_lines.append((x,line))\n",
      "61/29:\n",
      "from_lines = []\n",
      "for x,header_lines in enumerate(headers_lines):\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append((x,line))\n",
      "61/30: len(from_lines)\n",
      "61/31: from_lines[:10]\n",
      "61/32:\n",
      "for email  in from_lines[:10]:\n",
      "    x = re.findall(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", email)\n",
      "    print(x)\n",
      "61/33:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "61/34:\n",
      "for email in from_lines[:10]:\n",
      "    x = re.findall(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", email)\n",
      "    print(x)\n",
      "61/35:\n",
      "for email in from_lines[:10]:\n",
      "    x = re.findall(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", email[1])\n",
      "    print(x)\n",
      "61/36:\n",
      "for email in from_lines[:10]:\n",
      "    x = re.findall(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", email[1])\n",
      "    print(email[1])\n",
      "61/37:\n",
      "for email in from_lines[:10]:\n",
      "    x = re.findall(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", email[1])\n",
      "    print(x)\n",
      "61/38:\n",
      "for email in from_lines[:10]:\n",
      "    x = re.findall(r\"(@)\", email[1])\n",
      "    print(x)\n",
      "61/39:\n",
      "for email in from_lines[:10]:\n",
      "    x = re.findall(r\"([a-z]+@)\", email[1])\n",
      "    print(x)\n",
      "61/40:\n",
      "for email in from_lines[:10]:\n",
      "    x = re.findall(r\"([a-zA-Z0-9_.+-]+@)\", email[1])\n",
      "    print(x)\n",
      "61/41:\n",
      "for email in from_lines[:20]:\n",
      "    x = re.findall(r\"([a-zA-Z0-9_.+-]+@)\", email[1])\n",
      "    print(x)\n",
      "61/42:\n",
      "for email in from_lines[:20]:\n",
      "    x = re.findall(r\"([a-zA-Z0-9_.+-]+@+\\.[a-zA-Z0-9-.])\", email[1])\n",
      "    print(x)\n",
      "61/43:\n",
      "for email in from_lines[:20]:\n",
      "    x = re.findall(r\"([a-zA-Z0-9_.+-]+@+[a-zA-Z0-9-.])\", email[1])\n",
      "    print(x)\n",
      "61/44:\n",
      "for email in from_lines[:20]:\n",
      "    x = re.findall(r\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-.])\", email[1])\n",
      "    print(x)\n",
      "61/45:\n",
      "for email in from_lines[:20]:\n",
      "    x = re.findall(r\"(\\S+@\\S+)\", email[1])\n",
      "    print(x)\n",
      "61/46:\n",
      "for email in from_lines[:20]:\n",
      "    y = parseaddr(email)\n",
      "    x = re.findall(r\"(\\S+@\\S+)\", email[1])\n",
      "    print(y)\n",
      "61/47:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "\n",
      "from email.utils import parseaddr\n",
      "61/48:\n",
      "for email in from_lines[:20]:\n",
      "    y = parseaddr(email)\n",
      "    x = re.findall(r\"(\\S+@\\S+)\", email[1])\n",
      "    print(y)\n",
      "61/49:\n",
      "for email in from_lines[:20]:\n",
      "    y = parseaddr(email[1])\n",
      "    x = re.findall(r\"(\\S+@\\S+)\", email[1])\n",
      "    print(y)\n",
      "61/50:\n",
      "for email in from_lines[:20]:\n",
      "    y = parseaddr(email[1])\n",
      "    print(y[1])\n",
      "61/51:\n",
      "email_addresses = []\n",
      "for email in from_lines[:20]:\n",
      "    parser = parseaddr(email[1])\n",
      "    email_addresses.append[parser[1]]\n",
      "61/52:\n",
      "email_addresses = []\n",
      "for email in from_lines[:20]:\n",
      "    parser = parseaddr(email[1])\n",
      "    email_addresses.append(parser[1])\n",
      "61/53: email_addresses[:20]\n",
      "61/54: len(email_addresses)\n",
      "61/55:\n",
      "email_addresses = []\n",
      "for email in from_lines[]:\n",
      "    parser = parseaddr(email[1])\n",
      "    email_addresses.append(parser[1])\n",
      "61/56:\n",
      "email_addresses = []\n",
      "for email in from_lines:\n",
      "    parser = parseaddr(email[1])\n",
      "    email_addresses.append(parser[1])\n",
      "61/57: len(email_addresses)\n",
      "63/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "\n",
      "from email.utils import parseaddr\n",
      "63/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "63/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "63/4: headers, messages = get_emails(easyham_path)\n",
      "63/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "63/6:\n",
      "from_lines = []\n",
      "for header_lines in headers:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "63/7: from_lines[:10]\n",
      "63/8:\n",
      "email_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    email_addresses.append(parser_results[1])\n",
      "63/9: len(email_addresses)\n",
      "63/10: headers[0]\n",
      "63/11:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "63/12: from_lines[:10]\n",
      "63/13:\n",
      "email_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    email_addresses.append(parser_results[1])\n",
      "63/14: len(email_addresses)\n",
      "63/15: email_addresses[:10]\n",
      "63/16: headers[0]\n",
      "63/17: print(headers[0])\n",
      "63/18:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "63/19: print(subject_lines[:10])\n",
      "63/20: len(subject_lines)\n",
      "63/21:\n",
      "subjects=[]\n",
      "for subject_line in subjects_lines[:10]:\n",
      "    subject = re.findall('From: (\\w+)')\n",
      "    subjects.append(subject)\n",
      "63/22:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines[:10]:\n",
      "    subject = re.findall('From: (\\w+)')\n",
      "    subjects.append(subject)\n",
      "63/23:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines[:10]:\n",
      "    subject = re.findall('From: (\\w+)', subject_line)\n",
      "    subjects.append(subject)\n",
      "63/24: subjects\n",
      "63/25:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines[:10]:\n",
      "    subject = re.findall(r'From: (\\w+)', subject_line)\n",
      "    subjects.append(subject)\n",
      "63/26: subjects\n",
      "63/27:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines[:10]:\n",
      "    subject = re.findall(r'(\\w+)', subject_line)\n",
      "    subjects.append(subject)\n",
      "63/28: subjects\n",
      "63/29:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines[:10]:\n",
      "    subject = re.findall(r'Subject: (\\w+)', subject_line)\n",
      "    subjects.append(subject)\n",
      "63/30: subjects\n",
      "63/31:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines[:10]:\n",
      "    subject = re.findall(r'Subject: (\\.+)', subject_line)\n",
      "    subjects.append(subject)\n",
      "63/32: subjects\n",
      "63/33:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines[:10]:\n",
      "    subject = re.findall(r'Subject: (\\.)', subject_line)\n",
      "    subjects.append(subject)\n",
      "63/34: subjects\n",
      "63/35:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines[:10]:\n",
      "    subject = re.findall(r'Subject: (.)', subject_line)\n",
      "    subjects.append(subject)\n",
      "63/36: subjects\n",
      "63/37:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines[:10]:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject)\n",
      "63/38: subjects\n",
      "63/39: header[0]\n",
      "63/40: headers[0]\n",
      "63/41: print(headers[0])\n",
      "63/42:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines[]:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject)\n",
      "63/43:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject)\n",
      "63/44: subjects\n",
      "63/45: len(subjects)\n",
      "63/46: print(subjects[100])\n",
      "64/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "from collections import Counter\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "64/2:\n",
      "spam_path = 'Spam/data/spam/'\n",
      "spam2_path = 'Spam/data/spam2/'\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "hardham_path = 'Spam/data/hard_ham/'\n",
      "hardham2_path = 'Spam/data/har_ham_2/'\n",
      "64/3: stop_words = stopwords.words('english')\n",
      "64/4:\n",
      "def get_messages(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_messages\n",
      "64/5:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "64/6:\n",
      "def create_corpus_matrix(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "64/7:\n",
      "def create_message_matrix(message):\n",
      "    dicts = count_words(message)\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    return matrix\n",
      "64/8: all_spam = get_messages(spam_path)\n",
      "64/9: matrix_spam = create_corpus_matrix(all_spam)\n",
      "64/10: matrix_spam = matrix_spam.loc['documents_percentage']\n",
      "64/11: all_ham = get_messages(easyham_path)[:500]\n",
      "64/12: matrix_ham = create_corpus_matrix(all_ham)\n",
      "64/13:\n",
      "def classify_email(email,matrix_corpus, prior=0.5, c=0.0001, ):\n",
      "    matrix_email = create_message_matrix(email)\n",
      "\n",
      "    common_words = set(matrix_corpus.index).intersection(set(matrix_email.index))\n",
      "    common_words_frequencies = matrix_corpus.filter(items=list(common_words))\n",
      "    \n",
      "    number_of_words = len(matrix_email.index)\n",
      "    number_of_common_words = len(common_words)\n",
      "    number_of_uncommon_words = number_of_words - number_of_common_words\n",
      "    \n",
      "    if number_of_common_words < 1:\n",
      "        return prior*c**number_of_words\n",
      "    else:\n",
      "        probability_common = common_words_frequencies.prod()\n",
      "        \n",
      "        probability_uncommon = c**number_of_uncommon_words\n",
      "        \n",
      "        return(prior*probability_common*probability_uncommon)\n",
      "64/14:\n",
      "def spam_or_ham(email):\n",
      "    if classify_email(email, matrix_spam) > classify_email(email, matrix_ham):\n",
      "        return 0\n",
      "    else:\n",
      "        return 1\n",
      "64/15: hard_ham = get_messages(hardham_path)\n",
      "64/16: matrix_hardham = create_corpus_matrix(hard_ham)\n",
      "64/17:\n",
      "def evaluate_results(messages):\n",
      "    classification = (spam_or_ham(message) for message in messages)\n",
      "    classification_counter = Counter(classification)\n",
      "    results = (classification_counter[0]/len(messages), classification_counter[1]/len(messages))\n",
      "    print('Spam: ', results[0], ', Ham: ', results[1])\n",
      "64/18: evaluate_results(hard_ham)\n",
      "64/19: %time evaluate_results(all_spam)\n",
      "63/47: len(messages)\n",
      "63/48: print(messages[99])\n",
      "63/49: print(messages[199])\n",
      "66/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "\n",
      "from email.utils import parseaddr\n",
      "66/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "66/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "66/4: headers, messages = get_emails(easyham_path)\n",
      "66/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "66/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "66/7:\n",
      "email_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    email_addresses.append(parser_results[1])\n",
      "66/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "66/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject)\n",
      "66/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startwith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "66/11:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "66/12: print(dates_lines[:10])\n",
      "66/13:\n",
      "for x in dates_lines[:10]:\n",
      "    print(x)\n",
      "66/14: len(dates_lines)\n",
      "66/15:\n",
      "for x in dates_lines[:20]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    print(x)\n",
      "66/16:\n",
      "for x in dates_lines[100:120]:\n",
      "    print(x)\n",
      "66/17:\n",
      "for x in dates_lines[900:1200]:\n",
      "    print(x)\n",
      "66/18:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "66/19: dates_lines = [datestr[7:] for datestr in date_lines]\n",
      "66/20: dates_lines = [datestr[7:] for datestr in dates_lines]\n",
      "66/21: dates_lines\n",
      "66/22: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "66/23: dates_lines\n",
      "66/24:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "66/25:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "66/26: dates_lines = [datestr[:] for datestr in dates_lines]\n",
      "66/27: dates_lines\n",
      "66/28: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "66/29: dates_lines\n",
      "66/30: t = pd.to_datetime(dates_lines[0])\n",
      "66/31: t = pd.to_datetime(dates_lines[1])\n",
      "66/32: t\n",
      "66/33: t = pd.to_datetime(dates_lines[], error='coerce')\n",
      "66/34: t = pd.to_datetime(dates_lines, error='coerce')\n",
      "66/35: t = pd.to_datetime(dates_lines, errors='coerce')\n",
      "66/36: len(t)\n",
      "66/37: t\n",
      "66/38: t.count()\n",
      "66/39: t[:30]\n",
      "66/40: t[:50]\n",
      "66/41: t[:2]\n",
      "66/42: t[:200]\n",
      "66/43: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "66/44: send_datetime.info()\n",
      "68/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "\n",
      "from email.utils import parseaddr\n",
      "68/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "68/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "68/4: headers, messages = get_emails(easyham_path)\n",
      "68/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "68/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "68/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "68/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "68/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject)\n",
      "68/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "68/11:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "68/12: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "68/13: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "68/14: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "68/15: all_emails_data = pd.DataFrame(emails_data)\n",
      "68/16: all_emails_data.head()\n",
      "68/17:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data.transpose()\n",
      "68/18: all_emails_data.head()\n",
      "68/19: all_emails_data.head()\n",
      "68/20:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "68/21: all_emails_data.head()\n",
      "68/22:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "68/23: all_emails_data.head()\n",
      "68/24:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0])\n",
      "68/25:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "68/26:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "68/27: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "68/28: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "68/29: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "68/30:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "68/31: all_emails_data.head()\n",
      "68/32:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "68/33: headers, messages = get_emails(easyham_path)\n",
      "68/34: headers_lines = [header.split('\\n') for header in headers]\n",
      "68/35:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "68/36:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "68/37:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "68/38:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0])\n",
      "68/39:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "68/40:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "68/41: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "68/42: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "68/43: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "68/44:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "68/45: all_emails_data.head()\n",
      "68/46:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "68/47: headers, messages = get_emails(easyham_path)\n",
      "68/48: headers_lines = [header.split('\\n') for header in headers]\n",
      "68/49:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "68/50:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "68/51:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "68/52:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0])\n",
      "68/53:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "68/54:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "68/55: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "68/56: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "68/57: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "68/58:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "68/59: all_emails_data.head()\n",
      "68/60:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:]\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "68/61: headers, messages = get_emails(easyham_path)\n",
      "68/62: headers_lines = [header.split('\\n') for header in headers]\n",
      "68/63:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "68/64:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "68/65:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "68/66:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0])\n",
      "68/67:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "68/68:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "68/69: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "68/70: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "68/71: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "68/72:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "68/73: all_emails_data.head()\n",
      "68/74:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "68/75: headers, messages = get_emails(easyham_path)\n",
      "68/76: headers_lines = [header.split('\\n') for header in headers]\n",
      "68/77:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "68/78:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "68/79:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "68/80:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "68/81:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "68/82:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "68/83: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "68/84: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "68/85: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "68/86:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "68/87: all_emails_data.head()\n",
      "68/88:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "68/89: all_emails.head()\n",
      "68/90:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "68/91:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index()\n",
      "68/92: all_emails_data.head()\n",
      "68/93:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "68/94: all_emails_data.head()\n",
      "68/95: all_emails_training = all_emails_data.loc[:1250]\n",
      "68/96: all_emails_training.head()\n",
      "68/97: len(all_emails_training)\n",
      "68/98: all_emails_training.tail()\n",
      "68/99: all_emails_training.groupby(by=['From'])\n",
      "68/100: grouped = all_emails_training.groupby(by=['From'])\n",
      "68/101: grouped.head()\n",
      "68/102: grouped = all_emails_training.groupby(by=['From']).count()\n",
      "68/103: grouped.head()\n",
      "68/104: grouped.tail()\n",
      "68/105: grouped = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "68/106: grouped.tail()\n",
      "68/107: plt.scatter(grouped.Message)\n",
      "68/108: plt.scatter(grouped.From, grouped.Message)\n",
      "68/109: grouped.index\n",
      "68/110: plt.scatter(grouped.index, grouped.Message)\n",
      "68/111:\n",
      "plt.scatter(grouped.index, grouped.Message)\n",
      "plt.ylim(6,)\n",
      "68/112:\n",
      "plt.scatter(grouped[grouped.Message>6].index, grouped[grouped.Message>6].Message)\n",
      "plt.ylim(6,)\n",
      "68/113: plt.scatter(grouped[grouped.Message>10].index, grouped[grouped.Message>10].Message)\n",
      "70/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "\n",
      "from email.utils import parseaddr\n",
      "70/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "70/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "70/4: headers, messages = get_emails(easyham_path)\n",
      "70/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "70/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "70/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "70/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "70/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "70/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "70/11:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "70/12: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "70/13: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "70/14: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "70/15:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "70/16: all_emails_training = all_emails_data.loc[:1250]\n",
      "70/17: all_emails_training.tail()\n",
      "70/18: len(all_emails_training)\n",
      "70/19: grouped = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "70/20: grouped.tail()\n",
      "70/21: plt.scatter(grouped[grouped.Message>10].index, grouped[grouped.Message>10].Message)\n",
      "70/22: grouped.index\n",
      "70/23: plt.bar(grouped[grouped.Message>10].index, grouped[grouped.Message>10].Message)\n",
      "70/24: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "70/25: grouped_from.tail()\n",
      "70/26:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "\n",
      "from email.utils import parseaddr\n",
      "70/27: grouped_from['Weight'] = np.log[grouped_from['Message']]\n",
      "70/28: grouped_from['Weight'] = np.log(grouped_from['Message'])\n",
      "70/29: grouped_from.head()\n",
      "70/30: grouped_from.tail()\n",
      "70/31: grouped_from['Weight'] = np.ln(grouped_from['Message'])\n",
      "70/32: grouped_from.head()\n",
      "70/33: grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "70/34: grouped_from.head()\n",
      "70/35: plt.bar(grouped_from.index, grouped_from.Weight)\n",
      "70/36:\n",
      "plt.bar(grouped_from.index, grouped_from.Weight)\n",
      "plt.bar(grouped_from.index, grouped_from.Message)\n",
      "70/37:\n",
      "plt.bar(grouped_from.index, grouped_from.Weight)\n",
      "plt.bar(grouped_from.index, grouped_from.Message)\n",
      "plt.rcParams['figure.figsize']=(16,8)\n",
      "70/38:\n",
      "plt.bar(grouped_from.index, grouped_from.Weight)\n",
      "plt.bar(grouped_from.index, grouped_from.Message)\n",
      "plt.rcParams['figure.figsize']=(24,16)\n",
      "70/39:\n",
      "plt.bar(grouped_from.index, grouped_from.Weight, grouped_from.Message)\n",
      "#plt.bar(grouped_from.index, grouped_from.Message)\n",
      "plt.rcParams['figure.figsize']=(24,16)\n",
      "70/40:\n",
      "plt.scatter(grouped_from.index, grouped_from.Weight, grouped_from.Message)\n",
      "#plt.bar(grouped_from.index, grouped_from.Message)\n",
      "plt.rcParams['figure.figsize']=(24,16)\n",
      "70/41:\n",
      "plt.plot(grouped_from.index, grouped_from.Weight)\n",
      "plt.plot(grouped_from.index, grouped_from.Message)\n",
      "plt.rcParams['figure.figsize']=(24,16)\n",
      "70/42:\n",
      "plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "plt.rcParams['figure.figsize']=(24,16)\n",
      "70/43: responses = all_emails_training.loc[all_emails_training.Subject.startswith('re:')]\n",
      "70/44: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith('re:')]\n",
      "70/45: responses.head()\n",
      "70/46: responses.tail()\n",
      "70/47: responses_2 = responses.loc[responses.Subject.str.startswith('re:')]\n",
      "70/48: responses_2.head()\n",
      "70/49: responses.head()\n",
      "72/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "\n",
      "from email.utils import parseaddr\n",
      "72/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "72/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "72/4: headers, messages = get_emails(easyham_path)\n",
      "72/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "72/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "72/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "72/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "72/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "72/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "72/11:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "72/12: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "72/13: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "72/14: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "72/15:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "72/16: all_emails_training = all_emails_data.loc[:1250]\n",
      "72/17: all_emails_training.tail()\n",
      "72/18: len(all_emails_training)\n",
      "72/19: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "72/20: grouped_from.tail()\n",
      "72/21: grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "72/22: grouped_from.head()\n",
      "72/23:\n",
      "plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "plt.rcParams['figure.figsize']=(24,16)\n",
      "72/24: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith('re:')]\n",
      "72/25: len(responses)\n",
      "72/26: responses_2 = responses.loc[responses.Subject.str.startswith('re:')]\n",
      "72/27: responses_2.head()\n",
      "72/28: responses.head()\n",
      "72/29: responses_2 = [element[1:] for element in responses.subject.split('re:')]\n",
      "72/30: responses_2 = [element[1:] for element in responses.Subject.split('re:')]\n",
      "72/31: responses_2 = [element[1:] for element in responses.Subject.str.split('re:')]\n",
      "72/32: responses_2.head()\n",
      "72/33: responses_2\n",
      "72/34: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith('re')]\n",
      "72/35: len(responses)\n",
      "72/36: responses_2 = [element[1:] for element in responses.Subject.str.split('re')]\n",
      "72/37: responses_2\n",
      "72/38: responses_2 = [element[1:] for element in responses.Subject.str.split(r'(re...:)')]\n",
      "72/39: responses_2\n",
      "72/40: responses_2 = [element[1:] for element in responses.Subject.str.split(r'(re:)')]\n",
      "72/41: responses_2\n",
      "72/42: responses_2 = [element for element in responses.Subject.str.split(r'(re:)')]\n",
      "72/43: responses_2\n",
      "72/44: responses.Subject\n",
      "72/45: responses_2 = [element for element in responses.Subject.str.split(r'(re: )')]\n",
      "72/46: responses_2\n",
      "72/47: responses.Subject[0]\n",
      "72/48: responses.Subject[1]\n",
      "72/49: responses_2 = [element for element in responses.Subject[:2].str.split(r'(re: )')]\n",
      "72/50: responses_2\n",
      "72/51: responses.Subject[:2]\n",
      "72/52: responses_2 = [element for element in responses.Subject[:2].str.split('re:')]\n",
      "72/53: responses_2\n",
      "72/54: responses.Subject[:2].str.split('re:')\n",
      "72/55: responses.Subject[:2].str.split()\n",
      "72/56: responses.Subject[:2].str.split('please')\n",
      "72/57: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(r'(re[.]{0,3}:)')]\n",
      "72/58: responses.Subject[:2].str.split('please')\n",
      "72/59: responses\n",
      "72/60: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(r'(re:)')]\n",
      "72/61: responses\n",
      "72/62: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(r'(re)')]\n",
      "72/63: responses\n",
      "72/64: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(r're')]\n",
      "72/65: responses\n",
      "72/66: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(r're.?')]\n",
      "72/67: responses\n",
      "72/68: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith('re.?')]\n",
      "72/69: responses\n",
      "72/70: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith('re.')]\n",
      "72/71: responses\n",
      "72/72: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(r're.')]\n",
      "72/73: responses\n",
      "72/74:\n",
      "x = re.match('re')\n",
      "responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(x)]\n",
      "72/75: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(('re:', 're['))]\n",
      "72/76: responses\n",
      "72/77: responses = all_emails_training.loc[all_emails_training.Subject.str.startswith(('re', 're['))]\n",
      "72/78: responses\n",
      "72/79: responses.Subject[:2].str.split('re')\n",
      "72/80: responses.Subject.str.split('re')\n",
      "72/81: responses_2 = [element for element in responses.Subject.str.split('re')]\n",
      "72/82: responses_2\n",
      "72/83: x = responses.Subject.str.split('re')\n",
      "72/84: responses_cut_re = responses.Subject.str.split('re')\n",
      "72/85: responses_cut_re\n",
      "72/86: responses_cut_re.loc(response_cut_re.str.startswith('re'))\n",
      "72/87: responses_cut_re.loc(responses_cut_re.str.startswith('re'))\n",
      "72/88: [x for x in responses_cut_re]\n",
      "72/89: [x[1] for x in responses_cut_re]\n",
      "72/90: [x[1] for x in responses_cut_re if x[1].startswith('re')]\n",
      "72/91: all_emails_training.Subject.str.startswith('re')]\n",
      "72/92: all_emails_training.Subject.str.startswith('re')\n",
      "75/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "\n",
      "from email.utils import parseaddr\n",
      "75/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "75/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "75/4: headers, messages = get_emails(easyham_path)\n",
      "75/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "75/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "75/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "75/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "75/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "75/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "75/11:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "75/12: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "75/13: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "75/14: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "75/15:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "75/16: all_emails_training = all_emails_data.loc[:1250]\n",
      "75/17: all_emails_training.tail()\n",
      "75/18: len(all_emails_training)\n",
      "75/19: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "75/20: grouped_from.tail()\n",
      "75/21: grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "75/22: grouped_from.head()\n",
      "75/23:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "75/24: all_emails_training.Subject.str.startswith('re')\n",
      "75/25: responses\n",
      "75/26: all_emails_training['is_thread'] = all_emails_training.Subject.str.startswith('re')\n",
      "75/27: all_emails_training.head()\n",
      "75/28: all_emails_training.subject.str.split(':', 2)\n",
      "75/29: all_emails_training.Subject.str.split(':', 2)\n",
      "75/30: all_emails_training.Subject.str.split(':', 1)\n",
      "75/31: all_emails_training['thread_subject'] = all_emails_training.Subject.str.split(':', 1)[-1]\n",
      "75/32: all_emails_training['thread_subject'] = all_emails_training.Subject.str.split(':', 1)[0]\n",
      "75/33: all_emails_training['thread_subject'] = all_emails_training.Subject.str.split(':', 1)\n",
      "75/34: all_emails_training.Subject.str.split(':', 1)[0]\n",
      "75/35: thread_subjects = [x[1] for x in all_emails_training.Subject.str.split(':', 1) if x[1]]\n",
      "75/36: thread_subjects = [x[1] for x in all_emails_training.Subject.str.split(':', 1) if len(x)>1]\n",
      "75/37: thread_subjects\n",
      "75/38: thread_subjects = [x[1] for x in all_emails_training.Subject.str.split(':', 1) if len(x)>1 else 0]\n",
      "75/39: all_emails_training.Subject.str.split(':', 1)[2]\n",
      "75/40: len(thread_subjects)\n",
      "76/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "\n",
      "from email.utils import parseaddr\n",
      "76/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "76/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "76/4: headers, messages = get_emails(easyham_path)\n",
      "76/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "76/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "76/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "76/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "76/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "76/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "76/11:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "76/12: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "76/13: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "76/14: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "76/15:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "76/16: all_emails_training = all_emails_data.loc[:1250]\n",
      "76/17: all_emails_training.tail()\n",
      "76/18: len(all_emails_training)\n",
      "76/19: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "76/20: grouped_from.tail()\n",
      "76/21: grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "76/22: grouped_from.head()\n",
      "76/23:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "76/24: all_emails_training['is_thread'] = all_emails_training.Subject.str.startswith('re')\n",
      "76/25: all_emails_training.head()\n",
      "76/26: thread_subjects = [x[1] for x in all_emails_training.Subject.str.split(':', 1) if len(x)>1]\n",
      "76/27: all_emails_training['thread_subject'] = all_emails_training.Subject.str.split(':', 1)\n",
      "76/28: all_emails_training.Subject.str.split(':', 1)[2]\n",
      "76/29: len(thread_subjects)\n",
      "76/30: all_emails_training.head()\n",
      "76/31: all_emails_training.head(10)\n",
      "76/32: all_emails_training.head(20)\n",
      "76/33: all_emails_training.Subject.str.split(':', 1)[3]\n",
      "76/34: all_emails_training.Subject.str.split(':', 2)[3]\n",
      "76/35: all_emails_training.Subject.str.split(':', 1)[3]\n",
      "76/36: all_emails_training.Subject.str.split(':', 1)[3][1]\n",
      "76/37: all_emails_training.Subject.str.split(':', 1)[0][1]\n",
      "76/38: all_emails_training.Subject.str.split(':', 1)[4][1]\n",
      "76/39: all_emails_training.Subject.str.split(':', 1)[18][1]\n",
      "76/40: all_emails_training['thread_subject'] = all_emails_training.Subject.str.split(':', 1)[1] if all_emails_training['is_thread'] else Nan\n",
      "76/41: all_emails_training['thread_subject'] = all_emails_training.Subject.str.split(':', 1)[1] if all_emails_training['is_thread'] else 'x'\n",
      "76/42: thread_subject = [x.str.split(':', 1)[1] for x in all_emails_training.subject if all_emails_training['is_thread'] else 'x']\n",
      "76/43: thread_subject = [x.str.split(':', 1)[1] for x in all_emails_training.Subject if all_emails_training['is_thread'] else 'x']\n",
      "76/44: thread_subject = [x.str.split(':', 1)[1] for x,y in all_emails_training.Subject,all_emails_training.is_thread if y else 'x']\n",
      "76/45: thread_subject = [x.str.split(':', 1)[1] for x,y in (all_emails_training.Subject,all_emails_training.is_thread) if y else 'x']\n",
      "76/46: thread_subject = [x.str.split(':', 1)[1] for (x,y) in (all_emails_training.Subject,all_emails_training.is_thread) if y else 'x']\n",
      "76/47: thread_subject = [x.str.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject,all_emails_training.is_thread) if y else 'x']\n",
      "76/48: thread_subject = [x.str.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject,all_emails_training.is_thread) if y else 'x']\n",
      "76/49: z=[x.str.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject,all_emails_training.is_thread) if y else 'x']\n",
      "76/50: z=[x.str.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject,all_emails_training.is_thread) if y]\n",
      "76/51: z=[x.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject,all_emails_training.is_thread) if y]\n",
      "76/52: z=[x.split(':', 1)[0] for (x,y) in zip(all_emails_training.Subject,all_emails_training.is_thread) if y]\n",
      "76/53: z.head()\n",
      "76/54: z[:10]\n",
      "76/55: splited_subjects = [x.split(':', 1) for (x,y) in zip(all_emails_training.Subject, all_emails_training.is_thread) if y]\n",
      "76/56: splited_subjects\n",
      "76/57: splited_subjects_len = [len(x) for x in splitted_subjects]\n",
      "76/58: splited_subjects_len = [len(x) for x in splited_subjects]\n",
      "76/59: min(splited_subjects_len)\n",
      "76/60: sorted(splited_subjects_len)\n",
      "76/61: sorted(zip(splited_subjects_len, splited_subjects))\n",
      "76/62: all_emails_training.loc[all_emails_training.Subject.contains('recomended')]\n",
      "76/63: all_emails_training.loc[all_emails_training.Subject.contain('recomended')]\n",
      "76/64: all_emails_training.loc[all_emails_training.Subject.str.contain('recomended')]\n",
      "76/65: all_emails_training.loc[all_emails_training.Subject.str.contains('recomended')]\n",
      "76/66: all_emails_training.loc[all_emails_training.Subject.str.contains('recommended')]\n",
      "76/67: all_emails_training.loc[all_emails_training.Subject.str.contains('recommended viewing')]\n",
      "76/68: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1\\]{0,1}')\n",
      "76/69: all_emails_training.head()\n",
      "76/70: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re[{0,1}')\n",
      "76/71: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}')\n",
      "76/72: all_emails_training.head()\n",
      "76/73: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}')\n",
      "76/74: all_emails_training.head(10)\n",
      "76/75: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}')\n",
      "76/76: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}')\n",
      "76/77: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "76/78: all_emails_training.head(100)\n",
      "76/79: splited_subjects = [x.split(':', 1) for (x,y) in zip(all_emails_training.Subject, all_emails_training.is_thread) if y]\n",
      "76/80: splited_subjects_len = [len(x) for x in splited_subjects]\n",
      "76/81: all_emails_training.loc[all_emails_training.Subject.str.contains('recommended viewing')]\n",
      "76/82: sorted(zip(splited_subjects_len, splited_subjects))\n",
      "76/83: splited_subjects = [x.split(':', 1)[0] for (x,y) in zip(all_emails_training.Subject, all_emails_training.is_thread) if y]\n",
      "76/84: splitted_subjects\n",
      "76/85: splited_subjects\n",
      "76/86: splited_subjects = [x.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject, all_emails_training.is_thread) if y]\n",
      "76/87: splited_subjects\n",
      "76/88: all_emails_training['thread_subject'] = splited_subjects\n",
      "76/89: all_emails_training['thread_subject'] = x.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject, all_emails_training.is_thread) if y\n",
      "76/90: all_emails_training['thread_subject'] = [(x.split(':', 1)[1] for (x,y) in zip(all_emails_training.Subject, all_emails_training.is_thread) if y) or 'x']\n",
      "77/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "\n",
      "from email.utils import parseaddr\n",
      "77/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "77/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "77/4: headers, messages = get_emails(easyham_path)\n",
      "77/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "77/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "77/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "77/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "77/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "77/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "77/11:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "77/12: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "77/13: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "77/14: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "77/15:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "77/16: all_emails_training = all_emails_data.loc[:1250]\n",
      "77/17: all_emails_training.tail()\n",
      "77/18: len(all_emails_training)\n",
      "77/19: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "77/20: grouped_from.tail()\n",
      "77/21: grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "77/22: grouped_from.head()\n",
      "77/23:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "77/24: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "77/25: all_emails_training.head(100)\n",
      "77/26:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subject.append(None)\n",
      "    \n",
      "    return threads_subjects\n",
      "77/27: threads_subjects = get_threads(all_emails_training)\n",
      "77/28:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append(None)\n",
      "    \n",
      "    return threads_subjects\n",
      "77/29: threads_subjects = get_threads(all_emails_training)\n",
      "77/30: threads_subjects[:10]\n",
      "77/31: all_emails_training['thread_subject'] = threads_subjects\n",
      "77/32: all_emails_training.head()\n",
      "77/33: x=Nan\n",
      "77/34: x=pd.Nan\n",
      "77/35: x=NaN\n",
      "77/36: x=pd.NaN\n",
      "77/37: x=np.nan\n",
      "77/38: x=nan\n",
      "77/39:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append(np.nan)\n",
      "    \n",
      "    return threads_subjects\n",
      "77/40: threads_subjects = get_threads(all_emails_training)\n",
      "77/41: threads_subjects[:10]\n",
      "77/42: all_emails_training['thread_subject'] = threads_subjects\n",
      "77/43: all_emails_training.head()\n",
      "77/44: grouped_from = grouped_from.Weight\n",
      "77/45: grouped_from.head()\n",
      "77/46:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "77/47:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "\n",
      "from email.utils import parseaddr\n",
      "77/48:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "77/49:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "77/50: headers, messages = get_emails(easyham_path)\n",
      "77/51: headers_lines = [header.split('\\n') for header in headers]\n",
      "77/52:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "77/53:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "77/54:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "77/55:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "77/56:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "77/57:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "77/58: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "77/59: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "77/60: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "77/61:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "77/62: all_emails_training = all_emails_data.loc[:1250]\n",
      "77/63: all_emails_training.tail()\n",
      "77/64: len(all_emails_training)\n",
      "77/65: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "77/66: grouped_from.tail()\n",
      "77/67:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "77/68: grouped_from.head()\n",
      "77/69:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "77/70: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "77/71: all_emails_training.head(100)\n",
      "77/72:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append(np.nan)\n",
      "    \n",
      "    return threads_subjects\n",
      "77/73: threads_subjects = get_threads(all_emails_training)\n",
      "77/74: threads_subjects[:10]\n",
      "77/75: all_emails_training['thread_subject'] = threads_subjects\n",
      "77/76: all_emails_training.head()\n",
      "77/77: grouped_from.tail()\n",
      "77/78: all_emails_training.head()\n",
      "77/79: emails_threads = all_emails_training[all_emails_training.is_thread=True]\n",
      "77/80: emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "77/81: emaisl_thread.head()\n",
      "77/82: emails_thread.head()\n",
      "77/83: emails_threads.head()\n",
      "77/84:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_thread.drop(['Message', 'is_thread'], axis=1)\n",
      "77/85:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1)\n",
      "77/86: emails_threads.head()\n",
      "77/87:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "77/88: emails_threads.head()\n",
      "77/89: emails_threads_senders = emails_threads.groupby(by=['From'])\n",
      "77/90: emails_threads_senders.head()\n",
      "77/91: emails_threads_senders = emails_threads.groupby(by=['From'].count())\n",
      "77/92: emails_threads_senders = emails_threads.groupby(by=['From']).count()\n",
      "77/93: emails_threads_senders.head()\n",
      "77/94: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "77/95: emails_threads_senders.head()\n",
      "77/96: emails_threads_senders.tail()\n",
      "77/97:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "77/98: emails_threads_senders.head()\n",
      "77/99: emails_threads_senders.tail()\n",
      "78/1: all_emails_training.groupby(by=['thread_subject'])\n",
      "78/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "\n",
      "from email.utils import parseaddr\n",
      "78/3:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "78/4:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "78/5: headers, messages = get_emails(easyham_path)\n",
      "78/6: headers_lines = [header.split('\\n') for header in headers]\n",
      "78/7:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "78/8:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "78/9:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "78/10:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "78/11:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "78/12:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "78/13: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "78/14: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "78/15: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "78/16:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "78/17: all_emails_training = all_emails_data.loc[:1250]\n",
      "78/18: len(all_emails_training)\n",
      "78/19: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "78/20:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "78/21:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "78/22: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "78/23:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append(np.nan)\n",
      "    \n",
      "    return threads_subjects\n",
      "78/24: threads_subjects = get_threads(all_emails_training)\n",
      "78/25: all_emails_training['thread_subject'] = threads_subjects\n",
      "78/26:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "78/27: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "78/28: emails_threads_senders.tail()\n",
      "78/29:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "78/30: emails_threads_senders.tail()\n",
      "78/31: all_emails_training.groupby(by=['thread_subject'])\n",
      "78/32: def get_thread_intensity(emails_df):\n",
      "78/33: thread_subjects = all_emails_training.groupby(by=['thread_subject'])\n",
      "78/34: thread_subjects\n",
      "78/35: print(thread_subjects)\n",
      "78/36: thread_subjects = all_emails_training.groupby(by=['thread_subject']).count(['Send_datetime'])\n",
      "78/37: thread_subjects = all_emails_training.groupby(by=['thread_subject']).count('Send_datetime')\n",
      "78/38: thread_subjects = all_emails_training.groupby(by=['thread_subject']).count()\n",
      "78/39: print(thread_subjects)\n",
      "78/40: thread_subjects = all_emails_training.groupby(by=['thread_subject']).count('index')\n",
      "78/41: thread_subjects = all_emails_training.groupby(by=['thread_subject']).size()\n",
      "78/42: print(thread_subjects)\n",
      "78/43: thread_number_of_messages = all_emails_training.groupby(by=['thread_subject']).size()\n",
      "78/44: thread_number_of_messages\n",
      "78/45: thread_number_of_messages[:5]\n",
      "78/46: thread_number_of_messages = all_emails_training.groupby(by=['thread_subject']).min('Send_datetime')\n",
      "78/47: thread_number_of_messages = all_emails_training.groupby(by=['thread_subject']).size()\n",
      "78/48: thread_number_of_messages[:5]\n",
      "78/49: grouped_by_thread_subject = all_emails_training.groupby(by=['thread_subject'])\n",
      "78/50: thread_number_of_messages = .grouped_by_thread_subject.size()\n",
      "78/51: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "78/52: thread_number_of_messages[:5]\n",
      "78/53: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform.min()\n",
      "78/54: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "78/55: print(minimum_send_datetime)\n",
      "78/56:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "emails_threads.head()\n",
      "78/57: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "78/58: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "78/59: thread_number_of_messages[:5]\n",
      "78/60: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "78/61: print(minimum_send_datetime)\n",
      "78/62: print(minimum_send_datetime[:5])\n",
      "78/63: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "78/64: print(maximum_send_datetime[:5])\n",
      "78/65: emails_threads['send_min', 'send_max'] = [minimum_send_datetime, maximum_send_datetime]\n",
      "78/66: len(maximum_send_datetime)\n",
      "78/67: len(minimum_send_datetime)\n",
      "78/68: len(emails_threads)\n",
      "78/69: emails_threads['send_min'] = minimum_send_datetime\n",
      "78/70: emails_threads['send_max'] = maximum_send_datetime\n",
      "78/71: emails_threads.head()\n",
      "78/72: emails_threads[thread_time] = emails_threads.send_max - emails_threads.send_min\n",
      "78/73: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min\n",
      "78/74: emails_threads.head()\n",
      "79/1: emails_threads.thread_density = 1\n",
      "79/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "\n",
      "from email.utils import parseaddr\n",
      "79/3:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "79/4:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "79/5: headers, messages = get_emails(easyham_path)\n",
      "79/6: headers_lines = [header.split('\\n') for header in headers]\n",
      "79/7:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "79/8:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "79/9:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "79/10:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "79/11:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "79/12:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "79/13: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "79/14: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "79/15: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "79/16:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "79/17: all_emails_training = all_emails_data.loc[:1250]\n",
      "79/18: len(all_emails_training)\n",
      "79/19: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "79/20:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "79/21:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "79/22: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "79/23:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append(np.nan)\n",
      "    \n",
      "    return threads_subjects\n",
      "79/24: threads_subjects = get_threads(all_emails_training)\n",
      "79/25: all_emails_training['thread_subject'] = threads_subjects\n",
      "79/26:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "emails_threads.head()\n",
      "79/27: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "79/28: emails_threads_senders.tail()\n",
      "79/29:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "79/30: emails_threads_senders.tail()\n",
      "79/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "79/32: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "79/33: thread_number_of_messages[:5]\n",
      "79/34: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "79/35: len(minimum_send_datetime)\n",
      "79/36: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "79/37: len(maximum_send_datetime)\n",
      "79/38: len(emails_threads)\n",
      "79/39: emails_threads['send_min'] = minimum_send_datetime\n",
      "79/40: emails_threads['send_max'] = maximum_send_datetime\n",
      "79/41: emails_threads.head()\n",
      "79/42: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min\n",
      "79/43: emails_threads.thread_density = 1\n",
      "79/44: emails_threads.head()\n",
      "79/45: emails_threads.thread_density = 1\n",
      "79/46: emails_threads.head()\n",
      "79/47: emails_threads['thread_density'] = 1\n",
      "79/48: emails_threads.head()\n",
      "79/49: emails_threads['thread_density'] = emails_threads.send_max-emails_threads.send_min\n",
      "79/50: emails_threads.head()\n",
      "79/51: emails_threads['thread_density'] = emails_threads.thread_time/thread_number_of_messages[emails_threads.thread_subject]\n",
      "79/52:\n",
      "for email in emails_threads:\n",
      "    email['thread_density'] = 1 # emails_threads.thread_time/thread_number_of_messages[emails_threads.thread_subject]\n",
      "79/53:\n",
      "for email in emails_threads[:10]:\n",
      "    print(email) #['thread_density'] = 1 # emails_threads.thread_time/thread_number_of_messages[emails_threads.thread_subject]\n",
      "79/54:\n",
      "for email in emails_threads[:20]:\n",
      "    print(email) #['thread_density'] = 1 # emails_threads.thread_time/thread_number_of_messages[emails_threads.thread_subject]\n",
      "79/55: emails_threads.head()\n",
      "79/56:\n",
      "for email in emails_threads.thread_density[:10]:\n",
      "    print(email) #['thread_density'] = 1 # emails_threads.thread_time/thread_number_of_messages[emails_threads.thread_subject]\n",
      "79/57: df = emails_threads.merge(thread_number_of_messages, on='thread_subject', how='left')\n",
      "79/58: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "79/59: df = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "79/60: df\n",
      "79/61: df.head()\n",
      "79/62: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "79/63: emails_threads.head()\n",
      "79/64: emails_threads.thread_density = emails_threads.Number_of_messages/emails_threads.thread_time\n",
      "79/65: emails_threads.describe()\n",
      "79/66: emails_threads.columns\n",
      "79/67: emails_threads.dtypes\n",
      "79/68: emails_threads.thread_density = emails_threads.Number_of_messages/emails_threads.thread_time.seconds\n",
      "79/69: emails_threads.thread_density = emails_threads.Number_of_messages/emails_threads.thread_time\n",
      "79/70: emails_threads.thread_density.min()\n",
      "79/71: emails_threads.thread_density.max()\n",
      "79/72: emails_threads.thread_time.min()\n",
      "79/73: emails_threads.thread_time.max()\n",
      "79/74: emails_threads.sortby(by='thread_time').head()\n",
      "79/75: emails_threads.sort_by(by='thread_time').head()\n",
      "79/76: emails_threads.sort_values(by='thread_time').head()\n",
      "79/77: emails_threads.sort_values(by='thread_time').head(20)\n",
      "79/78: emails_threads.loc(emails_threads.send_min==emails_threads.send_max)\n",
      "79/79: emails_threads.loc(emails_threads.send_min=emails_threads.send_max)\n",
      "79/80: emails_threads.loc(emails_threads.send_min==emails_threads.send_max)\n",
      "79/81: emails_threads.sort_values(by='thread_time').head()\n",
      "79/82: emails_threads.loc(emails_threads.thread_time==0)\n",
      "79/83: emails_threads.loc(emails_threads.thread_time=datetime.timedelta(seconds=0))\n",
      "79/84: emails_threads.loc[emails_threads.thread_time=datetime.timedelta(seconds=0)]\n",
      "79/85: emails_threads[emails_threads.thread_time=datetime.timedelta(seconds=0)]\n",
      "79/86: emails_threads[emails_threads.thread_time==datetime.timedelta(seconds=0)]\n",
      "79/87:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "79/88: emails_threads[emails_threads.thread_time==datetime.timedelta(seconds=0)]\n",
      "79/89: all_emails_training[all_emails_training.str.contain('automated forwarding')]\n",
      "79/90: all_emails_training[all_emails_training['Subject'].str.contain('automated forwarding')]\n",
      "79/91: all_emails_training[all_emails_training['Subject'].str.contains('automated forwarding')]\n",
      "79/92: 'alexander' in all_emails_training.Subject\n",
      "79/93: 'alexander' in all_emails_training.Subject.strip()\n",
      "79/94: 'alexander' in all_emails_training.Subject.rstrip()\n",
      "79/95: 'alexander' in all_emails_training.Subject.str.rstrip()\n",
      "79/96: 'problems with apt update' in all_emails_training.Subject.str.rstrip()\n",
      "79/97: all_emails_training.Subject.str.rstrip()\n",
      "79/98: all_emails_training.loc[0].Subject.str.rstrip()\n",
      "79/99: all_emails_training.loc[0].Subject.rstrip()\n",
      "79/100: 'please help a newbie compile mplayer :-)' in all_emails_training.Subject\n",
      "79/101: 'please help a newbie compile mplayer :-)' in all_emails_training.Subject.rstrip()\n",
      "79/102: 'please help a newbie compile mplayer :-)' in all_emails_training.Subject.str.rstrip()\n",
      "79/103: all_emails_training.loc[0].Subject\n",
      "79/104: 'please help a newbie compile mplayer :-)' in all_emails_training.Subject.str.rstrip()\n",
      "79/105: all_emails_training['Subject'].str.contains(help)\n",
      "79/106: all_emails_training['Subject'].str.contains('help')\n",
      "79/107: all_emails_training['Subject'].str.contains('please help a newbie compile mplayer :-)')\n",
      "79/108: all_emails_training['Subject'].str.contains('please help a newbie compile mplayer :-\\)')\n",
      "80/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "80/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "80/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "80/4: headers, messages = get_emails(easyham_path)\n",
      "80/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "80/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "80/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "80/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "80/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "80/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "80/11:\n",
      "for x in dates_lines[:5]:\n",
      "    print(x)\n",
      "80/12: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "80/13: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "80/14: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "80/15:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "80/16: all_emails_training = all_emails_data.loc[:1250]\n",
      "80/17: len(all_emails_training)\n",
      "80/18: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "80/19:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "80/20:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "80/21: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "80/22:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append(np.nan)\n",
      "    \n",
      "    return threads_subjects\n",
      "80/23: threads_subjects = get_threads(all_emails_training)\n",
      "80/24: all_emails_training['thread_subject'] = threads_subjects\n",
      "80/25:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "emails_threads.head()\n",
      "80/26: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "80/27: emails_threads_senders.tail()\n",
      "80/28:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "80/29: emails_threads_senders.tail()\n",
      "80/30: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "80/31: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "80/32: thread_number_of_messages[:5]\n",
      "80/33: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "80/34: len(minimum_send_datetime)\n",
      "80/35: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "80/36: len(maximum_send_datetime)\n",
      "80/37: len(emails_threads)\n",
      "80/38: emails_threads['send_min'] = minimum_send_datetime\n",
      "80/39: emails_threads['send_max'] = maximum_send_datetime\n",
      "80/40: emails_threads.head()\n",
      "80/41: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min\n",
      "80/42: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "80/43: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "80/44: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "80/45: emails_threads.sort_values(by='thread_time').head()\n",
      "80/46: emails_threads.dtypes\n",
      "80/47: emails_threads.thread_time.min()\n",
      "80/48: emails_threads.thread_time.max()\n",
      "80/49: emails_threads[emails_threads.thread_time==datetime.timedelta(seconds=0)]\n",
      "80/50: all_emails_training[all_emails_training['Subject'].str.contains('automated forwarding')]\n",
      "80/51: all_emails_training.loc[0].Subject\n",
      "80/52: 'please help a newbie compile mplayer :-)' in all_emails_training.Subject.str.rstrip()\n",
      "80/53: all_emails_training['Subject'].str.contains('please help a newbie compile mplayer :-\\)')\n",
      "80/54: all_emails_training.loc[all_emails_training.is_thread].head()\n",
      "80/55: all_emails_training.loc[~all_emails_training.is_thread].head()\n",
      "80/56: all_emails_training.loc[~all_emails_training.is_thread].Subject\n",
      "80/57:\n",
      "for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:\n",
      "    print(subject)\n",
      "80/58:\n",
      "for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:\n",
      "    if subject in all_emails_training.Subject:\n",
      "        print('tak')\n",
      "80/59: all_emails_training.Subject\n",
      "80/60: 'problems with apt update' in all_emails_training.Subject\n",
      "80/61: 'problems with apt update' in all_emails_training.Subject.strip()\n",
      "80/62: 'problems with apt update' in all_emails_training.Subject.str.strip()\n",
      "80/63: 'problems with apt update' in all_emails_training.Subject.str.rstrip()\n",
      "80/64: 're' in all_emails_training.Subject.str.rstrip()\n",
      "80/65: x = pd.series('a','b', 'c')\n",
      "80/66: x = pd.Series('a','b', 'c')\n",
      "80/67: x = pd.Series(['a','b', 'c'])\n",
      "80/68: 'a' in x\n",
      "80/69: x\n",
      "80/70: 'a' isin x\n",
      "80/71: x.isin('a')\n",
      "80/72: x.isin(['a'])\n",
      "80/73: x.isin(['a']).any()\n",
      "80/74: x.isin(['a']).all()\n",
      "80/75:\n",
      "for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:\n",
      "    if all_emails_training.Subject.isin([subject]).any():\n",
      "        print('tak')\n",
      "80/76: all_emails_training.Subject[:5]\n",
      "80/77: all_emails_training.is_thread = all_emails_training.is_thread or all_emails_training.Subject.isin(all_emails_training.thread_subject).any()\n",
      "80/78:\n",
      "for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:\n",
      "    if all_emails_training.Subject.isin([subject]).any():\n",
      "        all_emails_training.at['is_thread','subject']=True\n",
      "80/79:\n",
      "for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:\n",
      "    if all_emails_training.Subject.isin([subject]).any():\n",
      "        all_emails_training.loc['subject','is_thread']=True\n",
      "80/80:\n",
      "for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:\n",
      "    if all_emails_training.Subject.isin([subject]).any():\n",
      "        all_emails_training.loc['subject','is_thread']=True\n",
      "80/81:\n",
      "for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:\n",
      "    if all_emails_training.Subject.isin([subject]).any():\n",
      "        print('x')\n",
      "        #all_emails_training.loc['subject','is_thread']=True\n",
      "80/82:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "80/83:\n",
      "for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:\n",
      "    if all_emails_training.Subject.isin([subject]).any():\n",
      "        print('x')\n",
      "        #all_emails_training.loc['subject','is_thread']=True\n",
      "80/84: all_emails_training.head()\n",
      "80/85: all_emails_training.drop(['subject'], axis=1, inplace=True)\n",
      "80/86: all_emails_training.head()\n",
      "80/87:\n",
      "for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:\n",
      "    if all_emails_training.Subject.isin([subject]).any():\n",
      "        print('x')\n",
      "        #all_emails_training.loc['subject','is_thread']=True\n",
      "80/88:\n",
      "for subject in all_emails_training.loc[~all_emails_training.is_thread].Subject:\n",
      "    #if all_emails_training.Subject.isin([subject]).any():\n",
      "    print('x')\n",
      "        #all_emails_training.loc['subject','is_thread']=True\n",
      "80/89: all_emails_training.dtypes\n",
      "80/90:\n",
      "for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:\n",
      "    #if all_emails_training.Subject.isin([subject]).any():\n",
      "    print('x')\n",
      "        #all_emails_training.loc['subject','is_thread']=True\n",
      "80/91: all_emails_training.loc[all_emails_training.is_thread]\n",
      "80/92: all_emails_training.is_thread\n",
      "80/93: all_emails_training.tail()\n",
      "80/94: all_emails_training.drop(['is_thread'], axis=0, inplace=True)\n",
      "80/95: all_emails_training.tail()\n",
      "80/96:\n",
      "for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:\n",
      "    if all_emails_training.Subject.isin([subject]).any():\n",
      "        all_emails_training.at[subject,'is_thread']=True\n",
      "80/97: all_emails_training.tail()\n",
      "80/98:\n",
      "#for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:\n",
      " #   if all_emails_training.Subject.isin([subject]).any():\n",
      "  #      all_emails_training.at[subject,'is_thread']=True\n",
      "80/99:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "80/100:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "80/101:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "80/102: headers, messages = get_emails(easyham_path)\n",
      "80/103: headers_lines = [header.split('\\n') for header in headers]\n",
      "80/104:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "80/105:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "80/106:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "80/107:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "80/108:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "80/109: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "80/110: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "80/111: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "80/112:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "80/113: all_emails_training = all_emails_data.loc[:1250]\n",
      "80/114: len(all_emails_training)\n",
      "80/115: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "80/116:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "80/117:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "80/118: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "80/119: x = pd.Series(['a','b', 'c'])\n",
      "80/120: x\n",
      "80/121: x.isin(['a']).all()\n",
      "80/122: all_emails_training.drop(['subject'], axis=1, inplace=True)\n",
      "80/123: all_emails_training.drop(['is_thread'], axis=0, inplace=True)\n",
      "80/124: all_emails_training.tail()\n",
      "80/125:\n",
      "#for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:\n",
      " #   if all_emails_training.Subject.isin([subject]).any():\n",
      "  #      all_emails_training.at[subject,'is_thread']=True\n",
      "80/126: all_emails_training.Subject[:5]\n",
      "80/127:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "80/128: threads_subjects = get_threads(all_emails_training)\n",
      "80/129: all_emails_training['thread_subject'] = threads_subjects\n",
      "80/130:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "emails_threads.head()\n",
      "80/131: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "80/132: emails_threads_senders.tail()\n",
      "80/133:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "80/134: emails_threads_senders.tail()\n",
      "80/135: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "80/136: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "80/137: thread_number_of_messages[:5]\n",
      "80/138: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "80/139: len(minimum_send_datetime)\n",
      "80/140: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "80/141: len(maximum_send_datetime)\n",
      "80/142: len(emails_threads)\n",
      "80/143: emails_threads['send_min'] = minimum_send_datetime\n",
      "80/144: emails_threads['send_max'] = maximum_send_datetime\n",
      "80/145: emails_threads.head()\n",
      "80/146: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min\n",
      "80/147: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "80/148: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "80/149: emails_threads.sort_values(by='thread_time').head()\n",
      "80/150: emails_threads.thread_time.min()\n",
      "80/151: emails_threads.thread_time.max()\n",
      "80/152: emails_threads[emails_threads.thread_time==datetime.timedelta(seconds=0)]\n",
      "80/153: all_emails_training[all_emails_training['Subject'].str.contains('automated forwarding')]\n",
      "80/154: all_emails_training.loc[0].Subject\n",
      "80/155: 'please help a newbie compile mplayer :-)' in all_emails_training.Subject.str.rstrip()\n",
      "80/156: all_emails_training['Subject'].str.contains('please help a newbie compile mplayer :-\\)')\n",
      "80/157:\n",
      "for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:\n",
      "    if all_emails_training.Subject.isin([subject]).any():\n",
      "        all_emails_training.at[all_emails_training.loc[all_emails_training.Subject==subject].index,'is_thread']=True\n",
      "80/158: all_emails_training.head()\n",
      "80/159: all_emails_training.tail()\n",
      "80/160: all_emails_training.tail(20)\n",
      "80/161: all_emails_training.threas_subject.isin('oh my').any()\n",
      "80/162: all_emails_training.thread_subject.isin('oh my').any()\n",
      "80/163: all_emails_training.thread_subject.isin(['oh my']).any()\n",
      "80/164: all_emails_training.thread_subject.isin(['oh my...']).any()\n",
      "80/165: all_emails_training.at[1234,'thread_subject']\n",
      "80/166: all_emails_training.thread_subject.isin([' oh my...']).any()\n",
      "80/167: all_emails_training.tail()\n",
      "80/168: threads_subjects = get_threads(all_emails_training)\n",
      "80/169: all_emails_training['thread_subject'] = threads_subjects\n",
      "80/170:\n",
      "for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:\n",
      "    if all_emails_training.Subject.isin([subject]).any():\n",
      "        all_emails_training.at[all_emails_training.loc[all_emails_training.Subject==subject].index,'is_thread']=True\n",
      "80/171: all_emails_training.head()\n",
      "80/172: all_emails_training.thread_subject.isin(['please help a newbie compile mplayer :-)']).any()\n",
      "80/173: all_emails_training.at[1,'thread_subject']\n",
      "80/174:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1].rstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "80/175: threads_subjects = get_threads(all_emails_training)\n",
      "80/176:\n",
      "for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:\n",
      "    if all_emails_training.Subject.isin([subject]).any():\n",
      "        all_emails_training.at[all_emails_training.loc[all_emails_training.Subject==subject].index,'is_thread']=True\n",
      "80/177: all_emails_training.head()\n",
      "80/178: all_emails_training.at[1,'thread_subject']\n",
      "80/179: threads_subjects = get_threads(all_emails_training)\n",
      "80/180: threads_subjects = get_threads(all_emails_training)\n",
      "80/181:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1].rstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    print(threads_subjects[:10])\n",
      "    return threads_subjects\n",
      "80/182: threads_subjects = get_threads(all_emails_training)\n",
      "80/183: x='   abc'\n",
      "80/184:\n",
      "print(x)\n",
      "x.rstrip()\n",
      "print(x)\n",
      "80/185:\n",
      "print(x)\n",
      "x.strip()\n",
      "print(x)\n",
      "80/186:\n",
      "print(x)\n",
      "x.strip('\\s')\n",
      "print(x)\n",
      "80/187:\n",
      "print(x)\n",
      "x.strip(' ')\n",
      "print(x)\n",
      "80/188: x='       abc'\n",
      "80/189:\n",
      "print(x)\n",
      "x.strip(' ')\n",
      "print(x)\n",
      "80/190:\n",
      "print(x)\n",
      "x.rstrip(' ')\n",
      "print(x)\n",
      "80/191:\n",
      "print(x)\n",
      "x.lstrip()\n",
      "print(x)\n",
      "80/192:\n",
      "print(x)\n",
      "x=x.lstrip()\n",
      "print(x)\n",
      "80/193:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]#.rstrip()\n",
      "            thread_subject = thread_subject.strip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    print(threads_subjects[:10])\n",
      "    return threads_subjects\n",
      "80/194: threads_subjects = get_threads(all_emails_training)\n",
      "80/195:\n",
      "for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:\n",
      "    if all_emails_training.Subject.isin([subject]).any():\n",
      "        all_emails_training.at[all_emails_training.loc[all_emails_training.Subject==subject].index,'is_thread']=True\n",
      "80/196: all_emails_training.head()\n",
      "80/197: all_emails_training.at[1,'thread_subject']\n",
      "80/198:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]#.rstrip()\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    print(threads_subjects[:10])\n",
      "    return threads_subjects\n",
      "80/199: threads_subjects = get_threads(all_emails_training)\n",
      "80/200: all_emails_training['thread_subject'] = threads_subjects\n",
      "80/201:\n",
      "for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:\n",
      "    if all_emails_training.Subject.isin([subject]).any():\n",
      "        all_emails_training.at[all_emails_training.loc[all_emails_training.Subject==subject].index,'is_thread']=True\n",
      "80/202: all_emails_training.head()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/203: all_emails_training.at[1,'thread_subject']\n",
      "81/1: import pandas as pd\n",
      "82/1: %run Priority_box\n",
      "82/2: %run Priority_box.py\n",
      "85/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "85/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "85/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "85/4: headers, messages = get_emails(easyham_path)\n",
      "85/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "85/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "85/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "85/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "85/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "85/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "85/11: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "85/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "85/13: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "85/14:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "85/15: all_emails_training = all_emails_data.loc[:1250]\n",
      "85/16: len(all_emails_training)\n",
      "85/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "85/18:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "85/19:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "85/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "85/21: all_emails_training.thread_subject.isin(['please help a newbie compile mplayer :-)']).any()\n",
      "85/22: all_emails_training?\n",
      "85/23:\n",
      "for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:\n",
      "    if all_emails_training.Subject.isin([subject]).any():\n",
      "        all_emails_training.at[all_emails_training.loc[all_emails_training.Subject==subject].index,'is_thread']=True\n",
      "85/24:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    print(threads_subjects[:10])\n",
      "    return threads_subjects\n",
      "85/25: threads_subjects = get_threads(all_emails_training)\n",
      "85/26: all_emails_training['thread_subject'] = threads_subjects\n",
      "85/27:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "emails_threads.head()\n",
      "85/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "85/29: emails_threads_senders.tail()\n",
      "85/30:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "85/31: emails_threads_senders.tail()\n",
      "85/32: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "85/33: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "85/34: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "85/35: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "85/36: emails_threads['send_min'] = minimum_send_datetime\n",
      "85/37: emails_threads['send_max'] = maximum_send_datetime\n",
      "85/38: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min\n",
      "85/39: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "85/40: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "85/41: emails_threads.thread_time.min()\n",
      "85/42: emails_threads.thread_time.max()\n",
      "85/43: all_emails_training.thread_subject.isin(['please help a newbie compile mplayer :-)']).any()\n",
      "85/44: all_emails_training.head()\n",
      "85/45: all_emails_training.head(20)\n",
      "85/46: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "85/47: all_emails_training.head(2)\n",
      "85/48: all_emails_training.is_thread.where(threads_subject.isin(all_emails_training.Subject), True, False)\n",
      "85/49: all_emails_training.is_thread.where(threads_subjects.isin(all_emails_training.Subject), True, False)\n",
      "85/50: all_emails_training.head()\n",
      "85/51: all_emails_training.is_thread.where(~threads_subjects.isin(all_emails_training.Subject), True, inplace=True)\n",
      "85/52: all_emails_training.head()\n",
      "85/53: all_emails_training.is_thread.where(threads_subjects.isin(all_emails_training.Subject), True, inplace=True)\n",
      "85/54: all_emails_training.head()\n",
      "85/55: all_emails_training.is_thread.where(threads_subjects.isin(all_emails_training.Subject).any(), True, inplace=True)\n",
      "85/56: threads_subjects.isin(all_emails_training.Subject)\n",
      "85/57: all_emails_training.is_thread.where(threads_subjects.isin(all_emails_training.Subject), True, inplace=True)\n",
      "85/58: all_emails_training.head()\n",
      "85/59: ~threads_subjects.isin(all_emails_training.Subject)\n",
      "85/60: all_emails_training.is_thread.where(threads_subjects.isin(all_emails_training.Subject), other=True, inplace=True)\n",
      "85/61: all_emails_training.head()\n",
      "85/62: all_emails_training.is_thread.where(threads_subjects.isin(all_emails_training.Subject), other=True,axis=1, inplace=True)\n",
      "85/63: all_emails_training.is_thread.where(threads_subjects.isin(all_emails_training.Subject), other='xxxx', inplace=True)\n",
      "85/64: all_emails_training.head()\n",
      "85/65: all_emails_training.is_thread.where(all_emails_training.is_thread, other='xxxx', inplace=True)\n",
      "85/66: all_emails_training.thread_subject.where(all_emails_training.is_thread, other='xxxx', inplace=True)\n",
      "85/67: all_emails_training.thread_subject.where(all_emails_training.is_thread==True, other='xxxx', inplace=True)\n",
      "85/68: all_emails_training.head()\n",
      "85/69: all_emails_training.is_thread.where(threads_subject.isin([all_emails_training.Subject]), other=True, inplace=True)\n",
      "85/70: all_emails_training.is_thread.where(threads_subjects.isin([all_emails_training.Subject]), other=True, inplace=True)\n",
      "85/71: all_emails_training.head()\n",
      "85/72: all_emails_training.is_thread.where(~threads_subjects.isin([all_emails_training.Subject]), other=True, inplace=True)\n",
      "85/73: all_emails_training.head()\n",
      "85/74: all_emails_training.head(10)\n",
      "85/75: all_emails_training.head(15)\n",
      "85/76: ~all_emails_training.Subject.isin(threads_subjects)\n",
      "85/77: threads_subjects.isin(r\"please help a newbie compile mplayer :-)\")\n",
      "85/78: threads_subjects.isin([r\"please help a newbie compile mplayer :-)\"])\n",
      "85/79: threads_subjects[:10]\n",
      "85/80: all_emails_training.Subject[:10]\n",
      "85/81: all_emails_training.Subject[:10].isin(subjects_threads[:10])\n",
      "85/82: all_emails_training.Subject[:10].isin(threads_subjects[:10])\n",
      "85/83: all_emails_training.Subject[:10].isin(threads_subjects[:10]).any()\n",
      "85/84: all_emails_training.Subject[:10].isin(threads_subjects[:10])\n",
      "85/85: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subject), other=True, inplace=True)\n",
      "85/86: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "85/87: all_emails_training.head(15)\n",
      "85/88: all_emails_training.head()\n",
      "85/89: all_emails_training\n",
      "85/90:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "85/91:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "85/92:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "85/93: headers, messages = get_emails(easyham_path)\n",
      "85/94: headers_lines = [header.split('\\n') for header in headers]\n",
      "85/95:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "85/96:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "85/97:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "85/98:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "85/99:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "85/100: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "85/101: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "85/102: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "85/103:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "85/104: all_emails_training = all_emails_data.loc[:1250]\n",
      "85/105: len(all_emails_training)\n",
      "85/106: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "85/107:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "85/108:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "85/109: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "85/110:\n",
      "for subject in all_emails_training.loc[all_emails_training.is_thread].Subject:\n",
      "    if all_emails_training.Subject.isin([subject]).any():\n",
      "        all_emails_training.at[all_emails_training.loc[all_emails_training.Subject==subject].index,'is_thread']=True\n",
      "85/111:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "85/112:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "85/113:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "85/114: headers, messages = get_emails(easyham_path)\n",
      "85/115: headers_lines = [header.split('\\n') for header in headers]\n",
      "85/116:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "85/117:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "85/118:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "85/119:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "85/120:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "85/121: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "85/122: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "85/123: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "85/124:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "85/125: all_emails_training = all_emails_data.loc[:1250]\n",
      "85/126: len(all_emails_training)\n",
      "85/127: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "85/128:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "85/129:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "85/130: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "85/131:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    print(threads_subjects[:10])\n",
      "    return threads_subjects\n",
      "85/132: threads_subjects = get_threads(all_emails_training)\n",
      "85/133: all_emails_training['thread_subject'] = threads_subjects\n",
      "85/134: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "85/135: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "85/136:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "emails_threads.head()\n",
      "85/137: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "85/138: emails_threads_senders.tail()\n",
      "85/139:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "85/140: emails_threads_senders.tail()\n",
      "85/141: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "85/142: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "85/143: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "85/144: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "85/145: emails_threads['send_min'] = minimum_send_datetime\n",
      "85/146: emails_threads['send_max'] = maximum_send_datetime\n",
      "85/147: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min\n",
      "85/148: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "85/149: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "85/150: emails_threads.thread_time.min()\n",
      "85/151: emails_threads.thread_time.max()\n",
      "85/152: emails_threads['thread_density']=emails_threads.thread_time/emails_threads.thread_number_of_messages\n",
      "85/153: emails_threads['thread_density']=emails_threads.thread_time/emails_threads.Number_of_messages\n",
      "85/154: emails_threads\n",
      "85/155: all_emails_training.head(15)\n",
      "85/156:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "85/157:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "85/158:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "85/159: headers, messages = get_emails(easyham_path)\n",
      "85/160: headers_lines = [header.split('\\n') for header in headers]\n",
      "85/161:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "85/162:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "85/163:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "85/164:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "85/165:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "85/166: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "85/167: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "85/168: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "85/169:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "85/170: all_emails_training = all_emails_data.loc[:1250]\n",
      "85/171: len(all_emails_training)\n",
      "85/172: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "85/173:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "85/174:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "85/175: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "85/176:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    print(threads_subjects[:10])\n",
      "    return threads_subjects\n",
      "85/177: threads_subjects = get_threads(all_emails_training)\n",
      "85/178: all_emails_training['thread_subject'] = threads_subjects\n",
      "85/179: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "85/180: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "85/181: threads_subjects = get_threads(all_emails_training)\n",
      "85/182: all_emails_training.head()\n",
      "85/183: all_emails_training.thread_subject.where(~(all_emails_training.is_thread&all_emails_training.thread_subject=='x'),all_emails_training.Subject, inplace=True)\n",
      "85/184: all_emails_training.thread_subject.where(~(all_emails_training.is_thread and all_emails_training.thread_subject=='x'),all_emails_training.Subject, inplace=True)\n",
      "85/185: all_emails_training.loc[all_emails_training.is_thread==True].thread_subject.where(~(all_emails_training.thread_subject=='x'),all_emails_training.Subject, inplace=True)\n",
      "85/186: all_emails_training.head()\n",
      "85/187: all_emails_training.loc[all_emails_training.is_thread==True].thread_subject.where(~(all_emails_training.thread_subject=='x'),'xxx', inplace=True)\n",
      "85/188: all_emails_training.head()\n",
      "85/189: all_emails_training.loc[all_emails_training.is_thread==True]\n",
      "85/190: all_emails_training.loc[all_emails_training.is_thread==True].thread_subject\n",
      "85/191: all_emails_training.loc[all_emails_training.is_thread==True].thread_subject.where(~(thread_subject=='x'),'xxx', inplace=True)\n",
      "85/192:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),'xxx', inplace=True)\n",
      "85/193: all_emails_training.head()\n",
      "85/194:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "85/195: all_emails_training['thread_subject'] = threads_subjects\n",
      "85/196:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='xxx'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "85/197: all_emails_training.head()\n",
      "85/198: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "85/199:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    print(threads_subjects[:10])\n",
      "    return threads_subjects\n",
      "85/200: threads_subjects = get_threads(all_emails_training)\n",
      "85/201: all_emails_training['thread_subject'] = threads_subjects\n",
      "85/202: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "85/203: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "85/204: all_emails_training.head()\n",
      "85/205:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "85/206: all_emails_training.head()\n",
      "85/207: all_emails_training['thread_subject'] = threads_subjects\n",
      "85/208:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "emails_threads.head()\n",
      "85/209: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "85/210: emails_threads_senders.tail()\n",
      "85/211:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "85/212: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "85/213: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "85/214: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "85/215: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "85/216: emails_threads['send_min'] = minimum_send_datetime\n",
      "85/217: emails_threads['send_max'] = maximum_send_datetime\n",
      "85/218: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min\n",
      "85/219: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "85/220: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "85/221: emails_threads.thread_time.min()\n",
      "85/222: emails_threads.thread_time.max()\n",
      "85/223: emails_threads['thread_density']=emails_threads.thread_time/emails_threads.Number_of_messages\n",
      "85/224: emails_threads\n",
      "85/225: all_emails_training.head(15)\n",
      "85/226: emails_threads.head(10)\n",
      "85/227:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "85/228: all_emails_training.head()\n",
      "85/229:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "85/230:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "85/231:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "85/232: headers, messages = get_emails(easyham_path)\n",
      "85/233: headers_lines = [header.split('\\n') for header in headers]\n",
      "85/234:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "85/235:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "85/236:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "85/237:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "85/238:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "85/239: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "85/240: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "85/241: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "85/242:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "85/243: all_emails_training = all_emails_data.loc[:1250]\n",
      "85/244: len(all_emails_training)\n",
      "85/245: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "85/246:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "85/247:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "85/248: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "85/249:\n",
      "def get_threads(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    print(threads_subjects[:10])\n",
      "    return threads_subjects\n",
      "85/250: threads_subjects = get_threads(all_emails_training)\n",
      "85/251: all_emails_training['thread_subject'] = threads_subjects\n",
      "85/252: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "85/253: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "85/254: all_emails_training.head()\n",
      "85/255:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "85/256:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "emails_threads.head()\n",
      "85/257: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "85/258: emails_threads_senders.tail()\n",
      "85/259:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "85/260: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "85/261: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "85/262: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "85/263: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "85/264: emails_threads['send_min'] = minimum_send_datetime\n",
      "85/265: emails_threads['send_max'] = maximum_send_datetime\n",
      "85/266: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min\n",
      "85/267: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "85/268: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "85/269: emails_threads.thread_time.min()\n",
      "85/270: emails_threads.thread_time.max()\n",
      "85/271: emails_threads['thread_density']=emails_threads.thread_time/emails_threads.Number_of_messages\n",
      "85/272: emails_threads.head(10)\n",
      "85/273: all_emails_training.head(15)\n",
      "85/274: emails_threads.groupby(by=thread_subject)\n",
      "85/275: emails_threads.groupby(by=[thread_subject])\n",
      "85/276: emails_threads.head()\n",
      "86/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "86/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "86/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "86/4: headers, messages = get_emails(easyham_path)\n",
      "86/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "86/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "86/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "86/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "86/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "86/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "86/11: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "86/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "86/13: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "86/14:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "86/15: all_emails_training = all_emails_data.loc[:1250]\n",
      "86/16: len(all_emails_training)\n",
      "86/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "86/18:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "86/19:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "86/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "86/21:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "86/22: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "86/23: all_emails_training['thread_subject'] = threads_subjects\n",
      "86/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "86/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "86/26:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "86/27:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "86/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "86/29: emails_threads_senders.tail()\n",
      "86/30:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "86/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "86/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "86/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "86/34: emails_threads['send_min'] = minimum_send_datetime\n",
      "86/35: emails_threads['send_max'] = maximum_send_datetime\n",
      "86/36: emails_threads['thread_time'] = emails_threads.send_max - emails_threads.send_min\n",
      "86/37: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "86/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "86/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "86/40: emails_threads.thread_time.min()\n",
      "86/41: emails_threads.thread_time.max()\n",
      "86/42: emails_threads['thread_density']=emails_threads.thread_time/emails_threads.Number_of_messages\n",
      "86/43: emails_threads.head()\n",
      "86/44: emails_threads.sort_values(by='thread_density')\n",
      "86/45: emails_threads.loc[emails_threads.thread_time=0]\n",
      "86/46: emails_threads.loc[emails_threads.thread_time==0]\n",
      "86/47: emails_threads.loc[emails_threads.thread_time==datetime(0)]\n",
      "86/48: emails_threads.loc[emails_threads.thread_time==datetime(0,0,0)]\n",
      "86/49: emails_threads.loc[emails_threads.thread_time==datetime.datetime(0,0,0)]\n",
      "86/50: emails_threads.loc[emails_threads.thread_time==datetime.timedelta(0)]\n",
      "86/51: 'snow' in all_emails_training.Subject\n",
      "86/52: 're: snow' in all_emails_training.Subject\n",
      "86/53: all_emails_training.loc[711, 'Subject']\n",
      "86/54: all_emails_training.Subject[all_emails_training.Subject.contains('snow')]\n",
      "86/55: all_emails_training.Subject[all_emails_training.Subject.str.contains('snow')]\n",
      "86/56: all_emails_training.Subject[all_emails_training.Subject.str.contains('underground')]\n",
      "86/57: all_emails_training.Subject[all_emails_training.Subject.str.contains('satalk')]\n",
      "86/58: all_emails_training.Subject[all_emails_training.Subject.str.contains('spamassasin')]\n",
      "86/59: all_emails_training.Subject[all_emails_training.Subject.str.contains('spamassassin')]\n",
      "86/60: emails_threads.thread_density.where(emails_threads.number_of_messages>1)\n",
      "86/61: emails_threads.thread_density.where(emails_threads.Number_of_messages>1)\n",
      "86/62: emails_threads.thread_density.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "86/63: emails_threads.loc[emails_threads.thread_time==datetime.timedelta(0)]\n",
      "86/64: emails_threads.head()\n",
      "86/65: emails_threads.head(10)\n",
      "86/66: emails_threads['thread_density']=(emails_threads.thread_time/emails_threads.Number_of_messages).total_seconds()\n",
      "86/67: emails_threads['thread_density']=(emails_threads.thread_time/emails_threads.Number_of_messages).dt.total_seconds()\n",
      "86/68: emails_threads.head(10)\n",
      "86/69: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "86/70: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "86/71: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "86/72: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "86/73: emails_threads.thread_time\n",
      "86/74: emails_threads.sort_values(by='thread_time')\n",
      "86/75: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "86/76: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "86/77: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "86/78: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "86/79: emails_threads.thread_time.min()\n",
      "86/80: emails_threads.thread_time.max()\n",
      "86/81: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "86/82: emails_threads.sort_values(by='thread_time')\n",
      "86/83:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "86/84:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "86/85:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "86/86: headers, messages = get_emails(easyham_path)\n",
      "86/87: headers_lines = [header.split('\\n') for header in headers]\n",
      "86/88:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "86/89:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "86/90:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "86/91:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "86/92:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "86/93: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "86/94: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "86/95: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "86/96:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "86/97: all_emails_training = all_emails_data.loc[:1250]\n",
      "86/98: len(all_emails_training)\n",
      "86/99: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "86/100:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "86/101:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "86/102: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "86/103:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "86/104: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "86/105: all_emails_training['thread_subject'] = threads_subjects\n",
      "86/106: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "86/107: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "86/108:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "86/109:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "86/110: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "86/111: emails_threads_senders.tail()\n",
      "86/112:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "86/113: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "86/114: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "86/115: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "86/116: emails_threads['send_min'] = minimum_send_datetime\n",
      "86/117: emails_threads['send_max'] = maximum_send_datetime\n",
      "86/118: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "86/119: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "86/120: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "86/121: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "86/122: emails_threads.thread_time.min()\n",
      "86/123: emails_threads.thread_time.max()\n",
      "86/124: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "86/125: emails_threads.sort_values(by='thread_time')\n",
      "86/126: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "86/127: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "86/128: emails_threads.head(10)\n",
      "86/129: emails_threads.sort_values(by='thread_density')\n",
      "86/130: emails_threads.sort_values(by='thread_density').max()\n",
      "86/131: emails_threads.loc[emails_threads.thread_subject=='whoa}']\n",
      "86/132: emails_threads.loc[emails_threads.thread_subject=='whoa']\n",
      "86/133: emails_threads.loc[emails_threads.thread_subject=='whoa}']\n",
      "86/134: emails_threads.tail()\n",
      "86/135: emails_threads.loc[emails_threads.thread_subject=='whoa']\n",
      "86/136: emails_threads.loc[2498]\n",
      "86/137: emails_threads.index.max()\n",
      "86/138: emails_threads.index\n",
      "86/139: emails_threads.loc['index']\n",
      "86/140: emails_threads.loc[:,'index']\n",
      "86/141: emails_threads.loc[:,'index'].max()\n",
      "86/142: emails_threads.sort_values(by='index')\n",
      "86/143: emails_threads.sort_values(by='thread_denisty').max()\n",
      "86/144: emails_threads.sort_values(by='thread_density').max()\n",
      "86/145: emails_threads.loc[emails_threads.Number_of_messages==42]\n",
      "87/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "87/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "87/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "87/4: headers, messages = get_emails(easyham_path)\n",
      "87/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "87/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "87/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "87/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "87/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "87/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "87/11: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "87/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "87/13: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "87/14:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "87/15: all_emails_training = all_emails_data.loc[:1250]\n",
      "87/16: len(all_emails_training)\n",
      "87/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "87/18:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "87/19:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "87/20: all_emails_training.loc[:,'is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "87/21: all_emails_training.loc[:,['is_thread']] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "87/22: all_emails_training.loc['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "87/23:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "87/24: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "87/25:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "87/26:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "87/27:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "87/28: headers, messages = get_emails(easyham_path)\n",
      "87/29: headers_lines = [header.split('\\n') for header in headers]\n",
      "87/30:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "87/31:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "87/32:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "87/33:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "87/34:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "87/35: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "87/36: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "87/37: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "87/38:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "87/39: all_emails_training = all_emails_data.loc[:1250]\n",
      "87/40: len(all_emails_training)\n",
      "87/41: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "87/42:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "87/43:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "87/44: all_emails_training.loc['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "87/45:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "87/46: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "87/47: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "87/48:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "87/49: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "87/50: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "87/51:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "87/52:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "87/53:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "87/54: headers, messages = get_emails(easyham_path)\n",
      "87/55: headers_lines = [header.split('\\n') for header in headers]\n",
      "87/56:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "87/57:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "87/58:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "87/59:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "87/60:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "87/61: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "87/62: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "87/63: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "87/64:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "87/65: all_emails_training = all_emails_data.loc[:1250]\n",
      "87/66: len(all_emails_training)\n",
      "87/67: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "87/68:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "87/69:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "87/70: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "87/71:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "87/72: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "87/73: all_emails_training['thread_subject'] = threads_subjects\n",
      "87/74: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "87/75: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "87/76:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "87/77:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "87/78: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "87/79: emails_threads_senders.tail()\n",
      "87/80:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "87/81: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "87/82: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "87/83: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "87/84: emails_threads['send_min'] = minimum_send_datetime\n",
      "87/85: emails_threads['send_max'] = maximum_send_datetime\n",
      "87/86: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "87/87: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "87/88: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "87/89: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "87/90: emails_threads.thread_time.min()\n",
      "87/91: emails_threads.thread_time.max()\n",
      "87/92: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "87/93: emails_threads.sort_values(by='thread_time')\n",
      "87/94: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "87/95: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "87/96: emails_threads.tail()\n",
      "87/97: emails_threads.sort_values(by='thread_density').max()\n",
      "87/98: emails_threads.loc[emails_threads.Number_of_messages==42]\n",
      "87/99: emails_threads.loc[:,'index'].max()\n",
      "87/100: emails_threads.sort_values(by='thread_density')\n",
      "87/101: help(pd.sort_values)\n",
      "87/102: help(pd.DataFrame.sort_values)\n",
      "87/103: emails_threads.sort_values(by='thread_density', ascending=False)\n",
      "87/104: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+10\n",
      "87/105: emails_threads.sort_values(by='thread_density', ascending=False)\n",
      "87/106: emails_threads.head()\n",
      "87/107: emails_threads.head(20)\n",
      "88/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "88/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "88/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "88/4: headers, messages = get_emails(easyham_path)\n",
      "88/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "88/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "88/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "88/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "88/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "88/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "88/11: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "88/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "88/13: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "88/14:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "88/15: all_emails_training = all_emails_data.loc[:1250]\n",
      "88/16: len(all_emails_training)\n",
      "88/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "88/18:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "88/19:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "88/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "88/21:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "88/22: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "88/23: all_emails_training['thread_subject'] = threads_subjects\n",
      "88/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "88/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "88/26:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "88/27:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "88/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "88/29: emails_threads_senders.tail()\n",
      "88/30:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "88/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "88/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "88/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "88/34: emails_threads['send_min'] = minimum_send_datetime\n",
      "88/35: emails_threads['send_max'] = maximum_send_datetime\n",
      "88/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "88/37: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "88/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "88/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "88/40: emails_threads.thread_time.min()\n",
      "88/41: emails_threads.thread_time.max()\n",
      "88/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "88/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "88/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "88/45: emails_threads.head()\n",
      "88/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+10\n",
      "88/47: def thread_tdm(df):\n",
      "88/48: emails_threads.head()\n",
      "88/49: all_emails_training.head()\n",
      "88/50:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "88/51: stop_words = stopwords.words('english')\n",
      "88/52:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "88/53:\n",
      "def thread_tdm(df):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "88/54:\n",
      "def thread_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "88/55: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].message)\n",
      "88/56: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "88/57: thread_tdm.head()\n",
      "88/58: thread_tdm.head(10)\n",
      "88/59: all_emails_training[all_emails_training.is_thread==True].Message\n",
      "88/60: all_emails_training[all_emails_training.is_thread==True].Message[:2]\n",
      "88/61: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[:1])\n",
      "88/62: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[:2])\n",
      "88/63: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "88/64:\n",
      "def thread_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "88/65: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "88/66: thread_tdm.head(10)\n",
      "88/67: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[:2])\n",
      "88/68: list(all_emails_training[all_emails_training.is_thread==True].Message[:2])\n",
      "88/69: thread_tdm = thread_tdm(list(all_emails_training[all_emails_training.is_thread==True].Message)[:2])\n",
      "88/70:\n",
      "def make_thread_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "88/71: thread_tdm = thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)[:2]\n",
      "88/72: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)[:2]\n",
      "88/73: thread_tdm.head(10)\n",
      "88/74: delete thread_tdm\n",
      "88/75: delete(thread_tdm)\n",
      "88/76: del(thread_tdm)\n",
      "88/77: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)[:2]\n",
      "88/78: thread_tdm.head(10)\n",
      "88/79:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "88/80:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "88/81:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "88/82: headers, messages = get_emails(easyham_path)\n",
      "88/83: headers_lines = [header.split('\\n') for header in headers]\n",
      "88/84:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "88/85:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "88/86:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "88/87:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "88/88:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "88/89: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "88/90: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "88/91: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "88/92:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "88/93: all_emails_training = all_emails_data.loc[:1250]\n",
      "88/94: len(all_emails_training)\n",
      "88/95: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "88/96:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "88/97:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "88/98: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "88/99:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "88/100: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "88/101: all_emails_training['thread_subject'] = threads_subjects\n",
      "88/102: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "88/103: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "88/104:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "88/105:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "88/106: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "88/107: emails_threads_senders.tail()\n",
      "88/108:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "88/109: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "88/110: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "88/111: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "88/112: emails_threads['send_min'] = minimum_send_datetime\n",
      "88/113: emails_threads['send_max'] = maximum_send_datetime\n",
      "88/114: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "88/115: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "88/116: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "88/117: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "88/118: emails_threads.thread_time.min()\n",
      "88/119: emails_threads.thread_time.max()\n",
      "88/120: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "88/121: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "88/122: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "88/123: emails_threads.head()\n",
      "88/124: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+10\n",
      "88/125: all_emails_training.head()\n",
      "88/126: stop_words = stopwords.words('english')\n",
      "88/127:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "88/128:\n",
      "def make_thread_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "88/129: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)[:2]\n",
      "88/130: thread_tdm.head(10)\n",
      "88/131: list(all_emails_training[all_emails_training.is_thread==True].Message[:2])\n",
      "88/132: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)[:1]\n",
      "88/133: thread_tdm.head(10)\n",
      "90/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "90/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "90/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "90/4: headers, messages = get_emails(easyham_path)\n",
      "90/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "90/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "90/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "90/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "90/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "90/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "90/11: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "90/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "90/13: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "90/14:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "90/15: all_emails_training = all_emails_data.loc[:1250]\n",
      "90/16: len(all_emails_training)\n",
      "90/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "90/18:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "90/19:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "90/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "90/21:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "90/22: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "90/23: all_emails_training['thread_subject'] = threads_subjects\n",
      "90/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "90/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "90/26:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "90/27:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "90/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "90/29: emails_threads_senders.tail()\n",
      "90/30:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "90/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "90/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "90/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "90/34: emails_threads['send_min'] = minimum_send_datetime\n",
      "90/35: emails_threads['send_max'] = maximum_send_datetime\n",
      "90/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "90/37: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "90/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "90/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "90/40: emails_threads.thread_time.min()\n",
      "90/41: emails_threads.thread_time.max()\n",
      "90/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "90/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "90/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "90/45: emails_threads.head()\n",
      "90/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+10\n",
      "90/47: all_emails_training.head()\n",
      "90/48: stop_words = stopwords.words('english')\n",
      "90/49:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "90/50:\n",
      "def make_thread_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "90/51: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)[:1]\n",
      "90/52: thread_tdm.head(10)\n",
      "90/53: list(all_emails_training[all_emails_training.is_thread==True].Message[:2])\n",
      "90/54:\n",
      "def make_thread_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "90/55: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)[:1]\n",
      "90/56: len(all_emails_training[all_emails_training.is_thread==True].Message[0])\n",
      "90/57: all_emails_trainins[all_emails_training.is_thread==True].Message[0]\n",
      "90/58: all_emails_trainins[all_emails_training.is_thread==True].Message[0]\n",
      "90/59: all_emails_training[all_emails_training.is_thread==True].Message[0]\n",
      "90/60: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[0])\n",
      "90/61: thread_tdm = make_thread_tdm(list[all_emails_training[all_emails_training.is_thread==True].Message[0]])\n",
      "90/62: thread_tdm = make_thread_tdm(list(all_emails_training[all_emails_training.is_thread==True].Message[0]))\n",
      "90/63: all_emails_training[all_emails_training.is_thread==True].Message[:2]\n",
      "90/64: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[:2])\n",
      "90/65: thread_tdm.head(10)\n",
      "90/66: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "90/67: thread_tdm.head(10)\n",
      "90/68: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[:10])\n",
      "90/69: thread_tdm.head(10)\n",
      "90/70:\n",
      "def make_thread_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "90/71:\n",
      "def make_thread_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "90/72: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[:10])\n",
      "90/73: thread_tdm.head(10)\n",
      "90/74: stop_words = stopwords.words('english')\n",
      "90/75:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "90/76:\n",
      "def make_thread_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "90/77: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message[:10])\n",
      "90/78: thread_tdm.head(10)\n",
      "90/79: thread_tdm.head()\n",
      "90/80: thread_tdm.head(20)\n",
      "90/81: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "90/82: thread_tdm.head(20)\n",
      "90/83: thread_tdm.tail(20)\n",
      "91/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "91/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "91/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "91/4: headers, messages = get_emails(easyham_path)\n",
      "91/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "91/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "91/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "91/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "91/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "91/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/11: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "91/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "91/13: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "91/14:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "91/15: all_emails_training = all_emails_data.loc[:1250]\n",
      "91/16: len(all_emails_training)\n",
      "91/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "91/18:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "91/19:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "91/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "91/21:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "91/22: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "91/23: all_emails_training['thread_subject'] = threads_subjects\n",
      "91/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "91/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "91/26:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "91/27:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "91/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "91/29: emails_threads_senders.tail()\n",
      "91/30:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "91/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "91/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "91/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "91/34: emails_threads['send_min'] = minimum_send_datetime\n",
      "91/35: emails_threads['send_max'] = maximum_send_datetime\n",
      "91/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "91/37: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "91/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "91/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "91/40: emails_threads.thread_time.min()\n",
      "91/41: emails_threads.thread_time.max()\n",
      "91/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "91/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "91/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "91/45: emails_threads.head()\n",
      "91/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+10\n",
      "91/47: all_emails_training.head()\n",
      "91/48: stop_words = stopwords.words('english')\n",
      "91/49:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "91/50:\n",
      "def make_thread_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    \n",
      "    return matrix\n",
      "91/51: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "91/52: thread_tdm.tail(20)\n",
      "91/53: len(all_emails_training[all_emails_training.is_thread==True].Message[0])\n",
      "91/54: all_emails_training[all_emails_training.is_thread==True].Message[:2]\n",
      "91/55: thread_tdm['help']\n",
      "91/56: emails_threads.head()\n",
      "91/57: emails_threads.head(8)\n",
      "91/58: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12\n",
      "91/59: emails_threads.head(8)\n",
      "91/60:\n",
      "def make_thread_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])\n",
      "    \n",
      "    return matrix\n",
      "91/61: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "91/62: thread_tdm.tail()\n",
      "91/63:\n",
      "def make_thread_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10\n",
      "    \n",
      "    return matrix\n",
      "91/64: thread_tdm = make_thread_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "91/65: thread_tdm.tail()\n",
      "91/66:\n",
      "emails_tdm = make_tdm(all_emails_training.Message)\n",
      "emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2\n",
      "91/67:\n",
      "def make_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10\n",
      "    \n",
      "    return matrix\n",
      "91/68:\n",
      "emails_tdm = make_tdm(all_emails_training.Message)\n",
      "emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2\n",
      "91/69: grouped_from.head()\n",
      "91/70: grouped_from.head(20)\n",
      "91/71: grouped_from.tail(20)\n",
      "91/72:\n",
      "#grouped_from.tail(20 -social activity measure\n",
      "emails_senders.head()\n",
      "91/73:\n",
      "#grouped_from.tail(20 -social activity measure\n",
      "emails_threads_senders.head()\n",
      "91/74:\n",
      "#grouped_from.tail(20 -social activity measure\n",
      "emails_threads_senders.tail()\n",
      "91/75:\n",
      "#grouped_from.tail(20 -social activity measure\n",
      "emails_threads_senders.tail()\n",
      "91/76:\n",
      "emails_tdm = make_tdm(all_emails_training.Message)\n",
      "emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2\n",
      "emails_tdm.tail()\n",
      "91/77:\n",
      "#grouped_from.tail(20 -social activity measure\n",
      "#emails_threads_senders.tail() -thread senders activity measure\n",
      "thread_tdm.weight.tail()\n",
      "92/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "92/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "92/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "92/4: headers, messages = get_emails(easyham_path)\n",
      "92/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "92/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "92/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "92/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "92/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "92/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "92/11: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "92/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "92/13: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "92/14:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "92/15: all_emails_training = all_emails_data.loc[:1250]\n",
      "92/16: len(all_emails_training)\n",
      "92/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "92/18:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "92/19:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "92/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "92/21:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "92/22: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "92/23: all_emails_training['thread_subject'] = threads_subjects\n",
      "92/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "92/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "92/26:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "92/27:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "92/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "92/29: emails_threads_senders.tail()\n",
      "92/30:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "92/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "92/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "92/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "92/34: emails_threads['send_min'] = minimum_send_datetime\n",
      "92/35: emails_threads['send_max'] = maximum_send_datetime\n",
      "92/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "92/37: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "92/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "92/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "92/40: emails_threads.thread_time.min()\n",
      "92/41: emails_threads.thread_time.max()\n",
      "92/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "92/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "92/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "92/45: emails_threads.head(8)\n",
      "92/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12\n",
      "92/47: all_emails_training.head()\n",
      "92/48: stop_words = stopwords.words('english')\n",
      "92/49:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "92/50:\n",
      "def make_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10\n",
      "    \n",
      "    return matrix\n",
      "92/51: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "92/52: thread_tdm.tail()\n",
      "92/53:\n",
      "emails_tdm = make_tdm(all_emails_training.Message)\n",
      "emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2\n",
      "emails_tdm.tail()\n",
      "92/54:\n",
      "#grouped_from.tail(20 -social activity measure\n",
      "#emails_threads_senders.tail() -thread senders activity measure\n",
      "#emails_threads.thread_density -thread activity measure\n",
      "#thread_tdm.weight -words in active threads mesure\n",
      "#emails_tdm.weight -words in all messages measure\n",
      "92/55:\n",
      "get_from_measure(senders):\n",
      "    x = [grouped_from[sender] for sender in senders if sender in grouped_from else 1]\n",
      "    from_measure = grouped_from[sender]\n",
      "92/56:\n",
      "get_from_measure(senders):\n",
      "    x = [grouped_from[sender] for sender in senders if sender in grouped_from else 1]\n",
      "92/57:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "92/58:\n",
      "get_from_measure(senders):\n",
      "    x = [grouped_from[sender] for sender in senders if sender in grouped_from else 1]\n",
      "92/59:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from[sender] for sender in senders if sender in grouped_from else 1]\n",
      "94/1:\n",
      "import pandas as pd\n",
      "a = pd.Series({'a':1, 'b':2})\n",
      "94/2: a\n",
      "94/3:\n",
      "def y(x):\n",
      "    result = [a.get(z,1) for z in x]\n",
      "    print(result)\n",
      "94/4: y(['a','c','b'])\n",
      "93/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "93/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "93/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "93/4: headers, messages = get_emails(easyham_path)\n",
      "93/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "93/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "93/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "93/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "93/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "93/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "93/11: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "93/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "93/13: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "93/14:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "93/15: all_emails_training = all_emails_data.loc[:1250]\n",
      "93/16: len(all_emails_training)\n",
      "93/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "93/18:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "93/19:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "93/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "93/21:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "93/22: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "93/23: all_emails_training['thread_subject'] = threads_subjects\n",
      "93/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "93/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "93/26:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "93/27:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "93/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "93/29: emails_threads_senders.tail()\n",
      "93/30:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "93/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "93/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "93/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "93/34: emails_threads['send_min'] = minimum_send_datetime\n",
      "93/35: emails_threads['send_max'] = maximum_send_datetime\n",
      "93/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "93/37: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "93/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "93/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "93/40: emails_threads.thread_time.min()\n",
      "93/41: emails_threads.thread_time.max()\n",
      "93/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "93/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "93/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "93/45: emails_threads.head(8)\n",
      "93/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12\n",
      "93/47: all_emails_training.head()\n",
      "93/48: stop_words = stopwords.words('english')\n",
      "93/49:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "93/50:\n",
      "def make_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10\n",
      "    \n",
      "    return matrix\n",
      "93/51: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "93/52: thread_tdm.tail()\n",
      "93/53:\n",
      "emails_tdm = make_tdm(all_emails_training.Message)\n",
      "emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2\n",
      "emails_tdm.tail()\n",
      "93/54:\n",
      "#grouped_from.tail(20 -social activity measure\n",
      "#emails_threads_senders.tail() -thread senders activity measure\n",
      "#emails_threads.thread_density -thread activity measure\n",
      "#thread_tdm.weight -words in active threads mesure\n",
      "#emails_tdm.weight -words in all messages measure\n",
      "93/55:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "93/56:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "93/57:\n",
      "test_emails = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails[test_emails_data]\n",
      "test_emails_data.sort_values(by=['rank'])\n",
      "93/58:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails[test_emails_data]\n",
      "test_emails_data.sort_values(by=['rank'])\n",
      "93/59:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = 1# get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = 1# get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = 1#get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "93/60:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'])\n",
      "93/61:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    print(x)\n",
      "93/62:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'])\n",
      "93/63:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    print(x)\n",
      "    return pd.Series(x)\n",
      "93/64:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print(result)\n",
      "    return result\n",
      "93/65:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'])\n",
      "93/66:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "93/67:\n",
      "def get_from_thread_measure(senders):\n",
      "    x = [emails_threads_senders.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print(result)\n",
      "    return result\n",
      "93/68:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = 1# get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = 1# get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = 1#get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "93/69:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = 1# get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = 1#get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "93/70:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "93/71:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print(result.sort_values())\n",
      "    return result\n",
      "93/72:\n",
      "def get_from_thread_measure(senders):\n",
      "    x = [emails_threads_senders.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print(result.sort_values())\n",
      "    return result\n",
      "93/73:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "93/74:\n",
      "def get_from_thread_measure(senders):\n",
      "    x = [emails_threads_senders.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print('get_from_thread_measure: ', result.sort_values())\n",
      "    return result\n",
      "93/75:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print('get_from_measure: ', result.sort_values())\n",
      "    return result\n",
      "93/76:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "93/77:\n",
      "def get_from_thread_measure(senders):\n",
      "    x = [emails_threads_senders.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print('get_from_thread_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "93/78:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print('get_from_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "93/79:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "94/5:\n",
      "a=pd.Series[1,2,3]\n",
      "b=pd.Series[1,2,3]\n",
      "z = a*b\n",
      "94/6:\n",
      "a=pd.Series([1,2,3])\n",
      "b=pd.Series([1,2,3])\n",
      "z = a*b\n",
      "94/7:\n",
      "a=pd.Series([1,2,3])\n",
      "b=pd.Series([1,2,3])\n",
      "z = a*b\n",
      "print(x)\n",
      "94/8:\n",
      "a=pd.Series([1,2,3])\n",
      "b=pd.Series([1,2,3])\n",
      "z = a*b\n",
      "print(z)\n",
      "95/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "95/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "95/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "95/4: headers, messages = get_emails(easyham_path)\n",
      "95/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "95/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "95/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "95/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "95/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "95/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "95/11: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "95/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "95/13: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "95/14:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "95/15: all_emails_training = all_emails_data.loc[:1250]\n",
      "95/16: len(all_emails_training)\n",
      "95/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "95/18:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "95/19:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "95/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "95/21:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "95/22: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "95/23: all_emails_training['thread_subject'] = threads_subjects\n",
      "95/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "95/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "95/26:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "95/27:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "95/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "95/29: #emails_threads_senders.tail()\n",
      "95/30:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "95/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "95/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "95/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "95/34: emails_threads['send_min'] = minimum_send_datetime\n",
      "95/35: emails_threads['send_max'] = maximum_send_datetime\n",
      "95/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "95/37: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "95/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "95/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "95/40: emails_threads.thread_time.min()\n",
      "95/41: emails_threads.thread_time.max()\n",
      "95/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "95/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "95/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "95/45: #emails_threads.head(8)\n",
      "95/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12\n",
      "95/47: #all_emails_training.head()\n",
      "95/48: stop_words = stopwords.words('english')\n",
      "95/49:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "95/50:\n",
      "def make_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10\n",
      "    \n",
      "    return matrix\n",
      "95/51: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "95/52: thread_tdm.tail()\n",
      "95/53:\n",
      "emails_tdm = make_tdm(all_emails_training.Message)\n",
      "emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2\n",
      "emails_tdm.tail()\n",
      "95/54:\n",
      "#grouped_from.tail(20 -social activity measure\n",
      "#emails_threads_senders.tail() -thread senders activity measure\n",
      "#emails_threads.thread_density -thread activity measure\n",
      "#thread_tdm.weight -words in active threads mesure\n",
      "#emails_tdm.weight -words in all messages measure\n",
      "95/55:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print('get_from_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "95/56:\n",
      "def get_from_thread_measure(senders):\n",
      "    x = [emails_threads_senders.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print('get_from_thread_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "95/57:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    all_emails_training['thread_subject'] = threads_subjects\n",
      "95/58:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = 1# get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = 1#get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "95/59:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "95/60:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    print(emails)\n",
      "95/61: x = all_emails_data[-10:]\n",
      "95/62: x\n",
      "95/63:\n",
      "#test_emails_data = all_emails_data[:10]\n",
      "#test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "#test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "95/64: get_thread_activity_measure(x)\n",
      "95/65:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    emails\n",
      "95/66: get_thread_activity_measure(x)\n",
      "95/67: x\n",
      "95/68:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails_threads = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails_threads.thread_density)+12\n",
      "95/69: x = all_emails_data[-10:]\n",
      "95/70: x.head()\n",
      "95/71: get_thread_activity_measure(x)\n",
      "95/72:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails_threads = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails_threads.thread_density)+12\n",
      "95/73:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails_threads = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails_threads.thread_density)+12\n",
      "95/74: get_thread_activity_measure(x)\n",
      "95/75:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails= emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails_threads.thread_density)+12\n",
      "95/76: get_thread_activity_measure(x)\n",
      "95/77: x\n",
      "95/78:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails= emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "95/79: get_thread_activity_measure(x)\n",
      "95/80: x\n",
      "96/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "96/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "96/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "96/4: headers, messages = get_emails(easyham_path)\n",
      "96/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "96/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "96/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "96/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "96/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "96/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "96/11: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "96/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "96/13: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "96/14:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "96/15: all_emails_training = all_emails_data.loc[:1250]\n",
      "96/16: len(all_emails_training)\n",
      "96/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "96/18:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "96/19:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "96/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "96/21:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "96/22: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "96/23: all_emails_training['thread_subject'] = threads_subjects\n",
      "96/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "96/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "96/26:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "96/27:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "96/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "96/29: #emails_threads_senders.tail()\n",
      "96/30:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "96/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "96/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "96/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "96/34: emails_threads['send_min'] = minimum_send_datetime\n",
      "96/35: emails_threads['send_max'] = maximum_send_datetime\n",
      "96/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "96/37: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "96/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "96/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "96/40: emails_threads.thread_time.min()\n",
      "96/41: emails_threads.thread_time.max()\n",
      "96/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "96/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "96/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "96/45: #emails_threads.head(8)\n",
      "96/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12\n",
      "96/47: #all_emails_training.head()\n",
      "96/48: stop_words = stopwords.words('english')\n",
      "96/49:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "96/50:\n",
      "def make_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10\n",
      "    \n",
      "    return matrix\n",
      "96/51: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "96/52: thread_tdm.tail()\n",
      "96/53:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails_tdm = make_tdm(all_emails_training.Message)\n",
      "emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2\n",
      "emails_tdm.tail()\n",
      "96/54:\n",
      "#grouped_from.tail(20 -social activity measure\n",
      "#emails_threads_senders.tail() -thread senders activity measure\n",
      "#emails_threads.thread_density -thread activity measure\n",
      "#thread_tdm.weight -words in active threads mesure\n",
      "#emails_tdm.weight -words in all messages measure\n",
      "96/55:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print('get_from_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "96/56:\n",
      "def get_from_thread_measure(senders):\n",
      "    x = [emails_threads_senders.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print('get_from_thread_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "96/57:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails= emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "96/58:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = 1# get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = 1#get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "96/59:\n",
      "#test_emails_data = all_emails_data[:10]\n",
      "#test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "#test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "96/60: x = all_emails_data[-10:]\n",
      "96/61: x.head()\n",
      "96/62: get_thread_activity_measure(x)\n",
      "96/63: x\n",
      "96/64:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    print(thread_number_of_messages)\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails= emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "96/65: get_thread_activity_measure(x)\n",
      "96/66:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    print(thread_number_of_messages_df)\n",
      "    emails= emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "96/67: get_thread_activity_measure(x)\n",
      "96/68:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails= emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "96/69: x = all_emails_data[-10:]\n",
      "96/70: x.head()\n",
      "96/71: get_thread_activity_measure(x)\n",
      "96/72: x\n",
      "96/73: x.head()\n",
      "96/74:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails= emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "96/75: x = all_emails_data[-10:]\n",
      "96/76: x.head()\n",
      "96/77: get_thread_activity_measure(x)\n",
      "96/78:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "96/79: x = all_emails_data[-10:]\n",
      "96/80: x.head()\n",
      "96/81: get_thread_activity_measure(x)\n",
      "96/82:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    #print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    #print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "    \n",
      "    return emails.thread_weight\n",
      "96/83: get_thread_activity_measure(x)\n",
      "96/84:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    #print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    #print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "    \n",
      "    return emails.thread_weight\n",
      "96/85:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = 1# get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = 1#get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "96/86:\n",
      "#test_emails_data = all_emails_data[:10]\n",
      "#test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "#test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "96/87: x = all_emails_data[-10:]\n",
      "96/88: x.head()\n",
      "96/89:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    #print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    #print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "    \n",
      "    return emails.thread_weight\n",
      "96/90: x = all_emails_data[-10:]\n",
      "96/91: x.head()\n",
      "96/92: get_thread_activity_measure(x)\n",
      "96/93: x.head()\n",
      "96/94:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    #print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    #print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "    emails.thread_weight.fillna(value=1)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "96/95: get_thread_activity_measure(x)\n",
      "96/96:\n",
      "def get_thread_activity_measure(emails):    \n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    #print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    #print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "    emails.thread_weight.fillna(value=1, inplace=True)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "96/97: get_thread_activity_measure(x)\n",
      "96/98:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = 1#get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "96/99:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "96/100:\n",
      "def get_thread_activity_measure(emails):\n",
      "    emails = emails.copy(deep+True)\n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    #print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    #print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "    emails.thread_weight.fillna(value=1, inplace=True)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "96/101:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = 1#get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "96/102:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "96/103:\n",
      "def get_thread_activity_measure(emails):\n",
      "    emails = emails.copy()\n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    #print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    #print(emails)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "    emails.thread_weight.fillna(value=1, inplace=True)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "96/104:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = 1#get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "96/105:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "96/106:\n",
      "def get_thread_activity_measure(emails):\n",
      "    emails = emails.copy()\n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "    emails.thread_weight.fillna(value=1, inplace=True)\n",
      "    \n",
      "    print(emails.thread_weight)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "96/107:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = 1#get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "96/108:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "97/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "97/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "97/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "97/4: headers, messages = get_emails(easyham_path)\n",
      "97/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "97/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "97/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "97/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "97/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "97/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "97/11: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "97/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "97/13: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "97/14:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "97/15: all_emails_training = all_emails_data.loc[:1250]\n",
      "97/16: len(all_emails_training)\n",
      "97/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "97/18:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "97/19:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "97/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "97/21:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "97/22: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "97/23: all_emails_training['thread_subject'] = threads_subjects\n",
      "97/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "97/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "97/26:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "97/27:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "97/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "97/29: #emails_threads_senders.tail()\n",
      "97/30:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "97/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "97/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "97/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "97/34: emails_threads['send_min'] = minimum_send_datetime\n",
      "97/35: emails_threads['send_max'] = maximum_send_datetime\n",
      "97/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "97/37: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "97/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "97/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "97/40: emails_threads.thread_time.min()\n",
      "97/41: emails_threads.thread_time.max()\n",
      "97/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "97/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "97/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "97/45: #emails_threads.head(8)\n",
      "97/46: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12\n",
      "97/47: #all_emails_training.head()\n",
      "97/48: stop_words = stopwords.words('english')\n",
      "97/49:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "97/50:\n",
      "def make_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #print(dicts)\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10\n",
      "    \n",
      "    return matrix\n",
      "97/51: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "97/52: thread_tdm.tail()\n",
      "97/53:\n",
      "emails_tdm = make_tdm(all_emails_training.Message)\n",
      "emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2\n",
      "emails_tdm.tail()\n",
      "97/54:\n",
      "#grouped_from.tail(20 -social activity measure\n",
      "#emails_threads_senders.tail() -thread senders activity measure\n",
      "#emails_threads.thread_density -thread activity measure\n",
      "#thread_tdm.weight -words in active threads mesure\n",
      "#emails_tdm.weight -words in all messages measure\n",
      "97/55:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print('get_from_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "97/56:\n",
      "def get_from_thread_measure(senders):\n",
      "    x = [emails_threads_senders.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print('get_from_thread_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "97/57:\n",
      "def get_thread_activity_measure(emails):\n",
      "    emails = emails.copy()\n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "    emails.thread_weight.fillna(value=1, inplace=True)\n",
      "    \n",
      "    print(emails.thread_weight)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "97/58:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = 1#get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "97/59:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "97/60:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.message.copy()\n",
      "97/61:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.message.copy()\n",
      "    print(messages)\n",
      "97/62: get_words_in_thread_measure(test_emails_data)\n",
      "97/63: test_emails_data\n",
      "97/64:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    print(messages)\n",
      "97/65: get_words_in_thread_measure(test_emails_data)\n",
      "97/66:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    dicts = count_words(messages)\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    print(matrix)\n",
      "97/67: get_words_in_thread_measure(test_emails_data)\n",
      "97/68:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #matrix = pd.DataFrame([dicts])\n",
      "    #matrix = matrix.transpose()\n",
      "    \n",
      "    print(matrix)\n",
      "97/69:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #matrix = pd.DataFrame([dicts])\n",
      "    #matrix = matrix.transpose()\n",
      "    \n",
      "    print(dicts)\n",
      "97/70: get_words_in_thread_measure(test_emails_data)\n",
      "97/71:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #matrix = pd.DataFrame([dicts])\n",
      "    #matrix = matrix.transpose()\n",
      "    \n",
      "    print(len(dicts)\n",
      "97/72: get_words_in_thread_measure(test_emails_data)\n",
      "97/73:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    #matrix = pd.DataFrame([dicts])\n",
      "    #matrix = matrix.transpose()\n",
      "    \n",
      "    print(len(dicts))\n",
      "97/74: get_words_in_thread_measure(test_emails_data)\n",
      "97/75:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    print(matrix)\n",
      "97/76: get_words_in_thread_measure(test_emails_data)\n",
      "97/77:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    print(matrix.loc[0])\n",
      "97/78: get_words_in_thread_measure(test_emails_data)\n",
      "97/79: test_emails_data.Message.loc[0]\n",
      "97/80:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    return matrix\n",
      "97/81: get_words_in_thread_measure(test_emails_data)\n",
      "97/82: x = get_words_in_thread_measure(test_emails_data)\n",
      "97/83: x\n",
      "97/84: x.loc[0]\n",
      "97/85:\n",
      "for z in x:\n",
      "    print(z)\n",
      "97/86:\n",
      "for z in x.loc[0:\n",
      "    print(z)\n",
      "97/87:\n",
      "for z in x.loc[0]:\n",
      "    print(z)\n",
      "97/88:\n",
      "for z in x.loc[0].sorted():\n",
      "    print(z)\n",
      "97/89:\n",
      "for z in x.loc[0].sort():\n",
      "    print(z)\n",
      "97/90:\n",
      "for z in x.loc[0].sort_values():\n",
      "    print(z)\n",
      "99/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "99/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "99/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "99/4: headers, messages = get_emails(easyham_path)\n",
      "99/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "99/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "99/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "99/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "99/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "99/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "99/11: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "99/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "99/13: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "99/14:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "99/15: all_emails_training = all_emails_data.loc[:1250]\n",
      "99/16: len(all_emails_training)\n",
      "99/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "99/18:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "99/19:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "99/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "99/21:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "99/22: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "99/23: all_emails_training['thread_subject'] = threads_subjects\n",
      "99/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "99/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "99/26:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "99/27:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "99/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "99/29: #emails_threads_senders.tail()\n",
      "99/30:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "99/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "99/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "99/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "99/34: emails_threads['send_min'] = minimum_send_datetime\n",
      "99/35: emails_threads['send_max'] = maximum_send_datetime\n",
      "99/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "99/37: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "99/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "99/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "99/40: emails_threads.thread_time.min()\n",
      "99/41: emails_threads.thread_time.max()\n",
      "99/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "99/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "99/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "99/45: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12\n",
      "99/46: stop_words = stopwords.words('english')\n",
      "99/47:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "99/48:\n",
      "def make_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10\n",
      "    \n",
      "    return matrix\n",
      "99/49: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "99/50: thread_tdm.tail()\n",
      "99/51:\n",
      "emails_tdm = make_tdm(all_emails_training.Message)\n",
      "emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2\n",
      "emails_tdm.tail()\n",
      "99/52:\n",
      "#grouped_from.tail(20 -social activity measure\n",
      "#emails_threads_senders.tail() -thread senders activity measure\n",
      "#emails_threads.thread_density -thread activity measure\n",
      "#thread_tdm.weight -words in active threads mesure\n",
      "#emails_tdm.weight -words in all messages measure\n",
      "99/53:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print('get_from_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "99/54:\n",
      "def get_from_thread_measure(senders):\n",
      "    x = [emails_threads_senders.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print('get_from_thread_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "99/55:\n",
      "def get_thread_activity_measure(emails):\n",
      "    emails = emails.copy()\n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "    emails.thread_weight.fillna(value=1, inplace=True)\n",
      "    \n",
      "    print(emails.thread_weight)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "99/56:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    return matrix\n",
      "99/57: x = get_words_in_thread_measure(test_emails_data)\n",
      "99/58:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = 1#get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "99/59:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "99/60: x = get_words_in_thread_measure(test_emails_data)\n",
      "99/61:\n",
      "for z in x.loc[0].sort_values():\n",
      "    print(z)\n",
      "99/62: test_emails_data.Message.loc[0]\n",
      "99/63: thread_tdm.player\n",
      "99/64: thread_tdm.linux\n",
      "99/65: x\n",
      "99/66: x.loc[0]\n",
      "99/67: x\n",
      "99/68:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix.weight = 0\n",
      "    \n",
      "    return matrix\n",
      "99/69: x = get_words_in_thread_measure(test_emails_data)\n",
      "99/70: x\n",
      "99/71:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['weight'] = 0\n",
      "    \n",
      "    return matrix\n",
      "99/72: x = get_words_in_thread_measure(test_emails_data)\n",
      "99/73: x\n",
      "99/74: thread_tdm.get('linux')\n",
      "99/75: thread_tdm.weight.get('lina',9)\n",
      "99/76: thread_tdm.weight.get('linux',9)\n",
      "99/77: thread_tdm.weight.get('linux')\n",
      "99/78: a=thread_tdm.weight.get('linux')\n",
      "99/79:\n",
      "a=thread_tdm.weight.get('linux')\n",
      "a\n",
      "99/80: a\n",
      "99/81: thread_tdm.weight\n",
      "99/82: thread_tdm.get('linux', 'weight')\n",
      "99/83: thread_tdm.get(['linux', 'weight'])\n",
      "99/84: thread_tdm.get(['weight', 'linux'])\n",
      "99/85: thread_tdm.at('weight','linux')\n",
      "99/86: thread_tdm.loc['weight']\n",
      "99/87: thread_tdm.loc['weight']['linux']\n",
      "99/88: thread_tdm.get(['weight']['linux'])\n",
      "100/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "100/2:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "100/3:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "100/4: headers, messages = get_emails(easyham_path)\n",
      "100/5: headers_lines = [header.split('\\n') for header in headers]\n",
      "100/6:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "100/7:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "100/8:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "100/9:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "100/10:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "100/11: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "100/12: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "100/13: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "100/14:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "100/15: all_emails_training = all_emails_data.loc[:1250]\n",
      "100/16: len(all_emails_training)\n",
      "100/17: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "100/18:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "100/19:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "100/20: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "100/21:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "100/22: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "100/23: all_emails_training['thread_subject'] = threads_subjects\n",
      "100/24: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "100/25: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "100/26:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "100/27:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "100/28: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "100/29: #emails_threads_senders.tail()\n",
      "100/30:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "100/31: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "100/32: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "100/33: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "100/34: emails_threads['send_min'] = minimum_send_datetime\n",
      "100/35: emails_threads['send_max'] = maximum_send_datetime\n",
      "100/36: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "100/37: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "100/38: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "100/39: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "100/40: emails_threads.thread_time.min()\n",
      "100/41: emails_threads.thread_time.max()\n",
      "100/42: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "100/43: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "100/44: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "100/45: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12\n",
      "100/46: stop_words = stopwords.words('english')\n",
      "100/47:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "100/48:\n",
      "def make_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10\n",
      "    \n",
      "    return matrix\n",
      "100/49: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "100/50: thread_tdm.tail()\n",
      "100/51: thread_tdm.get(['weight']['linux'])\n",
      "100/52: #thread_tdm.get(['weight']['linux'])\n",
      "100/53:\n",
      "emails_tdm = make_tdm(all_emails_training.Message)\n",
      "emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2\n",
      "emails_tdm.tail()\n",
      "100/54:\n",
      "#grouped_from.tail(20 -social activity measure\n",
      "#emails_threads_senders.tail() -thread senders activity measure\n",
      "#emails_threads.thread_density -thread activity measure\n",
      "#thread_tdm.weight -words in active threads mesure\n",
      "#emails_tdm.weight -words in all messages measure\n",
      "100/55:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print('get_from_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "100/56:\n",
      "def get_from_thread_measure(senders):\n",
      "    x = [emails_threads_senders.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print('get_from_thread_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "100/57:\n",
      "def get_thread_activity_measure(emails):\n",
      "    emails = emails.copy()\n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "    emails.thread_weight.fillna(value=1, inplace=True)\n",
      "    \n",
      "    print(emails.thread_weight)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "100/58:\n",
      "def count_words_in_thread_weight():\n",
      "    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = {word, thread_tdm.get}\n",
      "    return result\n",
      "100/59:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    matrix['weight'] = calculate_wor\n",
      "    \n",
      "    return matrix\n",
      "100/60: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/61:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = 1#get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/62:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/63: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/64:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    #matrix['weight'] = calculate_wor\n",
      "    \n",
      "    return matrix\n",
      "100/65: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/66: x\n",
      "100/67:\n",
      "def count_words_in_thread_weight():\n",
      "    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = {word, thread_tdm.get}\n",
      "    return words\n",
      "100/68:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    #matrix['weight'] = calculate_wor\n",
      "    \n",
      "    return matrix\n",
      "100/69: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/70: x\n",
      "100/71:\n",
      "def count_words_in_thread_weight():\n",
      "    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = {word, thread_tdm.get}\n",
      "    return words\n",
      "100/72:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame([dicts])\n",
      "    matrix = matrix.transpose()\n",
      "    \n",
      "    #matrix['weight'] = calculate_wor\n",
      "    \n",
      "    return matrix\n",
      "100/73: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/74: x\n",
      "100/75:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words(message) for message in messages]\n",
      "    #matrix = pd.DataFrame([dicts])\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    #matrix['weight'] = calculate_wor\n",
      "    \n",
      "    return messages_words\n",
      "100/76: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/77: x\n",
      "100/78:\n",
      "def count_words_in_thread_weight():\n",
      "    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = {word, thread_tdm.get}\n",
      "    return words\n",
      "100/79:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    #matrix = pd.DataFrame([dicts])\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    #matrix['weight'] = calculate_wor\n",
      "    \n",
      "    return messages_words\n",
      "100/80: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/81:\n",
      "def count_words_in_thread_weight(message):\n",
      "    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = {word, thread_tdm.get}\n",
      "    return words\n",
      "100/82:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    #matrix = pd.DataFrame([dicts])\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    #matrix['weight'] = calculate_wor\n",
      "    \n",
      "    return messages_words\n",
      "100/83: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/84:\n",
      "def count_words_in_thread_weight(message):\n",
      "    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    #result = {word, thread_tdm.get}\n",
      "    return words\n",
      "100/85:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    #matrix = pd.DataFrame([dicts])\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    #matrix['weight'] = calculate_wor\n",
      "    \n",
      "    return messages_words\n",
      "100/86: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/87: x\n",
      "100/88:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.DataFrame(message_words)\n",
      "    #matrix = pd.DataFrame([dicts])\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    #matrix['weight'] = calculate_wor\n",
      "    \n",
      "    return z\n",
      "100/89: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/90:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.DataFrame(messages_words)\n",
      "    #matrix = pd.DataFrame([dicts])\n",
      "   # matrix = matrix.transpose()\n",
      "    \n",
      "    #matrix['weight'] = calculate_wor\n",
      "    \n",
      "    return z\n",
      "100/91: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/92: x\n",
      "100/93:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    #matrix['weight'] = calculate_wor\n",
      "    \n",
      "    return z\n",
      "100/94: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/95: x\n",
      "100/96: emails_tdm.columns\n",
      "100/97: emails_tdm[emails_tdm.columns in x[0]]\n",
      "100/98: emails_tdm[emails_tdm.columns == 'linux']\n",
      "100/99: emails_tdm[emails_tdm.columns == 'aaron']\n",
      "100/100: emails_tdm[emails_tdm.columns == 'zoo']\n",
      "100/101: emails_tdm['zoo']\n",
      "100/102: emails_tdm['mamka']\n",
      "100/103: emails_tdm[{'aaron'}]\n",
      "100/104: emails_tdm[list({'aaron'})]\n",
      "100/105: set(emails_tdm.columns)\n",
      "100/106: emails_tdm[list(set(emails_tdm.columns).intersection{'aaron'})]\n",
      "100/107: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron'})]\n",
      "100/108: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron'}))]\n",
      "100/109: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron', 'sales'}))]\n",
      "100/110: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron', 'sales'}))].product()\n",
      "100/111: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron', 'sales'}))]\n",
      "100/112: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron', 'sales'}))].product()\n",
      "100/113: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron', 'sales'}))].product(axis=0)\n",
      "100/114: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron', 'sales'}))].product(axis=1)\n",
      "100/115: emails_tdm[list(set(emails_tdm.columns).intersection({'aaron', 'sales'}))].product(axis=1)['weight']\n",
      "100/116:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame()\n",
      "    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(z))].product(axis=1)['weight']\n",
      "    \n",
      "    return z\n",
      "100/117:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame()\n",
      "    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(z))].product(axis=1)['weight']\n",
      "    \n",
      "    return x\n",
      "100/118: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/119:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame()\n",
      "    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(z))].product(axis=1)['weight']\n",
      "    \n",
      "    return z\n",
      "100/120: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/121:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame()\n",
      " #   x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(z))].product(axis=1)['weight']\n",
      "    \n",
      "    return z\n",
      "100/122: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/123: x\n",
      "100/124: emails_tdm[list(set(emails_tdm.columns).intersection(z))].product(axis=1)['weight']\n",
      "100/125: emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']\n",
      "100/126:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      " #   x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(z))].product(axis=1)['weight']\n",
      "    \n",
      "    return z\n",
      "100/127: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/128: x\n",
      "100/129:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      " #   x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(z))].product(axis=1)['weight']\n",
      "    \n",
      "    return x\n",
      "100/130: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/131: x\n",
      "100/132:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(x[0]))].product(axis=1)['weight']\n",
      "    \n",
      "    return x\n",
      "100/133: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/134:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = emails_tdm[list(set(emails_tdm.columns)).intersection(x[0])].product(axis=1)['weight']\n",
      "    \n",
      "    return x\n",
      "100/135: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/136:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(x[0]))].product(axis=1)['weight']\n",
      "    \n",
      "    return x\n",
      "100/137: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/138:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    set(emails_tdm.columns).intersection(x[0]\n",
      "    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(x[0]))].product(axis=1)['weight']\n",
      "    \n",
      "    return x\n",
      "100/139:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    set(emails_tdm.columns).intersection(x[0])\n",
      "    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(x[0]))].product(axis=1)['weight']\n",
      "    \n",
      "    return x\n",
      "100/140: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/141: x[0]\n",
      "100/142: emails_tdm[list(set(emails_tdm.columns).intersection(x[0]))].product(axis=1)['weight']\n",
      "100/143: emails_tdm[list(set(emails_tdm.columns).intersection({'zoo'}))].product(axis=1)['weight']\n",
      "100/144: emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']\n",
      "100/145: x\n",
      "100/146:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    set(emails_tdm.columns).intersection(x[0])\n",
      "    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']\n",
      "    \n",
      "    return x\n",
      "100/147: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/148:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    #set(emails_tdm.columns).intersection(x[0])\n",
      "    x['weight'] = emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']\n",
      "    \n",
      "    return x\n",
      "100/149: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/150: x\n",
      "100/151:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    #set(emails_tdm.columns).intersection(x[0])\n",
      "    x['weight'] = x[0]*2 #emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']\n",
      "    \n",
      "    return x\n",
      "100/152: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/153:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    #set(emails_tdm.columns).intersection(x[0])\n",
      "    x['weight'] = x[0].intersection('now') #emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']\n",
      "    \n",
      "    return x\n",
      "100/154: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/155: x[0]\n",
      "100/156:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    #set(emails_tdm.columns).intersection(x[0])\n",
      "    x['weight'] = min(x[0])\n",
      "    \n",
      "    return x\n",
      "100/157: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/158:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    #set(emails_tdm.columns).intersection(x[0])\n",
      "    x['weight'] = min(x)\n",
      "    \n",
      "    return x\n",
      "100/159: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/160: x[0]\n",
      "100/161: x\n",
      "100/162:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    #set(emails_tdm.columns).intersection(x[0])\n",
      "    x['weight'] = max(x)\n",
      "    \n",
      "    return x\n",
      "100/163:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    #set(emails_tdm.columns).intersection(x[0])\n",
      "    x['weight'] = calculate_weight(x[0])\n",
      "    \n",
      "    return x\n",
      "100/164: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/165:\n",
      "def calculate_weight(words_set):\n",
      "    result = emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']\n",
      "    return result\n",
      "100/166:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    #set(emails_tdm.columns).intersection(x[0])\n",
      "    x['weight'] = calculate_weight(x[0])\n",
      "    \n",
      "    return x\n",
      "100/167: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/168: x\n",
      "100/169:\n",
      "test_emails_data = all_emails_data[:30]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/170: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/171: x\n",
      "100/172:\n",
      "def calculate_weight(words_set):\n",
      "    result = emails_tdm[list(set(emails_tdm.columns).intersection(word_set))].product(axis=1)['weight']\n",
      "    return result\n",
      "100/173:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    #set(emails_tdm.columns).intersection(x[0])\n",
      "    x['weight'] = calculate_weight(x[0])\n",
      "    \n",
      "    return x\n",
      "100/174: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/175:\n",
      "def calculate_weight(words_set):\n",
      "    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].product(axis=1)['weight']\n",
      "    return result\n",
      "100/176:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    #set(emails_tdm.columns).intersection(x[0])\n",
      "    x['weight'] = calculate_weight(x[0])\n",
      "    \n",
      "    return x\n",
      "100/177: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/178:\n",
      "def calculate_weight(words_set):\n",
      "    print(words_set)\n",
      "    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].product(axis=1)['weight']\n",
      "    return result\n",
      "100/179:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    #set(emails_tdm.columns).intersection(x[0])\n",
      "    x['weight'] = calculate_weight(x[0])\n",
      "    \n",
      "    return x\n",
      "100/180: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/181:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    #set(emails_tdm.columns).intersection(x[0])\n",
      "    x['weight'] = x[0].apply(calculate_weight)\n",
      "    \n",
      "    return x\n",
      "100/182: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/183: x\n",
      "100/184:\n",
      "test_emails_data = all_emails_data[:2]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/185:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_weight)\n",
      "    \n",
      "    return x\n",
      "100/186: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/187: x\n",
      "100/188: emails_tdm['source']\n",
      "100/189:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def calculate_weight(words_set):\n",
      "    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].mean(axis=1)['weight']\n",
      "    return result\n",
      "100/190:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_weight)\n",
      "    \n",
      "    return x\n",
      "100/191: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/192: x\n",
      "100/193:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/194:\n",
      "test_emails_data = all_emails_data[:2]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/195:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_weight)\n",
      "    \n",
      "    return x.weight\n",
      "100/196: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/197: x\n",
      "100/198: emails_tdm[list(set(emails_tdm.columns).intersection(x))].product(axis=1)['weight']\n",
      "100/199:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/200:\n",
      "test_emails_data = all_emails_data[:2]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/201:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/202:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)/2\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/203:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/204:\n",
      "def calculate_weight(words_set, tdm):\n",
      "    result = tdm[list(set(tdm.columns).intersection(words_set))].mean(axis=1)['weight']\n",
      "    return result\n",
      "100/205:\n",
      "def get_words_in_thread_measure(emails, tdm):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_weight,tdm)\n",
      "    \n",
      "    return x.weight\n",
      "100/206:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails,)/2\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/207:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/208:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails, emails_tdm)/2\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/209:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/210:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails, thread_tdm)/2\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/211:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/212:\n",
      "def get_words_in_thread_measure(emails, tdm):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_weight(tdm))\n",
      "    \n",
      "    return x.weight\n",
      "100/213: x = get_words_in_thread_measure(test_emails_data)\n",
      "100/214:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/215: #x = get_words_in_thread_measure(test_emails_data)\n",
      "100/216:\n",
      "def get_words_in_thread_measure(emails, tdm):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_weight(tdm))\n",
      "    \n",
      "    return x.weight\n",
      "100/217: #x = get_words_in_thread_measure(test_emails_data)\n",
      "100/218: #x\n",
      "100/219:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails, thread_tdm)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/220:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/221:\n",
      "def calculate_weight(words_set):\n",
      "    result = thread_tdm[list(set(thread_tdm.columns).intersection(words_set))].mean(axis=1)['weight']\n",
      "    return result\n",
      "100/222:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_weight)\n",
      "    \n",
      "    return x.weight\n",
      "100/223: #x = get_words_in_thread_measure(test_emails_data)\n",
      "100/224: #x\n",
      "100/225:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = 1#get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/226:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/227:\n",
      "def calculate_thread_weight(words_set):\n",
      "    result = thread_tdm[list(set(thread_tdm.columns).intersection(words_set))].mean(axis=1)['weight']\n",
      "    return result\n",
      "100/228:\n",
      "def calculate_thread_weight(words_set):\n",
      "    result = thread_tdm[list(set(thread_tdm.columns).intersection(words_set))].mean(axis=1)['weight']\n",
      "    return result\n",
      "100/229:\n",
      "def calculate_all_messages_weight(words_set):\n",
      "    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].mean(axis=1)['weight']\n",
      "    return result\n",
      "100/230:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_weight)\n",
      "    \n",
      "    return x.weight\n",
      "100/231:\n",
      "def get_words_in_all_messages_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_all_messages_weight)\n",
      "    \n",
      "    return x.weight\n",
      "100/232: #x = get_words_in_thread_measure(test_emails_data)\n",
      "100/233: #x\n",
      "100/234:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/235:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/236:\n",
      "def get_thread_activity_measure(emails):\n",
      "    emails = emails.copy()\n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+12\n",
      "    emails.thread_weight.fillna(value=1, inplace=True)\n",
      "    \n",
      "    #print(emails.thread_weight)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "100/237:\n",
      "def get_from_thread_measure(senders):\n",
      "    x = [emails_threads_senders.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    #print('get_from_thread_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "100/238:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    3print('get_from_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "100/239:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    #print('get_from_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "100/240:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/241:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/242:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure, from_thread_measure])\n",
      "    print('Test rank: ', test_rank)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/243:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/244:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure, from_thread_measure]).T\n",
      "    print('Test rank: ', test_rank)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/245:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/246:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure, from_thread_measure]).T\n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure']\n",
      "    print('Test rank: ', test_rank)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/247:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/248:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure, from_thread_measure]).T\n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure']\n",
      "    print('Test rank: \\n', test_rank)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/249:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/250:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure']\n",
      "    print('Test rank: \\n', test_rank)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/251:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/252:\n",
      "def get_thread_activity_measure(emails):\n",
      "    emails = emails.copy()\n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+15\n",
      "    emails.thread_weight.fillna(value=1, inplace=True)\n",
      "    \n",
      "    #print(emails.thread_weight)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "100/253:\n",
      "def count_words_in_thread_weight(message):\n",
      "    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    return words\n",
      "100/254:\n",
      "def calculate_thread_weight(words_set):\n",
      "    result = thread_tdm[list(set(thread_tdm.columns).intersection(words_set))].mean(axis=1)['weight']\n",
      "    return result\n",
      "100/255:\n",
      "def calculate_all_messages_weight(words_set):\n",
      "    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].mean(axis=1)['weight']\n",
      "    return result\n",
      "100/256:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_weight)\n",
      "    \n",
      "    return x.weight\n",
      "100/257:\n",
      "def get_words_in_all_messages_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_all_messages_weight)\n",
      "    \n",
      "    return x.weight\n",
      "100/258:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure']\n",
      "    print('Test rank: \\n', test_rank)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/259:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/260:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    test_rank['final_rank']=test_rank.product()\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']\n",
      "    \n",
      "    print('Test rank: \\n', test_rank)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/261:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/262:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    test_rank['final_rank']=test_rank.product(axis=0)\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']\n",
      "    \n",
      "    print('Test rank: \\n', test_rank)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/263:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/264:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    #test_rank['final_rank']=test_rank.product(axis=0)\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure',]# 'final_rank']\n",
      "    \n",
      "    print('Test rank: \\n', test_rank)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/265:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/266: a = DataFrame([1,2,3])\n",
      "100/267: a = pd.DataFrame([1,2,3])\n",
      "100/268: a\n",
      "100/269: a = pd.DataFrame([1,2,3], [4,5,6])\n",
      "100/270: a\n",
      "100/271: a = pd.DataFrame([[1,2,3],[4,5,6]])\n",
      "100/272: a\n",
      "100/273: a['suma'] = a.sum()\n",
      "100/274: a\n",
      "100/275:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    test_rank['final_rank']=test_rank.product(axis=0)\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure',]# 'final_rank']\n",
      "    \n",
      "    print('Test rank: \\n', test_rank)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/276:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/277:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    test_rank['final_rank']=test_rank.product(axis=1)\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure',]# 'final_rank']\n",
      "    \n",
      "    print('Test rank: \\n', test_rank)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/278:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/279: a['suma'] = a.sum(axis=0)\n",
      "100/280: a = pd.DataFrame([[1,2,3],[4,5,6]])\n",
      "100/281: a\n",
      "100/282: a['suma'] = a.sum(axis=0)\n",
      "100/283: a\n",
      "100/284: a = pd.DataFrame([[1,2,3],[4,5,6]])\n",
      "100/285: a\n",
      "100/286: a['suma'] = a.sum(axis=1)\n",
      "100/287: a\n",
      "100/288: a.loc['suma'] = a.sum(axis=1)\n",
      "100/289: a\n",
      "100/290: a = pd.DataFrame([[1,2,3],[4,5,6]])\n",
      "100/291: a\n",
      "100/292: a.loc['suma'] = a.sum()\n",
      "100/293: a\n",
      "100/294: a = pd.DataFrame([[1,2,3],[4,5,6]])\n",
      "100/295: a\n",
      "100/296: a['prod'] = a.product()\n",
      "100/297: a\n",
      "100/298: a = pd.DataFrame([[1,2,3],[4,5,6]])\n",
      "100/299: a\n",
      "100/300: a['prod'] = a.product(axis=0)\n",
      "100/301: a\n",
      "100/302: a = pd.DataFrame([[1,2,3],[4,5,6]])\n",
      "100/303: a\n",
      "100/304: a['prod'] = a.product(axis=1)\n",
      "100/305: a\n",
      "100/306:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/307:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    test_rank['final_rank']=test_rank.product(axis=1)\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']\n",
      "    \n",
      "    print('Test rank: \\n', test_rank)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/308:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/309: a = pd.DataFrame([[1,2,3],[4,5,6]])\n",
      "100/310: a\n",
      "100/311: a['prod'] = a.product()\n",
      "100/312: a\n",
      "100/313:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    test_rank['final_rank']=test_rank.product(axis=1)\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']\n",
      "    \n",
      "    #print('Test rank: \\n', test_rank)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/314:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/315: all_emails_data.describe\n",
      "100/316:\n",
      "test_emails_data = all_emails_data[1250:]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/317:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/318:\n",
      "test_emails_data = all_emails_data[1240:1260]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/319:\n",
      "test_emails_data = all_emails_data[1000:1260]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/320:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/321:\n",
      "test_emails_data = all_emails_data[:1000]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/322:\n",
      "test_emails_data = all_emails_data[800:1500]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/323:\n",
      "test_emails_data = all_emails_data[800:810]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/324:\n",
      "test_emails_data = all_emails_data[700:800]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/325:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    test_rank['final_rank']=test_rank.product(axis=1)\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']\n",
      "    \n",
      "    print('Test rank: \\n', test_rank)\n",
      "    \n",
      "    rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return rank\n",
      "100/326:\n",
      "test_emails_data = all_emails_data[700:800]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/327:\n",
      "test_emails_data = all_emails_data[0:800]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/328: test_emails_data.loc[790]\n",
      "100/329:\n",
      "test_emails_data = all_emails_data[790:791]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/330:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    test_rank['final_rank']=test_rank.product(axis=1)\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']\n",
      "    \n",
      "    print('Test rank: \\n', test_rank)\n",
      "    \n",
      "    #rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return test_rank['final_rank']\n",
      "100/331:\n",
      "test_emails_data = all_emails_data[790:791]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/332:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    test_rank['final_rank']=test_rank.product(axis=1)\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']\n",
      "    \n",
      "    print('Test rank: \\n', test_rank)\n",
      "    \n",
      "    #rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return test_rank['final_rank']\n",
      "100/333:\n",
      "test_emails_data = all_emails_data[790:791]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/334:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    test_rank['final_rank']=test_rank.product(axis=1)\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']\n",
      "    \n",
      "    print('Test rank: \\n', test_rank['final_rank'])\n",
      "    \n",
      "    #rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return test_rank['final_rank']\n",
      "100/335:\n",
      "test_emails_data = all_emails_data[790:791]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/336:\n",
      "test_emails_data = all_emails_data[0:1]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/337:\n",
      "test_emails_data = all_emails_data[100:101]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/338:\n",
      "test_emails_data = all_emails_data[90:101]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/339:\n",
      "test_emails_data = all_emails_data[0:101]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/340:\n",
      "test_emails_data = all_emails_data[675:676]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/341: test_emails_data.loc[675]\n",
      "100/342: test_emails_data.loc['562']\n",
      "100/343: test_emails_data.index\n",
      "100/344: test_emails_data.head()\n",
      "100/345: all_emails_data.head()\n",
      "100/346:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "#all_emails_data.reset_index(inplace=True)\n",
      "100/347: all_emails_training = all_emails_data.loc[:1250]\n",
      "100/348: len(all_emails_training)\n",
      "100/349: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "100/350:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "100/351:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "100/352: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "100/353:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "100/354: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "100/355: all_emails_training['thread_subject'] = threads_subjects\n",
      "100/356: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "100/357: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "100/358:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "100/359:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "100/360: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "100/361: #emails_threads_senders.tail()\n",
      "100/362:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "100/363: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "100/364: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "100/365: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "100/366: emails_threads['send_min'] = minimum_send_datetime\n",
      "100/367: emails_threads['send_max'] = maximum_send_datetime\n",
      "100/368: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "100/369: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "100/370: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "100/371: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "100/372: emails_threads.thread_time.min()\n",
      "100/373: emails_threads.thread_time.max()\n",
      "100/374: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "100/375: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "100/376: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "100/377: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12\n",
      "100/378: stop_words = stopwords.words('english')\n",
      "100/379:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "100/380:\n",
      "def make_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10\n",
      "    \n",
      "    return matrix\n",
      "100/381: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "100/382: thread_tdm.tail()\n",
      "100/383:\n",
      "emails_tdm = make_tdm(all_emails_training.Message)\n",
      "emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2\n",
      "emails_tdm.tail()\n",
      "100/384:\n",
      "#grouped_from.tail(20 -social activity measure\n",
      "#emails_threads_senders.tail() -thread senders activity measure\n",
      "#emails_threads.thread_density -thread activity measure\n",
      "#thread_tdm.weight -words in active threads mesure\n",
      "#emails_tdm.weight -words in all messages measure\n",
      "100/385:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    #print('get_from_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "100/386:\n",
      "def get_from_thread_measure(senders):\n",
      "    x = [emails_threads_senders.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    #print('get_from_thread_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "100/387:\n",
      "def get_thread_activity_measure(emails):\n",
      "    emails = emails.copy()\n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+15\n",
      "    emails.thread_weight.fillna(value=1, inplace=True)\n",
      "    \n",
      "    #print(emails.thread_weight)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "100/388:\n",
      "def count_words_in_thread_weight(message):\n",
      "    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    return words\n",
      "100/389:\n",
      "def calculate_thread_weight(words_set):\n",
      "    result = thread_tdm[list(set(thread_tdm.columns).intersection(words_set))].mean(axis=1)['weight']\n",
      "    return result\n",
      "100/390:\n",
      "def calculate_all_messages_weight(words_set):\n",
      "    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].mean(axis=1)['weight']\n",
      "    return result\n",
      "100/391:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_weight)\n",
      "    \n",
      "    return x.weight\n",
      "100/392:\n",
      "def get_words_in_all_messages_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_all_messages_weight)\n",
      "    \n",
      "    return x.weight\n",
      "100/393:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    test_rank['final_rank']=test_rank.product(axis=1)\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']\n",
      "    \n",
      "    print('Test rank: \\n', test_rank['final_rank'])\n",
      "    \n",
      "    #rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return test_rank['final_rank']\n",
      "100/394:\n",
      "test_emails_data = all_emails_data[675:676]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/395: test_emails_data.head()\n",
      "100/396: all_emails_data.head()\n",
      "100/397:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "100/398:\n",
      "easyham_path = 'Spam/data/easy_ham/'\n",
      "easyham2_path = 'Spam/data/easy_ham2/'\n",
      "100/399:\n",
      "def get_emails(path_to_directory):\n",
      "    file_names = os.listdir(path_to_directory)\n",
      "    path_to_all_files = [path_to_directory + file_name for file_name in file_names if file_name != 'cmds']\n",
      "    all_email_messages = []\n",
      "    all_email_headers = []\n",
      "    \n",
      "    for file in path_to_all_files:\n",
      "        email = open(file, encoding='latin1').read()\n",
      "        header_end_index = email.find('\\n\\n')\n",
      "        \n",
      "        email_header = email[:header_end_index]\n",
      "        all_email_headers.append(email_header)\n",
      "        \n",
      "        email_message = email[header_end_index+2:].lower()\n",
      "        all_email_messages.append(email_message)\n",
      "        \n",
      "    return all_email_headers,all_email_messages\n",
      "100/400: headers, messages = get_emails(easyham_path)\n",
      "100/401: headers_lines = [header.split('\\n') for header in headers]\n",
      "100/402:\n",
      "from_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('From:'):\n",
      "            from_lines.append(line)\n",
      "100/403:\n",
      "from_addresses = []\n",
      "for email in from_lines:\n",
      "    parser_results = parseaddr(email)\n",
      "    from_addresses.append(parser_results[1])\n",
      "100/404:\n",
      "subject_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Subject:'):\n",
      "            subject_lines.append(line)\n",
      "100/405:\n",
      "subjects=[]\n",
      "for subject_line in subject_lines:\n",
      "    subject = re.findall(r'Subject: (.+)', subject_line)\n",
      "    subjects.append(subject[0].lower())\n",
      "100/406:\n",
      "dates_lines = []\n",
      "for header_lines in headers_lines:\n",
      "    for line in header_lines:\n",
      "        if line.startswith('Date:'):\n",
      "            dates_lines.append(line)\n",
      "100/407: dates_lines = [datestr[6:] for datestr in dates_lines]\n",
      "100/408: send_datetime = pd.to_datetime(dates_lines, errors='coerce')\n",
      "100/409: emails_data = [send_datetime, from_addresses, subjects, messages]\n",
      "100/410:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "#all_emails_data.reset_index(inplace=True)\n",
      "100/411: all_emails_training = all_emails_data.loc[:1250]\n",
      "100/412: len(all_emails_training)\n",
      "100/413: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "100/414:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "100/415:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "100/416: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "100/417:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "100/418: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "100/419: all_emails_training['thread_subject'] = threads_subjects\n",
      "100/420: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "100/421: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "100/422:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "100/423:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "100/424: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "100/425: #emails_threads_senders.tail()\n",
      "100/426:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "100/427: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "100/428: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "100/429: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "100/430: emails_threads['send_min'] = minimum_send_datetime\n",
      "100/431: emails_threads['send_max'] = maximum_send_datetime\n",
      "100/432: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "100/433: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "100/434: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "100/435: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "100/436: emails_threads.thread_time.min()\n",
      "100/437: emails_threads.thread_time.max()\n",
      "100/438: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "100/439: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "100/440: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "100/441: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12\n",
      "100/442: stop_words = stopwords.words('english')\n",
      "100/443:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "100/444:\n",
      "def make_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10\n",
      "    \n",
      "    return matrix\n",
      "100/445: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "100/446: thread_tdm.tail()\n",
      "100/447:\n",
      "emails_tdm = make_tdm(all_emails_training.Message)\n",
      "emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2\n",
      "emails_tdm.tail()\n",
      "100/448:\n",
      "#grouped_from.tail(20 -social activity measure\n",
      "#emails_threads_senders.tail() -thread senders activity measure\n",
      "#emails_threads.thread_density -thread activity measure\n",
      "#thread_tdm.weight -words in active threads mesure\n",
      "#emails_tdm.weight -words in all messages measure\n",
      "100/449:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    #print('get_from_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "100/450:\n",
      "def get_from_thread_measure(senders):\n",
      "    x = [emails_threads_senders.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    #print('get_from_thread_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "100/451:\n",
      "def get_thread_activity_measure(emails):\n",
      "    emails = emails.copy()\n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+15\n",
      "    emails.thread_weight.fillna(value=1, inplace=True)\n",
      "    \n",
      "    #print(emails.thread_weight)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "100/452:\n",
      "def count_words_in_thread_weight(message):\n",
      "    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    return words\n",
      "100/453:\n",
      "def calculate_thread_weight(words_set):\n",
      "    result = thread_tdm[list(set(thread_tdm.columns).intersection(words_set))].mean(axis=1)['weight']\n",
      "    return result\n",
      "100/454:\n",
      "def calculate_all_messages_weight(words_set):\n",
      "    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].mean(axis=1)['weight']\n",
      "    return result\n",
      "100/455:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_weight)\n",
      "    \n",
      "    return x.weight\n",
      "100/456:\n",
      "def get_words_in_all_messages_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_all_messages_weight)\n",
      "    \n",
      "    return x.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/457:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    test_rank['final_rank']=test_rank.product(axis=1)\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']\n",
      "    \n",
      "    print('Test rank: \\n', test_rank['final_rank'])\n",
      "    \n",
      "    #rank = from_measure*from_thread_measure*thread_activity_measure*words_in_thread_measure*words_in_all_messages_measure\n",
      "    \n",
      "    return test_rank['final_rank']\n",
      "100/458:\n",
      "test_emails_data = all_emails_data[675:676]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/459: test_emails_data.head()\n",
      "100/460: all_emails_data.head()\n",
      "100/461:\n",
      "test_emails_data = all_emails_data[675:676]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/462:\n",
      "test_emails_data = all_emails_data[675:676]\n",
      "test_emails_data['rank']= 69 # rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/463:\n",
      "test_emails_data = all_emails_data[675:676]\n",
      "print('returned from function: ', rank_emails(test_emails_data))\n",
      "test_emails_data['rank']= 69 # rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/464:\n",
      "test_emails_data = all_emails_data[675:676]\n",
      "print('returned from function: \\n', rank_emails(test_emails_data))\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/465:\n",
      "test_emails_data = all_emails_data[675:676]\n",
      "print('returned from function: \\n', rank_emails(test_emails_data))\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/466: test_emails_data\n",
      "100/467:\n",
      "test_emails_data = all_emails_data[0:6]\n",
      "print('returned from function: \\n', rank_emails(test_emails_data))\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/468:\n",
      "test_emails_data = all_emails_data[:6]\n",
      "print('returned from function: \\n', rank_emails(test_emails_data))\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/469:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "print('returned from function: \\n', rank_emails(test_emails_data))\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/470:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "#print('returned from function: \\n', rank_emails(test_emails_data))\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/471:\n",
      "test_emails_data = all_emails_data[:1]\n",
      "#print('returned from function: \\n', rank_emails(test_emails_data))\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/472:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "#print('returned from function: \\n', rank_emails(test_emails_data))\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/473:\n",
      "all_emails_data = pd.DataFrame(emails_data)\n",
      "all_emails_data = all_emails_data.transpose()\n",
      "all_emails_data.columns = ['Send_datetime', 'From', 'Subject', 'Message']\n",
      "all_emails_data = all_emails_data.sort_values(by=['Send_datetime'])\n",
      "all_emails_data.reset_index(inplace=True)\n",
      "100/474: all_emails_training = all_emails_data.loc[:1250]\n",
      "100/475: len(all_emails_training)\n",
      "100/476: grouped_from = all_emails_training.groupby(by=['From']).count().sort_values(by=['Message'])\n",
      "100/477:\n",
      "grouped_from['Weight'] = np.log(grouped_from['Message']+1)\n",
      "grouped_from = grouped_from.Weight\n",
      "100/478:\n",
      "#plt.plot(grouped_from.index, grouped_from.Weight, 'or')\n",
      "#plt.plot(grouped_from.index, grouped_from.Message, 'ob')\n",
      "#plt.rcParams['figure.figsize']=(24,16)\n",
      "100/479: all_emails_training['is_thread'] = all_emails_training.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "100/480:\n",
      "def get_threads_subjects(emails_df):\n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails_df.Subject, emails_df.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "    \n",
      "    return threads_subjects\n",
      "100/481: threads_subjects = get_threads_subjects(all_emails_training)\n",
      "100/482: all_emails_training['thread_subject'] = threads_subjects\n",
      "100/483: threads_subjects = all_emails_training.thread_subject.loc[all_emails_training.is_thread]\n",
      "100/484: all_emails_training.is_thread.where(~all_emails_training.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "100/485:\n",
      "is_thread = all_emails_training.is_thread==True\n",
      "thread_subject_x = all_emails_training.thread_subject=='x'\n",
      "all_emails_training.thread_subject.where(~(is_thread & thread_subject_x),all_emails_training.Subject, inplace=True)\n",
      "100/486:\n",
      "emails_threads = all_emails_training[all_emails_training.is_thread==True]\n",
      "emails_threads.drop(['Message', 'is_thread'], axis=1, inplace=True)\n",
      "100/487: emails_threads_senders = emails_threads.groupby(by=['From']).count().sort_values(by=['Subject'])\n",
      "100/488: #emails_threads_senders.tail()\n",
      "100/489:\n",
      "emails_threads_senders['Weight'] = np.log(emails_threads_senders['Subject']+1)\n",
      "emails_threads_senders = emails_threads_senders.Weight\n",
      "100/490: grouped_by_thread_subject = emails_threads.groupby(by=['thread_subject'])\n",
      "100/491: minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "100/492: maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "100/493: emails_threads['send_min'] = minimum_send_datetime\n",
      "100/494: emails_threads['send_max'] = maximum_send_datetime\n",
      "100/495: emails_threads['thread_time'] = (emails_threads.send_max - emails_threads.send_min).dt.total_seconds()\n",
      "100/496: thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "100/497: thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "100/498: emails_threads = emails_threads.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "100/499: emails_threads.thread_time.min()\n",
      "100/500: emails_threads.thread_time.max()\n",
      "100/501: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "100/502: emails_threads['thread_density']=(emails_threads.Number_of_messages/emails_threads.thread_time)#.dt.total_seconds()\n",
      "100/503: emails_threads.thread_time.where(emails_threads.Number_of_messages>1, inplace=True)\n",
      "100/504: emails_threads['thread_weight'] = np.log(emails_threads.thread_density)+12\n",
      "100/505: stop_words = stopwords.words('english')\n",
      "100/506:\n",
      "def count_words(message):\n",
      "    occurances = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    result = dict.fromkeys(occurances, 1)\n",
      "    return result\n",
      "100/507:\n",
      "def make_tdm(messages):\n",
      "    dicts = [count_words(message) for message in messages]\n",
      "    matrix = pd.DataFrame(dicts)\n",
      "    \n",
      "    matrix.loc['summa']=matrix.sum()\n",
      "    \n",
      "    matrix.loc['number_of_documents']=(matrix.count()-1)\n",
      "    matrix.loc['documents_percentage']=matrix.loc['number_of_documents']/len(messages)\n",
      "    matrix.loc['weight'] = np.log(matrix.loc['documents_percentage'])+10\n",
      "    \n",
      "    return matrix\n",
      "100/508: thread_tdm = make_tdm(all_emails_training[all_emails_training.is_thread==True].Message)\n",
      "100/509: thread_tdm.tail()\n",
      "100/510:\n",
      "emails_tdm = make_tdm(all_emails_training.Message)\n",
      "emails_tdm.loc['weight'] = emails_tdm.loc['weight']/2\n",
      "emails_tdm.tail()\n",
      "100/511:\n",
      "#grouped_from.tail(20 -social activity measure\n",
      "#emails_threads_senders.tail() -thread senders activity measure\n",
      "#emails_threads.thread_density -thread activity measure\n",
      "#thread_tdm.weight -words in active threads mesure\n",
      "#emails_tdm.weight -words in all messages measure\n",
      "100/512:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    #print('get_from_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "100/513:\n",
      "def get_from_thread_measure(senders):\n",
      "    x = [emails_threads_senders.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    #print('get_from_thread_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "100/514:\n",
      "def get_thread_activity_measure(emails):\n",
      "    emails = emails.copy()\n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+15\n",
      "    emails.thread_weight.fillna(value=1, inplace=True)\n",
      "    \n",
      "    #print(emails.thread_weight)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "100/515:\n",
      "def count_words_in_thread_weight(message):\n",
      "    words = set([word.lower() for word in word_tokenize(message) if len(word)>2 and len(word)<15 and word.isalpha() and word.lower() not in stop_words])\n",
      "    return words\n",
      "100/516:\n",
      "def calculate_thread_weight(words_set):\n",
      "    result = thread_tdm[list(set(thread_tdm.columns).intersection(words_set))].mean(axis=1)['weight']\n",
      "    return result\n",
      "100/517:\n",
      "def calculate_all_messages_weight(words_set):\n",
      "    result = emails_tdm[list(set(emails_tdm.columns).intersection(words_set))].mean(axis=1)['weight']\n",
      "    return result\n",
      "100/518:\n",
      "def get_words_in_thread_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_weight)\n",
      "    \n",
      "    return x.weight\n",
      "100/519:\n",
      "def get_words_in_all_messages_measure(emails):\n",
      "    messages = emails.Message.copy()\n",
      "    messages_words = [count_words_in_thread_weight(message) for message in messages]\n",
      "    z=pd.Series(messages_words)\n",
      "    \n",
      "    x = pd.DataFrame(z)\n",
      "    x['weight'] = x[0].apply(calculate_all_messages_weight)\n",
      "    \n",
      "    return x.weight\n",
      "100/520:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    test_rank['final_rank']=test_rank.product(axis=1)\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']\n",
      "    \n",
      "    print('Test rank: \\n', test_rank['final_rank'])\n",
      "    \n",
      "    return test_rank['final_rank']\n",
      "100/521:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "#print('returned from function: \\n', rank_emails(test_emails_data))\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/522: test_emails_data\n",
      "100/523:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data.head()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/524:\n",
      "test_emails_data = all_emails_data[:100]\n",
      "test_emails_data.reset_index()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/525:\n",
      "test_emails_data = all_emails_data[:1]\n",
      "test_emails_data.reset_index()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/526:\n",
      "test_emails_data = all_emails_data[670:671]\n",
      "test_emails_data.reset_index()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/527:\n",
      "test_emails_data = all_emails_data[0:1]\n",
      "test_emails_data.reset_index()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/528:\n",
      "test_emails_data = all_emails_data[0:2]\n",
      "test_emails_data.reset_index()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/529:\n",
      "test_emails_data = all_emails_data[0:20]\n",
      "test_emails_data.reset_index()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/530:\n",
      "test_emails_data = all_emails_data[10:20]\n",
      "test_emails_data.reset_index()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/531:\n",
      "test_emails_data = all_emails_data[0:20]\n",
      "test_emails_data.reset_index()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/532:\n",
      "test_emails_data = all_emails_data[:20]\n",
      "test_emails_data.reset_index()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/533:\n",
      "test_emails_data = all_emails_data[1:20]\n",
      "test_emails_data.reset_index()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/534:\n",
      "test_emails_data = all_emails_data[:20]\n",
      "test_emails_data.reset_index()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/535:\n",
      "test_emails_data = all_emails_data[1:20]\n",
      "test_emails_data.reset_index()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/536:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    test_rank['final_rank']=test_rank.product(axis=1)\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']\n",
      "    \n",
      "    print('Test rank: \\n', test_rank)\n",
      "    \n",
      "    return test_rank['final_rank']\n",
      "100/537:\n",
      "test_emails_data = all_emails_data[1:20]\n",
      "test_emails_data.reset_index()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/538:\n",
      "test_emails_data = all_emails_data[1:20]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/539:\n",
      "test_emails_data = all_emails_data[10:20]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/540:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/541:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    print('get_from_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "100/542:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/543:\n",
      "def get_thread_activity_measure(emails):\n",
      "    emails = emails.copy()\n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+15\n",
      "    emails.thread_weight.fillna(value=1, inplace=True)\n",
      "    \n",
      "    print(emails.thread_weight)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "100/544:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/545:\n",
      "def get_thread_activity_measure(emails):\n",
      "    #emails = emails.copy()\n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+15\n",
      "    emails.thread_weight.fillna(value=1, inplace=True)\n",
      "    \n",
      "    print(emails.thread_weight)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "100/546:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/547:\n",
      "test_emails_data = all_emails_data[5:20].copy()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/548:\n",
      "test_emails_data = all_emails_data[5:20].copy()\n",
      "test_emails_data.reset_index()\n",
      "test_emails_data['rank']= rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/549:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = concat([test_emails_data, rank_df],)\n",
      "result.sort_values(by=['rank'], ascending=False)\n",
      "100/550:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],)\n",
      "result.sort_values(by=['rank'], ascending=False)\n",
      "100/551:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],)\n",
      "#result.sort_values(by=['rank'], ascending=False)\n",
      "100/552: result\n",
      "100/553:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df], join='inner')\n",
      "#result.sort_values(by=['rank'], ascending=False)\n",
      "100/554:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1, join='inner')\n",
      "#result.sort_values(by=['rank'], ascending=False)\n",
      "100/555:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_axis=True)\n",
      "#result.sort_values(by=['rank'], ascending=False)\n",
      "100/556:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=0, join='inner', ignore_axis=True)\n",
      "#result.sort_values(by=['rank'], ascending=False)\n",
      "100/557:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=0, join='inner', ignore_index=True)\n",
      "#result.sort_values(by=['rank'], ascending=False)\n",
      "100/558:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)\n",
      "#result.sort_values(by=['rank'], ascending=False)\n",
      "100/559: result\n",
      "100/560:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)\n",
      "result\n",
      "#result.sort_values(by=['rank'], ascending=False)\n",
      "100/561:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)\n",
      "result\n",
      "result.sort_values(by=[10], ascending=False)\n",
      "100/562:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)\n",
      "result.sort_values(by=[10], ascending=False)\n",
      "100/563:\n",
      "def rank_emails(emails):\n",
      "    from_measure = get_from_measure(emails.From)\n",
      "    from_thread_measure = get_from_thread_measure(emails.From)\n",
      "    thread_activity_measure = get_thread_activity_measure(emails)\n",
      "    words_in_thread_measure = get_words_in_thread_measure(emails)\n",
      "    words_in_all_messages_measure = get_words_in_all_messages_measure(emails)\n",
      "    \n",
      "    test_rank = pd.DataFrame([from_measure,\n",
      "                              from_thread_measure,\n",
      "                              thread_activity_measure,\n",
      "                              words_in_thread_measure,\n",
      "                              words_in_all_messages_measure]).T\n",
      "    \n",
      "    test_rank['final_rank']=test_rank.product(axis=1)\n",
      "    \n",
      "    test_rank.columns = ['from_measure', 'from_thread_measure','thread_activity_measure',\n",
      "                         'words_in_thread_measure', 'words_in_all_messages_measure', 'final_rank']\n",
      "    \n",
      "    #print('Test rank: \\n', test_rank)\n",
      "    \n",
      "    return test_rank['final_rank']\n",
      "100/564:\n",
      "def get_thread_activity_measure(emails):\n",
      "    #emails = emails.copy()\n",
      "    emails['is_thread'] = emails.Subject.str.contains('re\\[{0,1}[1-9]{0,1}[1-9]{0,1}\\]{0,1}:')\n",
      "    \n",
      "    threads_subjects=[]\n",
      "    for subject,isthread in zip(emails.Subject, emails.is_thread):\n",
      "        if isthread:\n",
      "            thread_subject = subject.split(':',1)[1]\n",
      "            thread_subject = thread_subject.lstrip()\n",
      "            threads_subjects.append(thread_subject)\n",
      "        else:\n",
      "            threads_subjects.append('x')\n",
      "            \n",
      "    emails['thread_subject'] = threads_subjects\n",
      "    threads_subjects = emails.thread_subject.loc[emails.is_thread]\n",
      "    emails.is_thread.where(~emails.Subject.isin(threads_subjects), other=True, inplace=True)\n",
      "    \n",
      "    is_thread = emails.is_thread==True\n",
      "    thread_subject_x = emails.thread_subject=='x'\n",
      "    emails.thread_subject.where(~(is_thread & thread_subject_x),emails.Subject, inplace=True)\n",
      "    \n",
      "    grouped_by_thread_subject = emails.groupby(by=['thread_subject'])\n",
      "    minimum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(min)\n",
      "    maximum_send_datetime = grouped_by_thread_subject['Send_datetime'].transform(max)\n",
      "    emails['send_min'] = minimum_send_datetime\n",
      "    emails['send_max'] = maximum_send_datetime\n",
      "    emails['thread_time'] = (emails.send_max - emails.send_min).dt.total_seconds()\n",
      "    \n",
      "    thread_number_of_messages = grouped_by_thread_subject.size()\n",
      "    thread_number_of_messages_df = pd.DataFrame({'Number_of_messages': thread_number_of_messages})\n",
      "    emails = emails.merge(thread_number_of_messages_df, on='thread_subject', how='left')\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_density']=(emails.Number_of_messages/emails.thread_time)\n",
      "    emails.thread_time.where(emails.Number_of_messages>1, inplace=True)\n",
      "    emails['thread_weight'] = np.log(emails.thread_density)+15\n",
      "    emails.thread_weight.fillna(value=1, inplace=True)\n",
      "    \n",
      "    #print(emails.thread_weight)\n",
      "    \n",
      "    return emails.thread_weight\n",
      "100/565:\n",
      "def get_from_measure(senders):\n",
      "    x = [grouped_from.get(sender,1) for sender in senders]\n",
      "    result = pd.Series(x)\n",
      "    #print('get_from_measure: \\n', result.sort_values())\n",
      "    return result\n",
      "100/566:\n",
      "test_emails_data = all_emails_data[5:20]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)\n",
      "result.sort_values(by=[10], ascending=False)\n",
      "100/567:\n",
      "test_emails_data = all_emails_data[1250:]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)\n",
      "result.sort_values(by=[10], ascending=False)\n",
      "100/568:\n",
      "test_emails_data = all_emails_data[1250:]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)\n",
      "result\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/569:\n",
      "test_emails_data = all_emails_data[:10]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)\n",
      "result\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/570:\n",
      "test_emails_data = all_emails_data[5:10]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1, join='inner', ignore_index=True)\n",
      "result\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/571: result\n",
      "100/572:\n",
      "test_emails_data = all_emails_data[5:10]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df], ignore_index=True)\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/573: result\n",
      "100/574:\n",
      "test_emails_data = all_emails_data[5:10]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1, ignore_index=True)\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/575: result\n",
      "100/576:\n",
      "test_emails_data = all_emails_data[5:10]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1,join=inner ignore_index=True)\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/577:\n",
      "test_emails_data = all_emails_data[5:10]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1,join=inner, ignore_index=True)\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/578: result\n",
      "100/579:\n",
      "test_emails_data = all_emails_data[5:10]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1,join='inner', ignore_index=True)\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/580: result\n",
      "100/581:\n",
      "test_emails_data = all_emails_data[5:10]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1,join='inner', ignore_index=True)\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/582: result\n",
      "100/583:\n",
      "test_emails_data = all_emails_data[0:10]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1,join='inner', ignore_index=True)\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/584: result\n",
      "100/585:\n",
      "test_emails_data = all_emails_data[3:10]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1,join='inner', ignore_index=True)\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/586: result\n",
      "100/587:\n",
      "test_emails_data = all_emails_data[3:10]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1, ignore_index=True)\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/588: result\n",
      "100/589:\n",
      "test_emails_data = all_emails_data[3:10]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = pd.concat([test_emails_data, rank_df],axis=1)\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/590: result\n",
      "100/591:\n",
      "test_emails_data = all_emails_data[3:10]\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = test_emails_data.join(rank_df)\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/592: result\n",
      "100/593:\n",
      "test_emails_data = all_emails_data[3:10]\n",
      "test_emails_data.reset_index()\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = test_emails_data.join(rank_df)\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/594: result\n",
      "100/595:\n",
      "test_emails_data = all_emails_data[3:10].copy()\n",
      "test_emails_data.reset_index()\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = test_emails_data.join(rank_df)\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/596: result\n",
      "100/597: test_emails_data.head()\n",
      "100/598:\n",
      "test_emails_data = all_emails_data[3:10].copy()\n",
      "test_emails_data.reset_index(inplace=True)\n",
      "rank_df= rank_emails(test_emails_data)\n",
      "result = test_emails_data.join(rank_df)\n",
      "#result.sort_values(by=[10], ascending=False)\n",
      "100/599: test_emails_data.head()\n",
      "100/600:\n",
      "test_emails_data = all_emails_data[3:10].reset_index(inplace=True)\n",
      "test_emails_data['rank']=rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=[10], ascending=False)\n",
      "100/601:\n",
      "test_emails_data = all_emails_data[3:10].reset_index(inplace=True)\n",
      "test_emails_data['rank']=rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=[10], ascending=False)\n",
      "100/602: test_emails_data.head()\n",
      "100/603:\n",
      "test_emails_data = all_emails_data[3:10].copy()\n",
      "test_emails_data.reset_index(inplace=True)\n",
      "test_emails_data['rank']=rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=[10], ascending=False)\n",
      "100/604:\n",
      "test_emails_data = all_emails_data[3:10].copy()\n",
      "test_emails_data.reset_index(inplace=True)\n",
      "test_emails_data['rank']=rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/605:\n",
      "test_emails_data = all_emails_data[3:10].copy()\n",
      "test_emails_data.reset_index(inplace=True)\n",
      "test_emails_data['rank']=rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/606:\n",
      "test_emails_data = all_emails_data[:1250].copy()\n",
      "test_emails_data.reset_index(inplace=True)\n",
      "test_emails_data['rank']=rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False)\n",
      "100/607: plt.plot(test_emails_data.rank)\n",
      "100/608: plt.plot(test_emails_data.index, test_emails_data.rank)\n",
      "100/609: plt.plot(test_emails_data.index, test_emails_data['rank'])\n",
      "100/610:\n",
      "test_emails_data = all_emails_data[:1250].copy()\n",
      "test_emails_data.reset_index(inplace=True)\n",
      "test_emails_data['rank']=rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False, inplace=True)\n",
      "100/611: plt.plot(test_emails_data.index, test_emails_data['rank'])\n",
      "100/612: test_emails_data.head()\n",
      "100/613:\n",
      "test_emails_data = all_emails_data[:125].copy()\n",
      "test_emails_data.reset_index(inplace=True)\n",
      "test_emails_data['rank']=rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False, inplace=True)\n",
      "test_emails_data.reset_index()\n",
      "100/614:\n",
      "test_emails_data = all_emails_data[:125].copy()\n",
      "test_emails_data.reset_index(inplace=True)\n",
      "test_emails_data['rank']=rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False, inplace=True)\n",
      "#test_emails_data.reset_index()\n",
      "100/615: plt.plot(test_emails_data.index, test_emails_data['rank'])\n",
      "100/616: plt.plot([test_emails_data.index, test_emails_data['rank']])\n",
      "100/617: plt.plot(test_emails_data['rank'], test_emails_data.index)\n",
      "100/618: plt.plot( test_emails_data.index, test_emails_data['rank'])\n",
      "100/619: test_emails_data.head()\n",
      "100/620:\n",
      "test_emails_data = all_emails_data[:125].copy()\n",
      "test_emails_data.reset_index(inplace=True)\n",
      "test_emails_data['rank']=rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False, inplace=True)\n",
      "test_emails_data.reset_index(inplace=True)\n",
      "100/621: plt.plot(test_emails_data['rank'])\n",
      "100/622:\n",
      "test_emails_data = all_emails_data[:125].copy()\n",
      "test_emails_data.reset_index(inplace=True)\n",
      "test_emails_data['rank']=rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False, inplace=True)\n",
      "#test_emails_data.reset_index(inplace=True)\n",
      "100/623: plt.plot(test_emails_data['rank'])\n",
      "100/624: plt.plot(range(0,125),test_emails_data['rank'])\n",
      "100/625:\n",
      "histogram = test_emails_data['rank'].hist(bins=20, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "100/626:\n",
      "histogram = test_emails_data['rank'].hist(bins=200, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "100/627:\n",
      "histogram = test_emails_data['rank'].hist(bins=10, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "100/628:\n",
      "histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "100/629:\n",
      "histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "plt.rc('figure', figsize=(10,10))\n",
      "100/630:\n",
      "histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "plt.rc('figure', figsize=(20,20))\n",
      "100/631:\n",
      "histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "plt.rc('figure', figsize=(20,40))\n",
      "100/632:\n",
      "histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "plt.rc('figure', figsize=(15,30))\n",
      "100/633:\n",
      "histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "plt.rc('figure', figsize=(10,10))\n",
      "100/634:\n",
      "histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "plt.rc('figure', figsize=(10,5))\n",
      "100/635:\n",
      "histogram = test_emails_data['rank'].hist(bins=50, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "plt.rc('figure', figsize=(15,5))\n",
      "100/636:\n",
      "histogram = test_emails_data['rank'].hist(bins=30, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "plt.rc('figure', figsize=(15,5))\n",
      "100/637: test_emails_data['rank'].plot.density()\n",
      "100/638: test_emails_data['rank'].plot.density(grid=True)\n",
      "100/639: test_emails_data.tail()\n",
      "100/640:\n",
      "histogram = test_emails_data['rank'].hist(bins=20, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "plt.rc('figure', figsize=(15,5))\n",
      "100/641: test_emails_data['rank'].describe()\n",
      "100/642:\n",
      "histogram = test_emails_data['rank'].hist(bins=20, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "test_emails_data['rank'].plot.density(grid=True)\n",
      "plt.rc('figure', figsize=(15,5))\n",
      "100/643:\n",
      "histogram = test_emails_data['rank'].hist(bins=20, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "plt.rc('figure', figsize=(15,5))\n",
      "100/644:\n",
      "test_emails_data = all_emails_data[:1250].copy()\n",
      "test_emails_data.reset_index(inplace=True)\n",
      "test_emails_data['rank']=rank_emails(test_emails_data)\n",
      "test_emails_data.sort_values(by=['rank'], ascending=False, inplace=True)\n",
      "#test_emails_data.reset_index(inplace=True)\n",
      "100/645:\n",
      "histogram = test_emails_data['rank'].hist(bins=20, label=['rank', 'number'])\n",
      "plt.title('Emails rank')\n",
      "plt.xlabel('rank')\n",
      "plt.ylabel('Observations')\n",
      "plt.rc('figure', figsize=(15,5))\n",
      "100/646: test_emails_data['rank'].plot.density(grid=True)\n",
      "100/647:\n",
      "test_emails_data['rank'].plot.density(grid=True)\n",
      "plt.axvline(x=750)\n",
      "100/648:\n",
      "test_emails_data['rank'].plot.density(grid=True)\n",
      "plt.axvline(x=750, 'r')\n",
      "100/649:\n",
      "test_emails_data['rank'].plot.density(grid=True)\n",
      "plt.axvline(x=750, color='r')\n",
      "100/650: test_emails_data['rank'].mean()\n",
      "100/651:\n",
      "test_emails_data['rank'].plot.density(grid=True)\n",
      "rank_mean = test_email_data[rank].mean()\n",
      "plt.axvline(x=rank_mean, color='r')\n",
      "100/652:\n",
      "test_emails_data['rank'].plot.density(grid=True)\n",
      "rank_mean = test_email_data['rank'].mean()\n",
      "plt.axvline(x=rank_mean, color='r')\n",
      "100/653:\n",
      "test_emails_data['rank'].plot.density(grid=True)\n",
      "rank_mean = test_emails_data['rank'].mean()\n",
      "plt.axvline(x=rank_mean, color='r')\n",
      "100/654:\n",
      "emails_data = all_emails_data[1250:].copy()\n",
      "emails_data.reset_index(inplace=True)\n",
      "emails_data['rank']=rank_emails(test_emails_data)\n",
      "emails_data.sort_values(by=['rank'], ascending=False, inplace=True)\n",
      "100/655:\n",
      "emails_data['rank'].plot.density(grid=True)\n",
      "rank_mean = test_emails_data['rank'].mean()\n",
      "plt.axvline(x=rank_mean, color='r')\n",
      "100/656:\n",
      "emails_data = all_emails_data[1250:].copy()\n",
      "emails_data.reset_index(inplace=True)\n",
      "emails_data['rank']=rank_emails(emails_data)\n",
      "emails_data.sort_values(by=['rank'], ascending=False, inplace=True)\n",
      "100/657:\n",
      "emails_data['rank'].plot.density(grid=True)\n",
      "rank_mean = test_emails_data['rank'].mean()\n",
      "plt.axvline(x=rank_mean, color='r')\n",
      "100/658:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from email.utils import parseaddr\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem import PorterStemmer\n",
      "\n",
      "from scipy import stats\n",
      "100/659: percentile = stats.percentileofscore(rank_mean, emails_data['rank'])\n",
      "100/660: percentile = stats.percentileofscore(emails_data['rank'], rank_mean)\n",
      "100/661: percentile\n",
      "103/1: path_to_data = 'Regression/longevity.csv'\n",
      "104/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "104/2: path_to_data = 'Regression/longevity.csv'\n",
      "104/3: longevity = pd.read_csv(path_to_data)\n",
      "104/4: longevity.head()\n",
      "104/5: longevity.loc[longevity.Smokes==1].plot.density()\n",
      "104/6: longevity.AgeAtDeath.loc[longevity.Smokes==1]#.plot.density()\n",
      "104/7: longevity.AgeAtDeath.loc[longevity.Smokes==1].plot.density()\n",
      "104/8: longevity.AgeAtDeath.loc[longevity.Smokes==1].plot.density(grid=True)\n",
      "104/9:\n",
      "longevity.AgeAtDeath.loc[longevity.Smokes==1].plot.density(grid=True)\n",
      "longevity.AgeAtDeath.loc[longevity.Smokes==0].plot.density(grid=True)\n",
      "104/10:\n",
      "longevity.AgeAtDeath.loc[longevity.Smokes==1].plot.density(grid=True)\n",
      "longevity.AgeAtDeath.loc[longevity.Smokes==0].plot.density(grid=True)\n",
      "legend()\n",
      "104/11:\n",
      "longevity.AgeAtDeath.loc[longevity.Smokes==1].plot.density(grid=True, label='Smokers')\n",
      "longevity.AgeAtDeath.loc[longevity.Smokes==0].plot.density(grid=True, label='NoSmokers')\n",
      "plt.legend()\n",
      "104/12:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn\n",
      "104/13:\n",
      "longevity.AgeAtDeath.loc[longevity.Smokes==1].plot.density(grid=True, label='Smokers')\n",
      "longevity.AgeAtDeath.loc[longevity.Smokes==0].plot.density(grid=True, label='NoSmokers')\n",
      "plt.legend()\n",
      "104/14: mean_age_at_death = longevity.AgeAtDeath.mean()\n",
      "104/15: mean_age_at_death\n",
      "104/16:\n",
      "mean_age_at_death = longevity.AgeAtDeath.mean()\n",
      "mean_age_at_death\n",
      "104/17: guess = 73\n",
      "104/18: longevity.AgeAtDeath-guess\n",
      "104/19: (longevity.AgeAtDeath-guess)^2\n",
      "104/20: (longevity.AgeAtDeath-guess)**2\n",
      "104/21: ((longevity.AgeAtDeath-guess)**2).mean()\n",
      "104/22: [((longevity.AgeAtDeath-x)**2).mean() for x in range(63,84)]\n",
      "104/23: guesses = [((longevity.AgeAtDeath-x)**2).mean() for x in range(63,84)]\n",
      "104/24: guesses.plot()\n",
      "104/25: plot(list(range(63,84)), guesses)\n",
      "104/26: plt.plot(list(range(63,84)), guesses)\n",
      "104/27: plt.plot(list(range(63,84)), guesses, grid=True)\n",
      "104/28: plt.plot(list(range(63,84)), guesses)\n",
      "104/29:\n",
      "plt.plot(list(range(63,84)), guesses)\n",
      "plt.grid()\n",
      "106/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn\n",
      "106/2: path_to_data = 'Regression/longevity.csv'\n",
      "106/3: longevity = pd.read_csv(path_to_data)\n",
      "106/4: longevity.head()\n",
      "106/5:\n",
      "longevity.AgeAtDeath.loc[longevity.Smokes==1].plot.density(grid=True, label='Smokers')\n",
      "longevity.AgeAtDeath.loc[longevity.Smokes==0].plot.density(grid=True, label='NoSmokers')\n",
      "plt.legend()\n",
      "106/6:\n",
      "mean_age_at_death = longevity.AgeAtDeath.mean()\n",
      "mean_age_at_death\n",
      "106/7: guess = 73\n",
      "106/8:\n",
      "#rms\n",
      "((longevity.AgeAtDeath-guess)**2).mean()\n",
      "106/9:\n",
      "# rms for random guesses\n",
      "guesses = [((longevity.AgeAtDeath-x)**2).mean() for x in range(63,84)]\n",
      "106/10:\n",
      "plt.plot(list(range(63,84)), guesses)\n",
      "plt.grid()\n",
      "106/11: ((longevity.AgeAtDeath-guess)**2).mean()**(0.5)\n",
      "106/12: smokers_guess = longevity.loc[smokes==1].AgeAtDeath.mean()\n",
      "106/13: smokers_guess = longevity.loc[Smokes==1].AgeAtDeath.mean()\n",
      "106/14: smokers_guess = longevity.loc[longevity.Smokes==1].AgeAtDeath.mean()\n",
      "106/15:\n",
      "smokers_guess = longevity.loc[longevity.Smokes==1].AgeAtDeath.mean()\n",
      "smokers_guess\n",
      "106/16:\n",
      "no_smokers_guess = longevity.loc[longevity.Smokes==0].AgeAtDeath.mean()\n",
      "no_smokers_guess\n",
      "106/17: longevity.loc[longevity.Smokes==1]-smokers_guess\n",
      "106/18: smokers = longevity.loc[longevity.Smokes==1]-smokers_guess\n",
      "106/19:\n",
      "smokers = longevity.loc[longevity.Smokes==1]-smokers_guess\n",
      "smokers\n",
      "106/20:\n",
      "smokers = longevity.loc[longevity.Smokes==1].AgeAtDeath-smokers_guess\n",
      "smokers\n",
      "106/21:\n",
      "smokers = (longevity.loc[longevity.Smokes==1].AgeAtDeath-smokers_guess)**2\n",
      "smokers\n",
      "106/22:\n",
      "smokers = (longevity.loc[longevity.Smokes==1].AgeAtDeath-smokers_guess)**2\n",
      "no_smokers = (longevity.loc[longevity.Smokes==0].AgeAtDeath-no_smokers_guess)**2\n",
      "106/23: smokers.append(no_smokers)\n",
      "106/24: longevity.describe()\n",
      "106/25: smokers.append(no_smokers).mean().**(0.5)\n",
      "106/26: smokers.append(no_smokers).mean()**(0.5)\n",
      "107/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as stats\n",
      "from scipy.optimize import curve_fit\n",
      "import numpy as np\n",
      "\n",
      "import statsmodels.api as sm\n",
      "\n",
      "%matplotlib inline\n",
      "107/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as stats\n",
      "from scipy.optimize import curve_fit\n",
      "import numpy as np\n",
      "\n",
      "import statsmodels.api as sm\n",
      "\n",
      "%matplotlib inline\n",
      "107/3: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'\n",
      "107/4:\n",
      "hwg_df = pd.read_csv(\n",
      "    heights_weights_gender_data)\n",
      "107/5: hwg_df.head()\n",
      "107/6: hwg_df.Height =hwg_df.Height * 2.54\n",
      "107/7: hwg_df.Weight = hwg_df.Weight * 0.454\n",
      "107/8: hwg_df.describe()\n",
      "107/9: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)\n",
      "107/10:\n",
      "def my_var(x):\n",
      "    m = sum(x)/len(x)\n",
      "    return sum([(element-m)**2 for element in x])/(len(x)-1)\n",
      "107/11:\n",
      "def my_sd(x):\n",
      "    return my_var(x)**(0.5)\n",
      "107/12: my_var([1,2,3,4,5])\n",
      "107/13: my_var(hwg_df.Height)\n",
      "107/14: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()\n",
      "107/15: my_sd(hwg_df.Height) - hwg_df.Height.std()\n",
      "107/16: df = hwg_df\n",
      "107/17: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()\n",
      "107/18: from scipy import stats\n",
      "107/19: stats.percentileofscore(df.Height, 158.8)\n",
      "107/20: stats.percentileofscore(df.Height, 178.34)\n",
      "107/21: 82.5-17.31\n",
      "107/22: df.describe()\n",
      "107/23: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])\n",
      "107/24:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.title('KDE')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Probability')\n",
      "107/25: df.head()\n",
      "107/26:\n",
      "df.Height[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Height[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Height')\n",
      "107/27:\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Weight')\n",
      "107/28:\n",
      "plt.figure(figsize=(10,10))\n",
      "ax1 = plt.subplot(2,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.grid(True)\n",
      "plt.legend()\n",
      "ax2 = plt.subplot(2,1,2, sharex = ax1)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.grid(True)\n",
      "plt.xlabel('Weight')\n",
      "plt.legend()\n",
      "107/29:\n",
      "mu = 10\n",
      "variance = 5\n",
      "sigma = variance**0.5\n",
      "x = np.linspace(mu-3*sigma, mu+3*sigma, 100)\n",
      "plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Gauss')\n",
      "plt.plot(x, stats.cauchy.pdf(x, mu, sigma), label='Cauchy')\n",
      "plt.legend()\n",
      "107/30:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "def f1(x,a,b):\n",
      "    return a*x + b\n",
      "\n",
      "popt,pcov = curve_fit(f1, df.Height, df.Weight )\n",
      "plt.plot(df.Height, f1(df.Height, *popt), color = 'red')\n",
      "plt.xlabel('Height [cm]')\n",
      "plt.ylabel('Weight [kg]')\n",
      "107/31: popt\n",
      "107/32: 1.38*160-159\n",
      "107/33: colors = dict(Male='red', Female='blue')\n",
      "107/34: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x]))\n",
      "107/35: colors\n",
      "107/36: df.dtypes\n",
      "107/37: df.Gender.loc[9970]\n",
      "107/38:\n",
      "plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')\n",
      "plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Weight')\n",
      "107/39: df.head()\n",
      "107/40:\n",
      "# Note the difference in argument order\n",
      "model = sm.OLS(df.Weight, df.Height).fit()\n",
      "predictions = model.predict(df.Height) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "model.summary()\n",
      "107/41:\n",
      "# Note the difference in argument order\n",
      "model = sm.OLS(df.Weight, df.Height).fit()\n",
      "predictions = model.predict(df.Height) # make the predictions by the model\n",
      "df.Height = sm.add_constant(df.Height)\n",
      "\n",
      "# Print out the statistics\n",
      "model.summary()\n",
      "107/42:\n",
      "# Note the difference in argument order\n",
      "model = sm.OLS(df.Weight, df.Height).fit()\n",
      "predictions = model.predict(df.Height) # make the predictions by the model\n",
      "df.Height = sm.add_constant(df.Height)\n",
      "\n",
      "# Print out the statistics\n",
      "model.summary()\n",
      "107/43:\n",
      "# Note the difference in argument order\n",
      "df.Height = sm.add_constant(df.Height)\n",
      "model = sm.OLS(df.Weight, df.Height).fit()\n",
      "predictions = model.predict(df.Height) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "model.summary()\n",
      "107/44:\n",
      "df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "model = sm.OLS(df.Weight, df.Height).fit()\n",
      "predictions = model.predict(df.Height) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "model.summary()\n",
      "107/45:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as stats\n",
      "from scipy.optimize import curve_fit\n",
      "import numpy as np\n",
      "\n",
      "import statsmodels.api as sm\n",
      "\n",
      "%matplotlib inline\n",
      "107/46: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'\n",
      "107/47:\n",
      "hwg_df = pd.read_csv(\n",
      "    heights_weights_gender_data)\n",
      "107/48: hwg_df.head()\n",
      "107/49: hwg_df.Height =hwg_df.Height * 2.54\n",
      "107/50: hwg_df.Weight = hwg_df.Weight * 0.454\n",
      "107/51: hwg_df.describe()\n",
      "107/52: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)\n",
      "107/53:\n",
      "def my_var(x):\n",
      "    m = sum(x)/len(x)\n",
      "    return sum([(element-m)**2 for element in x])/(len(x)-1)\n",
      "107/54:\n",
      "def my_sd(x):\n",
      "    return my_var(x)**(0.5)\n",
      "107/55: my_var([1,2,3,4,5])\n",
      "107/56: my_var(hwg_df.Height)\n",
      "107/57: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()\n",
      "107/58: my_sd(hwg_df.Height) - hwg_df.Height.std()\n",
      "107/59: df = hwg_df\n",
      "107/60: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()\n",
      "107/61: from scipy import stats\n",
      "107/62: stats.percentileofscore(df.Height, 158.8)\n",
      "107/63: stats.percentileofscore(df.Height, 178.34)\n",
      "107/64: 82.5-17.31\n",
      "107/65: df.describe()\n",
      "107/66: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])\n",
      "107/67:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.title('KDE')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Probability')\n",
      "107/68: df.head()\n",
      "107/69:\n",
      "df.Height[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Height[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Height')\n",
      "107/70:\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Weight')\n",
      "107/71:\n",
      "plt.figure(figsize=(10,10))\n",
      "ax1 = plt.subplot(2,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.grid(True)\n",
      "plt.legend()\n",
      "ax2 = plt.subplot(2,1,2, sharex = ax1)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.grid(True)\n",
      "plt.xlabel('Weight')\n",
      "plt.legend()\n",
      "107/72:\n",
      "mu = 10\n",
      "variance = 5\n",
      "sigma = variance**0.5\n",
      "x = np.linspace(mu-3*sigma, mu+3*sigma, 100)\n",
      "plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Gauss')\n",
      "plt.plot(x, stats.cauchy.pdf(x, mu, sigma), label='Cauchy')\n",
      "plt.legend()\n",
      "107/73:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "def f1(x,a,b):\n",
      "    return a*x + b\n",
      "\n",
      "popt,pcov = curve_fit(f1, df.Height, df.Weight )\n",
      "plt.plot(df.Height, f1(df.Height, *popt), color = 'red')\n",
      "plt.xlabel('Height [cm]')\n",
      "plt.ylabel('Weight [kg]')\n",
      "107/74: popt\n",
      "107/75: 1.38*160-159\n",
      "107/76: colors = dict(Male='red', Female='blue')\n",
      "107/77: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x]))\n",
      "107/78: colors\n",
      "107/79: df.dtypes\n",
      "107/80: df.Gender.loc[9970]\n",
      "107/81:\n",
      "plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')\n",
      "plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Weight')\n",
      "107/82:\n",
      "df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "model = sm.OLS(df.Weight, df.Height).fit()\n",
      "predictions = model.predict(df.Height) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "model.summary()\n",
      "107/83: df.head()\n",
      "107/84:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "model = sm.OLS(y, y).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "model.summary()\n",
      "107/85:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "model.summary()\n",
      "107/86:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "model.summary()\n",
      "107/87:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "#x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "model.summary()\n",
      "107/88:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "#x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/89:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/90:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "#x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/91:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height[:10]\n",
      "y = df.Weight[:10]\n",
      "\n",
      "#x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/92:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height[:10]\n",
      "y = df.Weight[:10]\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/93:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height[:20]\n",
      "y = df.Weight[:20]\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/94:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height[:20]\n",
      "y = df.Weight[:20]\n",
      "\n",
      "x = sm.tools.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/95:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height[:20]\n",
      "y = df.Weight[:20]\n",
      "\n",
      "x = sm.tools.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/96:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height[:20]\n",
      "y = df.Weight[:20]\n",
      "\n",
      "x = sm.tools.tools.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/97:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height[:20]\n",
      "y = df.Weight[:20]\n",
      "\n",
      "x = sm.tools.tools.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/98:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height[:]\n",
      "y = df.Weight[]\n",
      "\n",
      "x = sm.tools.tools.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/99:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height[]\n",
      "y = df.Weight[]\n",
      "\n",
      "x = sm.tools.tools.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/100:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.tools.tools.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/101:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/102:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as stats\n",
      "from scipy.optimize import curve_fit\n",
      "import numpy as np\n",
      "\n",
      "import statsmodels as sm\n",
      "\n",
      "%matplotlib inline\n",
      "107/103:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/104:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.tools.tools.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/105:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as stats\n",
      "from scipy.optimize import curve_fit\n",
      "import numpy as np\n",
      "\n",
      "import statsmodels.api as sm\n",
      "\n",
      "%matplotlib inline\n",
      "107/106:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/107: x = df.Height\n",
      "107/108:\n",
      "x = df.Height\n",
      "x\n",
      "107/109: x = df.Height\n",
      "107/110:\n",
      "x = df.Height\n",
      "x\n",
      "107/111:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as stats\n",
      "from scipy.optimize import curve_fit\n",
      "import numpy as np\n",
      "\n",
      "import statsmodels.api as sm\n",
      "\n",
      "%matplotlib inline\n",
      "107/112: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'\n",
      "107/113:\n",
      "hwg_df = pd.read_csv(\n",
      "    heights_weights_gender_data)\n",
      "107/114: hwg_df.head()\n",
      "107/115: hwg_df.Height =hwg_df.Height * 2.54\n",
      "107/116: hwg_df.Weight = hwg_df.Weight * 0.454\n",
      "107/117: hwg_df.describe()\n",
      "107/118: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)\n",
      "107/119:\n",
      "def my_var(x):\n",
      "    m = sum(x)/len(x)\n",
      "    return sum([(element-m)**2 for element in x])/(len(x)-1)\n",
      "107/120:\n",
      "def my_sd(x):\n",
      "    return my_var(x)**(0.5)\n",
      "107/121: my_var([1,2,3,4,5])\n",
      "107/122: my_var(hwg_df.Height)\n",
      "107/123: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()\n",
      "107/124: my_sd(hwg_df.Height) - hwg_df.Height.std()\n",
      "107/125: df = hwg_df\n",
      "107/126: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()\n",
      "107/127: from scipy import stats\n",
      "107/128: stats.percentileofscore(df.Height, 158.8)\n",
      "107/129: stats.percentileofscore(df.Height, 178.34)\n",
      "107/130: 82.5-17.31\n",
      "107/131: df.describe()\n",
      "107/132: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])\n",
      "107/133:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.title('KDE')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Probability')\n",
      "107/134: df.head()\n",
      "107/135:\n",
      "df.Height[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Height[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Height')\n",
      "107/136:\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Weight')\n",
      "107/137:\n",
      "plt.figure(figsize=(10,10))\n",
      "ax1 = plt.subplot(2,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.grid(True)\n",
      "plt.legend()\n",
      "ax2 = plt.subplot(2,1,2, sharex = ax1)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.grid(True)\n",
      "plt.xlabel('Weight')\n",
      "plt.legend()\n",
      "107/138:\n",
      "mu = 10\n",
      "variance = 5\n",
      "sigma = variance**0.5\n",
      "x = np.linspace(mu-3*sigma, mu+3*sigma, 100)\n",
      "plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Gauss')\n",
      "plt.plot(x, stats.cauchy.pdf(x, mu, sigma), label='Cauchy')\n",
      "plt.legend()\n",
      "107/139:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "def f1(x,a,b):\n",
      "    return a*x + b\n",
      "\n",
      "popt,pcov = curve_fit(f1, df.Height, df.Weight )\n",
      "plt.plot(df.Height, f1(df.Height, *popt), color = 'red')\n",
      "plt.xlabel('Height [cm]')\n",
      "plt.ylabel('Weight [kg]')\n",
      "107/140: popt\n",
      "107/141: 1.38*160-159\n",
      "107/142: colors = dict(Male='red', Female='blue')\n",
      "107/143: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x]))\n",
      "107/144: colors\n",
      "107/145: df.dtypes\n",
      "107/146: df.Gender.loc[9970]\n",
      "107/147:\n",
      "plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')\n",
      "plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Weight')\n",
      "107/148:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "print(print_model)\n",
      "107/149:\n",
      "x = df.Height\n",
      "x\n",
      "107/150: x = sm.add_constant(x)\n",
      "107/151:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "#print(print_model)\n",
      "107/152:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "print_model = model.summary()\n",
      "#print(print_model)\n",
      "107/153:\n",
      "#df.Height = sm.add_constant(df.Height)\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "model.summary()\n",
      "#print(print_model)\n",
      "107/154: model.__dir__()\n",
      "107/155: model.coef\n",
      "107/156: model.coef()\n",
      "107/157: model['coeff']\n",
      "107/158:\n",
      "# Note the difference in argument order\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "model.summary()\n",
      "107/159: model\n",
      "107/160: model.params\n",
      "107/161: print(model.params)\n",
      "107/162: model.params[0]\n",
      "107/163: model.params[1]\n",
      "107/164:\n",
      "model.params\n",
      "model.params[0]\n",
      "model.params[1]\n",
      "107/165:\n",
      "model.params\n",
      "model.params[0]\n",
      "model.params[1]\n",
      "107/166:\n",
      "print(model.params)\n",
      "model.params[0]\n",
      "model.params[1]\n",
      "107/167:\n",
      "print(model.params)\n",
      "print(model.params[0])\n",
      "print(model.params[1])\n",
      "107/168:\n",
      "print('''\n",
      "model.params\n",
      "model.params[0]\n",
      "model.params[1]\n",
      "''')\n",
      "107/169: model.params\n",
      "107/170:\n",
      "#Linear regression calculation: statsmodels\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model = sm.OLS(y, x).fit()\n",
      "predictions = model.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "model.summary()\n",
      "107/171:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as stats\n",
      "from scipy.optimize import curve_fit\n",
      "import numpy as np\n",
      "\n",
      "import statsmodels.api as sm\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "%matplotlib inline\n",
      "107/172:\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x,y)\n",
      "107/173:\n",
      "predictions = lm.predict(X)\n",
      "print(predictions)[0:5]\n",
      "107/174:\n",
      "predictions = lm.predict(x)\n",
      "print(predictions)[0:5]\n",
      "107/175:\n",
      "predictions = lm.predict(x)\n",
      "print(predictions[0:5])\n",
      "107/176: df.head()\n",
      "107/177:\n",
      "#Linear regression calculation: statsmodels\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model_statsmodels = sm.OLS(y, x).fit()\n",
      "predictions = model_statmodels.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "model_statmodels.summary()\n",
      "107/178:\n",
      "#Linear regression calculation: statsmodels\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model_statsmodels = sm.OLS(y, x).fit()\n",
      "predictions = model_statsmodels.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "model_statsmodels.summary()\n",
      "107/179: model_statsmodels.params\n",
      "107/180:\n",
      "lm = linear_model.LinearRegression()\n",
      "model_sklearn = lm.fit(x,y)\n",
      "107/181: model_sklearn\n",
      "107/182: model_sklearn.coef\n",
      "107/183: model_sklearn._coef\n",
      "107/184: model_sklearn.__dir__\n",
      "107/185: model_sklearn.__dir__()\n",
      "107/186: model_sklearn.coef_#__dir__()\n",
      "107/187: model_sklearn.coef_ * 2#__dir__()\n",
      "107/188: model_sklearn.coef_ #__dir__()\n",
      "107/189: model_sklearn.intercept\n",
      "107/190: help(model_sklearn)\n",
      "107/191: model_sklearn.intercept_\n",
      "107/192: plt.plot(df.Height, df.Weight)\n",
      "107/193: df.sort_values(by='Height')\n",
      "107/194: df.sort_values(by='Height', inplace=True)\n",
      "107/195: plt.plot(df.Height, df.Weight)\n",
      "107/196:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as stats\n",
      "from scipy.optimize import curve_fit\n",
      "import numpy as np\n",
      "\n",
      "import statsmodels.api as sm\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "%matplotlib inline\n",
      "107/197: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'\n",
      "107/198:\n",
      "hwg_df = pd.read_csv(\n",
      "    heights_weights_gender_data)\n",
      "107/199: hwg_df.head()\n",
      "107/200: hwg_df.Height =hwg_df.Height * 2.54\n",
      "107/201: hwg_df.Weight = hwg_df.Weight * 0.454\n",
      "107/202: hwg_df.describe()\n",
      "107/203: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)\n",
      "107/204:\n",
      "def my_var(x):\n",
      "    m = sum(x)/len(x)\n",
      "    return sum([(element-m)**2 for element in x])/(len(x)-1)\n",
      "107/205:\n",
      "def my_sd(x):\n",
      "    return my_var(x)**(0.5)\n",
      "107/206: my_var([1,2,3,4,5])\n",
      "107/207: my_var(hwg_df.Height)\n",
      "107/208: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()\n",
      "107/209: my_sd(hwg_df.Height) - hwg_df.Height.std()\n",
      "107/210: df = hwg_df\n",
      "107/211: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()\n",
      "107/212: from scipy import stats\n",
      "107/213: stats.percentileofscore(df.Height, 158.8)\n",
      "107/214: stats.percentileofscore(df.Height, 178.34)\n",
      "107/215: 82.5-17.31\n",
      "107/216: df.describe()\n",
      "107/217: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])\n",
      "107/218:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.title('KDE')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Probability')\n",
      "107/219: df.head()\n",
      "107/220:\n",
      "df.Height[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Height[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Height')\n",
      "107/221:\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Weight')\n",
      "107/222:\n",
      "plt.figure(figsize=(10,10))\n",
      "ax1 = plt.subplot(2,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.grid(True)\n",
      "plt.legend()\n",
      "ax2 = plt.subplot(2,1,2, sharex = ax1)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.grid(True)\n",
      "plt.xlabel('Weight')\n",
      "plt.legend()\n",
      "107/223:\n",
      "mu = 10\n",
      "variance = 5\n",
      "sigma = variance**0.5\n",
      "x = np.linspace(mu-3*sigma, mu+3*sigma, 100)\n",
      "plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Gauss')\n",
      "plt.plot(x, stats.cauchy.pdf(x, mu, sigma), label='Cauchy')\n",
      "plt.legend()\n",
      "107/224:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "def f1(x,a,b):\n",
      "    return a*x + b\n",
      "\n",
      "popt,pcov = curve_fit(f1, df.Height, df.Weight )\n",
      "plt.plot(df.Height, f1(df.Height, *popt), color = 'red')\n",
      "plt.xlabel('Height [cm]')\n",
      "plt.ylabel('Weight [kg]')\n",
      "107/225: popt\n",
      "107/226: 1.38*160-159\n",
      "107/227: colors = dict(Male='red', Female='blue')\n",
      "107/228: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x]))\n",
      "107/229: colors\n",
      "107/230: df.dtypes\n",
      "107/231: df.Gender.loc[9970]\n",
      "107/232:\n",
      "plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')\n",
      "plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Weight')\n",
      "107/233:\n",
      "#Linear regression calculation: statsmodels\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model_statsmodels = sm.OLS(y, x).fit()\n",
      "predictions = model_statsmodels.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "model_statsmodels.summary()\n",
      "107/234: model_statsmodels.params\n",
      "107/235:\n",
      "lm = linear_model.LinearRegression()\n",
      "model_sklearn = lm.fit(x,y)\n",
      "107/236:\n",
      "predictions = lm.predict(x)\n",
      "print(predictions[0:5])\n",
      "107/237: model_sklearn.coef_ #__dir__()\n",
      "107/238: model_sklearn.intercept_\n",
      "107/239: squares=[df.weights]\n",
      "107/240: squares=df.Weight - predictions\n",
      "107/241: squares\n",
      "107/242: squares.mean()\n",
      "107/243: squares\n",
      "107/244: squares[500:520]\n",
      "107/245: squares[500:550]\n",
      "107/246: squares**2\n",
      "107/247: squares**2.sum()\n",
      "107/248: (squares**2).sum()\n",
      "107/249: (squares**2).sum()**(0.5)\n",
      "107/250: (squares**2).sum()/len(squares)\n",
      "107/251: ((squares**2).sum()/len(squares))**0.5\n",
      "107/252: difference=df.Weight - predictions\n",
      "107/253: rms = ((difference**2).sum()/len(difference))**0.5\n",
      "107/254:\n",
      "rms = ((difference**2).sum()/len(difference))**0.5\n",
      "rms\n",
      "107/255: nominator = rms**2\n",
      "107/256:\n",
      "mean_weight = df.Weight.mean()\n",
      "mean_weight\n",
      "107/257:\n",
      "nominator = ((predictions-mean_weight)**2).sum()\n",
      "nominator\n",
      "107/258: denominator = ((df.Weight-mean_weight)**2).sum()\n",
      "107/259:\n",
      "denominator = ((df.Weight-mean_weight)**2).sum()\n",
      "denominator\n",
      "107/260: r_square = nominator/denominator\n",
      "107/261: r_square\n",
      "107/262:\n",
      "r_square = nominator/denominator\n",
      "r_square\n",
      "107/263:\n",
      "difference=df.Weight - predictions\n",
      "rms = ((difference**2).sum()/len(difference))**0.5\n",
      "rms\n",
      "107/264: (difference**2).sum()\n",
      "107/265: (difference**2)[1:10].sum()\n",
      "107/266: (difference**2)[0:10].sum()\n",
      "107/267: (difference**2)[0:11].sum()\n",
      "107/268: (difference**2)[1:10].sum()\n",
      "107/269:\n",
      "difference=df.Weight - predictions\n",
      "rms_model = ((difference**2).sum()/len(difference))**0.5\n",
      "rms_model\n",
      "107/270:\n",
      "rmse_mean = (((mean_weight - df.Weight)**2).mean())**(0.5)\n",
      "rmse_mean\n",
      "107/271: r_square_2 = 1-(rms_model/rmse_mean)\n",
      "107/272:\n",
      "r_square_2 = 1-(rms_model/rmse_mean)\n",
      "r_square_2\n",
      "107/273:\n",
      "rmse_mean = (((mean_weight - df.Weight)**2).sum()/len(df.Weight))**(0.5)\n",
      "rmse_mean\n",
      "107/274:\n",
      "r_square_2 = 1-(rms_model/rmse_mean)\n",
      "r_square_2\n",
      "107/275: mean_weight - df.Weight\n",
      "107/276: (mean_weight - df.Weight)**2.sum()\n",
      "107/277: ((mean_weight - df.Weight)**2).sum()\n",
      "107/278: ((mean_weight - df.Weight)**2).sum()/len(mean_weight)\n",
      "107/279: ((mean_weight - df.Weight)**2).sum()/len(df.Weight)\n",
      "108/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "108/2: help(pd.read_csv)\n",
      "108/3: top_sites = pd.read_csv('\\Regression\\top_1000_sites.tsv', delimiter='\\t' )\n",
      "108/4: import os\n",
      "108/5: help(os)\n",
      "108/6: os.get_cwd()\n",
      "108/7: os.__dir__\n",
      "108/8: os.__dir__()\n",
      "108/9: os.getcwd()\n",
      "108/10: top_sites = pd.read_csv('Regression\\top_1000_sites.tsv', delimiter='\\t' )\n",
      "108/11: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\\t' )\n",
      "108/12: top_sites.head()\n",
      "108/13: top_sites = pd.read_csv('Regression/top_1000_sites.tsv')#, delimiter='\\t' )\n",
      "108/14: top_sites.head()\n",
      "108/15: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\\t' )\n",
      "108/16: top_sites.head()\n",
      "108/17: help(plt.scatter)\n",
      "108/18: plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )\n",
      "108/19:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid=True\n",
      "108/20: help(plt.grid)\n",
      "108/21:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )\n",
      "grid(True)\n",
      "108/22:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )\n",
      "plt.grid\n",
      "108/23:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )\n",
      "plt.grid = True\n",
      "108/24:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )\n",
      "plt.grid = on\n",
      "108/25:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )\n",
      "plt.grid = True\n",
      "108/26:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )\n",
      "plt.grid()\n",
      "110/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "110/2: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\\t' )\n",
      "110/3: top_sites.head()\n",
      "110/4: help(plt.grid)\n",
      "110/5:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )\n",
      "plt.grid()\n",
      "110/6: help(plt.xlabel)\n",
      "110/7:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )\n",
      "plt.grid()\n",
      "plt(xlabel='Unique_visitors')\n",
      "110/8:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )\n",
      "plt.grid()\n",
      "plt.xlabel='Unique_visitors'\n",
      "plt.show()\n",
      "110/9:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )\n",
      "plt.grid()\n",
      "plt.xlabel('Unique_visitors')\n",
      "plt.show()\n",
      "110/10: help(plt.set_xlabel)\n",
      "111/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "111/2: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\\t' )\n",
      "111/3: top_sites.head()\n",
      "111/4: help(plt.set_xlabel)\n",
      "111/5:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )\n",
      "plt.grid()\n",
      "plt.xlabel('Unique_visitors')\n",
      "plt.show()\n",
      "111/6:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews )\n",
      "plt.grid()\n",
      "plt.xlabel('Unique_visitors')\n",
      "plt.ylabel('Page_views')\n",
      "plt.show()\n",
      "111/7: help(plt.kde)\n",
      "111/8: help(kde)\n",
      "111/9: top_sites.PageViews.plot()\n",
      "111/10: top_sites.PageViews.plot.kde()\n",
      "111/11:\n",
      "top_sites.PageViews.plot.kde()\n",
      "xlabe('Unique_visitors')\n",
      "ylabel('Density')\n",
      "111/12:\n",
      "top_sites.PageViews.plot.kde()\n",
      "plt.xlabe('Unique_visitors')\n",
      "plt.ylabel('Density')\n",
      "111/13:\n",
      "top_sites.PageViews.plot.kde()\n",
      "plt.xlabel('Unique_visitors')\n",
      "plt.ylabel('Density')\n",
      "111/14:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('Unique_visitors')\n",
      "plt.ylabel('Page_views')\n",
      "plt.show()\n",
      "111/15:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "111/16:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('Unique_visitors')\n",
      "plt.ylabel('Page_views')\n",
      "plt.show()\n",
      "111/17:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.show()\n",
      "111/18:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.show()\n",
      "111/19:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)\n",
      "plt.yscale('log')\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.show()\n",
      "111/20:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)\n",
      "plt.yscale('log')\n",
      "plt.xscale('log')\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.show()\n",
      "111/21:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.show()\n",
      "111/22:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "111/23: help(sm)\n",
      "111/24: help(sm.ols)\n",
      "111/25: help(sm.OLS)\n",
      "111/26:\n",
      "x = top_sites.UniqueVisitors\n",
      "y = top_sites.PageViews\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS.fit(y,x)\n",
      "111/27:\n",
      "x = top_sites.UniqueVisitors\n",
      "y = top_sites.PageViews\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "111/28:\n",
      "x = top_sites.UniqueVisitors\n",
      "y = top_sites.PageViews\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "print(model)\n",
      "111/29:\n",
      "x = top_sites.UniqueVisitors\n",
      "y = top_sites.PageViews\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "model.summary()\n",
      "111/30:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "\n",
      "import seaborn as sns\n",
      "111/31: sns.regplot(data=top_sites[columns=['UniqueVisitor','PageViews']])\n",
      "111/32: top_sites(columns='UniqueVisitors')\n",
      "111/33: top_sites[columns='UniqueVisitors']\n",
      "111/34: top_sites[columns=='UniqueVisitors']\n",
      "111/35: top_sites['UniqueVisitors']\n",
      "111/36: top_sites['UniqueVisitors', 'PageViews']\n",
      "111/37: top_sites[['UniqueVisitors', 'PageViews']]\n",
      "111/38: sns.regplot(data=top_sites[['UniqueVisitor','PageViews']])\n",
      "111/39: sns.regplot(data=top_sites[['UniqueVisitors','PageViews']])\n",
      "111/40: sns.regplot(x='Unique_visitors', y='Page_views', data=top_sites[['UniqueVisitors','PageViews']])\n",
      "111/41: sns.regplot(x='UniqueVisitors', y='PageViews', data=top_sites[['UniqueVisitors','PageViews']])\n",
      "111/42: sns.regplot(x='UniqueVisitors', y='PageViews', data=top_sites)\n",
      "112/1: z = sns.regplot(x='UniqueVisitors', y='PageViews', data=top_sites)\n",
      "112/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "\n",
      "import seaborn as sns\n",
      "112/3: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\\t' )\n",
      "112/4: top_sites.head()\n",
      "112/5:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.show()\n",
      "112/6:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)\n",
      "plt.yscale('log')\n",
      "plt.xscale('log')\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.show()\n",
      "112/7:\n",
      "x = top_sites.UniqueVisitors\n",
      "y = top_sites.PageViews\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "model.summary()\n",
      "112/8: help(sm.OLS).\n",
      "112/9: z\n",
      "112/10: z = sns.regplot(x='UniqueVisitors', y='PageViews', data=top_sites)\n",
      "112/11:\n",
      "z = sns.regplot(x='UniqueVisitors', y='PageViews', data=top_sites)\n",
      "z.set_yscale('log')\n",
      "z.set_xscale('log')\n",
      "112/12: logs = log(top_sites['Unigue'])\n",
      "112/13: logs = log(top_sites['UniqueVisitors'])\n",
      "112/14:\n",
      "logs = log(top_sites['UniqueVisitors'])\n",
      "logs\n",
      "112/15:\n",
      "logs = log(top_sites['UniqueVisitors', 'PageViews'])\n",
      "logs\n",
      "112/16:\n",
      "logs = log(top_sites[['UniqueVisitors', 'PageViews']])\n",
      "logs\n",
      "112/17:\n",
      "z = sns.regplot(x='UniqueVisitors', y='PageViews', data=logs)\n",
      "#z.set_yscale('log')\n",
      "#z.set_xscale('log')\n",
      "112/18: logs = log(top_sites[['UniqueVisitors', 'PageViews']])\n",
      "112/19:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)\n",
      "plt.yscale('log')\n",
      "plt.xscale('log')\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.show()\n",
      "112/20: plt.gcf().set_size_inches(8,4)\n",
      "112/21:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "%matplotlib inline\n",
      "112/22: plt.gcf().set_size_inches(8,4)\n",
      "112/23:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)\n",
      "plt.yscale('log')\n",
      "plt.xscale('log')\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.show()\n",
      "112/24: plt.gcf().set_size_inches(8,4)\n",
      "112/25: plt.gcf().set_size_inches(10,5)\n",
      "112/26: plt.gcf().set_size_inches(12,6)\n",
      "112/27:\n",
      "plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)\n",
      "plt.yscale('log')\n",
      "plt.xscale('log')\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.gcf().set_size_inches(12,6)\n",
      "plt.show()\n",
      "112/28:\n",
      "figure = plt.scatter(top_sites.UniqueVisitors, top_sites.PageViews)\n",
      "plt.yscale('log')\n",
      "plt.xscale('log')\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.gcf().set_size_inches(12,6)\n",
      "plt.show()\n",
      "112/29: figure\n",
      "112/30: help(figure)\n",
      "112/31:\n",
      "x_min = log(top_sites.UniqueVisiotors).min()\n",
      "x_min\n",
      "112/32:\n",
      "x_min = log(top_sites.UniqueVisitors).min()\n",
      "x_min\n",
      "112/33:\n",
      "x_min = log(top_sites.UniqueVisitors).min()\n",
      "x_max = log(top_sites.UniqueVisitors).max()\n",
      "x_min, x_max\n",
      "112/34:\n",
      "x = log(top_sites.UniqueVisitors)\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "model.summary()\n",
      "112/35: model.predict([x_min, x_max])\n",
      "112/36:\n",
      "predictions = model.predict([x_min, x_max])\n",
      "predictions\n",
      "112/37:\n",
      "predictions = model.predict([x_min, x_max])\n",
      "print(predictions)\n",
      "112/38:\n",
      "predictions = model.predict([x_min, x_max])\n",
      "print(predictions)\n",
      "help(predictions)\n",
      "112/39:\n",
      "predictions = model.predict([x_min, x_max])\n",
      "print(predictions)\n",
      "help(model.predict())\n",
      "112/40:\n",
      "predictions = model.predict([x_min, x_max])\n",
      "print(predictions)\n",
      "help(model.predict())\n",
      "112/41:\n",
      "predictions = model.predict([x_min, x_max])\n",
      "print(predictions)\n",
      "help(model)\n",
      "112/42:\n",
      "predictions = model.predict(x_min, x_max)\n",
      "print(predictions)\n",
      "#help(model)\n",
      "112/43:\n",
      "predictions = model.predict([x_min, x_max])\n",
      "print(predictions)\n",
      "#help(model)\n",
      "112/44:\n",
      "predictions = model.predict([x_min, x_max])\n",
      "predictions\n",
      "#help(model)\n",
      "112/45:\n",
      "predictions = model.predict([1, 2])\n",
      "predictions\n",
      "#help(model)\n",
      "112/46:\n",
      "x = log(top_sites.UniqueVisitors)\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "model.summary()\n",
      "112/47:\n",
      "predictions = model.predict([15, 20])\n",
      "predictions\n",
      "#help(model)\n",
      "112/48:\n",
      "predictions = model.predict([15, 20])\n",
      "predictions = 1.3363*15 - 2.8344\n",
      "#help(model)\n",
      "112/49:\n",
      "predictions = model.predict([15, 20])\n",
      "predictions = 1.3363*15 - 2.8344\n",
      "predictions\n",
      "112/50:\n",
      "predictions = model.predict([15, 20])\n",
      "predictions = 1.3363*20 - 2.8344\n",
      "predictions\n",
      "112/51:\n",
      "predictions = model.predict([15, 20])\n",
      "predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]\n",
      "predictions\n",
      "112/52:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "112/53:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.plot([x_min, x_max],predictions)\n",
      "112/54:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.plot([x_min, x_max],predictions, color='red')\n",
      "112/55:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.plot(minmax,predictions, color='red')\n",
      "112/56:\n",
      "x_min = log(top_sites.UniqueVisitors).min()\n",
      "x_max = log(top_sites.UniqueVisitors).max()\n",
      "minmax = [x_min, x_max]\n",
      "112/57:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.plot(minmax,predictions, color='red')\n",
      "112/58:\n",
      "predictions = model.predict([15, 20])\n",
      "predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]\n",
      "predictions_2 = model.predict([15,20])\n",
      "112/59:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.plot(minmax,predictions_2, color='red')\n",
      "112/60:\n",
      "predictions = model.predict(minmax)\n",
      "predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]\n",
      "predictions_2 = model.predict(minmax)\n",
      "predictions_2\n",
      "112/61: minmax_2 = pd.DataFrame(minmax)\n",
      "112/62:\n",
      "predictions = model.predict(minmax)\n",
      "predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]\n",
      "predictions_2 = model.predict(minmax_2)\n",
      "predictions_2\n",
      "112/63:\n",
      "minmax_2 = pd.DataFrame(minmax)\n",
      "minmax_2\n",
      "112/64:\n",
      "minmax_2 = pd.DataFrame(minmax)\n",
      "minmax_2.transpose()\n",
      "112/65:\n",
      "minmax_2 = pd.DataFrame(minmax)\n",
      "minmax_2.transpose(inplace=True)\n",
      "112/66:\n",
      "minmax_2 = pd.DataFrame(minmax)\n",
      "minmax_2 = minmax_2.transpose()\n",
      "112/67:\n",
      "predictions = model.predict(minmax)\n",
      "predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]\n",
      "predictions_2 = model.predict(minmax_2)\n",
      "predictions_2\n",
      "112/68: minmax_2 = pd.DataFrame(minmax)\n",
      "112/69:\n",
      "predictions = model.predict(minmax)\n",
      "predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]\n",
      "predictions_2 = model.predict(minmax_2)\n",
      "predictions_2\n",
      "112/70: minmax_2 = x\n",
      "112/71:\n",
      "predictions = model.predict(minmax)\n",
      "predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]\n",
      "predictions_2 = model.predict(minmax_2)\n",
      "predictions_2\n",
      "112/72: minmax_2 = sm.add_constant(minmax)\n",
      "112/73:\n",
      "predictions = model.predict(minmax)\n",
      "predictions = [1.3363*15 - 2.8344, 1.3363*20-2.8344]\n",
      "predictions_2 = model.predict(minmax_2)\n",
      "predictions_2\n",
      "112/74:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.plot(minmax,predictions_2, color='red')\n",
      "112/75:\n",
      "minmax_2 = sm.add_constant(minmax)\n",
      "predictions = model.predict(minmax_2)\n",
      "predictions\n",
      "112/76:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.plot(minmax,predictions, color='red')\n",
      "112/77:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.plot(minmax,predictions, color='red')\n",
      "plt.gcf.set_size_inches(12,6)\n",
      "112/78:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.plot(minmax,predictions, color='red')\n",
      "plt.gcf().set_size_inches(12,6)\n",
      "112/79:\n",
      "x = log(top_sites.UniqueVisitors)\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "model.summary()\n",
      "model.information()\n",
      "112/80:\n",
      "x = log(top_sites.UniqueVisitors)\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "model.summary()\n",
      "112/81: help(model)\n",
      "112/82:\n",
      "x = log(top_sites.UniqueVisitors)\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "y_predictions = model.predict(x)\n",
      "rmse = rmse(y, y_predictions)\n",
      "print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/83:\n",
      "x = log(top_sites.UniqueVisitors)\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "y_predictions = model.predict(x)\n",
      "rmse = sm.rmse(y, y_predictions)\n",
      "print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/84:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.tools.eval_measures import rmse\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "%matplotlib inline\n",
      "112/85:\n",
      "x = log(top_sites.UniqueVisitors)\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "y_predictions = model.predict(x)\n",
      "rmse = sm.rmse(y, y_predictions)\n",
      "print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/86:\n",
      "x = log(top_sites.UniqueVisitors)\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "y_predictions = model.predict(x)\n",
      "rmse = rmse(y, y_predictions)\n",
      "print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/87:\n",
      "x = [log(top_sites.UniqueVisitors), top_sites.InEnglish]\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "y_predictions = model.predict(x)\n",
      "rmse = rmse(y, y_predictions)\n",
      "print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/88:\n",
      "x = [log(top_sites.UniqueVisitors), top_sites.InEnglish]\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "y_predictions = model.predict(x)\n",
      "rmse = rmse(y, y_predictions)\n",
      "print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/89:\n",
      "m,c = np.polyfit(x,y)\n",
      "plt.plot(x, m*x+c)\n",
      "112/90:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.tools.eval_measures import rmse\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "%matplotlib inline\n",
      "112/91:\n",
      "m,c = np.polyfit(x,y)\n",
      "plt.plot(x, m*x+c)\n",
      "112/92:\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c)\n",
      "112/93:\n",
      "m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)\n",
      "plt.plot(x, m*x+c)\n",
      "112/94:\n",
      "m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)\n",
      "plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c)\n",
      "112/95:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)\n",
      "plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c)\n",
      "112/96:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)\n",
      "plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c, color='red')\n",
      "112/97:\n",
      "x = [log(top_sites.UniqueVisitors)]\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "y_predictions = model.predict(x)\n",
      "rmse = rmse(y, y_predictions)\n",
      "print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/98:\n",
      "x = log(top_sites.UniqueVisitors)\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "y_predictions = model.predict(x)\n",
      "rmse = rmse(y, y_predictions)\n",
      "print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/99:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.plot(log(top_sites.UniqueVisitors),model.predict(log(top_sites.UniqueVisitors)), color='red')\n",
      "plt.gcf().set_size_inches(12,6)\n",
      "112/100:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.plot(minmax,predictions, color='red')\n",
      "plt.gcf().set_size_inches(12,6)\n",
      "112/101:\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)\n",
      "plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c, color='red')\n",
      "plt.gcf().set_size_inches(12,6)\n",
      "112/102: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).append(tope_sites['IsEnglish'])\n",
      "112/103: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).append(top_sites['IsEnglish'])\n",
      "112/104: top_sites.head()\n",
      "112/105: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).append(top_sites['InEnglish'])\n",
      "112/106: logs.head()\n",
      "112/107: logs = log(top_sites[['UniqueVisitors', 'PageViews']])#.append(top_sites['InEnglish'])\n",
      "112/108: logs.head()\n",
      "112/109: logs.summary()\n",
      "112/110: logs.describe()\n",
      "112/111: logs.info()\n",
      "112/112: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).append(top_sites['InEnglish'], axis=1)\n",
      "112/113: help(pd.DataFrame.append())\n",
      "112/114: help(pd.DataFrame.append\n",
      "112/115: help(pd.DataFrame.append)\n",
      "112/116: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish)\n",
      "112/117: logs.head()\n",
      "112/118: z = sns.regplot(x='UniqueVisitors', y='PageViews',hue='InEnglish' data=logs)\n",
      "112/119: z = sns.regplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)\n",
      "112/120: z = sns.lmplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)\n",
      "112/121:\n",
      "#x = log(top_sites.UniqueVisitors)\n",
      "#y = log(top_sites.PageViews)\n",
      "\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(formula='PageViews ~ UniqueVisitors + InEnglish').fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/122:\n",
      "#x = log(top_sites.UniqueVisitors)\n",
      "#y = log(top_sites.PageViews)\n",
      "\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(formula='PageViews ~ UniqueVisitors + InEnglish', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/123:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.tools.eval_measures import rmse\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "%matplotlib inline\n",
      "112/124:\n",
      "#x = log(top_sites.UniqueVisitors)\n",
      "#y = log(top_sites.PageViews)\n",
      "\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "model = smf.OLS(formula='PageViews ~ UniqueVisitors + InEnglish', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/125:\n",
      "#x = log(top_sites.UniqueVisitors)\n",
      "#y = log(top_sites.PageViews)\n",
      "\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/126:\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "#model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/127:\n",
      "import smpi.statsmodels as ssm\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = ssm.add_constant(x)\n",
      "\n",
      "model = ssm.OLS(y,x).fit()\n",
      "\n",
      "#model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/128:\n",
      "#import smpi.statsmodels as ssm\n",
      "#x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "#y = log(top_sites.PageViews)\n",
      "\n",
      "#x = ssm.add_constant(x)\n",
      "\n",
      "#model = ssm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ UniqueVisitors + c(InEnglish)', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/129:\n",
      "#import smpi.statsmodels as ssm\n",
      "#x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "#y = log(top_sites.PageViews)\n",
      "\n",
      "#x = ssm.add_constant(x)\n",
      "\n",
      "#model = ssm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ UniqueVisitors + C(InEnglish)', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/130:\n",
      "#import smpi.statsmodels as ssm\n",
      "#x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "#y = log(top_sites.PageViews)\n",
      "\n",
      "#x = ssm.add_constant(x)\n",
      "\n",
      "#model = ssm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/131:\n",
      "logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'], drop_first=True)), axis=1)\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/132:\n",
      "logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'], drop_first=True)), axis=1)\n",
      "\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "#model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/133:\n",
      "logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'], drop_first=True)), axis=1)\n",
      "\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x.astype(float)).fit()\n",
      "\n",
      "#model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/134: logs.info()\n",
      "112/135:\n",
      "logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'])), axis=1)\n",
      "\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x.astype(float)).fit()\n",
      "\n",
      "#model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/136: logs.info()\n",
      "112/137: logs.head()\n",
      "112/138: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish)\n",
      "112/139: logs.head()\n",
      "112/140:\n",
      "#logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'])), axis=1)\n",
      "logs['IsEnglish']=logs.InEnglish.map({'Yes':0,'No':1})\n",
      "\n",
      "x = logs[['UniqueVisitors', 'IsEnglish']]\n",
      "y = logs['PageViews']\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x.astype(float)).fit()\n",
      "\n",
      "#model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/141:\n",
      "#logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'])), axis=1)\n",
      "logs.dropna()\n",
      "logs['IsEnglish']=logs.InEnglish.map({'Yes':0,'No':1})\n",
      "\n",
      "x = logs[['UniqueVisitors', 'IsEnglish']]\n",
      "y = logs['PageViews']\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x.astype(float)).fit()\n",
      "\n",
      "#model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/142:\n",
      "#logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'])), axis=1)\n",
      "logs.dropna()\n",
      "logs['IsEnglish']=logs.InEnglish.map({'Yes':0,'No':1})\n",
      "\n",
      "x = logs[['UniqueVisitors', 'IsEnglish']]\n",
      "y = logs['PageViews']\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "#model = smf.ols(formula='PageViews ~ C(InEnglish)', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/143: logs.group_by(by='InEnglish').count()\n",
      "112/144: logs.groupby(by='InEnglish').count()\n",
      "112/145: logs.groupby(by='UniqueVisitors').count()\n",
      "112/146:\n",
      "#logs = pd.concat((logs,pd.get_dummies(logs['InEnglish'])), axis=1)\n",
      "logs.dropna()\n",
      "logs['IsEnglish']=logs.InEnglish.map({'Yes':0,'No':1})\n",
      "\n",
      "x = logs[['UniqueVisitors', 'IsEnglish']]\n",
      "y = logs['PageViews']\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "#model = sm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ C(IsEnglish)', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/147:\n",
      "x = logs[['UniqueVisitors', 'IsEnglish']]\n",
      "y = logs['PageViews']\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "#model = sm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ C(IsEnglish)', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/148:\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "#model = sm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ InEnglish', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/149:\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "#model = sm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/150: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish)\n",
      "112/151: logs.head()\n",
      "112/152:\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish).labels\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "#model = sm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish_ord', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/153:\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "#model = sm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish_ord', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/154: logs.head()\n",
      "112/155: logs.info()\n",
      "112/156:\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "#model = sm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ UniqueVisitors + C(InEnglish_ord)', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "112/157:\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "#model = sm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ (InEnglish_ord)', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "113/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as stats\n",
      "from scipy.optimize import curve_fit\n",
      "import numpy as np\n",
      "\n",
      "import statsmodels.api as sm\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "%matplotlib inline\n",
      "113/2: heights_weights_gender_data = '/users/wioletanytko/documents/workspace/r/umapro/02-Exploration/data/01_heights_weights_genders.csv'\n",
      "113/3:\n",
      "hwg_df = pd.read_csv(\n",
      "    heights_weights_gender_data)\n",
      "113/4: hwg_df.head()\n",
      "113/5: hwg_df.Height =hwg_df.Height * 2.54\n",
      "113/6: hwg_df.Weight = hwg_df.Weight * 0.454\n",
      "113/7: hwg_df.describe()\n",
      "113/8: hwg_df.Height.quantile(q=0.05), hwg_df.Height.quantile(q=0.95)\n",
      "113/9:\n",
      "def my_var(x):\n",
      "    m = sum(x)/len(x)\n",
      "    return sum([(element-m)**2 for element in x])/(len(x)-1)\n",
      "113/10:\n",
      "def my_sd(x):\n",
      "    return my_var(x)**(0.5)\n",
      "113/11: my_var([1,2,3,4,5])\n",
      "113/12: my_var(hwg_df.Height)\n",
      "113/13: hwg_df.Height.mean() -hwg_df.Height.var(), hwg_df.Height.mean() +hwg_df.Height.var()\n",
      "113/14: my_sd(hwg_df.Height) - hwg_df.Height.std()\n",
      "113/15: df = hwg_df\n",
      "113/16: df.Height.mean() - df.Height.std(), df.Height.mean()+ df.Height.std()\n",
      "113/17: from scipy import stats\n",
      "113/18: stats.percentileofscore(df.Height, 158.8)\n",
      "113/19: stats.percentileofscore(df.Height, 178.34)\n",
      "113/20: 82.5-17.31\n",
      "113/21: df.describe()\n",
      "113/22: histogram = df.Height.hist(bins=64, label=['Height', 'Number'])\n",
      "113/23:\n",
      "kde = df.Height.plot.kde()\n",
      "plt.title('KDE')\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Probability')\n",
      "113/24: df.head()\n",
      "113/25:\n",
      "df.Height[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Height[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Height')\n",
      "113/26:\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Weight')\n",
      "113/27:\n",
      "plt.figure(figsize=(10,10))\n",
      "ax1 = plt.subplot(2,1,1)\n",
      "df.Weight[df.Gender=='Male'].plot.kde(label='Male')\n",
      "plt.grid(True)\n",
      "plt.legend()\n",
      "ax2 = plt.subplot(2,1,2, sharex = ax1)\n",
      "df.Weight[df.Gender=='Female'].plot.kde(label='Female')\n",
      "plt.grid(True)\n",
      "plt.xlabel('Weight')\n",
      "plt.legend()\n",
      "113/28:\n",
      "mu = 10\n",
      "variance = 5\n",
      "sigma = variance**0.5\n",
      "x = np.linspace(mu-3*sigma, mu+3*sigma, 100)\n",
      "plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Gauss')\n",
      "plt.plot(x, stats.cauchy.pdf(x, mu, sigma), label='Cauchy')\n",
      "plt.legend()\n",
      "113/29:\n",
      "plt.scatter(df.Height, df.Weight)\n",
      "def f1(x,a,b):\n",
      "    return a*x + b\n",
      "\n",
      "popt,pcov = curve_fit(f1, df.Height, df.Weight )\n",
      "plt.plot(df.Height, f1(df.Height, *popt), color = 'red')\n",
      "plt.xlabel('Height [cm]')\n",
      "plt.ylabel('Weight [kg]')\n",
      "113/30: popt\n",
      "113/31: 1.38*160-159\n",
      "113/32: colors = dict(Male='red', Female='blue')\n",
      "113/33: plt.scatter(df.Height, df.Weight, c=df.Gender.apply(lambda x: colors[x]))\n",
      "113/34: colors\n",
      "113/35: df.dtypes\n",
      "113/36: df.Gender.loc[9970]\n",
      "113/37:\n",
      "plt.scatter(df[df.Gender=='Male']['Height'], df[df.Gender=='Male']['Weight'], label='Male')\n",
      "plt.scatter(df[df.Gender=='Female']['Height'], df[df.Gender=='Female']['Weight'], label='Female')\n",
      "plt.legend()\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('Weight')\n",
      "113/38:\n",
      "#Linear regression calculation: statsmodels\n",
      "x = df.Height\n",
      "y = df.Weight\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "model_statsmodels = sm.OLS(y, x).fit()\n",
      "predictions = model_statsmodels.predict(x) # make the predictions by the model\n",
      "\n",
      "# Print out the statistics\n",
      "model_statsmodels.summary()\n",
      "113/39: model_statsmodels.params\n",
      "113/40:\n",
      "lm = linear_model.LinearRegression()\n",
      "model_sklearn = lm.fit(x,y)\n",
      "113/41:\n",
      "predictions = lm.predict(x)\n",
      "print(predictions[0:5])\n",
      "113/42: model_sklearn.coef_ #__dir__()\n",
      "113/43: model_sklearn.intercept_\n",
      "113/44:\n",
      "difference=df.Weight - predictions\n",
      "rms_model = ((difference**2).sum()/len(difference))**0.5\n",
      "rms_model\n",
      "113/45: (difference**2)[1:10].sum()\n",
      "113/46:\n",
      "mean_weight = df.Weight.mean()\n",
      "mean_weight\n",
      "113/47:\n",
      "nominator = ((predictions-mean_weight)**2).sum()\n",
      "nominator\n",
      "113/48:\n",
      "denominator = ((df.Weight-mean_weight)**2).sum()\n",
      "denominator\n",
      "113/49:\n",
      "r_square = nominator/denominator\n",
      "r_square\n",
      "113/50:\n",
      "rmse_mean = (((mean_weight - df.Weight)**2).sum()/len(df.Weight))**(0.5)\n",
      "rmse_mean\n",
      "113/51:\n",
      "r_square_2 = 1-(rms_model/rmse_mean)\n",
      "r_square_2\n",
      "113/52: ((mean_weight - df.Weight)**2).sum()/len(df.Weight)\n",
      "114/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.tools.eval_measures import rmse\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "%matplotlib inline\n",
      "114/2: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\\t' )\n",
      "114/3: top_sites.head()\n",
      "114/4:\n",
      "x_min = log(top_sites.UniqueVisitors).min()\n",
      "x_max = log(top_sites.UniqueVisitors).max()\n",
      "minmax = [x_min, x_max]\n",
      "114/5:\n",
      "#regression plot from statsmodels predict function\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "plt.plot(minmax,predictions, color='red')\n",
      "plt.gcf().set_size_inches(12,6)\n",
      "114/6:\n",
      "#regression from numpy polyfit function\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)\n",
      "plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c, color='red')\n",
      "plt.gcf().set_size_inches(12,6)\n",
      "114/7:\n",
      "x = log(top_sites.UniqueVisitors)\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "y_predictions = model.predict(x)\n",
      "rmse = rmse(y, y_predictions)\n",
      "print('rmse: ', rmse)\n",
      "model.summary()\n",
      "114/8:\n",
      "minmax_2 = sm.add_constant(minmax)\n",
      "predictions = model.predict(minmax_2)\n",
      "predictions\n",
      "114/9: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish)\n",
      "114/10: z = sns.lmplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)\n",
      "114/11: logs.info()\n",
      "114/12:\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "#model = sm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ (InEnglish_ord)', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "114/13: logs.head()\n",
      "114/14: logs.info()\n",
      "114/15:\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "#model = sm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~UniqueVisitors + InEnglish_ord', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "114/16:\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "#model = sm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~UniqueVisitors + InEnglish', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "114/17:\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "#model = sm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ InEnglish', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "114/18:\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "#model = sm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ InEnglish + UniqueVisitors', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "114/19:\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "#model = sm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ UniqueVisitors', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "114/20: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish, top_sites.HasAdvertising)\n",
      "114/21: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).concat(top_sites.InEnglish, top_sites.HasAdvertising)\n",
      "114/22: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish, top_sites.HasAdvertising)\n",
      "114/23: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish)\n",
      "114/24:\n",
      "#regression plot from statsmodels predict function\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "plt.grid()\n",
      "plt.xlabel('log(Unique_visitors)')\n",
      "plt.ylabel('log(Page_views)')\n",
      "#plt.plot(minmax,predictions, color='red')\n",
      "plt.gcf().set_size_inches(12,6)\n",
      "114/25:\n",
      "#regression from numpy polyfit function\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)\n",
      "plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c, color='red')\n",
      "plt.gcf().set_size_inches(12,6)\n",
      "114/26: logs = pd.concat(log(top_sites[['UniqueVisitors', 'PageViews']]),(top_sites.InEnglish))\n",
      "114/27: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish)\n",
      "114/28: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish, top_sitez.HasAdvertising)\n",
      "114/29: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish, top_sites.HasAdvertising)\n",
      "114/30: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish).join(top_sites.HasAdvertising)\n",
      "114/31:\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "logs['InEnglish_ord'] = pd.Categorical(logs.InEnglish)\n",
      "#x = sm.add_constant(x)\n",
      "\n",
      "#model = sm.OLS(y,x).fit()\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish + HasAdvertising', data=logs).fit()\n",
      "\n",
      "#y_predictions = model.predict(x)\n",
      "#rmse = rmse(y, y_predictions)\n",
      "#print('rmse: ', rmse)\n",
      "model.summary()\n",
      "114/32: top_sites.info()\n",
      "115/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.tools.eval_measures import rmse\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "%matplotlib inline\n",
      "115/2: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\\t' )\n",
      "115/3: top_sites.info()\n",
      "115/4:\n",
      "x_min = log(top_sites.UniqueVisitors).min()\n",
      "x_max = log(top_sites.UniqueVisitors).max()\n",
      "minmax = [x_min, x_max]\n",
      "115/5:\n",
      "#regression from numpy polyfit function\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)\n",
      "plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c, color='red')\n",
      "plt.gcf().set_size_inches(12,6)\n",
      "115/6:\n",
      "x = log(top_sites.UniqueVisitors)\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "y_predictions = model.predict(x)\n",
      "rmse = rmse(y, y_predictions)\n",
      "print('rmse: ', rmse)\n",
      "model.summary()\n",
      "115/7:\n",
      "minmax_2 = sm.add_constant(minmax)\n",
      "predictions = model.predict(minmax_2)\n",
      "predictions\n",
      "115/8: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish).join(top_sites.HasAdvertising)\n",
      "115/9: z = sns.lmplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)\n",
      "115/10: logs.info()\n",
      "115/11:\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish + HasAdvertising', data=logs).fit()\n",
      "\n",
      "model.summary()\n",
      "115/12: logs.head()\n",
      "115/13:\n",
      "z = sns.lmplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)\n",
      "help(z)\n",
      "115/14: z = sns.lmplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)\n",
      "115/15:\n",
      "#multiple regression model with statsmodels -formula interface\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish + HasAdvertising', data=logs).fit()\n",
      "\n",
      "model.summary()\n",
      "115/16:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.tools.eval_measures import rmse\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "from sklearn import linearmodel\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "%matplotlib inline\n",
      "115/17:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.tools.eval_measures import rmse\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "%matplotlib inline\n",
      "115/18:\n",
      "y_sk = logs.PageViews\n",
      "x_sk = logs.UniqueVisitors\n",
      "lm = linera_model.LinearRegression()\n",
      "model = lm.fit(x,y)\n",
      "lm.coef()\n",
      "115/19:\n",
      "y_sk = logs.PageViews\n",
      "x_sk = logs.UniqueVisitors\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x,y)\n",
      "lm.coef()\n",
      "115/20:\n",
      "y_sk = logs.PageViews\n",
      "x_sk = logs.UniqueVisitors\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "lm.coef()\n",
      "115/21:\n",
      "y_sk = [1,2,3,4] #logs.PageViews\n",
      "x_sk = [1,4,6,8] #logs.UniqueVisitors\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "lm.coef()\n",
      "115/22:\n",
      "Stock_Market = {'Year': [2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016],\n",
      "                'Month': [12, 11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1],\n",
      "                'Interest_Rate': [2.75,2.5,2.5,2.5,2.5,2.5,2.5,2.25,2.25,2.25,2,2,2,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75],\n",
      "                'Unemployment_Rate': [5.3,5.3,5.3,5.3,5.4,5.6,5.5,5.5,5.5,5.6,5.7,5.9,6,5.9,5.8,6.1,6.2,6.1,6.1,6.1,5.9,6.2,6.2,6.1],\n",
      "                'Stock_Index_Price': [1464,1394,1357,1293,1256,1254,1234,1195,1159,1167,1130,1075,1047,965,943,958,971,949,884,866,876,822,704,719]        \n",
      "                }\n",
      "\n",
      "df = pd.DataFrame(Stock_Market,columns=['Year','Month','Interest_Rate','Unemployment_Rate','Stock_Index_Price'])\n",
      "\n",
      "X = df[['Interest_Rate','Unemployment_Rate']] # here we have 2 variables for multiple regression. If you just want to use one variable for simple linear regression, then use X = df['Interest_Rate'] for example.Alternatively, you may add additional variables within the brackets\n",
      "Y = df['Stock_Index_Price']\n",
      "115/23: X\n",
      "115/24: Y\n",
      "115/25:\n",
      "y_sk = logs.PageViews\n",
      "x_sk = logs.UniqueVisitors\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "lm.coef()\n",
      "115/26:\n",
      "y_sk = logs.PageViews.to_numpy()\n",
      "x_sk = logs.UniqueVisitors.to_numpy()\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "lm.coef()\n",
      "115/27:\n",
      "y_sk = logs.PageViews.as_matrix()\n",
      "x_sk = logs.UniqueVisitors.as_matrix()\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "lm.coef()\n",
      "115/28:\n",
      "y_sk = logs.PageViews.reshape(-1,1)\n",
      "x_sk = logs.UniqueVisitors.reshape(-1,1)\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "lm.coef()\n",
      "115/29:\n",
      "y_sk = logs.PageViews#.reshape(-1,1)\n",
      "x_sk = logs.UniqueVisitors#.reshape(-1,1)\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "lm.coef()\n",
      "115/30:\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs.UniqueVisitors.values.reshape(-1,1)\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "lm.coef()\n",
      "115/31:\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs.UniqueVisitors.values.reshape(-1,1)\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "lm.coef_()\n",
      "115/32:\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs.UniqueVisitors.values.reshape(-1,1)\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "lm.coef_\n",
      "115/33:\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs.UniqueVisitors.values.reshape(-1,1)\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "lm.coef_, lm.intercept_\n",
      "115/34:\n",
      "#linear regression with scikit-learn\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs.UniqueVisitors.values.reshape(-1,1)\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "print('model coefficient and intereception: ',lm.coef_, lm.intercept_)\n",
      "115/35:\n",
      "#linear regression with scikit-learn\n",
      "y_sk = logs.PageViews.values.to_numpy()\n",
      "x_sk = logs.UniqueVisitors.values.reshape(-1,1)\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)\n",
      "115/36:\n",
      "#linear regression with scikit-learn\n",
      "y_sk = pd.DataFrame(logs.PageViews).to_numpy()#.values.to_numpy()\n",
      "x_sk = logs.UniqueVisitors.values.reshape(-1,1)\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)\n",
      "115/37: pd.DataFrame(logs.PageViews)\n",
      "115/38: pd.DataFrame(logs.PageViews).to_numpy()\n",
      "115/39:\n",
      "#linear regression with scikit-learn\n",
      "y_sk = logs.PageViews.values.to_numpy()\n",
      "x_sk = logs.UniqueVisitors.values.reshape(-1,1)\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)\n",
      "115/40:\n",
      "#linear regression with scikit-learn\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs.UniqueVisitors.values.reshape(-1,1)\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)\n",
      "115/41: y_sk\n",
      "115/42: y_sk, x_sk\n",
      "115/43: pd.version()\n",
      "115/44:\n",
      "#linear regression with scikit-learn\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs.UniqueVisitors.values.reshape(-1,1)\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)\n",
      "115/45: model_sk.predict(x_sk)\n",
      "115/46:\n",
      "#linear regression with scikit-learn\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs.UniqueVisitors.values.reshape(-1,1)\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)\n",
      "print('r2 for the model: ', model_sk.score())\n",
      "115/47:\n",
      "#linear regression with scikit-learn\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs.UniqueVisitors.values.reshape(-1,1)\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)\n",
      "print('r2 for the model: ', model_sk.score)\n",
      "115/48:\n",
      "#linear regression with scikit-learn\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs.UniqueVisitors.values.reshape(-1,1)\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "115/49:\n",
      "#linear regression with scikit-learn\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs['UniqueVisitors','InEnglish'].values.reshape(-1,1)\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "115/50:\n",
      "#linear regression with scikit-learn\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs['UniqueVisitors','InEnglish']\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "115/51:\n",
      "#linear regression with scikit-learn\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs[['UniqueVisitors','InEnglish']]\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "115/52:\n",
      "#linear regression with scikit-learn\n",
      "dummies = logs.get_dummies(logs.InEnglish, drop_first=True)\n",
      "logs.join(dummies)\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs[['UniqueVisitors','InEnglish']]\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "115/53:\n",
      "#linear regression with scikit-learn\n",
      "dummies = pd.get_dummies(data=logs.InEnglish, drop_first=True)\n",
      "logs.join(dummies)\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs[['UniqueVisitors','InEnglish']]\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "115/54:\n",
      "#linear regression with scikit-learn\n",
      "dummies = pd.get_dummies(data=logs.InEnglish, drop_first=True)\n",
      "logs.join(dummies)\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = dummies#logs[['UniqueVisitors','InEnglish']]\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm.coef_, lm.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "115/55: dummies\n",
      "115/56:\n",
      "#linear regression with scikit-learn\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs.UniqueVisitors.values.reshape(-1,1)\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "115/57:\n",
      "#linear regression with scikit-learn\n",
      "dummies = pd.get_dummies(data=logs.InEnglish, drop_first=True)\n",
      "logs.join(dummies)\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = dummies#logs[['UniqueVisitors','InEnglish']]\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "115/58: logs\n",
      "115/59: logs.join(dummies)\n",
      "115/60:\n",
      "#linear regression with scikit-learn\n",
      "dummies = pd.get_dummies(data=logs.InEnglish, drop_first=True)\n",
      "logs=logs.join(dummies)\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs[['UniqueVisitors','Yes']]\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "115/61:\n",
      "#linear regression with scikit-learn\n",
      "dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)\n",
      "logs=logs.join(dummies_english)\n",
      "\n",
      "dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)\n",
      "logs=logs.join(dummies_english)\n",
      "\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs[['UniqueVisitors','Yes']]\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "115/62: logs\n",
      "115/63:\n",
      "dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)\n",
      "logs=logs.join(dummies_english)\n",
      "logs\n",
      "115/64:\n",
      "dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)\n",
      "logs=logs.join(dummies_advertising)\n",
      "logs\n",
      "115/65:\n",
      "#linear regression with scikit-learn\n",
      "dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)\n",
      "logs=logs.join(dummies_english)\n",
      "logs.rename(columns={'Yes': 'English_yes'})\n",
      "\n",
      "dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)\n",
      "logs=logs.join(dummies_advertising)\n",
      "\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs[['UniqueVisitors','Yes']]\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "115/66:\n",
      "#linear regression with scikit-learn\n",
      "dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)\n",
      "logs=logs.join(dummies_english)\n",
      "logs.rename(columns={'Yes': 'English_yes'})\n",
      "\n",
      "dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)\n",
      "logs=logs.join(dummies_advertising)\n",
      "logs.rename(columns={'Yes': 'Advertising_yes'})\n",
      "\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs[['UniqueVisitors','Yes']]\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "115/67:\n",
      "#linear regression with scikit-learn\n",
      "dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)\n",
      "logs=logs.join(dummies_english)\n",
      "logs.rename(columns={'Yes': 'English_yes'})\n",
      "115/68: logs.head()\n",
      "115/69: logs.drop_column('Yes')\n",
      "115/70: logs.drop(column=['Yes'])\n",
      "115/71: logs.drop(columns=['Yes'])\n",
      "115/72:\n",
      "#linear regression with scikit-learn\n",
      "dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)\n",
      "logs=logs.join(dummies_english)\n",
      "logs.rename(columns={'Yes': 'English_yes'})\n",
      "115/73: logs.drop(columns=['Yes'], inplace=True)\n",
      "115/74:\n",
      "#linear regression with scikit-learn\n",
      "dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)\n",
      "logs=logs.join(dummies_english)\n",
      "logs.rename(columns={'Yes': 'English_yes'})\n",
      "115/75:\n",
      "dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)\n",
      "logs=logs.join(dummies_advertising)\n",
      "logs.rename(columns={'Yes': 'Advertising_yes'})\n",
      "\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs[['UniqueVisitors','English_yes']]\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "115/76: logs.head()\n",
      "115/77: logs.drop(columns=['Yes'], inplace=True)\n",
      "115/78:\n",
      "#multiple linear regression with scikit-learn\n",
      "dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)\n",
      "logs=logs.join(dummies_english)\n",
      "logs.rename(columns={'Yes': 'English_yes'}, inplae=True)\n",
      "115/79:\n",
      "#multiple linear regression with scikit-learn\n",
      "dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)\n",
      "logs=logs.join(dummies_english)\n",
      "logs.rename(columns={'Yes': 'English_yes'}, inplace=True)\n",
      "115/80: logs.drop(columns=['Yes'], inplace=True)\n",
      "115/81:\n",
      "#multiple linear regression with scikit-learn\n",
      "dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)\n",
      "logs=logs.join(dummies_english)\n",
      "logs.rename(columns={'Yes': 'English_yes'}, inplace=True)\n",
      "115/82:\n",
      "dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)\n",
      "logs=logs.join(dummies_advertising)\n",
      "logs.rename(columns={'Yes': 'Advertising_yes'}, inplace=True)\n",
      "\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs[['UniqueVisitors','English_yes']]\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "115/83:\n",
      "dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)\n",
      "logs=logs.join(dummies_advertising)\n",
      "logs.rename(columns={'Yes': 'Advertising_yes'}, inplace=True)\n",
      "\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs[['UniqueVisitors','English_yes', 'Advertising_yes']]\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "117/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.tools.eval_measures import rmse\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "%matplotlib inline\n",
      "117/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.tools.eval_measures import rmse\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "%matplotlib inline\n",
      "117/3: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\\t' )\n",
      "117/4:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.tools.eval_measures import rmse\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "%matplotlib inline\n",
      "117/5:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.tools.eval_measures import rmse\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "%matplotlib inline\n",
      "117/6:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.tools.eval_measures import rmse\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "%matplotlib inline\n",
      "117/7:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy import log\n",
      "\n",
      "import statsmodels.api as sm\n",
      "from statsmodels.tools.eval_measures import rmse\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "%matplotlib inline\n",
      "117/8: top_sites = pd.read_csv('Regression/top_1000_sites.tsv', delimiter='\\t' )\n",
      "117/9: top_sites.info()\n",
      "117/10:\n",
      "x_min = log(top_sites.UniqueVisitors).min()\n",
      "x_max = log(top_sites.UniqueVisitors).max()\n",
      "minmax = [x_min, x_max]\n",
      "117/11:\n",
      "#regression from numpy polyfit function\n",
      "plt.scatter(log(top_sites.UniqueVisitors), log(top_sites.PageViews))\n",
      "m,c = np.polyfit(log(top_sites.UniqueVisitors),log(top_sites.PageViews), deg=1)\n",
      "plt.plot(log(top_sites.UniqueVisitors), m*log(top_sites.UniqueVisitors)+c, color='red')\n",
      "plt.gcf().set_size_inches(12,6)\n",
      "117/12:\n",
      "#regression with statsmodels\n",
      "x = log(top_sites.UniqueVisitors)\n",
      "y = log(top_sites.PageViews)\n",
      "\n",
      "x = sm.add_constant(x)\n",
      "\n",
      "model = sm.OLS(y,x).fit()\n",
      "\n",
      "y_predictions = model.predict(x)\n",
      "rmse = rmse(y, y_predictions)\n",
      "print('rmse: ', rmse)\n",
      "model.summary()\n",
      "117/13:\n",
      "minmax_2 = sm.add_constant(minmax)\n",
      "predictions = model.predict(minmax_2)\n",
      "predictions\n",
      "117/14: logs = log(top_sites[['UniqueVisitors', 'PageViews']]).join(top_sites.InEnglish).join(top_sites.HasAdvertising)\n",
      "117/15: z = sns.lmplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)\n",
      "117/16:\n",
      "#multiple regression model with statsmodels -formula interface\n",
      "x = logs[['UniqueVisitors', 'InEnglish']]\n",
      "y = logs['PageViews']\n",
      "\n",
      "model = smf.ols(formula='PageViews ~ UniqueVisitors + InEnglish + HasAdvertising', data=logs).fit()\n",
      "\n",
      "model.summary()\n",
      "117/17: logs.head()\n",
      "117/18:\n",
      "#linear regression with scikit-learn\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs.UniqueVisitors.values.reshape(-1,1)\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "117/19:\n",
      "#multiple linear regression with scikit-learn\n",
      "dummies_english = pd.get_dummies(data=logs.InEnglish, drop_first=True)\n",
      "logs=logs.join(dummies_english)\n",
      "logs.rename(columns={'Yes': 'English_yes'}, inplace=True)\n",
      "117/20:\n",
      "dummies_advertising = pd.get_dummies(data=logs.HasAdvertising, drop_first=True)\n",
      "logs=logs.join(dummies_advertising)\n",
      "logs.rename(columns={'Yes': 'Advertising_yes'}, inplace=True)\n",
      "\n",
      "y_sk = logs.PageViews.values.reshape(-1,1)\n",
      "x_sk = logs[['UniqueVisitors','English_yes', 'Advertising_yes']]\n",
      "lm_sk = linear_model.LinearRegression()\n",
      "model_sk = lm_sk.fit(x_sk,y_sk)\n",
      "print('Model coefficient and intereception: ',lm_sk.coef_, lm_sk.intercept_)\n",
      "print('r2 for the model: ', model_sk.score(x_sk, y_sk))\n",
      "117/21: logs.head()\n",
      "117/22: z = sns.lmplot(x='Log UniqueVisitors', y='Log PageViews',hue='InEnglish', data=logs)\n",
      "117/23: z = sns.lmplot(x='UniqueVisitors', y='PageViews',hue='InEnglish', data=logs)\n",
      "117/24:\n",
      "model = smf.ols(formula='PageViews ~ UniqueVisitors', data=logs).fit()\n",
      "\n",
      "model.summary()\n",
      "117/25:\n",
      "model = smf.ols(formula='PageViews ~InEnglish', data=logs).fit()\n",
      "\n",
      "model.summary()\n",
      "117/26:\n",
      "#Linear regression -Unique visitors correlation\n",
      "model = smf.ols(formula='PageViews ~ UniqueVisitors', data=logs).fit()\n",
      "model.summary()\n",
      "117/27:\n",
      "#Linear regression -In English correlation\n",
      "model = smf.ols(formula='PageViews ~InEnglish', data=logs).fit()\n",
      "model.summary()\n",
      "117/28:\n",
      "#Linear regression -Has advertising correlation\n",
      "model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()\n",
      "model.summary()\n",
      "117/29: np.correlate(logs.UniqueVisitors, logs.PageViews)\n",
      "117/30: np.corrcoef(logs.UniqueVisitors, logs.PageViews)\n",
      "117/31: 0.6794**2\n",
      "117/32:\n",
      "#Współczynnik korelacji\n",
      "np.corrcoef(logs.UniqueVisitors, logs.PageViews)\n",
      "117/33:\n",
      "#Linear regression -Has advertising correlation\n",
      "model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()\n",
      "model.summary()\n",
      "model.raquared\n",
      "117/34:\n",
      "#Linear regression -Has advertising correlation\n",
      "model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()\n",
      "model.summary()\n",
      "model.raquared()\n",
      "117/35:\n",
      "#Linear regression -Has advertising correlation\n",
      "model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()\n",
      "model.summary()\n",
      "model.rsquared()\n",
      "117/36:\n",
      "#Linear regression -Has advertising correlation\n",
      "model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()\n",
      "model.summary()\n",
      "model.rsquared\n",
      "117/37:\n",
      "#Linear regression -Has advertising correlation\n",
      "model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()\n",
      "print(model.summary())\n",
      "model.rsquared\n",
      "117/38:\n",
      "#Linear regression -Has advertising correlation\n",
      "model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()\n",
      "print(model.summary())\n",
      "#r2 parameter  = model.rsquared\n",
      "117/39:\n",
      "#Linear regression -Has advertising correlation\n",
      "model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()\n",
      "print(model.summary())\n",
      "#r2 parameter  = model.rsquared\n",
      "model.resid_pearson\n",
      "117/40:\n",
      "#Linear regression -Has advertising correlation\n",
      "model = smf.ols(formula='PageViews ~HasAdvertising', data=logs).fit()\n",
      "print(model.summary())\n",
      "#r2 parameter  = model.rsquared\n",
      "118/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "118/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "118/3: normal_distribution = np.random.normal()\n",
      "118/4: plt.plot(normal_distribution)\n",
      "118/5: plt.scatter(normal_distribution)\n",
      "118/6: normal_distribution\n",
      "118/7: normal_distribution = np.random.normal(0, 0.1, 100)\n",
      "118/8: plt.scatter(normal_distribution)\n",
      "118/9: normal_distribution\n",
      "118/10: x = pd.series[list(range(100))]\n",
      "118/11: x = pd.Series[list(range(100))]\n",
      "118/12: x = pd.Series(list(range(100)))\n",
      "118/13: x\n",
      "118/14: normal_distribution = np.random.normal(0, 0.1, len(x))\n",
      "118/15: plt.scatter(normal_distribution)\n",
      "118/16: normal_distribution\n",
      "118/17: plt.scatter(x,normal_distribution)\n",
      "118/18: normal_distribution = np.random.normal(0, 1, len(x))\n",
      "118/19: plt.scatter(x,normal_distribution)\n",
      "118/20: y = 1 - x**2 + normal_distribution\n",
      "118/21: plt.scatter(x,y)\n",
      "118/22: x = pd.Series(list(range(-10, 10)))\n",
      "118/23: normal_distribution = np.random.normal(0, 1, len(x))\n",
      "118/24: y = 1 - x**2 + normal_distribution\n",
      "118/25: plt.scatter(x,y)\n",
      "118/26: x = pd.Series(list(range(-10, 10, 0.1)))\n",
      "118/27: x = pd.Series(np.arange(-10, 10, 0.1))\n",
      "118/28: normal_distribution = np.random.normal(0, 1, len(x))\n",
      "118/29: y = 1 - x**2 + normal_distribution\n",
      "118/30: plt.scatter(x,y)\n",
      "118/31: x = pd.Series(np.arange(-10, 10, 0.01))\n",
      "118/32: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "118/33: y = 1 - x**2 + normal_distribution\n",
      "118/34: plt.scatter(x,y)\n",
      "118/35: plt.rcParams[\"figure.figsize\"] = (8,4)\n",
      "118/36: plt.scatter(x,y)\n",
      "118/37: plt.rcParams[\"figure.figsize\"] = (12,6)\n",
      "118/38: plt.scatter(x,y)\n",
      "118/39:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "118/40:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid = True\n",
      "118/41:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "118/42:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid\n",
      "118/43:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid(True)\n",
      "118/44:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid(b=True)\n",
      "118/45:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "118/46:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "118/47: plt.rcParams[\"figure.figsize\"] = (12,6)\n",
      "118/48: x = pd.Series(np.arange(-10, 10, 0.01))\n",
      "118/49: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "118/50: y = 1 - x**2 + normal_distribution\n",
      "118/51:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "119/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "119/2: plt.rcParams[\"figure.figsize\"] = (12,6)\n",
      "119/3: x = pd.Series(np.arange(-10, 10, 0.01))\n",
      "119/4: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "119/5: y = 1 - x**2 + normal_distribution\n",
      "119/6:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "119/7: normal_distribution\n",
      "119/8: x\n",
      "119/9:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=2)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "119/10:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "119/11: x_squared = x**2\n",
      "119/12: plt.scatter(x_squared, y)\n",
      "120/1:\n",
      "m,c = np.polyfit(x_squared, y)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y_model)\n",
      "r**2\n",
      "120/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "120/3: plt.rcParams[\"figure.figsize\"] = (12,6)\n",
      "120/4: x = pd.Series(np.arange(-10, 10, 0.01))\n",
      "120/5: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "120/6: y = 1 - x**2 + normal_distribution\n",
      "120/7:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "120/8: x_squared = x**2\n",
      "120/9: plt.scatter(x_squared, y)\n",
      "120/10:\n",
      "m,c = np.polyfit(x_squared, y)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y_model)\n",
      "r**2\n",
      "120/11:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y_model)\n",
      "r**2\n",
      "120/12:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y_model)\n",
      "print(r)\n",
      "120/13: y_model - x_squared\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/14: y_model - y\n",
      "120/15:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "120/16: (y_model - y).mean()\n",
      "120/17: (y_model - y).max()\n",
      "120/18: df = pd.DataFrame(x_squared, y)\n",
      "120/19: df = pd.DataFrame(x_squared, y, columns=['x_squared', 'y'])\n",
      "120/20: df = pd.DataFrame([x_squared, y], columns=['x_squared', 'y'])\n",
      "120/21: df\n",
      "120/22: df = pd.DataFrame(x_squared, columns=['x_squared'])\n",
      "120/23: df\n",
      "120/24: df = pd.DataFrame(x_squared).join(y, inplace=True)\n",
      "120/25: df = pd.DataFrame(x_squared).join(y)\n",
      "120/26: y\n",
      "120/27: df = pd.DataFrame({'x_squared': x_squared, 'y': y})\n",
      "120/28: df\n",
      "120/29:\n",
      "model = smf.ols(formula='y ~ x_squared', model=df)\n",
      "model.summary()\n",
      "120/30:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df)\n",
      "model.summary()\n",
      "120/31:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "120/32: x_squared.corr(y)\n",
      "120/33: x_squared.corr(y)**2\n",
      "120/34: x_squared.corr(y)\n",
      "120/35:\n",
      "#r2\n",
      "x_squared.corr(y)**2\n",
      "120/36:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "print(r)\n",
      "120/37:\n",
      "#r2\n",
      "x_squared.corr(y)\n",
      "120/38:\n",
      "#r2\n",
      "x_squared.corr(y)**2\n",
      "120/39:\n",
      "#r2\n",
      "x_squared.corr(y_model)**2\n",
      "120/40:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "#print(r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "120/41:\n",
      "model = smf.ols(formula='y_model ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "120/42:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "120/43: my_r2 = (y - y.mean()).sum()\n",
      "120/44: my_r2\n",
      "120/45: my_r2 = (y - y_model).sum() / (y - y.mean()).sum()\n",
      "120/46: my_r2\n",
      "120/47: my_r2 = (y_model - y.mean()).sum() / (y - y.mean()).sum()\n",
      "120/48: my_r2\n",
      "120/49: y_model - y.mean()\n",
      "120/50: (y_model - y.mean()).sum()\n",
      "120/51:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "120/52: plt.rcParams[\"figure.figsize\"] = (12,6)\n",
      "120/53: x = pd.Series(np.arange(-10, 10, 0.1))\n",
      "120/54: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "120/55: y = 1 - x**2 + normal_distribution\n",
      "120/56:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "120/57: x_squared = x**2\n",
      "120/58: plt.scatter(x_squared, y)\n",
      "120/59:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "#print(r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "120/60:\n",
      "#r2\n",
      "x_squared.corr(y_model)**2\n",
      "120/61: df = pd.DataFrame({'x_squared': x_squared, 'y': y})\n",
      "120/62: df\n",
      "120/63:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "120/64: my_r2 = (y_model - y.mean()).sum() / (y - y.mean()).sum()\n",
      "120/65: my_r2\n",
      "120/66: (y_model - y.mean()).sum()\n",
      "120/67: (y-y.mean()).sum()\n",
      "120/68: (y_model - y.mean())**2.sum()\n",
      "120/69: ((y_model - y.mean())**2).sum()\n",
      "120/70: my_r2 = ((y_model - y.mean()))**2.sum() / ((y - y.mean()))**2.sum()\n",
      "120/71: my_r2 = ((y_model - y.mean()))**2.sum() / ((y - y.mean()))**2.sum()\n",
      "120/72: my_r2 = ((y_model - y.mean())**2.sum()) / ((y - y.mean())**2).sum()\n",
      "120/73: my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())\n",
      "120/74: my_r2\n",
      "120/75: my_r2**(0.5)\n",
      "120/76: my_r2\n",
      "120/77:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "print(r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "120/78:\n",
      "r2\n",
      "#x_squared.corr(y_model)**2\n",
      "120/79:\n",
      "r**2\n",
      "#x_squared.corr(y_model)**2\n",
      "120/80:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "120/81: plt.rcParams[\"figure.figsize\"] = (12,6)\n",
      "120/82: x = pd.Series(np.arange(-10, 10, 0.01))\n",
      "120/83: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "120/84: y = 1 - x**2 + normal_distribution\n",
      "120/85:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "120/86: x_squared = x**2\n",
      "120/87: plt.scatter(x_squared, y)\n",
      "120/88:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "print(r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "120/89: r**2\n",
      "120/90: df = pd.DataFrame({'x_squared': x_squared, 'y': y})\n",
      "120/91: df\n",
      "120/92:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "120/93: my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())\n",
      "120/94: my_r2\n",
      "120/95:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef([x_squared, y])\n",
      "print(r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "120/96:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef([x_squared, y_model])\n",
      "print(r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "120/97: x = pd.Series(np.arange(-10, 10, 0.1))\n",
      "120/98: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "120/99: y = 1 - x**2 + normal_distribution\n",
      "120/100:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "120/101: x_squared = x**2\n",
      "120/102: plt.scatter(x_squared, y)\n",
      "120/103:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef([x_squared, y_model])\n",
      "print(r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "120/104: r**2\n",
      "120/105: r**2\n",
      "120/106: df = pd.DataFrame({'x_squared': x_squared, 'y': y})\n",
      "120/107: df\n",
      "120/108:\n",
      "model = smf.ols(formula='y_model ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "120/109:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "120/110:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef([x_squared, y])\n",
      "print(r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "120/111: r**2\n",
      "120/112: df = pd.DataFrame({'x_squared': x_squared, 'y': y})\n",
      "120/113: df\n",
      "120/114:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "120/115: my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())\n",
      "120/116: my_r2\n",
      "120/117: df.head()\n",
      "120/118:\n",
      "plt.scatter(x_squared, y)\n",
      "plt.plot(x_squared, y_model)\n",
      "120/119:\n",
      "plt.scatter(x_squared, y)\n",
      "plt.plot(x_squared, y_model, color = 'red')\n",
      "122/1: df.head()\n",
      "122/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "122/3: plt.rcParams[\"figure.figsize\"] = (12,6)\n",
      "122/4: x = pd.Series(np.arange(-10, 10, 0.01))\n",
      "122/5: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "122/6: y = 1 - x**2 + normal_distribution\n",
      "122/7:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "122/8: x_squared = x**2\n",
      "122/9: plt.scatter(x_squared, y)\n",
      "122/10:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "print(r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "122/11: r**2\n",
      "122/12: df = pd.DataFrame({'x_squared': x_squared, 'y': y})\n",
      "122/13: df.head()\n",
      "122/14:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "122/15: my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())\n",
      "122/16: my_r2\n",
      "122/17:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "print('r matrix: ', r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "122/18: r**2\n",
      "122/19: x = pd.Series(np.arange(0,1,0.01))\n",
      "122/20:\n",
      "my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())\n",
      "my_r2\n",
      "122/21: np.pi\n",
      "122/22: np.sin(pi)\n",
      "122/23: np.sin(np.pi)\n",
      "122/24: pd.pi\n",
      "122/25: pd.sin(np.pi)\n",
      "122/26: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))\n",
      "122/27: plt.plot(x,y)\n",
      "122/28: plt.scatter(x,y)\n",
      "122/29: m,c = np.polyfit(x,y, deg=1)\n",
      "122/30: plt.plot(x*m+c)\n",
      "122/31:\n",
      "cf = plt.gcf()\n",
      "cf.plot(x*m+c)\n",
      "122/32:\n",
      "plt.gcf()\n",
      "plt.plot(x*m+c)\n",
      "122/33: fig, ax = plt.scatter(x,y)\n",
      "122/34:\n",
      "ax=plt.gca()\n",
      "ax.plot(x*m+c)\n",
      "122/35:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "%matplotlib inline\n",
      "122/36:\n",
      "ax=plt.gca()\n",
      "ax.plot(x*m+c)\n",
      "122/37: plt.scatter(x,y)\n",
      "122/38:\n",
      "ax=plt.gca()\n",
      "ax.plot(x*m+c)\n",
      "122/39:\n",
      "fig, ax = plt.subplot()\n",
      "ax.scatter(x,y)\n",
      "122/40:\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(x,y)\n",
      "122/41:\n",
      "#ax=plt.gca()\n",
      "ax.plot(x*m+c)\n",
      "122/42:\n",
      "#ax=plt.gca()\n",
      "ax.plot(x*m+c)\n",
      "plt.show()\n",
      "122/43:\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(x,y)\n",
      "122/44: m,c = np.polyfit(x,y, deg=1)\n",
      "122/45:\n",
      "#ax=plt.gca()\n",
      "ax.plot(x*m+c)\n",
      "plt.show()\n",
      "122/46:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "#%matplotlib inline\n",
      "122/47:\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(x,y)\n",
      "122/48:\n",
      "#ax=plt.gca()\n",
      "ax.plot(x*m+c)\n",
      "plt.show()\n",
      "122/49:\n",
      "#ax=plt.gca()\n",
      "ax.plot(x*m+c)\n",
      "fig.show()\n",
      "122/50:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "%matplotlib inline\n",
      "122/51:\n",
      "#ax=plt.gca()\n",
      "ax.plot(x*m+c)\n",
      "fig.show()\n",
      "122/52: ax.plot(x*m+c)\n",
      "122/53:\n",
      "%matplotlib inline\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(x,y)\n",
      "122/54: ax.plot(x*m+c)\n",
      "122/55:\n",
      "ax.plot(x*m+c)\n",
      "fig\n",
      "122/56: x*m+c\n",
      "122/57:\n",
      "ax.plot(x,x*m+c)\n",
      "fig\n",
      "122/58:\n",
      "ax.plot(x,x*m+c)\n",
      "fig\n",
      "122/59:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "\n",
      "%matplotlib inline\n",
      "122/60: plt.rcParams[\"figure.figsize\"] = (12,6)\n",
      "122/61: x = pd.Series(np.arange(-10, 10, 0.01))\n",
      "122/62: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "122/63: y = 1 - x**2 + normal_distribution\n",
      "122/64:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "122/65: x_squared = x**2\n",
      "122/66: plt.scatter(x_squared, y)\n",
      "122/67:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "print('r matrix: ', r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "122/68: r**2\n",
      "122/69: df = pd.DataFrame({'x_squared': x_squared, 'y': y})\n",
      "122/70: df.head()\n",
      "122/71:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "122/72:\n",
      "my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())\n",
      "my_r2\n",
      "122/73: x = pd.Series(np.arange(0,1,0.01))\n",
      "122/74: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))\n",
      "122/75:\n",
      "%matplotlib inline\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(x,y)\n",
      "122/76: m,c = np.polyfit(x,y, deg=1)\n",
      "122/77:\n",
      "ax.plot(x,x*m+c)\n",
      "fig\n",
      "122/78: x*m+c\n",
      "122/79:\n",
      "ax.plot(x,x*m+c, color=red)\n",
      "fig\n",
      "122/80:\n",
      "ax.plot(x,x*m+c, color='red')\n",
      "fig\n",
      "122/81:\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(x,y)\n",
      "122/82:\n",
      "ax.plot(x,x*m+c, color='red')\n",
      "fig\n",
      "122/83:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "\n",
      "%matplotlib inline\n",
      "122/84: sk_model = linear_model()\n",
      "122/85: sk_model = linear_model.linear_regression()\n",
      "122/86: sk_model = linear_model.LinearRegression()\n",
      "122/87: sk_model(x,y).fit()\n",
      "122/88: sk_model.fit(x,y)\n",
      "122/89:\n",
      "y_sk = y.reshape(-1,1)\n",
      "sk_model.fit(x,y)\n",
      "122/90:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "sk_model.fit(x,y)\n",
      "122/91:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "x.sk = x.values.reshape(-1,1)\n",
      "sk_model.fit(x,y)\n",
      "122/92: x_sk\n",
      "122/93:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "x_sk = x\n",
      "sk_model.fit(x_sk,y_sk)\n",
      "122/94: x_sk\n",
      "122/95: y_sk\n",
      "122/96:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "x_sk = x_sk.values.reshape(-1,1)\n",
      "sk_model.fit(x_sk,y_sk)\n",
      "122/97:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "x_sk = x_sk.values.reshape(-1,1)\n",
      "sk_result = sk_model.fit(x_sk,y_sk)\n",
      "122/98:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "sk_result = sk_model.fit(x_sk,y_sk)\n",
      "122/99: x_sk\n",
      "122/100:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "sk_result = sk_model.fit(x_sk,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(x_sk,y_sk))\n",
      "122/101:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "sk_result = sk_model.fit(x_sk,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(x_sk,y_sk))\n",
      "print('coeffs for sk_model: ', sk_result.coef_)\n",
      "122/102:\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "print(m,c)\n",
      "122/103:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "sk_result = sk_model.fit(x_sk,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(x_sk,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/104:\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "print('Coeff and intercept for np model: 'm,c)\n",
      "122/105:\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "print('Coeff and intercept for np model: ',m,c)\n",
      "122/106:\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(x,y)\n",
      "fig.grid()\n",
      "122/107:\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(x,y)\n",
      "ax.grid()\n",
      "122/108:\n",
      "ax.plot(x,x*m+c, color='red')\n",
      "fig\n",
      "122/109: x_sk2 = x_sk**2\n",
      "122/110: x_sk2\n",
      "122/111:\n",
      "X = [x_sk, x_sk2]\n",
      "X\n",
      "122/112:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(x_sk,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/113: X = pd.DataFrame({'x1':x_sk, 'x2': x_sk2})\n",
      "122/114: x_sk\n",
      "122/115: help(x_sk)\n",
      "122/116: X = np.concatenate(x_sk, x_sk2)\n",
      "122/117: X = np.concatenate(x_sk, x_sk2, axis=1)\n",
      "122/118: X = np.concatenate((x_sk, x_sk2), axis=1)\n",
      "122/119: X\n",
      "122/120:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(x_sk,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/121:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/122:\n",
      "x_sk2 = x_sk**2\n",
      "x_sk3 = x_sk**3\n",
      "122/123: X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)\n",
      "122/124: X\n",
      "122/125:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/126: X.head()\n",
      "122/127: np.concatenate(X, x_sk)\n",
      "122/128: np.concatenate(x_sk, x_sk)\n",
      "122/129: np.concatenate(x_sk, x_sk2)\n",
      "122/130:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "x_sk2 = x_sk**2\n",
      "x_sk3 = x_sk**3\n",
      "X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)\n",
      "\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/131: np.concatenate(x_sk, x_sk2)\n",
      "122/132: np.concatenate((X, x_sk2), axis=1)\n",
      "122/133:\n",
      "X = x_sk\n",
      "for i in range(2,15):\n",
      "    xi = x_sk**i\n",
      "    X = np.concatenate((X, xi), axis=1)\n",
      "122/134: X\n",
      "122/135:\n",
      "X = x_sk\n",
      "for i in range(2,15):\n",
      "    xi = x_sk**i\n",
      "    X = np.concatenate((X, xi), axis=1)\n",
      "    \n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/136:\n",
      "ax.plot(x, sk_result.predict(x))\n",
      "fig\n",
      "122/137:\n",
      "ax.plot(x_sk, sk_result.predict(x_sk))\n",
      "fig\n",
      "122/138: sk_result.predict(x_sk)\n",
      "122/139: x_sk\n",
      "122/140: sk_result.predict([0])\n",
      "122/141:\n",
      "ax.plot(x_sk, sk_result.predict(X))\n",
      "fig\n",
      "122/142:\n",
      "ax.plot(x_sk, sk_result.predict(X), color='green')\n",
      "fig\n",
      "122/143:\n",
      "ax.plot(x_sk, sk_result.predict(X), color='green')\n",
      "fig\n",
      "122/144:\n",
      "ax.plot(x_sk, sk_result.predict(X), color='red')\n",
      "fig\n",
      "122/145:\n",
      "ax.plot(x_sk, sk_result.predict(X), color='red', label='pow 14')\n",
      "fig\n",
      "122/146: help(fig)\n",
      "122/147:\n",
      "ax.plot(x_sk, sk_result.predict(X), color='red', label='pow 14')\n",
      "ax.legend()\n",
      "fig.leg\n",
      "122/148:\n",
      "ax.plot(x_sk, sk_result.predict(X), color='red', label='pow 14')\n",
      "ax.legend()\n",
      "fig\n",
      "122/149:\n",
      "ax.plot(x,x*m+c, color='red', legend='linear')\n",
      "ax.legend()\n",
      "fig\n",
      "122/150:\n",
      "ax.plot(x,x*m+c, color='red', label='linear')\n",
      "ax.legend()\n",
      "fig\n",
      "122/151:\n",
      "ax.plot(x_sk, sk_result.predict(X), color='red', label='pow 14')\n",
      "ax.legend()\n",
      "fig\n",
      "122/152:\n",
      "ax.plot(x,x*m+c, color='red', label='linear')\n",
      "ax.legend()\n",
      "fig\n",
      "122/153:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "\n",
      "%matplotlib inline\n",
      "122/154: x = pd.Series(np.arange(-10, 10, 0.01))\n",
      "122/155: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "122/156: y = 1 - x**2 + normal_distribution\n",
      "122/157:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "122/158: x_squared = x**2\n",
      "122/159: plt.scatter(x_squared, y)\n",
      "122/160:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "print('r matrix: ', r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "122/161: r**2\n",
      "122/162: df = pd.DataFrame({'x_squared': x_squared, 'y': y})\n",
      "122/163: df.head()\n",
      "122/164:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "122/165:\n",
      "my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())\n",
      "my_r2\n",
      "122/166: x = pd.Series(np.arange(0,1,0.01))\n",
      "122/167: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))\n",
      "122/168:\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(x,y)\n",
      "ax.grid()\n",
      "122/169:\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "print('Coeff and intercept for np model: ',m,c)\n",
      "122/170:\n",
      "ax.plot(x,x*m+c, color='red', label='linear')\n",
      "ax.legend()\n",
      "fig\n",
      "122/171: sk_model = linear_model.LinearRegression()\n",
      "122/172:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "x_sk2 = x_sk**2\n",
      "x_sk3 = x_sk**3\n",
      "X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)\n",
      "\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/173:\n",
      "X = x_sk\n",
      "for i in range(2,15):\n",
      "    xi = x_sk**i\n",
      "    X = np.concatenate((X, xi), axis=1)\n",
      "    \n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/174:\n",
      "ax.plot(x_sk, sk_result.predict(X), color='red',)\n",
      "ax.legend()\n",
      "fig\n",
      "122/175: help(fig)\n",
      "122/176: label\n",
      "122/177:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.scatter(x,y)\n",
      "ax.grid()\n",
      "122/178:\n",
      "ax.plot(x,x*m+c, color='red', label='linear')\n",
      "ax.legend()\n",
      "fig\n",
      "122/179:\n",
      "ax.plot(x_sk, sk_result.predict(X), color='green', label=label.append)\n",
      "ax.legend()\n",
      "fig\n",
      "122/180:\n",
      "ax.plot(x_sk, sk_result.predict(X), color='green')\n",
      "ax.legend()\n",
      "fig\n",
      "122/181:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "line, = ax.scatter(x,y)\n",
      "line.set_label('signal')\n",
      "ax.grid()\n",
      "122/182:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "line = ax.scatter(x,y)\n",
      "line.set_label('signal')\n",
      "ax.grid()\n",
      "122/183:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "line = ax.scatter(x,y)\n",
      "line.set_label('signal')\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "122/184:\n",
      "ax.plot(x,x*m+c, color='red', label='linear')\n",
      "ax.legend()\n",
      "fig\n",
      "122/185:\n",
      "ax.plot(x,x*m+c, color='red', label='linear regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/186:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "line = ax.scatter(x,y)\n",
      "line.set_label('signal')\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "122/187:\n",
      "line = ax.plot(x,x*m+c, color='red')\n",
      "line.set_label('linear regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/188:\n",
      "line, = ax.plot(x,x*m+c, color='red')\n",
      "line.set_label('linear regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/189:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='green')\n",
      "line.set_label('polynomial regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/190:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xlabel('x')\n",
      "line = ax.scatter(x,y)\n",
      "line.set_label('signal')\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "122/191:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xlabel('x')\n",
      "ax.set_ylabel('y')\n",
      "line = ax.scatter(x,y)\n",
      "line.set_label('signal')\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "122/192:\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "print('Coeff and intercept for np model: ',m,c)\n",
      "122/193:\n",
      "line, = ax.plot(x,x*m+c, color='red')\n",
      "line.set_label('linear regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/194: sk_model = linear_model.LinearRegression()\n",
      "122/195:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "x_sk2 = x_sk**2\n",
      "x_sk3 = x_sk**3\n",
      "X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)\n",
      "\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/196:\n",
      "X = x_sk\n",
      "for i in range(2,15):\n",
      "    xi = x_sk**i\n",
      "    X = np.concatenate((X, xi), axis=1)\n",
      "    \n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/197:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='green')\n",
      "line.set_label('polynomial regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/198: X_ort = PolynomialFeature(2).fit(x_sk)\n",
      "122/199:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "from sklearn.preprocessing import PolynomialFeature\n",
      "\n",
      "%matplotlib inline\n",
      "122/200:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "%matplotlib inline\n",
      "122/201: X_ort = PolynomialFeature(2).fit(x_sk)\n",
      "122/202: X_ort = PolynomialFeatures(2).fit(x_sk)\n",
      "122/203: X_ort\n",
      "122/204: X_ort = PolynomialFeatures(2).fit_transform(x_sk)\n",
      "122/205: X_ort\n",
      "122/206:\n",
      "sk_ort_result = sk_model.fit(X_ort,y_sk)\n",
      "print('r2 for sk_model: ', sk_ort_result.score(X_ort,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_ort_result.coef_, sk_ort_result.intercept_)\n",
      "122/207:\n",
      "line, = ax.plot(x_sk, sk_ort_result.predict(X_ort), color='orange')\n",
      "line.set_label('ort polynomial regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/208: X_ort = PolynomialFeatures(3).fit_transform(x_sk)\n",
      "122/209:\n",
      "sk_ort_result = sk_model.fit(X_ort,y_sk)\n",
      "print('r2 for sk_model: ', sk_ort_result.score(X_ort,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_ort_result.coef_, sk_ort_result.intercept_)\n",
      "122/210:\n",
      "line, = ax.plot(x_sk, sk_ort_result.predict(X_ort), color='orange')\n",
      "line.set_label('ort polynomial regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/211: ax.lines\n",
      "122/212: ax.lines[2].pop()\n",
      "122/213: ax.lines.pop(2)\n",
      "122/214: fig\n",
      "122/215: ax.labels()\n",
      "122/216: ax.label\n",
      "122/217: ax.lagend\n",
      "122/218: ax.legend()\n",
      "122/219: ax.legend.pop(2)\n",
      "122/220: help(ax.legend)\n",
      "122/221: help(ax.legends)\n",
      "122/222: help(ax.legend)\n",
      "122/223: ax.legend.labels()\n",
      "122/224: ax.legend.labels\n",
      "122/225: ax.legend().labels\n",
      "122/226: h,l - ax.get_legend_handels_labels()\n",
      "122/227: h,l = ax.get_legend_handels_labels()\n",
      "122/228: h,l = ax.get_legend_handles_labels()\n",
      "122/229: h\n",
      "122/230: l\n",
      "122/231: fig\n",
      "122/232: fig\n",
      "122/233:\n",
      "line, = ax.plot(x_sk, sk_ort_result.predict(X_ort), color='orange')\n",
      "line.set_label('ort polynomial regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/234: fig\n",
      "122/235:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "%matplotlib inline\n",
      "122/236: x = pd.Series(np.arange(-10, 10, 0.01))\n",
      "122/237: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "122/238: y = 1 - x**2 + normal_distribution\n",
      "122/239:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "122/240: x_squared = x**2\n",
      "122/241: plt.scatter(x_squared, y)\n",
      "122/242:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "print('r matrix: ', r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "122/243: r**2\n",
      "122/244: df = pd.DataFrame({'x_squared': x_squared, 'y': y})\n",
      "122/245: df.head()\n",
      "122/246:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "122/247:\n",
      "my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())\n",
      "my_r2\n",
      "122/248: x = pd.Series(np.arange(0,1,0.01))\n",
      "122/249: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))\n",
      "122/250:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xlabel('x')\n",
      "ax.set_ylabel('y')\n",
      "line = ax.scatter(x,y)\n",
      "line.set_label('signal')\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "122/251:\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "print('Coeff and intercept for np model: ',m,c)\n",
      "122/252:\n",
      "line, = ax.plot(x,x*m+c, color='red')\n",
      "line.set_label('linear regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/253: sk_model = linear_model.LinearRegression()\n",
      "122/254:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "x_sk2 = x_sk**2\n",
      "x_sk3 = x_sk**3\n",
      "X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)\n",
      "\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/255:\n",
      "X = x_sk\n",
      "for i in range(2,15):\n",
      "    xi = x_sk**i\n",
      "    X = np.concatenate((X, xi), axis=1)\n",
      "    \n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/256:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='green')\n",
      "line.set_label('polynomial regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/257: X_ort = PolynomialFeatures(3).fit_transform(x_sk)\n",
      "122/258:\n",
      "sk_ort_result = sk_model.fit(X_ort,y_sk)\n",
      "print('r2 for sk_model: ', sk_ort_result.score(X_ort,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_ort_result.coef_, sk_ort_result.intercept_)\n",
      "122/259:\n",
      "line, = ax.plot(x_sk, sk_ort_result.predict(X_ort), color='orange')\n",
      "line.set_label('ort polynomial regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/260: ax.lines.pop(2)\n",
      "122/261:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "%matplotlib inline\n",
      "122/262: x = pd.Series(np.arange(-10, 10, 0.01))\n",
      "122/263: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "122/264: y = 1 - x**2 + normal_distribution\n",
      "122/265:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "122/266: x_squared = x**2\n",
      "122/267: plt.scatter(x_squared, y)\n",
      "122/268:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "print('r matrix: ', r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "122/269: r**2\n",
      "122/270: df = pd.DataFrame({'x_squared': x_squared, 'y': y})\n",
      "122/271: df.head()\n",
      "122/272:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "122/273:\n",
      "my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())\n",
      "my_r2\n",
      "122/274: x = pd.Series(np.arange(0,1,0.01))\n",
      "122/275: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))\n",
      "122/276:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xlabel('x')\n",
      "ax.set_ylabel('y')\n",
      "line = ax.scatter(x,y)\n",
      "line.set_label('signal')\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "122/277:\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "print('Coeff and intercept for np model: ',m,c)\n",
      "122/278:\n",
      "line, = ax.plot(x,x*m+c, color='red')\n",
      "line.set_label('linear regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/279: sk_model = linear_model.LinearRegression()\n",
      "122/280:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "x_sk2 = x_sk**2\n",
      "x_sk3 = x_sk**3\n",
      "X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)\n",
      "\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/281:\n",
      "X = x_sk\n",
      "for i in range(2,15):\n",
      "    xi = x_sk**i\n",
      "    X = np.concatenate((X, xi), axis=1)\n",
      "    \n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/282:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='green')\n",
      "line.set_label('polynomial regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/283: X_ort = PolynomialFeatures(3).fit_transform(x_sk)\n",
      "122/284:\n",
      "sk_ort_result = sk_model.fit(X_ort,y_sk)\n",
      "print('r2 for sk_model: ', sk_ort_result.score(X_ort,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_ort_result.coef_, sk_ort_result.intercept_)\n",
      "122/285:\n",
      "line, = ax.plot(x_sk, sk_ort_result.predict(X_ort), color='orange')\n",
      "line.set_label('ort polynomial regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/286:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='black')\n",
      "line.set_label('polynomial regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/287:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "%matplotlib inline\n",
      "122/288: x = pd.Series(np.arange(-10, 10, 0.01))\n",
      "122/289: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "122/290: y = 1 - x**2 + normal_distribution\n",
      "122/291:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "122/292: x_squared = x**2\n",
      "122/293: plt.scatter(x_squared, y)\n",
      "122/294:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "print('r matrix: ', r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "122/295: r**2\n",
      "122/296: df = pd.DataFrame({'x_squared': x_squared, 'y': y})\n",
      "122/297: df.head()\n",
      "122/298:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "122/299:\n",
      "my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())\n",
      "my_r2\n",
      "122/300: x = pd.Series(np.arange(0,1,0.01))\n",
      "122/301: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))\n",
      "122/302:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xlabel('x')\n",
      "ax.set_ylabel('y')\n",
      "line = ax.scatter(x,y)\n",
      "line.set_label('signal')\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "122/303:\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "print('Coeff and intercept for np model: ',m,c)\n",
      "122/304:\n",
      "line, = ax.plot(x,x*m+c, color='red')\n",
      "line.set_label('linear regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/305: sk_model = linear_model.LinearRegression()\n",
      "122/306:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "x_sk2 = x_sk**2\n",
      "x_sk3 = x_sk**3\n",
      "X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)\n",
      "\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/307:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='black')\n",
      "line.set_label('polynomial regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/308:\n",
      "X = x_sk\n",
      "for i in range(2,15):\n",
      "    xi = x_sk**i\n",
      "    X = np.concatenate((X, xi), axis=1)\n",
      "    \n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "122/309:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='green')\n",
      "line.set_label('polynomial regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/310: X_ort = PolynomialFeatures(3).fit_transform(x_sk)\n",
      "122/311:\n",
      "sk_ort_result = sk_model.fit(X_ort,y_sk)\n",
      "print('r2 for sk_model: ', sk_ort_result.score(X_ort,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_ort_result.coef_, sk_ort_result.intercept_)\n",
      "122/312:\n",
      "line, = ax.plot(x_sk, sk_ort_result.predict(X_ort), color='orange')\n",
      "line.set_label('ort polynomial regression')\n",
      "ax.legend()\n",
      "fig\n",
      "122/313:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "%matplotlib inline\n",
      "122/314:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = test_train_split(X, y, test_size=0.7)\n",
      "122/315:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.7)\n",
      "122/316: plt.plot(X_train,  Y_train)\n",
      "122/317: plt.scater(X_train,  Y_train)\n",
      "122/318: plt.scatter(X_train,  Y_train)\n",
      "122/319: plt.scatter(X_test,  Y_test)\n",
      "122/320: X_train.size()\n",
      "122/321:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "122/322: plt.scatter(X_test,  Y_test)\n",
      "122/323: plt.scatter(X_train,  Y_train)\n",
      "122/324: sk_result.predict(X)\n",
      "122/325: Y\n",
      "122/326: y_sk\n",
      "122/327:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "%matplotlib inline\n",
      "122/328:\n",
      "x_sk2 = x_sk**2\n",
      "x_sk3 = x_sk**3\n",
      "X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "\n",
      "mean_squared_error(y_sk, sk_result.predict(X))\n",
      "122/329: X_train\n",
      "122/330: pd.concatenate(X_train, X_train**2)\n",
      "122/331: np.concatenate(X_train, X_train**2)\n",
      "122/332: np.concatenate(X_train, X_train**2, axis=1)\n",
      "122/333: np.concatenate((X_train, X_train**2), axis=1)\n",
      "122/334:\n",
      "errors = []\n",
      "for i in range(2,15):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i))\n",
      "    sk_result = sk_model.fit(Xtrain, y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "    plt.scatter(i, error)\n",
      "122/335:\n",
      "errors = []\n",
      "for i in range(2,15):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i))\n",
      "    sk_result = sk_model.fit(X_train, y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "    plt.scatter(i, error)\n",
      "122/336:\n",
      "errors = []\n",
      "for i in range(2,15):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i))\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "    plt.scatter(i, error)\n",
      "122/337:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "122/338:\n",
      "errors = []\n",
      "for i in range(2,15):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i))\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "    plt.scatter(i, error)\n",
      "122/339: X_train\n",
      "122/340: X_test\n",
      "122/341:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "122/342:\n",
      "errors = []\n",
      "for i in range(2,15):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "    plt.scatter(i, error)\n",
      "122/343:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "122/344:\n",
      "errors = []\n",
      "for i in range(2,5):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "    plt.scatter(i, error)\n",
      "122/345:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "plt.scatter(1, error)\n",
      "\n",
      "errors = []\n",
      "for i in range(2,5):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "    plt.scatter(i, error)\n",
      "122/346:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "plt.scatter(1, error)\n",
      "\n",
      "errors = []\n",
      "for i in range(2,5):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "    plt.scatter(i, error)\n",
      "    plt.grid()\n",
      "122/347:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "plt.scatter(1, error)\n",
      "\n",
      "errors = []\n",
      "for i in range(2,8):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "    plt.scatter(i, error)\n",
      "    plt.grid()\n",
      "122/348:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "plt.scatter(1, error)\n",
      "\n",
      "errors = []\n",
      "for i in range(2,8):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "    plt.scatter(i, error)\n",
      "    plt.grid()\n",
      "122/349:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "plt.scatter(1, error)\n",
      "\n",
      "errors = []\n",
      "for i in range(2,6):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "    plt.scatter(i, error)\n",
      "    plt.grid()\n",
      "122/350:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "\n",
      "errors = [error]\n",
      "for i in range(2,6):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "plt.scatter(errors)\n",
      "plt.grid()\n",
      "122/351:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "\n",
      "errors = [error]\n",
      "for i in range(2,6):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "plt.scatter(range(6),errors)\n",
      "plt.grid()\n",
      "122/352: errors\n",
      "122/353:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "\n",
      "errors = [error]\n",
      "for i in range(2,6):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "plt.scatter(range(5),errors)\n",
      "plt.grid()\n",
      "122/354:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 7\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "\n",
      "errors = [error]\n",
      "for i in range(2,MAX_POWER):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "plt.scatter(range(MAX_POWER),errors)\n",
      "plt.grid()\n",
      "122/355:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 7\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "\n",
      "errors = [error]\n",
      "for i in range(2,MAX_POWER):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "plt.scatter(range(MAX_POWER-1),errors)\n",
      "plt.grid()\n",
      "122/356:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 7\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "\n",
      "errors = [error]\n",
      "for i in range(2,MAX_POWER):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "plt.scatter(range(1,MAX_POWER),errors)\n",
      "plt.grid()\n",
      "122/357:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 6\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "\n",
      "errors = [error]\n",
      "for i in range(2,MAX_POWER):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "plt.scatter(range(1,MAX_POWER),errors)\n",
      "plt.grid()\n",
      "122/358:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 6\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "\n",
      "errors = [error]\n",
      "for i in range(2,MAX_POWER):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "plt.plot(range(1,MAX_POWER),errors)\n",
      "plt.grid()\n",
      "122/359:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 7\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "\n",
      "errors = [error]\n",
      "for i in range(2,MAX_POWER):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "plt.plot(range(1,MAX_POWER),errors)\n",
      "plt.grid()\n",
      "122/360:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 6\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "\n",
      "errors = [error]\n",
      "for i in range(2,MAX_POWER):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "plt.plot(range(1,MAX_POWER),errors)\n",
      "plt.grid()\n",
      "122/361:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 6\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "\n",
      "errors = [error]\n",
      "for i in range(2,MAX_POWER):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "plt.plot(range(1,MAX_POWER),errors)\n",
      "plt.xscale('X to power n')\n",
      "plt.yscale('MSE')\n",
      "plt.grid()\n",
      "122/362:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 6\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "\n",
      "errors = [error]\n",
      "for i in range(2,MAX_POWER):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "plt.plot(range(1,MAX_POWER),errors)\n",
      "plt.x_label('X to power n')\n",
      "plt.y_label('MSE')\n",
      "plt.grid()\n",
      "122/363:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 6\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "\n",
      "errors = [error]\n",
      "for i in range(2,MAX_POWER):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "plt.plot(range(1,MAX_POWER),errors)\n",
      "plt.xlabel('X to power n')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "122/364: errors\n",
      "124/1: sk_result.coef_\n",
      "124/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "%matplotlib inline\n",
      "124/3: x = pd.Series(np.arange(-10, 10, 0.01))\n",
      "124/4: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "124/5: y = 1 - x**2 + normal_distribution\n",
      "124/6:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "124/7: x_squared = x**2\n",
      "124/8: plt.scatter(x_squared, y)\n",
      "124/9:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "print('r matrix: ', r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "124/10: r**2\n",
      "124/11: df = pd.DataFrame({'x_squared': x_squared, 'y': y})\n",
      "124/12: df.head()\n",
      "124/13:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "124/14:\n",
      "my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())\n",
      "my_r2\n",
      "124/15: x = pd.Series(np.arange(0,1,0.01))\n",
      "124/16: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))\n",
      "124/17:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xlabel('x')\n",
      "ax.set_ylabel('y')\n",
      "line = ax.scatter(x,y)\n",
      "line.set_label('signal')\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "124/18:\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "print('Coeff and intercept for np model: ',m,c)\n",
      "124/19:\n",
      "line, = ax.plot(x,x*m+c, color='red')\n",
      "line.set_label('linear regression')\n",
      "ax.legend()\n",
      "fig\n",
      "124/20: sk_model = linear_model.LinearRegression()\n",
      "124/21:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "x_sk2 = x_sk**2\n",
      "x_sk3 = x_sk**3\n",
      "X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)\n",
      "\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "124/22:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='black')\n",
      "line.set_label('polynomial regression: pow=3')\n",
      "ax.legend()\n",
      "fig\n",
      "124/23:\n",
      "X = x_sk\n",
      "for i in range(2,15):\n",
      "    xi = x_sk**i\n",
      "    X = np.concatenate((X, xi), axis=1)\n",
      "    \n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "124/24:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='green')\n",
      "line.set_label('polynomial regression: pow=14')\n",
      "ax.legend()\n",
      "fig\n",
      "124/25:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "124/26:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 6\n",
      "\n",
      "sk_result = sk_model.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "\n",
      "errors = [error]\n",
      "for i in range(2,MAX_POWER):\n",
      "    X_train = np.concatenate((X_train, X_train**i), axis=1)\n",
      "    X_test = np.concatenate((X_test, X_test**i), axis=1)\n",
      "    sk_result = sk_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "plt.plot(range(1,MAX_POWER),errors)\n",
      "plt.xlabel('X to power n')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "124/27: sk_result.coef_\n",
      "124/28: l2_model_complexity = sk_result.coef_\n",
      "124/29: l2_model_complexity = sk_result.coef_**2.sum()\n",
      "124/30: l2_model_complexity = (sk_result.coef_)**2.sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/31: (sk_result.coef_)**2\n",
      "124/32: sk_result.coef_\n",
      "124/33: l2_model_complexity = (sk_result.coef_)[0]**2.sum()\n",
      "124/34: sk_result.coef_[0]\n",
      "124/35: sk_result.coef_[0].sum()\n",
      "124/36: sk_result.coef_[0]**2\n",
      "124/37: sk_result.coef_[0]**2.sum()\n",
      "124/38: (sk_result.coef_[0]**2).sum()\n",
      "124/39: l2_model_complexity = (sk_result.coef_[0]**2).sum()\n",
      "124/40: l1_model_complexity = (abs(sk_result.coef_)).sum()\n",
      "124/41: l1_model_complexity, l2_model_complxity\n",
      "124/42: l1_model_complexity, l2_model_complexity\n",
      "124/43:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from skelearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "%matplotlib inline\n",
      "124/44:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "%matplotlib inline\n",
      "124/45: x_sk\n",
      "124/46:\n",
      "poly = PolynomialFeatures(degree=1)\n",
      "poly.fit_transport(x_sk)\n",
      "124/47:\n",
      "poly = PolynomialFeatures(degree=1)\n",
      "poly.fit_transform(x_sk)\n",
      "124/48: X_sk\n",
      "124/49: x_sk\n",
      "124/50:\n",
      "poly = PolynomialFeatures(degree=0)\n",
      "poly.fit_transform(x_sk)\n",
      "124/51:\n",
      "poly = PolynomialFeatures(degree=1)\n",
      "poly.fit_transform(x_sk)\n",
      "124/52:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 6\n",
      "errors = []\n",
      "\n",
      "for i in range(1,MAX_POWER):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors.append(error)\n",
      "plt.plot(range(1,MAX_POWER),errors)\n",
      "plt.xlabel('X to power n')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "124/53:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 6\n",
      "errors = []\n",
      "\n",
      "for i in range(1,MAX_POWER):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(1,MAX_POWER),errors)\n",
      "plt.xlabel('X to power n')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "124/54:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 7\n",
      "errors = []\n",
      "\n",
      "for i in range(1,MAX_POWER):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(1,MAX_POWER),errors)\n",
      "plt.xlabel('X to power n')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "124/55:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 10\n",
      "errors = []\n",
      "\n",
      "for i in range(1,MAX_POWER):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(1,MAX_POWER),errors)\n",
      "plt.xlabel('X to power n')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "124/56:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 15\n",
      "errors = []\n",
      "\n",
      "for i in range(1,MAX_POWER):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(1,MAX_POWER),errors)\n",
      "plt.xlabel('X to power n')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "124/57:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 15\n",
      "errors = []\n",
      "\n",
      "for i in range(4,MAX_POWER):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(1,MAX_POWER),errors)\n",
      "plt.xlabel('X to power n')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "124/58:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 15\n",
      "errors = []\n",
      "\n",
      "for i in range(4,MAX_POWER):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(4,MAX_POWER),errors)\n",
      "plt.xlabel('X to power n')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "124/59:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 20\n",
      "errors = []\n",
      "\n",
      "for i in range(4,MAX_POWER):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(4,MAX_POWER),errors)\n",
      "plt.xlabel('X to power n')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "124/60:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MIN_DEGREE = 1\n",
      "MAX_DEGREE = 20\n",
      "errors = []\n",
      "\n",
      "for i in range(MIN_DEGREE,MAX_POWER):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)\n",
      "plt.xlabel('X to power n')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "124/61:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MIN_DEGREE = 3\n",
      "MAX_DEGREE = 20\n",
      "errors = []\n",
      "\n",
      "for i in range(MIN_DEGREE,MAX_POWER):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)\n",
      "plt.xlabel('X to power n')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "124/62: sk_model_ridge = linear_model.Ridge()\n",
      "125/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "%matplotlib inline\n",
      "125/2: x = pd.Series(np.arange(-10, 10, 0.01))\n",
      "125/3: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "125/4: y = 1 - x**2 + normal_distribution\n",
      "125/5:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "125/6: x_squared = x**2\n",
      "125/7: plt.scatter(x_squared, y)\n",
      "125/8:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "print('r matrix: ', r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "125/9: r**2\n",
      "125/10: df = pd.DataFrame({'x_squared': x_squared, 'y': y})\n",
      "125/11: df.head()\n",
      "125/12:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "125/13:\n",
      "my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())\n",
      "my_r2\n",
      "125/14: x = pd.Series(np.arange(0,1,0.01))\n",
      "125/15: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))\n",
      "125/16:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xlabel('x')\n",
      "ax.set_ylabel('y')\n",
      "line = ax.scatter(x,y)\n",
      "line.set_label('signal')\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "125/17:\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "print('Coeff and intercept for np model: ',m,c)\n",
      "125/18:\n",
      "line, = ax.plot(x,x*m+c, color='red')\n",
      "line.set_label('linear regression')\n",
      "ax.legend()\n",
      "fig\n",
      "125/19: sk_model = linear_model.LinearRegression()\n",
      "125/20:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "x_sk2 = x_sk**2\n",
      "x_sk3 = x_sk**3\n",
      "X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)\n",
      "\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "125/21:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='black')\n",
      "line.set_label('polynomial regression: pow=3')\n",
      "ax.legend()\n",
      "fig\n",
      "125/22:\n",
      "X = x_sk\n",
      "for i in range(2,15):\n",
      "    xi = x_sk**i\n",
      "    X = np.concatenate((X, xi), axis=1)\n",
      "    \n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "125/23:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='green')\n",
      "line.set_label('polynomial regression: pow=14')\n",
      "ax.legend()\n",
      "fig\n",
      "125/24:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "125/25:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MIN_DEGREE = 3\n",
      "MAX_DEGREE = 20\n",
      "errors = []\n",
      "\n",
      "for i in range(MIN_DEGREE,MAX_POWER):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)\n",
      "plt.xlabel('X to power n')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "125/26:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MIN_DEGREE = 3\n",
      "MAX_DEGREE = 20\n",
      "errors = []\n",
      "\n",
      "for i in range(MIN_DEGREE,MAX_DEGREE):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)\n",
      "plt.xlabel('X to power n')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "125/27:\n",
      "sk_model_ridge = linear_model.Ridge()\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 6\n",
      "\n",
      "ridge_result = sk_model_ridge.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "125/28: error\n",
      "125/29:\n",
      "sk_model_ridge = linear_model.Ridge()\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MAX_POWER = 6\n",
      "\n",
      "ridge_result = sk_model_ridge.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "\n",
      "print('Ridge MSE: ', error)\n",
      "print('Ridge coeffs: ', ridge_result.coef_)\n",
      "125/30:\n",
      "l = []\n",
      "l.append(a,b)\n",
      "125/31:\n",
      "l = []\n",
      "l.append(1,2)\n",
      "125/32:\n",
      "l = []\n",
      "l.append((1,2))\n",
      "125/33:\n",
      "l = []\n",
      "l.append((1,2))\n",
      "l.append((3,4))\n",
      "125/34: l.min()\n",
      "125/35: min(l)\n",
      "125/36:\n",
      "l = []\n",
      "l.append((1,2))\n",
      "l.append((3,4))\n",
      "l.append(0,10)\n",
      "125/37:\n",
      "l = []\n",
      "l.append((1,2))\n",
      "l.append((3,4))\n",
      "l.append((0,10))\n",
      "125/38: min(l)\n",
      "125/39:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,1, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errorrs_alpha[1])\n",
      "#print('Ridge coeffs: ', ridge_result.coef_)\n",
      "125/40:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,1, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errorrs_alpha[1])\n",
      "#print('Ridge coeffs: ', ridge_result.coef_)\n",
      "125/41:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,1, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errorrs_alpha[1])\n",
      "#print('Ridge coeffs: ', ridge_result.coef_)\n",
      "125/42:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,1, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "#print('Ridge coeffs: ', ridge_result.coef_)\n",
      "125/43:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MIN_DEGREE = 3\n",
      "MAX_DEGREE = 8\n",
      "errors = []\n",
      "\n",
      "for i in range(MIN_DEGREE,MAX_DEGREE):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)\n",
      "plt.xlabel('X to power n')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "125/44:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "#print('Ridge coeffs: ', ridge_result.coef_)\n",
      "125/45:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,1, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "#print('Ridge coeffs: ', ridge_result.coef_)\n",
      "125/46:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.001,1, 0.001)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "#print('Ridge coeffs: ', ridge_result.coef_)\n",
      "125/47:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.00001,1, 0.001)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "#print('Ridge coeffs: ', ridge_result.coef_)\n",
      "125/48: ridge_cv_result.coef_\n",
      "125/49:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.1,1, 0.001)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "#print('Ridge coeffs: ', ridge_result.coef_)\n",
      "125/50:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.001,1, 0.001)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "#print('Ridge coeffs: ', ridge_result.coef_)\n",
      "125/51: errors_alphas\n",
      "125/52: errors, alphas = zip(errors_alphas)\n",
      "125/53: zip(*errors_alphas)\n",
      "125/54: list(zip(*errors_alphas))\n",
      "125/55: er,al = zip(*errors_alphas)\n",
      "125/56: er\n",
      "125/57:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.plot(al, er)\n",
      "125/58:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.0001,0.01, 0.001)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "#print('Ridge coeffs: ', ridge_result.coef_)\n",
      "125/59:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "125/60: ridge_cv_result.coef_\n",
      "125/61:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.plot(al, er)\n",
      "125/62:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.0001,1, 0.001)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "#print('Ridge coeffs: ', ridge_result.coef_)\n",
      "125/63:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "125/64:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.0001,1, 0.001)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "#print('Ridge coeffs: ', ridge_result.coef_)\n",
      "125/65:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "125/66:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.plot(al, er)\n",
      "125/67:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, y_sk))\n",
      "125/68:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/69:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.0001,1, 0.001)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "125/70:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.0001,1, 0.001)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "125/71:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/72:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.0001,1, 0.001)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "125/73:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/74:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/75: ridge_cv_result.coef_\n",
      "125/76:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/77:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/78: ridge_cv_result.coef_\n",
      "125/79:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.plot(al, er)\n",
      "125/80:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.0001,1, 0.001)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "125/81:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/82:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/83: ridge_cv_result.coef_\n",
      "125/84:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.plot(al, er)\n",
      "125/85:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.0001,1, 0.001)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "125/86:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/87:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MIN_DEGREE = 3\n",
      "MAX_DEGREE = 8\n",
      "errors = []\n",
      "\n",
      "for i in range(MIN_DEGREE,MAX_DEGREE):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)\n",
      "plt.xlabel('Degree of polynomial')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "125/88:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,10, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "125/89:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/90:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/91:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/92:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,10, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "125/93:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,1, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "125/94: x = pd.Series(np.arange(0,1,0.001))\n",
      "125/95: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))\n",
      "125/96:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xlabel('x')\n",
      "ax.set_ylabel('y')\n",
      "line = ax.scatter(x,y)\n",
      "line.set_label('signal')\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "125/97:\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "print('Coeff and intercept for np model: ',m,c)\n",
      "125/98:\n",
      "line, = ax.plot(x,x*m+c, color='red')\n",
      "line.set_label('linear regression')\n",
      "ax.legend()\n",
      "fig\n",
      "125/99: sk_model = linear_model.LinearRegression()\n",
      "125/100:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "x_sk2 = x_sk**2\n",
      "x_sk3 = x_sk**3\n",
      "X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)\n",
      "\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "125/101:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='black')\n",
      "line.set_label('polynomial regression: pow=3')\n",
      "ax.legend()\n",
      "fig\n",
      "125/102:\n",
      "X = x_sk\n",
      "for i in range(2,15):\n",
      "    xi = x_sk**i\n",
      "    X = np.concatenate((X, xi), axis=1)\n",
      "    \n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "125/103:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='green')\n",
      "line.set_label('polynomial regression: pow=14')\n",
      "ax.legend()\n",
      "fig\n",
      "125/104:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "125/105:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MIN_DEGREE = 3\n",
      "MAX_DEGREE = 8\n",
      "errors = []\n",
      "\n",
      "for i in range(MIN_DEGREE,MAX_DEGREE):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)\n",
      "plt.xlabel('Degree of polynomial')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "125/106:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,1, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "125/107:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/108: ridge_cv_result.coef_\n",
      "125/109:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.plot(al, er)\n",
      "125/110:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,1, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "125/111:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/112:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,1, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "125/113:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/114:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,1, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "125/115:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/116:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,1, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "125/117:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "125/118:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.plot(al, er)\n",
      "plt.xlabel('alpha')\n",
      "plt.ylabel('MSE')\n",
      "125/119:\n",
      "paht_to_file = 'Regularization/oreilly.csv'\n",
      "ranks = pd.read(path_to_file)\n",
      "ranks.head()\n",
      "125/120:\n",
      "paht_to_file = 'Regularization/oreilly.csv'\n",
      "ranks = pd.read_csv(path_to_file)\n",
      "ranks.head()\n",
      "125/121:\n",
      "path_to_file = 'Regularization/oreilly.csv'\n",
      "ranks = pd.read_csv(path_to_file)\n",
      "ranks.head()\n",
      "125/122:\n",
      "with open(path_to_file) as f:\n",
      "    print(f)\n",
      "125/123:\n",
      "path_to_file = 'Regularization/oreilly.csv'\n",
      "ranks = pd.read_csv(path_to_file, error_bad_lines=False)\n",
      "ranks.head()\n",
      "125/124:\n",
      "with open(path_to_file) as f:\n",
      "    for line in f:\n",
      "        print(line)\n",
      "125/125:\n",
      "path_to_file = 'Regularization/oreilly.csv'\n",
      "ranks = pd.read_csv(path_to_file, encoding='latin1')\n",
      "ranks.head()\n",
      "125/126: ranks.isnull.sum()\n",
      "125/127: ranks.isnull().sum()\n",
      "125/128:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "%matplotlib inline\n",
      "125/129: count = CountVectorizer()\n",
      "125/130:\n",
      "bag = count(ranks[0,'BOOK title'])\n",
      "bag\n",
      "125/131:\n",
      "bag = count(ranks.loc[0,'BOOK title'])\n",
      "bag\n",
      "125/132:\n",
      "bag = count.fit_transform(ranks.loc[0,'BOOK title'])\n",
      "bag\n",
      "125/133: count.fit_transform('aa bb cc aa')\n",
      "125/134: count.fit_transform(['aa bb cc aa'])\n",
      "125/135: list('abc')\n",
      "125/136:\n",
      "x = count.fit_transform(['aa bb cc aa'])\n",
      "x\n",
      "125/137:\n",
      "x = count.fit_transform(['aa bb cc aa'])\n",
      "print(x.vocabulary_)\n",
      "125/138:\n",
      "x = count.fit_transform(['aa bb cc aa'])\n",
      "print(count.vocabulary_)\n",
      "125/139: ranks.loc([0, 'BOOK title'])\n",
      "125/140: ranks.loc[0, 'BOOK title']\n",
      "125/141:\n",
      "bag = count.fit_transform([ranks.loc[0,'BOOK title']])\n",
      "bag\n",
      "125/142: count.vocabulary_\n",
      "125/143:\n",
      "bag = count.fit_transform([ranks.loc[0,'BOOK title']])\n",
      "count.vocabulary_\n",
      "125/144: count = CountVectorizer()\n",
      "125/145: ranks.loc['BOOK title']\n",
      "125/146:\n",
      "bag = count.fit_transform([ranks['BOOK title']])\n",
      "count.vocabulary_\n",
      "125/147: count.fit_transform(['aa', 'bb', 'aa'])\n",
      "125/148: count.vocabulary_\n",
      "125/149: count.fit_transform(['aa', 'bb', 'aa', 'bb'])\n",
      "125/150: count.vocabulary_\n",
      "125/151: count = CountVectorizer()\n",
      "125/152: count.fit_transform(['aa', 'bb', 'aa', 'bb'])\n",
      "125/153: count.vocabulary_\n",
      "125/154: count.fit_transform(['aa', 'bb', 'aa', 'bb', 'cc'])\n",
      "125/155: count.vocabulary_\n",
      "125/156: count.fit_transform(['cc', 'bb', 'aa', 'bb', 'cc'])\n",
      "125/157: count.vocabulary_\n",
      "125/158: count.fit_transform(['cc', 'bb', 'aaa', 'bb', 'cc'])\n",
      "125/159: count.vocabulary_\n",
      "125/160: count.fit_transform(['cc', 'bb', 'aaa', 'bbb', 'cc'])\n",
      "125/161: count.vocabulary_\n",
      "125/162: print(count.vocabulary_)\n",
      "125/163:\n",
      "bag = count.fit_transform([ranks['BOOK title']])\n",
      "count.vocabulary_\n",
      "125/164:\n",
      "bag = count.fit_transform(ranks['BOOK title'])\n",
      "count.vocabulary_\n",
      "125/165:\n",
      "count = CountVectorizer()\n",
      "bag = count.fit_transform(ranks['Long Desc'])\n",
      "count.vocabulary_\n",
      "125/166:\n",
      "count = CountVectorizer()\n",
      "bag = count.fit_transform(ranks['Long Desc.'])\n",
      "count.vocabulary_\n",
      "128/1:\n",
      "count = CountVectorizer()\n",
      "count.fit_transform(['cc', 'bb', 'aaa', 'bbb', 'cc'])\n",
      "print(count.vocabulary_)\n",
      "128/2:\n",
      "count = CountVectorizer()\n",
      "bag = count.fit_transform(ranks['Long Desc.'])\n",
      "count.vocabulary_\n",
      "128/3:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "%matplotlib inline\n",
      "128/4: x = pd.Series(np.arange(-10, 10, 0.01))\n",
      "128/5: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "128/6: y = 1 - x**2 + normal_distribution\n",
      "128/7:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "128/8: x_squared = x**2\n",
      "128/9: plt.scatter(x_squared, y)\n",
      "128/10:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "print('r matrix: ', r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "128/11: r**2\n",
      "128/12: df = pd.DataFrame({'x_squared': x_squared, 'y': y})\n",
      "128/13: df.head()\n",
      "128/14:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "128/15:\n",
      "my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())\n",
      "my_r2\n",
      "128/16: x = pd.Series(np.arange(0,1,0.001))\n",
      "128/17: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))\n",
      "128/18:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xlabel('x')\n",
      "ax.set_ylabel('y')\n",
      "line = ax.scatter(x,y)\n",
      "line.set_label('signal')\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "128/19:\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "print('Coeff and intercept for np model: ',m,c)\n",
      "128/20:\n",
      "line, = ax.plot(x,x*m+c, color='red')\n",
      "line.set_label('linear regression')\n",
      "ax.legend()\n",
      "fig\n",
      "128/21: sk_model = linear_model.LinearRegression()\n",
      "128/22:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "x_sk2 = x_sk**2\n",
      "x_sk3 = x_sk**3\n",
      "X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)\n",
      "\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "128/23:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='black')\n",
      "line.set_label('polynomial regression: pow=3')\n",
      "ax.legend()\n",
      "fig\n",
      "128/24:\n",
      "X = x_sk\n",
      "for i in range(2,15):\n",
      "    xi = x_sk**i\n",
      "    X = np.concatenate((X, xi), axis=1)\n",
      "    \n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "128/25:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='green')\n",
      "line.set_label('polynomial regression: pow=14')\n",
      "ax.legend()\n",
      "fig\n",
      "128/26:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "128/27:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MIN_DEGREE = 3\n",
      "MAX_DEGREE = 8\n",
      "errors = []\n",
      "\n",
      "for i in range(MIN_DEGREE,MAX_DEGREE):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)\n",
      "plt.xlabel('Degree of polynomial')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "128/28:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,1, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "128/29:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "128/30: ridge_cv_result.coef_\n",
      "128/31:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.plot(al, er)\n",
      "plt.xlabel('alpha')\n",
      "plt.ylabel('MSE')\n",
      "128/32:\n",
      "path_to_file = 'Regularization/oreilly.csv'\n",
      "ranks = pd.read_csv(path_to_file, encoding='latin1')\n",
      "ranks.head()\n",
      "128/33: ranks.isnull().sum()\n",
      "128/34:\n",
      "count = CountVectorizer()\n",
      "bag = count.fit_transform(ranks['Long Desc.'])\n",
      "count.vocabulary_\n",
      "128/35:\n",
      "count = CountVectorizer()\n",
      "count.fit_transform(['cc', 'bb', 'aaa', 'bbb', 'cc'])\n",
      "print(count.vocabulary_)\n",
      "128/36:\n",
      "count = CountVectorizer()\n",
      "count.fit_transform(['ja', 'ty', 'ona', 'ty', 'ja'])\n",
      "print(count.vocabulary_)\n",
      "128/37:\n",
      "count = CountVectorizer()\n",
      "count.fit_transform(['mama', 'ja', 'ty', 'ona', 'ty', 'ja'])\n",
      "print(count.vocabulary_)\n",
      "128/38:\n",
      "count = CountVectorizer()\n",
      "count.fit_transform(['mama', 'ja', 'ty', 'ona', 'ty', 'ona'])\n",
      "print(count.vocabulary_)\n",
      "128/39:\n",
      "count = CountVectorizer()\n",
      "count.fit_transform(['mama', 'ja', 'ty', 'ona', 'ty', 'mama'])\n",
      "print(count.vocabulary_)\n",
      "128/40: ranks.shape()\n",
      "128/41: ranks.shape\n",
      "128/42:\n",
      "x = CountVectorizer()\n",
      "x.fit_transform(['mama', 'ja', 'ty', 'ona', 'ty', 'mama'])\n",
      "print(count.vocabulary_)\n",
      "128/43:\n",
      "x = CountVectorizer()\n",
      "x.fit_transform(['This is the first document.',\n",
      "...     'This document is the second document.',\n",
      "...     'And this is the third one.',\n",
      "...     'Is this the first document?',])\n",
      "print(count.vocabulary_)\n",
      "128/44:\n",
      "x = CountVectorizer()\n",
      "x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])\n",
      "print(count.vocabulary_)\n",
      "128/45:\n",
      "x = CountVectorizer()\n",
      "x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])\n",
      "print(x.vocabulary_)\n",
      "128/46:\n",
      "x = CountVectorizer()\n",
      "X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])\n",
      "print(X.vocabulary_)\n",
      "128/47:\n",
      "x = CountVectorizer()\n",
      "X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])\n",
      "print(X)\n",
      "128/48:\n",
      "x = CountVectorizer()\n",
      "X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])\n",
      "print(x.vocabulary)\n",
      "128/49:\n",
      "x = CountVectorizer()\n",
      "X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])\n",
      "print(x.vocabulary_)\n",
      "128/50: count.vocabulary_.get('second')\n",
      "128/51: count.vocabulary_.get('the')\n",
      "128/52: count.get_features_names()\n",
      "128/53: count.get_feature_names()\n",
      "128/54: x.get_feature_names()\n",
      "128/55: x.vocabulary_.get('second')\n",
      "128/56:\n",
      "x = CountVectorizer()\n",
      "X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])\n",
      "print(x.vocabulary_)\n",
      "128/57:\n",
      "x = CountVectorizer()\n",
      "X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])\n",
      "print(x.vocabulary_)\n",
      "128/58: x.vocabulary_.get('second')\n",
      "128/59:\n",
      "x = CountVectorizer()\n",
      "X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])\n",
      "print(x.vocabulary_)\n",
      "128/60: x.vocabulary_.get('second')\n",
      "128/61:\n",
      "count = CountVectorizer()\n",
      "bag = count.fit_transform(ranks['Long Desc.'])\n",
      "count.vocabulary_\n",
      "128/62:\n",
      "count = CountVectorizer()\n",
      "bag = count.fit_transform(ranks['Long Desc.'])\n",
      "count.vocabulary_\n",
      "128/63: count.vocabulary_.get('the')\n",
      "128/64: count.vocabulary_.get('perl')\n",
      "128/65: count.vocabulary_.get('python')\n",
      "128/66:\n",
      "x = CountVectorizer()\n",
      "X = x.fit_transform(['This is the first document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])\n",
      "print(x.vocabulary_)\n",
      "128/67:\n",
      "x = CountVectorizer()\n",
      "X = x.fit_transform(['This is the document.', 'This document is the second document.','And this is the third one.','Is this the first document?',])\n",
      "print(x.vocabulary_)\n",
      "128/68:\n",
      "x = CountVectorizer()\n",
      "X = x.fit_transform(['This is the document.', 'This document is the document.','And this is the third one.','Is this the first document?',])\n",
      "print(x.vocabulary_)\n",
      "128/69:\n",
      "x = CountVectorizer()\n",
      "X = x.fit_transform(['This is second the document.', 'This document is the document.','And this is the third one.','Is this the first document?',])\n",
      "print(x.vocabulary_)\n",
      "128/70:\n",
      "x = CountVectorizer()\n",
      "X = x.fit_transform(['This is the first document.', 'This is the second document.','And this is the third one.','Is this the first document?',])\n",
      "print(x.vocabulary_)\n",
      "128/71: X\n",
      "128/72: X.to_array()\n",
      "128/73: X.toarray()\n",
      "128/74: X.toarray()[5]\n",
      "128/75: X.toarray()[:,5]\n",
      "128/76: X.toarray()[:]\n",
      "128/77: X.toarray()[:,5]\n",
      "128/78:\n",
      "count = CountVectorizer()\n",
      "bag = count.fit_transform(ranks['Long Desc.'])\n",
      "128/79:\n",
      "count = CountVectorizer()\n",
      "bag = count.fit_transform(ranks['Long Desc.'], stopwords='english')\n",
      "128/80:\n",
      "count = CountVectorizer(stopwords='english')\n",
      "bag = count.fit_transform(ranks['Long Desc.'])\n",
      "128/81:\n",
      "count = CountVectorizer(stop_words='english')\n",
      "bag = count.fit_transform(ranks['Long Desc.'])\n",
      "128/82: bag\n",
      "128/83: bag.to_array()\n",
      "128/84: bag.toarray()\n",
      "128/85: pd.DataFrame.sparse.from_matrix(bag)\n",
      "128/86: pd.DataFrame.sparse.from_spmatrix(bag)\n",
      "128/87: bag_df = pd.DataFrame.sparse.from_spmatrix(bag)\n",
      "128/88: bsg_df.columns = count.get_feature_names()\n",
      "128/89: bag_df.columns = count.get_feature_names()\n",
      "128/90: bag_df.head()\n",
      "128/91: bag_df.head(10)\n",
      "128/92: bag_df.index\n",
      "128/93:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X, y)\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "128/94:\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "128/95:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,1, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "128/96: ridge_cv_result.coef_\n",
      "128/97:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,10, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "128/98:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,10, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "128/99:\n",
      "#Ridge regression cross validation\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "128/100:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.01,10, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "128/101:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.001,1, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "128/102:\n",
      "#Ridge regression cross validation\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])\n",
      "poly= PolynomialFeatures(degree=5)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "128/103:\n",
      "#Ridge regression cross validation\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "128/104:\n",
      "#Ridge regression cross validation\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "128/105:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.plot(al, er)\n",
      "plt.xlabel('alpha')\n",
      "plt.ylabel('MSE')\n",
      "128/106: np.arange(1,100).reshape(-1,1)\n",
      "128/107:\n",
      "clf_rank = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])\n",
      "poly_rank= PolynomialFeatures(degree=10)\n",
      "\n",
      "X_rank = bag.to_array\n",
      "y_rank = np.arange(1,100).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "X_train = poly_rank.fit_transform(X_train)\n",
      "X_test = poly_rank.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_rank_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_rank_result.score(X_test, Y_test))\n",
      "128/108:\n",
      "clf_rank = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])\n",
      "poly_rank= PolynomialFeatures(degree=10)\n",
      "\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.arange(1,100).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "X_train = poly_rank.fit_transform(X_train)\n",
      "X_test = poly_rank.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_rank_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv rank min MSE: ', error)\n",
      "print('Ridge cv rank r2: ', ridge_cv_rank_result.score(X_test, Y_test))\n",
      "128/109:\n",
      "clf_rank = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])\n",
      "poly_rank= PolynomialFeatures(degree=10)\n",
      "\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.arange(1,101).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "X_train = poly_rank.fit_transform(X_train)\n",
      "X_test = poly_rank.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_rank_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv rank min MSE: ', error)\n",
      "print('Ridge cv rank r2: ', ridge_cv_rank_result.score(X_test, Y_test))\n",
      "128/110:\n",
      "clf_rank = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])\n",
      "poly_rank= PolynomialFeatures(degree=2)\n",
      "\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.arange(1,101).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "X_train = poly_rank.fit_transform(X_train)\n",
      "X_test = poly_rank.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_rank_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv rank min MSE: ', error)\n",
      "print('Ridge cv rank r2: ', ridge_cv_rank_result.score(X_test, Y_test))\n",
      "128/111:\n",
      "%time\n",
      "clf_rank = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])\n",
      "poly_rank= PolynomialFeatures(degree=2)\n",
      "\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.arange(1,101).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "X_train = poly_rank.fit_transform(X_train)\n",
      "X_test = poly_rank.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_rank_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv rank min MSE: ', error)\n",
      "print('Ridge cv rank r2: ', ridge_cv_rank_result.score(X_test, Y_test))\n",
      "128/112:\n",
      "%timeit\n",
      "clf_rank = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])\n",
      "poly_rank= PolynomialFeatures(degree=2)\n",
      "\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.arange(1,101).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "X_train = poly_rank.fit_transform(X_train)\n",
      "X_test = poly_rank.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_rank_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv rank min MSE: ', error)\n",
      "print('Ridge cv rank r2: ', ridge_cv_rank_result.score(X_test, Y_test))\n",
      "128/113:\n",
      "%timeit\n",
      "clf_rank = linear_model.RidgeCV(alphas=[1e-2, 1e-1, 1, 10])\n",
      "poly_rank= PolynomialFeatures(degree=2)\n",
      "\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.arange(1,101).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "X_train = poly_rank.fit_transform(X_train)\n",
      "X_test = poly_rank.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_rank_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv rank min MSE: ', error)\n",
      "print('Ridge cv rank mean accuracy: ', ridge_cv_rank_result.score(X_test, Y_test))\n",
      "128/114:\n",
      "%%timeit\n",
      "clf_rank = linear_model.RidgeCV(alphas=[1e-2, 1e-1, 1, 10])\n",
      "poly_rank= PolynomialFeatures(degree=2)\n",
      "\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.arange(1,101).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "X_train = poly_rank.fit_transform(X_train)\n",
      "X_test = poly_rank.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_rank_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv rank min MSE: ', error)\n",
      "print('Ridge cv rank mean accuracy: ', ridge_cv_rank_result.score(X_test, Y_test))\n",
      "128/115:\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.arange(1,101).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "X_train = poly_rank.fit_transform(X_train)\n",
      "X_test = poly_rank.fit_transform(X_test)\n",
      "128/116:\n",
      "clf_rank = linear_model.RidgeCV(alphas=[1e-2, 1e-1, 1, 10])\n",
      "poly_rank= PolynomialFeatures(degree=2)\n",
      "\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.arange(1,101).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "X_train = poly_rank.fit_transform(X_train)\n",
      "X_test = poly_rank.fit_transform(X_test)\n",
      "\n",
      "%time ridge_cv_rank_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv rank min MSE: ', error)\n",
      "print('Ridge cv rank mean accuracy: ', ridge_cv_rank_result.score(X_test, Y_test))\n",
      "128/117:\n",
      "alphas = [0.1, 0.25, 0.5, 1, 2, 5]\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "128/118:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.plot(al, er)\n",
      "plt.xlabel('alpha')\n",
      "plt.ylabel('MSE')\n",
      "128/119:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.scatter(al, er)\n",
      "plt.xlabel('alpha')\n",
      "plt.ylabel('MSE')\n",
      "128/120: er\n",
      "128/121: er**(0.5)\n",
      "128/122: list(er)**(0.5)\n",
      "128/123:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.scatter(al, [e*0.5 for e in er])\n",
      "plt.xlabel('alpha')\n",
      "plt.ylabel('MSE')\n",
      "128/124:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.scatter(al, er)\n",
      "plt.xlabel('alpha')\n",
      "plt.ylabel('MSE')\n",
      "128/125:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.scatter(al, [e**0.5 for e in er])\n",
      "plt.xlabel('alpha')\n",
      "plt.ylabel('MSE')\n",
      "128/126: [e**0.5 for e in er]\n",
      "128/127: al\n",
      "128/128: er\n",
      "128/129:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.scatter(al, [e**0.5 for e in er])\n",
      "plt.xlabel('alpha')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "128/130:\n",
      "alphas = [0.1, 0.5, 1, 10, 20]\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "128/131:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.scatter(al, [e**0.5 for e in er])\n",
      "plt.xlabel('alpha')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "128/132: er\n",
      "128/133: [e**0.5 for e in er]\n",
      "128/134:\n",
      "alphas = [1, 10, 20]\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "128/135:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.scatter(al, [e**0.5 for e in er])\n",
      "plt.xlabel('alpha')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "128/136: er\n",
      "128/137: [e**0.5 for e in er]\n",
      "128/138:\n",
      "alphas = [0.005, 0.01, 0.1]\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "128/139:\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.arange(1,101).reshape(-1,1)\n",
      "128/140: 50*[0]\n",
      "128/141: np.arange(1,50).join(50*[0])\n",
      "128/142: np.arange(1,50)\n",
      "128/143: 50*[1]+50*[0]\n",
      "128/144: (50*[1]+50*[0]).reshape(-1,1)\n",
      "128/145: np.arange(1,50)\n",
      "128/146: np.array(50*[1]+50*[0]).reshape(-1,1)\n",
      "128/147:\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)\n",
      "128/148: clf = linear_model.LogisticRegression(X_rank,y_rank)\n",
      "128/149: X_rank[39]\n",
      "128/150: clf.predict(X_rank[39])\n",
      "128/151: clf = linear_model.LogisticRegression.fit(X_rank,y_rank)\n",
      "128/152: clf = linear_model.LogisticRegression().fit(X_rank,y_rank)\n",
      "128/153: clf.predict(X_rank[39])\n",
      "128/154:\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.arange(1,101).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "X_train = poly_rank.fit_transform(X_train)\n",
      "X_test = poly_rank.fit_transform(X_test)\n",
      "128/155: y_rank\n",
      "128/156:\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)\n",
      "y_rank\n",
      "128/157: clf = linear_model.LogisticRegression(random_state=0).fit(X_rank,y_rank)\n",
      "128/158: np.arange(1,10)\n",
      "128/159: np.arange(1,10).reshape(-1,1)\n",
      "128/160: clf.predict(X_rank[39])\n",
      "128/161:\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)\n",
      "128/162: clf.predict(X_rank)\n",
      "128/163: clf.score()\n",
      "128/164: clf.score(X_rank, y_rank)\n",
      "128/165: clf = linear_model.LogisticRegression(random_state=0).fit(X_train,y_train)\n",
      "128/166: clf = linear_model.LogisticRegression(random_state=0).fit(X_train,Y_train)\n",
      "128/167: clf = linear_model.LogisticRegression(random_state=0).fit(X_rank,y_rank)\n",
      "128/168: clf = linear_model.LogisticRegression().fit(X_train,y_train)\n",
      "128/169: clf = linear_model.LogisticRegression().fit(X_train,Y_train)\n",
      "129/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf\n",
      "from sklearn import linear_model\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "%matplotlib inline\n",
      "129/2: x = pd.Series(np.arange(-10, 10, 0.01))\n",
      "129/3: normal_distribution = np.random.normal(0, 5, len(x))\n",
      "129/4: y = 1 - x**2 + normal_distribution\n",
      "129/5:\n",
      "plt.scatter(x,y)\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "plt.plot(x, m*x+c, color='red')\n",
      "plt.grid()\n",
      "129/6: x_squared = x**2\n",
      "129/7: plt.scatter(x_squared, y)\n",
      "129/8:\n",
      "m,c = np.polyfit(x_squared, y, deg=1)\n",
      "y_model = x_squared*m +c\n",
      "r = np.corrcoef(x_squared, y)\n",
      "print('r matrix: ', r)\n",
      "print('m: ', m, ', c: ', c)\n",
      "129/9: r**2\n",
      "129/10: df = pd.DataFrame({'x_squared': x_squared, 'y': y})\n",
      "129/11: df.head()\n",
      "129/12:\n",
      "model = smf.ols(formula='y ~ x_squared', data=df).fit()\n",
      "model.summary()\n",
      "129/13:\n",
      "my_r2 = (((y_model - y.mean())**2).sum()) / (((y - y.mean())**2).sum())\n",
      "my_r2\n",
      "129/14: x = pd.Series(np.arange(0,1,0.001))\n",
      "129/15: y = np.sin(2*np.pi*x) + np.random.normal(0,0.1,len(x))\n",
      "129/16:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xlabel('x')\n",
      "ax.set_ylabel('y')\n",
      "line = ax.scatter(x,y)\n",
      "line.set_label('signal')\n",
      "ax.legend()\n",
      "ax.grid()\n",
      "129/17:\n",
      "m,c = np.polyfit(x,y, deg=1)\n",
      "print('Coeff and intercept for np model: ',m,c)\n",
      "129/18:\n",
      "line, = ax.plot(x,x*m+c, color='red')\n",
      "line.set_label('linear regression')\n",
      "ax.legend()\n",
      "fig\n",
      "129/19: sk_model = linear_model.LinearRegression()\n",
      "129/20:\n",
      "y_sk = y.values.reshape(-1,1)\n",
      "\n",
      "x_sk = x.values.reshape(-1,1)\n",
      "x_sk2 = x_sk**2\n",
      "x_sk3 = x_sk**3\n",
      "X = np.concatenate((x_sk, x_sk2, x_sk3), axis=1)\n",
      "\n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "129/21:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='black')\n",
      "line.set_label('polynomial regression: pow=3')\n",
      "ax.legend()\n",
      "fig\n",
      "129/22:\n",
      "X = x_sk\n",
      "for i in range(2,15):\n",
      "    xi = x_sk**i\n",
      "    X = np.concatenate((X, xi), axis=1)\n",
      "    \n",
      "sk_result = sk_model.fit(X,y_sk)\n",
      "print('r2 for sk_model: ', sk_result.score(X,y_sk))\n",
      "print('coeff and intercept for sk_model: ', sk_result.coef_, sk_result.intercept_)\n",
      "129/23:\n",
      "line, = ax.plot(x_sk, sk_result.predict(X), color='green')\n",
      "line.set_label('polynomial regression: pow=14')\n",
      "ax.legend()\n",
      "fig\n",
      "129/24:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "129/25:\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "\n",
      "MIN_DEGREE = 3\n",
      "MAX_DEGREE = 8\n",
      "errors = []\n",
      "\n",
      "for i in range(MIN_DEGREE,MAX_DEGREE):\n",
      "    poly= PolynomialFeatures(degree=i)\n",
      "    X_train_i = poly.fit_transform(X_train)\n",
      "    X_test_i = poly.fit_transform(X_test)\n",
      "    \n",
      "    sk_result = sk_model.fit(X_train_i, Y_train)\n",
      "    error = mean_squared_error(Y_test, sk_result.predict(X_test_i))\n",
      "    errors.append(error)\n",
      "plt.plot(range(MIN_DEGREE,MAX_DEGREE),errors)\n",
      "plt.xlabel('Degree of polynomial')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "129/26:\n",
      "#Ridge regression \n",
      "\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "alphas = np.arange(0.001,1, 0.01)\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "129/27:\n",
      "#Ridge regression cross validation\n",
      "clf = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10])\n",
      "poly= PolynomialFeatures(degree=10)\n",
      "\n",
      "X = x_sk\n",
      "y = y_sk\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
      "X_train = poly.fit_transform(X_train)\n",
      "X_test = poly.fit_transform(X_test)\n",
      "\n",
      "ridge_cv_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv min MSE: ', error)\n",
      "print('Ridge r2: ', ridge_cv_result.score(X_test, Y_test))\n",
      "129/28: ridge_cv_result.coef_\n",
      "129/29:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.plot(al, er)\n",
      "plt.xlabel('alpha')\n",
      "plt.ylabel('MSE')\n",
      "129/30:\n",
      "path_to_file = 'Regularization/oreilly.csv'\n",
      "ranks = pd.read_csv(path_to_file, encoding='latin1')\n",
      "ranks.head()\n",
      "129/31: ranks.isnull().sum()\n",
      "129/32:\n",
      "count = CountVectorizer(stop_words='english')\n",
      "bag = count.fit_transform(ranks['Long Desc.'])\n",
      "129/33: bag.toarray()\n",
      "129/34: bag_df = pd.DataFrame.sparse.from_spmatrix(bag)\n",
      "129/35: bag_df.columns = count.get_feature_names()\n",
      "129/36: bag_df.head(10)\n",
      "129/37: bag_df.index\n",
      "129/38:\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.arange(1,101).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "X_train = poly_rank.fit_transform(X_train)\n",
      "X_test = poly_rank.fit_transform(X_test)\n",
      "129/39:\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.arange(1,101).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "129/40: y_rank\n",
      "129/41:\n",
      "clf_rank = linear_model.RidgeCV(alphas=[1e-2, 1e-1, 1, 10], cv=5)\n",
      "poly_rank= PolynomialFeatures(degree=2)\n",
      "\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.arange(1,101).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "X_train = poly_rank.fit_transform(X_train)\n",
      "X_test = poly_rank.fit_transform(X_test)\n",
      "\n",
      "%time ridge_cv_rank_result = clf.fit(X_train, Y_train)\n",
      "error = mean_squared_error(Y_test, ridge_cv_rank_result.predict(X_test))\n",
      "\n",
      "print('Ridge cv rank min MSE: ', error)\n",
      "print('Ridge cv rank mean accuracy: ', ridge_cv_rank_result.score(X_test, Y_test))\n",
      "129/42:\n",
      "alphas = [0.005, 0.01, 0.1]\n",
      "errors_alphas = []\n",
      "\n",
      "for alpha in alphas:\n",
      "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
      "    ridge_result = ridge_model.fit(X_train, Y_train)\n",
      "    error = mean_squared_error(Y_test, ridge_result.predict(X_test))\n",
      "    errors_alphas.append((error, alpha))\n",
      "    \n",
      "min_errors_alpha = min(errors_alphas)\n",
      "\n",
      "print('Ridge min MSE: ', min_errors_alpha[0], ' for alpha: ', min_errors_alpha[1])\n",
      "print('Ridge r2: ', ridge_result.score(X_test,Y_test))\n",
      "129/43:\n",
      "er,al = zip(*errors_alphas)\n",
      "plt.scatter(al, [e**0.5 for e in er])\n",
      "plt.xlabel('alpha')\n",
      "plt.ylabel('MSE')\n",
      "plt.grid()\n",
      "129/44: er\n",
      "129/45: [e**0.5 for e in er]\n",
      "129/46:\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "129/47: clf = linear_model.LogisticRegression().fit(X_train,Y_train)\n",
      "129/48: clf.predict(X_rank)\n",
      "129/49: clf.score(X_test, T_test)\n",
      "129/50: clf.score(X_test, Y_test)\n",
      "129/51: logistic_regression_cv = linear_model.LogisticRegressionCV().fit(X_train,Y_train)\n",
      "129/52: logistic_regression_cv.score(X_test, Y_test)\n",
      "129/53: logistic_regression_cv = linear_model.LogisticRegressionCV(cv=5).fit(X_train,Y_train)\n",
      "129/54: logistic_regression_cv.score(X_test, Y_test)\n",
      "129/55: logistic_regression_cv = linear_model.LogisticRegressionCV(cv=5, penalty='elasticnet').fit(X_train,Y_train)\n",
      "129/56: logistic_regression_cv = linear_model.LogisticRegressionCV(cv=5, penalty='l1').fit(X_train,Y_train)\n",
      "129/57: clf = linear_model.LogisticRegression().fit(X_train,Y_train)\n",
      "129/58: clf.predict(X_rank)\n",
      "129/59: clf.score(X_test, Y_test)\n",
      "129/60: logistic_regression_cv = linear_model.LogisticRegressionCV(cv=5, penalty='l1').fit(X_train,Y_train)\n",
      "129/61: logistic_regression_cv = linear_model.LogisticRegressionCV().fit(X_train,Y_train)\n",
      "129/62: logistic_regression_cv.score(X_test, Y_test)\n",
      "129/63:\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank)\n",
      "129/64: clf = linear_model.LogisticRegression().fit(X_train,Y_train)\n",
      "129/65: clf.predict(X_rank)\n",
      "129/66: clf.score(X_test, Y_test)\n",
      "129/67: logistic_regression_cv = linear_model.LogisticRegressionCV().fit(X_train,Y_train)\n",
      "129/68: logistic_regression_cv.score(X_test, Y_test)\n",
      "129/69: logistic_regression_cv.score(X_train, Y_train)\n",
      "129/70: X_test\n",
      "129/71: Y_test.shape()\n",
      "129/72: Y_test.shape\n",
      "129/73:\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank, test_size=0.15)\n",
      "129/74: clf = linear_model.LogisticRegression().fit(X_train,Y_train)\n",
      "129/75: clf.predict(X_rank)\n",
      "129/76: clf.score(X_test, Y_test)\n",
      "129/77: logistic_regression_cv = linear_model.LogisticRegressionCV().fit(X_train,Y_train)\n",
      "129/78: logistic_regression_cv.score(X_test, Y_test)\n",
      "129/79: logistic_regression_cv.score(X_train, Y_train)\n",
      "129/80: Y_test.shape\n",
      "129/81:\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank, test_size=0.5)\n",
      "129/82: clf = linear_model.LogisticRegression().fit(X_train,Y_train)\n",
      "129/83: clf.predict(X_rank)\n",
      "129/84: clf.score(X_test, Y_test)\n",
      "129/85: logistic_regression_cv = linear_model.LogisticRegressionCV().fit(X_train,Y_train)\n",
      "129/86: logistic_regression_cv.score(X_test, Y_test)\n",
      "129/87: logistic_regression_cv.score(X_train, Y_train)\n",
      "129/88:\n",
      "X_rank = bag.toarray()\n",
      "y_rank = np.array(50*[1]+50*[0]).reshape(-1,1)\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X_rank, y_rank, test_size=0.1)\n",
      "129/89: clf = linear_model.LogisticRegression().fit(X_train,Y_train)\n",
      "129/90: clf.predict(X_rank)\n",
      "129/91: clf.score(X_test, Y_test)\n",
      "129/92: logistic_regression_cv = linear_model.LogisticRegressionCV().fit(X_train,Y_train)\n",
      "129/93: logistic_regression_cv.score(X_test, Y_test)\n",
      "129/94: logistic_regression_cv.score(X_train, Y_train)\n",
      "129/95: Y_test.shape\n",
      "130/1: print('3')\n",
      "130/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "130/3:\n",
      "def one_dict(list_dict):\n",
      "    keys=list_dict[0].keys()\n",
      "    out_dict={key:[] for key in keys}\n",
      "    for dict_ in list_dict:\n",
      "        for key, value in dict_.items():\n",
      "            out_dict[key].append(value)\n",
      "    return out_dict\n",
      "130/4:\n",
      "from nba_api.stats.static import teams\n",
      "import matplotlib.pyplot as plt\n",
      "130/5: !pip3 install nba_api\n",
      "130/6:\n",
      "from nba_api.stats.static import teams\n",
      "import matplotlib.pyplot as plt\n",
      "130/7: from nba_api.stats.endpoints import playercareerstats\n",
      "130/8: nba_api\n",
      "130/9:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import nba_api\n",
      "130/10: !pip install nba_api\n",
      "130/11:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import nba_api\n",
      "131/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import nba_api\n",
      "132/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import nba_api\n",
      "133/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import nba_api\n",
      "134/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import nba_api\n",
      "134/2: !pip install nba_api\n",
      "134/3:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import nba_api\n",
      "134/4:\n",
      "def one_dict(list_dict):\n",
      "    keys=list_dict[0].keys()\n",
      "    out_dict={key:[] for key in keys}\n",
      "    for dict_ in list_dict:\n",
      "        for key, value in dict_.items():\n",
      "            out_dict[key].append(value)\n",
      "    return out_dict\n",
      "134/5:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import nba_api\n",
      "135/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import nba_api\n",
      "135/2: !pip install nba_api\n",
      "135/3:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import nba_api\n",
      "135/4:\n",
      "def one_dict(list_dict):\n",
      "    keys=list_dict[0].keys()\n",
      "    out_dict={key:[] for key in keys}\n",
      "    for dict_ in list_dict:\n",
      "        for key, value in dict_.items():\n",
      "            out_dict[key].append(value)\n",
      "    return out_dict\n",
      "135/5:\n",
      "from nba_api.stats.static import teams\n",
      "import matplotlib.pyplot as plt\n",
      "135/6: from nba_api.stats.endpoints import playercareerstats\n",
      "135/7: nba_teams = teams.get_teams()\n",
      "135/8: nba_teams[0:3]\n",
      "135/9:\n",
      "dict_nba_team=one_dict(nba_teams)\n",
      "df_teams=pd.DataFrame(dict_nba_team)\n",
      "df_teams.head()\n",
      "135/10: dict_nba_team\n",
      "135/11:\n",
      "df_warriors=df_teams[df_teams['nickname']=='Warriors']\n",
      "df_warriors\n",
      "135/12:\n",
      "id_warriors=df_warriors[['id']].values[0][0]\n",
      "#we now have an integer that can be used   to request the Warriors information \n",
      "id_warriors\n",
      "135/13: id_warriors=df_warriors[['id']]\n",
      "135/14:\n",
      "id_warriors=df_warriors[['id']]\n",
      "id_warriors\n",
      "135/15:\n",
      "id_warriors=df_warriors[['id']].values\n",
      "id_warriors\n",
      "135/16:\n",
      "id_warriors=df_warriors[['id']].values[0]\n",
      "id_warriors\n",
      "135/17:\n",
      "id_warriors=df_warriors[['id']].values[0][0]\n",
      "id_warriors\n",
      "135/18: from nba_api.stats.endpoints import leaguegamefinder\n",
      "135/19:\n",
      "# Since https://stats.nba.com does lot allow api calls from Cloud IPs and Skills Network Labs uses a Cloud IP.\n",
      "# The following code is comment out, you can run it on jupyter labs on your own computer.\n",
      "gamefinder = leaguegamefinder.LeagueGameFinder(team_id_nullable=id_warriors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/20:\n",
      "# Since https://stats.nba.com does lot allow api calls from Cloud IPs and Skills Network Labs uses a Cloud IP.\n",
      "# The following code is comment out, you can run it on jupyter labs on your own computer.\n",
      "gamefinder.get_json()\n",
      "135/21:\n",
      "# Since https://stats.nba.com does lot allow api calls from Cloud IPs and Skills Network Labs uses a Cloud IP.\n",
      "# The following code is comment out, you can run it on jupyter labs on your own computer.\n",
      "games = gamefinder.get_data_frames()[0]\n",
      "games.head()\n",
      "135/22:\n",
      "games_home=games [games ['MATCHUP']=='GSW vs. TOR']\n",
      "games_away=games [games ['MATCHUP']=='GSW @ TOR']\n",
      "135/23: games_home.mean()['PLUS_MINUS']\n",
      "135/24: games_away.mean()['PLUS_MINUS']\n",
      "135/25:\n",
      "fig, ax = plt.subplots()\n",
      "\n",
      "games_away.plot(x='GAME_DATE',y='PLUS_MINUS', ax=ax)\n",
      "games_home.plot(x='GAME_DATE',y='PLUS_MINUS', ax=ax)\n",
      "ax.legend([\"away\", \"home\"])\n",
      "plt.show()\n",
      "136/1:\n",
      "import pandas as pd\n",
      "from bokeh.plotting import figure, output_file, show, output_notebook\n",
      "output_notebook()\n",
      "136/2:\n",
      "def make_dashboard(x, gdp_change, unemployment, title, file_name):\n",
      "    output_file(file_name)\n",
      "    p = figure(title=title, x_axis_label='year', y_axis_label='%')\n",
      "    p.line(x.squeeze(), gdp_change.squeeze(), color=\"firebrick\", line_width=4, legend=\"% GDP change\")\n",
      "    p.line(x.squeeze(), unemployment.squeeze(), line_width=4, legend=\"% unemployed\")\n",
      "    show(p)\n",
      "136/3:\n",
      "links={'GDP':'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/PY0101EN/projects/coursera_project/clean_gdp.csv',\\\n",
      "       'unemployment':'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/PY0101EN/projects/coursera_project/clean_unemployment.csv'}\n",
      "136/4:\n",
      "# Type your code here\n",
      "df_gdp = pd.read_csv(links['GDP'])\n",
      "136/5:\n",
      "# Type your code here\n",
      "df_gdp.head()\n",
      "136/6:\n",
      "# Type your code here\n",
      "df_unemployment = pd.read_csv(links['unemployment'])\n",
      "136/7:\n",
      "# Type your code here\n",
      "df_unemployment.head()\n",
      "136/8: df_unemployment.sort_values(by='unemployment')\n",
      "136/9: df_unemployment.sort_values(by='unemployment', ascending=False)\n",
      "136/10: df_unemployment.sort_values(by='unemployment', ascending=False)[:10]\n",
      "137/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "import folium\n",
      "137/2: path_to_file = 'Police_Department_Incidents_-_Previous_Year__2016_'\n",
      "137/3: df_incidents = pd.read_csv(path_to_file)\n",
      "137/4: path_to_file = 'Police_Department_Incidents_-_Previous_Year__2016_.csv'\n",
      "137/5: df_incidents = pd.read_csv(path_to_file)\n",
      "137/6: df_incidents.head()\n",
      "137/7: df_incidents.PdDistrict.unique()\n",
      "137/8: df_incidents.groupby(['PdDistrict']).count()\n",
      "137/9: df_by_district = df_incidents.groupby(['PdDistrict']).count()\n",
      "137/10: df_by_district.head()\n",
      "137/11: df_by_district\n",
      "137/12: df_by_district['Total'] = df_by_district['Location']\n",
      "137/13: df_by_district\n",
      "137/14: df_by_district = df_incidents.groupby(['PdDistrict']).count()\n",
      "137/15: df_by_district.reset_index\n",
      "137/16: df_by_district.reset_index()\n",
      "137/17: df_by_district.reset_index(inplace=True)\n",
      "137/18: df_by_district = df_by_district[['Neighborhood', 'Count']]\n",
      "137/19: df_by_district.rename(columns={'PdDistrict': 'Neighborhood'})\n",
      "137/20: df_by_district = df_by_district[['Neighborhood', 'Count']]\n",
      "137/21: df_by_district = df_incidents.groupby(['PdDistrict']).count()\n",
      "137/22: df_by_district['Count'] = df_by_district['Location']\n",
      "137/23: df_by_district.reset_index(inplace=True)\n",
      "137/24: df_by_district.rename(columns={'PdDistrict': 'Neighborhood'})\n",
      "137/25: df_by_district = df_by_district[['Neighborhood', 'Count']]\n",
      "137/26: df_by_district = df_by_district['Neighborhood', 'Count']\n",
      "137/27: df_by_district = df_by_district.rename(columns={'PdDistrict': 'Neighborhood'})\n",
      "137/28: df_by_district = df_by_district['Neighborhood', 'Count']\n",
      "137/29: df_by_district = df_by_district[['Neighborhood', 'Count']]\n",
      "137/30: df_by_district\n",
      "137/31: df_by_district = df_by_district.sort_values(by='Neighborhood')\n",
      "137/32: df_by_district\n",
      "137/33: df_by_district = df_by_district.sort_values(by='Count')\n",
      "137/34: df_by_district\n",
      "137/35: df_by_district = df_by_district[['Neighborhood', 'Count']]\n",
      "137/36: df_by_district\n",
      "137/37:\n",
      "latitude = 37.77\n",
      "longitude = -122.42\n",
      "sf_map = folium.map(location=[latitude, longitude], zoom_start=12, tiles='Mapbox Bright')\n",
      "137/38: sf_gjson_file = 'san-francisco.geojson'\n",
      "137/39:\n",
      "latitude = 37.77\n",
      "longitude = -122.42\n",
      "sf_map = folium.Map(location=[latitude, longitude], zoom_start=12, tiles='Mapbox Bright')\n",
      "137/40: sf_geodata = 'san-francisco.geojson'\n",
      "137/41:\n",
      "sf_map.choropleth(geo_data=sf_geodata,\n",
      "                 data=df_by_district,\n",
      "                 columns=['Neighborhood', 'Count'],\n",
      "                 key_on='feature.properties.DISTRICT',\n",
      "                 fill_color='YlOrRd',\n",
      "                 fill_opacity=0.7,\n",
      "                 line_opacity=0.2,\n",
      "                 )\n",
      "sf_map\n",
      "137/42:\n",
      "sf_map.choropleth(geo_data=sf_geodata,\n",
      "                 data=df_by_district,\n",
      "                 columns=['Neighborhood', 'Count'],\n",
      "                 key_on='feature.properties.DISTRICT',\n",
      "                 fill_color='YlOrRd',\n",
      "                 fill_opacity=0.7,\n",
      "                 line_opacity=0.2,\n",
      "                  legend_name='Crime rate in San Francisco'\n",
      "                 )\n",
      "sf_map\n",
      "138/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "import folium\n",
      "138/2: path_to_file = 'Police_Department_Incidents_-_Previous_Year__2016_.csv'\n",
      "138/3: df_incidents = pd.read_csv(path_to_file)\n",
      "138/4: df_incidents.head()\n",
      "138/5: df_by_district = df_incidents.groupby(['PdDistrict']).count()\n",
      "138/6: df_by_district['Count'] = df_by_district['Location']\n",
      "138/7: df_by_district.reset_index(inplace=True)\n",
      "138/8: df_by_district = df_by_district.rename(columns={'PdDistrict': 'Neighborhood'})\n",
      "138/9: df_by_district = df_by_district[['Neighborhood', 'Count']]\n",
      "138/10: df_by_district\n",
      "138/11: sf_geodata = 'san-francisco.geojson'\n",
      "138/12:\n",
      "latitude = 37.77\n",
      "longitude = -122.42\n",
      "sf_map = folium.Map(location=[latitude, longitude], zoom_start=12, tiles='Mapbox Bright')\n",
      "138/13:\n",
      "sf_map.choropleth(geo_data=sf_geodata,\n",
      "                 data=df_by_district,\n",
      "                 columns=['Neighborhood', 'Count'],\n",
      "                 key_on='feature.properties.DISTRICT',\n",
      "                 fill_color='YlOrRd',\n",
      "                 fill_opacity=0.7,\n",
      "                 line_opacity=0.2,\n",
      "                  legend_name='Crime rate in San Francisco'\n",
      "                 )\n",
      "sf_map\n",
      "138/14:\n",
      "latitude = 37.77\n",
      "longitude = -122.42\n",
      "sf_map = folium.Map(location=[latitude, longitude], zoom_start=12)\n",
      "138/15:\n",
      "sf_map.choropleth(geo_data=sf_geodata,\n",
      "                 data=df_by_district,\n",
      "                 columns=['Neighborhood', 'Count'],\n",
      "                 key_on='feature.properties.DISTRICT',\n",
      "                 fill_color='YlOrRd',\n",
      "                 fill_opacity=0.,\n",
      "                 line_opacity=0.2,\n",
      "                  legend_name='Crime rate in San Francisco'\n",
      "                 )\n",
      "sf_map\n",
      "138/16:\n",
      "sf_map.choropleth(geo_data=sf_geodata,\n",
      "                 data=df_by_district,\n",
      "                 columns=['Neighborhood', 'Count'],\n",
      "                 key_on='feature.properties.DISTRICT',\n",
      "                 fill_color='YlOrRd',\n",
      "                 fill_opacity=0.,\n",
      "                 line_opacity=0.2,\n",
      "                  legend_name='Crime rate in San Francisco'\n",
      "                 )\n",
      "sf_map\n",
      "138/17:\n",
      "sf_map.choropleth(geo_data=sf_geodata,\n",
      "                 data=df_by_district,\n",
      "                 columns=['Neighborhood', 'Count'],\n",
      "                 key_on='feature.properties.DISTRICT',\n",
      "                 fill_color='YlOrRd',\n",
      "                 fill_opacity=0.7,\n",
      "                 line_opacity=0.2,\n",
      "                  legend_name='Crime rate in San Francisco'\n",
      "                 )\n",
      "sf_map\n",
      "138/18:\n",
      "latitude = 37.77\n",
      "longitude = -122.42\n",
      "sf_map = folium.Map(location=[latitude, longitude], zoom_start=12)\n",
      "138/19:\n",
      "sf_map.choropleth(geo_data=sf_geodata,\n",
      "                 data=df_by_district,\n",
      "                 columns=['Neighborhood', 'Count'],\n",
      "                 key_on='feature.properties.DISTRICT',\n",
      "                 fill_color='YlOrRd',\n",
      "                 fill_opacity=0.7,\n",
      "                 line_opacity=0.2,\n",
      "                  legend_name='Crime rate in San Francisco'\n",
      "                 )\n",
      "sf_map\n",
      "139/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "139/2: print('Hello Capstone Project Course!')\n",
      "140/1: df = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')\n",
      "140/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "140/3: print('Hello Capstone Project Course!')\n",
      "140/4: df = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')\n",
      "140/5: df = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')\n",
      "141/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "141/2: print('Hello Capstone Project Course!')\n",
      "141/3: df = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')\n",
      "141/4: df\n",
      "141/5: df.columns\n",
      "141/6: type(df)\n",
      "141/7: df[0]\n",
      "141/8: df = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')[0]\n",
      "141/9: type(df)\n",
      "141/10: df[0]\n",
      "141/11: df\n",
      "141/12: df['Borough'].isnull().sum()\n",
      "141/13: df['Borough'].isnull().count()\n",
      "141/14: df['Borough'].isnull()\n",
      "141/15: df['Borough']=='Not assigned'\n",
      "141/16: df[df['Borough']=='Not assigned']\n",
      "141/17: df[df['Borough']=='Not assigned'].count()\n",
      "141/18: df[df['Borough']=='Not assigned'].sum()\n",
      "141/19: df[df['Borough']=='Not assigned']\n",
      "141/20: df[df['Borough']=='Not assigned'].shape\n",
      "141/21: df[~df['Borough']=='Not assigned']\n",
      "141/22: df[~df['Borough']=='Not assigned']\n",
      "141/23: df[~(df['Borough']=='Not assigned')]\n",
      "141/24: df[~(df['Borough']=='Not assigned')].isnull()\n",
      "141/25: df[~(df['Borough']=='Not assigned')].isnull().count()\n",
      "141/26: df = df[~(df['Borough']=='Not assigned')]\n",
      "141/27: df.isnull()\n",
      "141/28: df.isnull().sum()\n",
      "141/29: df = df[(df['Borough']=='Not assigned')]\n",
      "141/30: df.isnull().sum()\n",
      "141/31: df\n",
      "141/32: df = df[(df['Borough']=='Not assigned')]\n",
      "141/33: df.isnull().sum()\n",
      "141/34: df\n",
      "141/35: df = df[~(df['Borough']=='Not assigned')]\n",
      "141/36: df.isnull().sum()\n",
      "141/37: df\n",
      "141/38: df = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')[0]\n",
      "141/39: type(df)\n",
      "141/40: df\n",
      "141/41: df = df[~(df['Borough']=='Not assigned')]\n",
      "141/42: df.isnull().sum()\n",
      "141/43: df\n",
      "141/44: df.count_values()\n",
      "141/45: df.unique()\n",
      "141/46: df.PostalCode.unique()\n",
      "141/47: df.PostalCode.unique()\n",
      "141/48: df.PostalCode\n",
      "141/49: df['Postal Code'].unique()\n",
      "141/50: df['Postal Code'].unique().count()\n",
      "141/51: df['Postal Code'].unique().count_values()\n",
      "141/52: df['Postal Code'].value_counts()\n",
      "141/53: df['Postal Code'].value_counts()>1\n",
      "141/54: df[df['Postal Code'].value_counts()>1]\n",
      "141/55: df['Postal Code'].value_counts().sort()\n",
      "141/56: df['Postal Code'].value_counts().sorted()\n",
      "141/57: df['Postal Code'].value_counts().sort_values()\n",
      "141/58: df.loc['M5A']\n",
      "141/59: df[df['Postal Code']=='M5A']\n",
      "141/60: df_list = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')\n",
      "141/61: df_postal_codes = df_list[0]\n",
      "141/62: df_postal_code\n",
      "141/63: df_postal_codes\n",
      "141/64: >This method return *list*, and correct dataframe is it's first element\n",
      "141/65: df_postal_codes.head()\n",
      "141/66: df_list = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')\n",
      "141/67: df_postal_codes = df_list[0]\n",
      "141/68: df.postal_codes.isnull().sum()\n",
      "141/69: df_postal_codes.isnull().sum()\n",
      "141/70: df_postal_codes.isnull().count()\n",
      "141/71: df_postal_codes.isnull().sum()\n",
      "141/72: df_postal_codes['Neighborhood']\n",
      "141/73: df_postal_codes['Neighborhood']=='Not assigned'\n",
      "141/74: dfdf_[postal_codes['Neighborhood']=='Not assigned']\n",
      "141/75: df_postal_codes[df_postal_codes['Neighborhood']=='Not assigned']\n",
      "141/76: df_postal_codes[df_postal_codes['Neighborhood']=='Not assigned']\n",
      "141/77: df_postal_codes[df_postal_codes['Neighborhood']=='Not assigned'].count()\n",
      "141/78: df_postal_codes = df_postal_codes[~(df_postal_codes['Borough']=='Not assigned')]\n",
      "141/79: df_postal_codes.isnull().sum()\n",
      "141/80: df_postal_codes[df_postal_codes['Neighborhood']=='Not assigned'].count()\n",
      "141/81: df[df['Postal Code']=='M5A']\n",
      "141/82: df_postal_codes[df_postal_codes['Postal Code']=='M5A']\n",
      "141/83: df_postal_codes.shape\n",
      "141/84:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import geocoder\n",
      "141/85: g = geocoder.google('M5A, Toronto, Ontario')\n",
      "141/86: g\n",
      "141/87: g = geocoder.google('M5A, Toronto, Ontario')\n",
      "141/88: g\n",
      "141/89: g = geocoder.google('M1A, Toronto, Ontario')\n",
      "141/90: g\n",
      "141/91: g.json\n",
      "141/92: g = geocoder.google('M1A, Toronto, Ontario')\n",
      "141/93: g.json\n",
      "141/94:  g = geocoder.google('Mountain View, CA')\n",
      "141/95: g\n",
      "141/96: g.city\n",
      "141/97: g.__dir__\n",
      "141/98: g.__dir__()\n",
      "141/99: g.status\n",
      "141/100: g = geocoder.google('M1A, Toronto, Ontario')\n",
      "141/101: g.status\n",
      "141/102:  g = geocoder.google('Mountain View, CA')\n",
      "141/103: g.geojson\n",
      "141/104: g.geojson['features']\n",
      "141/105: g = geocoder.canadapost('M1A, Toronto, Ontario')\n",
      "141/106: g\n",
      "141/107: g.json\n",
      "141/108: g = geocoder.arcgis('M1A, Toronto, Ontario')\n",
      "141/109: g.json\n",
      "141/110:  g = geocoder.arcgis('M5A, Toronto, Ontario')\n",
      "141/111: g.geojson['features']\n",
      "141/112:\n",
      "for postal_code in df_postal_codes['Postal Codes']:\n",
      "    print(postal_code)\n",
      "141/113:\n",
      "for postal_code in df_postal_codes['Postal Code']:\n",
      "    print(postal_code)\n",
      "141/114:\n",
      "for postal_code in df_postal_codes['Postal Code']:\n",
      "    print(type(postal_code))\n",
      "141/115:\n",
      "for postal_code in df_postal_codes['Postal Code']:\n",
      "    print('{}, Toronto, Ontario'.format(postal_code))\n",
      "141/116:\n",
      "for postal_code in df_postal_codes['Postal Code']:\n",
      "    address = '{}, Toronto, Ontario'.format(postal_code)\n",
      "    postal_code_coordinates = geocoder.arcgis(address)\n",
      "    postal_codes_coordinates.append(postal_code_coordinates)\n",
      "141/117: postal_codes_coordinates=[]\n",
      "141/118:\n",
      "for postal_code in df_postal_codes['Postal Code']:\n",
      "    address = '{}, Toronto, Ontario'.format(postal_code)\n",
      "    postal_code_coordinates = geocoder.arcgis(address)\n",
      "    postal_codes_coordinates.append(postal_code_coordinates)\n",
      "141/119: postal_codes_coordinates\n",
      "141/120: df_postal_codes['Postal Code']\n",
      "141/121: df_postal_codes['Postal Code'][-30:-10]\n",
      "141/122: df_postal_codes['Postal Code'][-50:-10]\n",
      "141/123: len(postal_codes_coordinates)\n",
      "141/124: df_postal_codes['Postal Code']\n",
      "141/125: zipped_postal_codes_coordinates = list(zip(df_postal_codes['Postal Code'], postal_codes_coordinates)\n",
      "141/126: zipped_postal_codes_coordinates = list(zip(df_postal_codes['Postal Code'], postal_codes_coordinates))\n",
      "141/127: zipped_postal_codes_coordinates\n",
      "141/128: print(postal_codes_coordinates[0])\n",
      "141/129: postal_codes_coordinates[0]\n",
      "141/130: postal_codes_coordinates[0].geojson\n",
      "141/131:\n",
      "longitudes = []\n",
      "latitudes = []\n",
      "141/132: postal_codes_coordinates[0].geojson.keys()\n",
      "141/133: postal_codes_coordinates[0].geojson['features']   #.keys()\n",
      "141/134: postal_codes_coordinates[0].geojson['features']['coordinates']   #.keys()\n",
      "141/135: postal_codes_coordinates[0].geojson['features']   #.keys()\n",
      "141/136: len(postal_codes_coordinates[0].geojson['features'])   #.keys()\n",
      "141/137: postal_codes_coordinates[0].geojson['features'][0]['coordinates']   #.keys()\n",
      "141/138: postal_codes_coordinates[0].geojson['features'][0]['coordinate']   #.keys()\n",
      "141/139: postal_codes_coordinates[0].geojson['features'][0]#['coordinate']   #.keys()\n",
      "141/140: postal_codes_coordinates[0].geojson['features'][0].keys() #['coordinate']   #.keys()\n",
      "141/141: postal_codes_coordinates[0].geojson['features'][0]['geometry'] #['coordinate']   #.keys()\n",
      "141/142: postal_codes_coordinates[0].geojson['features'][0]['geometry']['coordinates'] #['coordinate']   #.keys()\n",
      "141/143:\n",
      "for coordinates in postal_codes_coordinates:\n",
      "    longitude = coordinates.geojson['features'][0]['geometry']['coordinates'][0]\n",
      "    longitudes.append(longitude)\n",
      "    \n",
      "    latitude = coordinates.geojson['features'][0]['geometry']['coordinates'][1]\n",
      "    latitudes.append(lattitude)\n",
      "141/144:\n",
      "for coordinates in postal_codes_coordinates:\n",
      "    longitude = coordinates.geojson['features'][0]['geometry']['coordinates'][0]\n",
      "    longitudes.append(longitude)\n",
      "    \n",
      "    latitude = coordinates.geojson['features'][0]['geometry']['coordinates'][1]\n",
      "    latitudes.append(latitude)\n",
      "141/145: longitudes\n",
      "141/146: df_postal_codes = df_postal_code.append(latitudes, axis=1)\n",
      "141/147: df_postal_codes = df_postal_codes.append(latitudes, axis=1)\n",
      "141/148: df_postal_codes['Latitude'] = pd.Series(latitudes)\n",
      "141/149: df_postal_codes\n",
      "141/150: df_postal_codes.loc[:,'Longitude'] = pd.Series(longitudes)\n",
      "141/151: df_postal_codes\n",
      "141/152: df_postal_codes.isnull().sum()\n",
      "141/153: latitudes\n",
      "141/154: len(latitudes)\n",
      "141/155: df_postal_codes.shape\n",
      "141/156: df_postal_codes\n",
      "141/157: df_postal_codes.iloc[-10:]\n",
      "141/158: df_postal_codes.iloc[-20:]\n",
      "141/159: df_postal_codes.iloc[-40:]\n",
      "141/160: postal_codes_coordinates=[]\n",
      "141/161:\n",
      "for postal_code in df_postal_codes['Postal Code']:\n",
      "    address = '{}, Toronto, Ontario'.format(postal_code)\n",
      "    postal_code_coordinates = geocoder.arcgis(address)\n",
      "    postal_codes_coordinates.append(postal_code_coordinates)\n",
      "141/162: len(postal_codes_coordinates)\n",
      "141/163: df_postal_codes['Postal Code']\n",
      "141/164: zipped_postal_codes_coordinates = list(zip(df_postal_codes['Postal Code'], postal_codes_coordinates))\n",
      "141/165: postal_codes_coordinates[0].geojson['features'][0]['geometry']['coordinates'] #['coordinate']   #.keys()\n",
      "141/166:\n",
      "longitudes = []\n",
      "latitudes = []\n",
      "141/167:\n",
      "for coordinates in postal_codes_coordinates:\n",
      "    longitude = coordinates.geojson['features'][0]['geometry']['coordinates'][0]\n",
      "    longitudes.append(longitude)\n",
      "    \n",
      "    latitude = coordinates.geojson['features'][0]['geometry']['coordinates'][1]\n",
      "    latitudes.append(latitude)\n",
      "141/168: df_postal_codes['Latitude'] = pd.Series(latitudes)\n",
      "141/169: df_postal_codes.loc[:,'Longitude'] = pd.Series(longitudes)\n",
      "141/170: df_postal_codes.isnull().sum()\n",
      "141/171:  g = geocoder.baidu('M5A, Toronto, Ontario')\n",
      "141/172:  g = geocoder.geocodefarm('M5A, Toronto, Ontario')\n",
      "141/173: g.geojson['features']\n",
      "141/174:  g = geocoder.geocodefarm('M5A, Toronto, Ontario')\n",
      "141/175:  g = geocoder.geolytica('M5A, Toronto, Ontario')\n",
      "141/176: g.geojson['features']\n",
      "141/177: postal_codes_coordinates=[]\n",
      "141/178:\n",
      "for postal_code in df_postal_codes['Postal Code']:\n",
      "    address = '{}, Toronto, Ontario'.format(postal_code)\n",
      "    postal_code_coordinates = geocoder.arcgis(address)\n",
      "    postal_codes_coordinates.append(postal_code_coordinates)\n",
      "141/179: len(postal_codes_coordinates)\n",
      "141/180: df_postal_codes['Postal Code']\n",
      "141/181: zipped_postal_codes_coordinates = list(zip(df_postal_codes['Postal Code'], postal_codes_coordinates))\n",
      "141/182: postal_codes_coordinates[0].geojson['features'][0]['geometry']['coordinates'] #['coordinate']   #.keys()\n",
      "141/183:\n",
      "longitudes = []\n",
      "latitudes = []\n",
      "141/184:\n",
      "for coordinates in postal_codes_coordinates:\n",
      "    longitude = coordinates.geojson['features'][0]['geometry']['coordinates'][0]\n",
      "    longitudes.append(longitude)\n",
      "    \n",
      "    latitude = coordinates.geojson['features'][0]['geometry']['coordinates'][1]\n",
      "    latitudes.append(latitude)\n",
      "141/185: df_postal_codes['Latitude'] = pd.Series(latitudes)\n",
      "141/186: df_postal_codes.loc[:,'Longitude'] = pd.Series(longitudes)\n",
      "141/187: df_postal_codes.isnull().sum()\n",
      "141/188: len(latitudes)\n",
      "141/189: df_postal_codes.iloc[-40:]\n",
      "141/190:  g = geocoder.geolytica('M5P, Toronto, Ontario')\n",
      "141/191: g.geojson['features']\n",
      "141/192:  g = geocoder.geolytica('M6P, Toronto, Ontario')\n",
      "141/193: g.geojson['features']\n",
      "141/194:  g = geocoder.geolytica('M5P, Toronto, Ontario')\n",
      "141/195: g.geojson['features']\n",
      "141/196:  g = geocoder.geolytica('M7R, Toronto, Ontario')\n",
      "141/197: g.geojson['features']\n",
      "141/198: postal_codes_coordinates[100].geojson['features'][0]['geometry']['coordinates'] #['coordinate']   #.keys()\n",
      "141/199: postal_codes_coordinates[101].geojson['features'][0]['geometry']['coordinates'] #['coordinate']   #.keys()\n",
      "141/200: postal_codes_coordinates[80].geojson['features'][0]['geometry']['coordinates'] #['coordinate']   #.keys()\n",
      "141/201: df_postal_codes.reset_index(inplace=True)\n",
      "141/202: df_postal_codes['Latitude'] = pd.Series(latitudes)\n",
      "141/203: df_postal_codes.loc[:,'Longitude'] = pd.Series(longitudes)\n",
      "141/204: df_postal_codes.isnull().sum()\n",
      "141/205: len(latitudes)\n",
      "141/206: df_postal_codes.iloc[-40:]\n",
      "141/207: df_postal_codes.to_csv('Toronto_postal_codes.csv')\n",
      "141/208: pd.DataFrame(Series(latitudes))\n",
      "141/209: pd.DataFrame(pd.Series(latitudes))\n",
      "141/210: df_postal_codes\n",
      "141/211: df_postal_codes.drop(columns=['Latitude', 'Longitutde'])\n",
      "141/212: df_postal_codes.drop(columns=['Latitude', 'Longitude'])\n",
      "141/213: df_test = df_postal_codes.drop(columns=['Latitude', 'Longitude'])\n",
      "141/214: df_test\n",
      "141/215: df_test = df_postal_codes.drop(columns=['Latitude', 'Longitude', 'index'])\n",
      "141/216: df_test\n",
      "141/217: df_test.columns\n",
      "141/218: df_test.columns = ['x','y','z']\n",
      "141/219: df_test\n",
      "141/220: df_test.columns = ['x','y','z','h']\n",
      "141/221: df_test.reindex(columns = ['x','y','z','h'])\n",
      "141/222: df_test['h'] = Longitudes\n",
      "141/223: df_test['h'] = longitudes\n",
      "141/224: df_test\n",
      "141/225: df_test.reindex(columns = ['x','y','z','h','l'])\n",
      "141/226: df_test[['h','l']] = [longitudes, latitudes]\n",
      "141/227: df_long = pd.DataFrame(pd.Series(longitudes))\n",
      "141/228: df_test.append(df_long)\n",
      "141/229: df_test\n",
      "141/230: df_test.append(df_long, inplace=True)\n",
      "141/231: df_3 = df_test.append(df_long)\n",
      "141/232: df_3\n",
      "141/233: df_test = df_postal_codes.drop(columns=['Latitude', 'Longitude', 'index'])\n",
      "141/234: df_test.reindex(columns = ['x','y','z','h','l'])\n",
      "141/235: df_test = df_postal_codes.drop(columns=['Latitude', 'Longitude', 'index'])\n",
      "141/236: df_test.reindex(columns = ['x','y','z','h','l'])\n",
      "141/237: df_postal_codes\n",
      "141/238: df_test = df_postal_codes.drop(columns=['Latitude', 'Longitude', 'index'])\n",
      "141/239: df_test.reindex(columns = df_test.columns+['h','l'])\n",
      "141/240: df_test.columns\n",
      "141/241: df_test.columns + ['a']\n",
      "141/242: df_test.columns.append('a')\n",
      "141/243: df_test['a']=np.NaN\n",
      "141/244: df_test['a']=longitudes\n",
      "141/245: df_test['r']=longitudes\n",
      "141/246: df_postal_codes['Latitude'] = latitudes\n",
      "141/247: df_postal_codes.loc[:,['Longitude']] = pd.Series(longitudes)\n",
      "141/248: df_postal_codes.loc[:,['Longitude']] = longitudes\n",
      "141/249: df_postal_codes[:,['Latitude']] = latitudes\n",
      "141/250: df_postal_codes.loc[:,['Latitude']] = latitudes\n",
      "141/251: g.geojson\n",
      "141/252: postal_codes_coordinates[0].geojson['features'][0]['geometry']['coordinates'] #['coordinate']   #.keys()\n",
      "141/253: df_postal_codes\n",
      "141/254: postal_codes_coordinates[0].geojson['features'][0]['geometry']['coordinates']\n",
      "141/255: df_postal_codes.loc[df_postal_codes['Postal Code'].isin['M5G', 'M2H','M4M','M9L', 'M1B']]\n",
      "141/256: df_postal_codes.loc[df_postal_codes['Postal Code'].isin(['M5G', 'M2H','M4M','M9L', 'M1B'])]\n",
      "141/257: postal_codes_coordinates[1]\n",
      "141/258: x = geocoder.geoname('M1B, Toronto, Ontario')\n",
      "141/259: x = geocoder.geonames('M1B, Toronto, Ontario')\n",
      "141/260: x = geocoder.here('M1B, Toronto, Ontario')\n",
      "141/261: x = geocoder.osm('M1B, Toronto, Ontario')\n",
      "141/262: x\n",
      "141/263: x.geojson\n",
      "141/264: x = geocoder.yahoo('M1B, Toronto, Ontario')\n",
      "141/265: x = geocoder.yahoo('M1B, Toronto, Ontario')\n",
      "141/266: x = geocoder.yandex('M1B, Toronto, Ontario')\n",
      "141/267: x = geocoder.tgos('M1B, Toronto, Ontario')\n",
      "141/268: x = geocoder.canadapost('M1B, Toronto, Ontario')\n",
      "141/269: x.geojson\n",
      "141/270: x = geocoder.google('M1B, Toronto, Ontario')\n",
      "141/271: x.geojson\n",
      "141/272: x\n",
      "141/273:  g = geocoder.geolytica('M7R, Toronto, Ontario')\n",
      "141/274: g.geojson\n",
      "141/275:  g = geocoder.geolytica('M1B, Toronto, Ontario')\n",
      "141/276: g.geojson\n",
      "141/277:  g = geocoder.arcgis('M1B, Toronto, Ontario')\n",
      "141/278: g.geojson\n",
      "141/279:  g = geocoder.arcgis('M5G, Toronto, Ontario')\n",
      "141/280: g.geojson\n",
      "141/281:\n",
      "toronto_coordinates = ['43.7325', '-79.3993']\n",
      "map_toronto = folium.Map(location=toronto_coordinates, zoom_start=10)\n",
      "\n",
      "# add markers to map\n",
      "for lat, lng, postal_code, borough, neighborhood in zip(df_postal_codes['Latitude'], df_postal_codes['Longitude'],df_postal_codes['Postal Code'], df_postal_codes['Borough'], df_postal_codes['Neighborhood']):\n",
      "    label = '{}, {}, {}'.format(postal_code,neighborhood, borough)\n",
      "    label = folium.Popup(label, parse_html=True)\n",
      "    folium.CircleMarker(\n",
      "        [lat, lng],\n",
      "        radius=5,\n",
      "        popup=label,\n",
      "        color='blue',\n",
      "        fill=True,\n",
      "        fill_color='#3186cc',\n",
      "        fill_opacity=0.7,\n",
      "        parse_html=False).add_to(map_tronto)  \n",
      "    \n",
      "map_toronto\n",
      "141/282:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import geocoder\n",
      "import folium\n",
      "141/283:\n",
      "toronto_coordinates = ['43.7325', '-79.3993']\n",
      "map_toronto = folium.Map(location=toronto_coordinates, zoom_start=10)\n",
      "\n",
      "# add markers to map\n",
      "for lat, lng, postal_code, borough, neighborhood in zip(df_postal_codes['Latitude'], df_postal_codes['Longitude'],df_postal_codes['Postal Code'], df_postal_codes['Borough'], df_postal_codes['Neighborhood']):\n",
      "    label = '{}, {}, {}'.format(postal_code,neighborhood, borough)\n",
      "    label = folium.Popup(label, parse_html=True)\n",
      "    folium.CircleMarker(\n",
      "        [lat, lng],\n",
      "        radius=5,\n",
      "        popup=label,\n",
      "        color='blue',\n",
      "        fill=True,\n",
      "        fill_color='#3186cc',\n",
      "        fill_opacity=0.7,\n",
      "        parse_html=False).add_to(map_tronto)  \n",
      "    \n",
      "map_toronto\n",
      "141/284:\n",
      "toronto_coordinates = [43.7325, -79.3993]\n",
      "map_toronto = folium.Map(location=toronto_coordinates, zoom_start=10)\n",
      "\n",
      "# add markers to map\n",
      "for lat, lng, postal_code, borough, neighborhood in zip(df_postal_codes['Latitude'], df_postal_codes['Longitude'],df_postal_codes['Postal Code'], df_postal_codes['Borough'], df_postal_codes['Neighborhood']):\n",
      "    label = '{}, {}, {}'.format(postal_code,neighborhood, borough)\n",
      "    label = folium.Popup(label, parse_html=True)\n",
      "    folium.CircleMarker(\n",
      "        [lat, lng],\n",
      "        radius=5,\n",
      "        popup=label,\n",
      "        color='blue',\n",
      "        fill=True,\n",
      "        fill_color='#3186cc',\n",
      "        fill_opacity=0.7,\n",
      "        parse_html=False).add_to(map_tronto)  \n",
      "    \n",
      "map_toronto\n",
      "141/285:\n",
      "toronto_coordinates = [43.7325, -79.3993]\n",
      "map_toronto = folium.Map(location=toronto_coordinates, zoom_start=10)\n",
      "\n",
      "# add markers to map\n",
      "for lat, lng, postal_code, borough, neighborhood in zip(df_postal_codes['Latitude'], df_postal_codes['Longitude'],df_postal_codes['Postal Code'], df_postal_codes['Borough'], df_postal_codes['Neighborhood']):\n",
      "    label = '{}, {}, {}'.format(postal_code,neighborhood, borough)\n",
      "    label = folium.Popup(label, parse_html=True)\n",
      "    folium.CircleMarker(\n",
      "        [lat, lng],\n",
      "        radius=5,\n",
      "        popup=label,\n",
      "        color='blue',\n",
      "        fill=True,\n",
      "        fill_color='#3186cc',\n",
      "        fill_opacity=0.7,\n",
      "        parse_html=False).add_to(map_toronto)  \n",
      "    \n",
      "map_toronto\n",
      "141/286: postal_codes_coordinates[0].geojson\n",
      "141/287:  g = geocoder.arcgis('Scarborough, Ontario M1J')\n",
      "141/288: g.geojson\n",
      "141/289: postal_codes_coordinates[0].geojson['features'][0]['properties']['quality']\n",
      "141/290:\n",
      "for z in postal_codes_coordinates:\n",
      "    print(z.geojson['features'][0]['properties']['quality'])\n",
      "141/291:\n",
      "for i,z in enumerate(postal_codes_coordinates):\n",
      "    print(i,' ',z.geojson['features'][0]['properties']['quality'])\n",
      "141/292:\n",
      "pcc = []\n",
      "for n,z in zip(df_postal_codes['Neighborhood'], df_postal_codes['Postal Code']:\n",
      "    address = '{}, Ontario, {}'.format(n, z)\n",
      "    geojson = geocoder.arcgis(address)           \n",
      "    pcc.append(geojson)\n",
      "141/293:\n",
      "pcc = []\n",
      "for n,z in zip(df_postal_codes['Neighborhood'], df_postal_codes['Postal Code']):\n",
      "    address = '{}, Ontario, {}'.format(n, z)\n",
      "    geojson = geocoder.arcgis(address)           \n",
      "    pcc.append(geojson)\n",
      "141/294: pcc\n",
      "141/295:\n",
      "for i,z in enumerate(pcc):\n",
      "    print(i,' ',z.geojson['features'][0]['properties']['quality'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/296:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import geocoder\n",
      "import folium\n",
      "import os\n",
      "141/297:\n",
      "CLIENT_ID = os.environ.get('FOURSQAUREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 50\n",
      "141/298:\n",
      "def get_nearby_venues(neighborhood, latitudes, longitudes, radius=500):\n",
      "    venues = []\n",
      "    \n",
      "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
      "    print(name)\n",
      "\n",
      "    # create the API request URL\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "        CLIENT_ID, \n",
      "        CLIENT_SECRET, \n",
      "        VERSION, \n",
      "        lat, \n",
      "        lng, \n",
      "        radius, \n",
      "        LIMIT)\n",
      "    \n",
      "    results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
      "\n",
      "    venues_list.append([(\n",
      "        name, \n",
      "        lat, \n",
      "        lng, \n",
      "        v['venue']['name'], \n",
      "        v['venue']['location']['lat'], \n",
      "        v['venue']['location']['lng'],  \n",
      "        v['venue']['categories'][0]['name']) for v in results])\n",
      "\n",
      "nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
      "nearby_venues.columns = ['Neighborhood', \n",
      "              'Neighborhood Latitude', \n",
      "              'Neighborhood Longitude', \n",
      "              'Venue', \n",
      "              'Venue Latitude', \n",
      "              'Venue Longitude', \n",
      "              'Venue Category']\n",
      "\n",
      "return(nearby_venues)\n",
      "141/299:\n",
      "def get_nearby_venues(neighborhood, latitudes, longitudes, radius=500):\n",
      "    venues = []\n",
      "    \n",
      "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
      "        print(name)\n",
      "\n",
      "        # create the API request URL\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            radius, \n",
      "            LIMIT)\n",
      "\n",
      "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
      "\n",
      "        venues_list.append([(\n",
      "            name, \n",
      "            lat, \n",
      "            lng, \n",
      "            v['venue']['name'], \n",
      "            v['venue']['location']['lat'], \n",
      "            v['venue']['location']['lng'],  \n",
      "            v['venue']['categories'][0]['name']) for v in results])\n",
      "\n",
      "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
      "    nearby_venues.columns = ['Neighborhood', \n",
      "              'Neighborhood Latitude', \n",
      "              'Neighborhood Longitude', \n",
      "              'Venue', \n",
      "              'Venue Latitude', \n",
      "              'Venue Longitude', \n",
      "              'Venue Category']\n",
      "\n",
      "    return(nearby_venues)\n",
      "141/300:\n",
      "toronto_venues = getNearbyVenues(names=df_postal_codes['Neighborhood'],\n",
      "                                   latitudes=df_postal_codes['Latitude'],\n",
      "                                   longitudes=df_postal_codes['Longitude']\n",
      "                                  )\n",
      "141/301:\n",
      "toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'],\n",
      "                                   latitudes=df_postal_codes['Latitude'],\n",
      "                                   longitudes=df_postal_codes['Longitude']\n",
      "                                  )\n",
      "141/302:\n",
      "def get_nearby_venues(names, latitudes, longitudes, radius=500):\n",
      "    venues = []\n",
      "    \n",
      "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
      "        print(name)\n",
      "\n",
      "        # create the API request URL\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            radius, \n",
      "            LIMIT)\n",
      "\n",
      "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
      "\n",
      "        venues_list.append([(\n",
      "            name, \n",
      "            lat, \n",
      "            lng, \n",
      "            v['venue']['name'], \n",
      "            v['venue']['location']['lat'], \n",
      "            v['venue']['location']['lng'],  \n",
      "            v['venue']['categories'][0]['name']) for v in results])\n",
      "\n",
      "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
      "    nearby_venues.columns = ['Neighborhood', \n",
      "              'Neighborhood Latitude', \n",
      "              'Neighborhood Longitude', \n",
      "              'Venue', \n",
      "              'Venue Latitude', \n",
      "              'Venue Longitude', \n",
      "              'Venue Category']\n",
      "\n",
      "    return(nearby_venues)\n",
      "141/303:\n",
      "toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'],\n",
      "                                   latitudes=df_postal_codes['Latitude'],\n",
      "                                   longitudes=df_postal_codes['Longitude']\n",
      "                                  )\n",
      "141/304:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import geocoder\n",
      "import folium\n",
      "import os\n",
      "import requests\n",
      "141/305:\n",
      "toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'],\n",
      "                                   latitudes=df_postal_codes['Latitude'],\n",
      "                                   longitudes=df_postal_codes['Longitude']\n",
      "                                  )\n",
      "141/306:\n",
      "toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'][:1],\n",
      "                                   latitudes=df_postal_codes['Latitude'][:1],\n",
      "                                   longitudes=df_postal_codes['Longitude'][:1]\n",
      "                                  )\n",
      "141/307:\n",
      "def get_nearby_venues(names, latitudes, longitudes, radius=500):\n",
      "    venues = []\n",
      "    \n",
      "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
      "        print(name)\n",
      "\n",
      "        # create the API request URL\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            radius, \n",
      "            LIMIT)\n",
      "\n",
      "        results = requests.get(url).json()#[\"response\"]['groups'][0]['items']\n",
      "        print(result)\n",
      "        \n",
      "        venues_list.append([(\n",
      "            name, \n",
      "            lat, \n",
      "            lng, \n",
      "            v['venue']['name'], \n",
      "            v['venue']['location']['lat'], \n",
      "            v['venue']['location']['lng'],  \n",
      "            v['venue']['categories'][0]['name']) for v in results])\n",
      "\n",
      "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
      "    nearby_venues.columns = ['Neighborhood', \n",
      "              'Neighborhood Latitude', \n",
      "              'Neighborhood Longitude', \n",
      "              'Venue', \n",
      "              'Venue Latitude', \n",
      "              'Venue Longitude', \n",
      "              'Venue Category']\n",
      "\n",
      "    return(nearby_venues)\n",
      "141/308:\n",
      "toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'][:1],\n",
      "                                   latitudes=df_postal_codes['Latitude'][:1],\n",
      "                                   longitudes=df_postal_codes['Longitude'][:1]\n",
      "                                  )\n",
      "141/309:\n",
      "def get_nearby_venues(names, latitudes, longitudes, radius=500):\n",
      "    venues = []\n",
      "    \n",
      "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
      "        print(name)\n",
      "\n",
      "        # create the API request URL\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            radius, \n",
      "            LIMIT)\n",
      "\n",
      "        results = requests.get(url).json()#[\"response\"]['groups'][0]['items']\n",
      "        print(results)\n",
      "        \n",
      "        venues_list.append([(\n",
      "            name, \n",
      "            lat, \n",
      "            lng, \n",
      "            v['venue']['name'], \n",
      "            v['venue']['location']['lat'], \n",
      "            v['venue']['location']['lng'],  \n",
      "            v['venue']['categories'][0]['name']) for v in results])\n",
      "\n",
      "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
      "    nearby_venues.columns = ['Neighborhood', \n",
      "              'Neighborhood Latitude', \n",
      "              'Neighborhood Longitude', \n",
      "              'Venue', \n",
      "              'Venue Latitude', \n",
      "              'Venue Longitude', \n",
      "              'Venue Category']\n",
      "\n",
      "    return(nearby_venues)\n",
      "141/310:\n",
      "toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'][:1],\n",
      "                                   latitudes=df_postal_codes['Latitude'][:1],\n",
      "                                   longitudes=df_postal_codes['Longitude'][:1]\n",
      "                                  )\n",
      "141/311: print(CLIENT_ID)\n",
      "141/312:\n",
      "CLIENT_ID = os.environ.get('FOURSQAUREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 50\n",
      "141/313: print(CLIENT_ID)\n",
      "141/314:\n",
      "CLIENT_ID = os.environ.get('FOURSQAUREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 50\n",
      "141/315: print(CLIENT_ID)\n",
      "141/316: print(CLIENT_ID)\n",
      "142/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import geocoder\n",
      "import folium\n",
      "import os\n",
      "import requests\n",
      "142/2: df_list = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')\n",
      "142/3: df_postal_codes = df_list[0]\n",
      "142/4: df_postal_codes.head()\n",
      "142/5: df_postal_codes = df_postal_codes[~(df_postal_codes['Borough']=='Not assigned')]\n",
      "142/6: df_postal_codes.isnull().sum()\n",
      "142/7: df_postal_codes[df_postal_codes['Neighborhood']=='Not assigned'].count()\n",
      "142/8: df_postal_codes[df_postal_codes['Postal Code']=='M5A']\n",
      "142/9: df_postal_codes.shape\n",
      "142/10:  g = geocoder.arcgis('M5G, Toronto, Ontario')\n",
      "142/11: g.geojson\n",
      "142/12: postal_codes_coordinates[0].geojson['features'][0]['geometry']['coordinates']\n",
      "142/13: postal_codes_coordinates=[]\n",
      "142/14:\n",
      "for postal_code in df_postal_codes['Postal Code']:\n",
      "    address = '{}, Toronto, Ontario'.format(postal_code)\n",
      "    postal_code_coordinates = geocoder.arcgis(address)\n",
      "    postal_codes_coordinates.append(postal_code_coordinates)\n",
      "142/15: len(postal_codes_coordinates)\n",
      "142/16: df_postal_codes['Postal Code']\n",
      "142/17:\n",
      "longitudes = []\n",
      "latitudes = []\n",
      "142/18:\n",
      "for coordinates in postal_codes_coordinates:\n",
      "    longitude = coordinates.geojson['features'][0]['geometry']['coordinates'][0]\n",
      "    longitudes.append(longitude)\n",
      "    \n",
      "    latitude = coordinates.geojson['features'][0]['geometry']['coordinates'][1]\n",
      "    latitudes.append(latitude)\n",
      "142/19: df_postal_codes.reset_index(inplace=True)\n",
      "142/20: df_postal_codes.loc[:,['Latitude']] = latitudes\n",
      "142/21: df_postal_codes[['Latitude']] = latitudes\n",
      "142/22: df_postal_codes['Latitude'] = latitudes\n",
      "142/23: df_postal_codes.loc[:,['Longitude']] = longitudes\n",
      "142/24: df_postal_codes.loc['Longitude'] = longitudes\n",
      "142/25: df_postal_codes['Longitude'] = longitudes\n",
      "142/26: df_postal_codes.isnull().sum()\n",
      "142/27: df_postal_codes\n",
      "142/28: df_postal_codes.to_csv('Toronto_postal_codes.csv')\n",
      "142/29: df_postal_codes.loc[df_postal_codes['Postal Code'].isin(['M5G', 'M2H','M4M','M9L', 'M1B'])]\n",
      "142/30:\n",
      "toronto_coordinates = [43.7325, -79.3993]\n",
      "map_toronto = folium.Map(location=toronto_coordinates, zoom_start=10)\n",
      "\n",
      "# add markers to map\n",
      "for lat, lng, postal_code, borough, neighborhood in zip(df_postal_codes['Latitude'], df_postal_codes['Longitude'],df_postal_codes['Postal Code'], df_postal_codes['Borough'], df_postal_codes['Neighborhood']):\n",
      "    label = '{}, {}, {}'.format(postal_code,neighborhood, borough)\n",
      "    label = folium.Popup(label, parse_html=True)\n",
      "    folium.CircleMarker(\n",
      "        [lat, lng],\n",
      "        radius=5,\n",
      "        popup=label,\n",
      "        color='blue',\n",
      "        fill=True,\n",
      "        fill_color='#3186cc',\n",
      "        fill_opacity=0.7,\n",
      "        parse_html=False).add_to(map_toronto)  \n",
      "    \n",
      "map_toronto\n",
      "142/31:\n",
      "CLIENT_ID = os.environ.get('FOURSQAUREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 50\n",
      "142/32:\n",
      "def get_nearby_venues(names, latitudes, longitudes, radius=500):\n",
      "    venues = []\n",
      "    \n",
      "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
      "        print(name)\n",
      "\n",
      "        # create the API request URL\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            radius, \n",
      "            LIMIT)\n",
      "\n",
      "        results = requests.get(url).json()#[\"response\"]['groups'][0]['items']\n",
      "        print(results)\n",
      "        \n",
      "        venues_list.append([(\n",
      "            name, \n",
      "            lat, \n",
      "            lng, \n",
      "            v['venue']['name'], \n",
      "            v['venue']['location']['lat'], \n",
      "            v['venue']['location']['lng'],  \n",
      "            v['venue']['categories'][0]['name']) for v in results])\n",
      "\n",
      "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
      "    nearby_venues.columns = ['Neighborhood', \n",
      "              'Neighborhood Latitude', \n",
      "              'Neighborhood Longitude', \n",
      "              'Venue', \n",
      "              'Venue Latitude', \n",
      "              'Venue Longitude', \n",
      "              'Venue Category']\n",
      "\n",
      "    return(nearby_venues)\n",
      "142/33:\n",
      "def get_nearby_venues(names, latitudes, longitudes, radius=500):\n",
      "    venues_list = []\n",
      "    \n",
      "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
      "        print(name)\n",
      "\n",
      "        # create the API request URL\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            radius, \n",
      "            LIMIT)\n",
      "\n",
      "        results = requests.get(url).json()#[\"response\"]['groups'][0]['items']\n",
      "        print(results)\n",
      "        \n",
      "        venues_list.append([(\n",
      "            name, \n",
      "            lat, \n",
      "            lng, \n",
      "            v['venue']['name'], \n",
      "            v['venue']['location']['lat'], \n",
      "            v['venue']['location']['lng'],  \n",
      "            v['venue']['categories'][0]['name']) for v in results])\n",
      "\n",
      "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
      "    nearby_venues.columns = ['Neighborhood', \n",
      "              'Neighborhood Latitude', \n",
      "              'Neighborhood Longitude', \n",
      "              'Venue', \n",
      "              'Venue Latitude', \n",
      "              'Venue Longitude', \n",
      "              'Venue Category']\n",
      "\n",
      "    return(nearby_venues)\n",
      "142/34:\n",
      "toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'][:1],\n",
      "                                   latitudes=df_postal_codes['Latitude'][:1],\n",
      "                                   longitudes=df_postal_codes['Longitude'][:1]\n",
      "                                  )\n",
      "142/35: print(CLIENT_ID)\n",
      "143/1:\n",
      "CLIENT_ID = os.environ.get('FOURSQAUREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 50\n",
      "143/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import geocoder\n",
      "import folium\n",
      "import os\n",
      "import requests\n",
      "144/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import geocoder\n",
      "import folium\n",
      "import os\n",
      "import requests\n",
      "144/2:\n",
      "CLIENT_ID = os.environ.get('FOURSQAUREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 50\n",
      "144/3: print(CLIENT_ID)\n",
      "144/4:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 50\n",
      "144/5: print(CLIENT_ID)\n",
      "144/6:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import geocoder\n",
      "import folium\n",
      "import os\n",
      "import requests\n",
      "144/7: df_list = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')\n",
      "144/8: df_postal_codes = df_list[0]\n",
      "144/9: df_postal_codes.head()\n",
      "144/10: df_postal_codes = df_postal_codes[~(df_postal_codes['Borough']=='Not assigned')]\n",
      "144/11: df_postal_codes.isnull().sum()\n",
      "144/12: df_postal_codes[df_postal_codes['Neighborhood']=='Not assigned'].count()\n",
      "144/13: df_postal_codes[df_postal_codes['Postal Code']=='M5A']\n",
      "144/14: df_postal_codes.shape\n",
      "144/15:  g = geocoder.arcgis('M5G, Toronto, Ontario')\n",
      "144/16: g.geojson\n",
      "144/17: postal_codes_coordinates=[]\n",
      "144/18:\n",
      "for postal_code in df_postal_codes['Postal Code']:\n",
      "    address = '{}, Toronto, Ontario'.format(postal_code)\n",
      "    postal_code_coordinates = geocoder.arcgis(address)\n",
      "    postal_codes_coordinates.append(postal_code_coordinates)\n",
      "144/19: len(postal_codes_coordinates)\n",
      "144/20: df_postal_codes['Postal Code']\n",
      "144/21:\n",
      "longitudes = []\n",
      "latitudes = []\n",
      "144/22:\n",
      "for coordinates in postal_codes_coordinates:\n",
      "    longitude = coordinates.geojson['features'][0]['geometry']['coordinates'][0]\n",
      "    longitudes.append(longitude)\n",
      "    \n",
      "    latitude = coordinates.geojson['features'][0]['geometry']['coordinates'][1]\n",
      "    latitudes.append(latitude)\n",
      "144/23: df_postal_codes.reset_index(inplace=True)\n",
      "144/24: df_postal_codes['Latitude'] = latitudes\n",
      "144/25: df_postal_codes['Longitude'] = longitudes\n",
      "144/26: df_postal_codes.isnull().sum()\n",
      "144/27: df_postal_codes\n",
      "144/28: df_postal_codes.to_csv('Toronto_postal_codes.csv')\n",
      "144/29: df_postal_codes.loc[df_postal_codes['Postal Code'].isin(['M5G', 'M2H','M4M','M9L', 'M1B'])]\n",
      "144/30:\n",
      "toronto_coordinates = [43.7325, -79.3993]\n",
      "map_toronto = folium.Map(location=toronto_coordinates, zoom_start=10)\n",
      "\n",
      "# add markers to map\n",
      "for lat, lng, postal_code, borough, neighborhood in zip(df_postal_codes['Latitude'], df_postal_codes['Longitude'],df_postal_codes['Postal Code'], df_postal_codes['Borough'], df_postal_codes['Neighborhood']):\n",
      "    label = '{}, {}, {}'.format(postal_code,neighborhood, borough)\n",
      "    label = folium.Popup(label, parse_html=True)\n",
      "    folium.CircleMarker(\n",
      "        [lat, lng],\n",
      "        radius=5,\n",
      "        popup=label,\n",
      "        color='blue',\n",
      "        fill=True,\n",
      "        fill_color='#3186cc',\n",
      "        fill_opacity=0.7,\n",
      "        parse_html=False).add_to(map_toronto)  \n",
      "    \n",
      "map_toronto\n",
      "144/31:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 50\n",
      "144/32:\n",
      "def get_nearby_venues(names, latitudes, longitudes, radius=500):\n",
      "    venues_list = []\n",
      "    \n",
      "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
      "        print(name)\n",
      "\n",
      "        # create the API request URL\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            radius, \n",
      "            LIMIT)\n",
      "\n",
      "        results = requests.get(url).json()#[\"response\"]['groups'][0]['items']\n",
      "        print(results)\n",
      "        \n",
      "        venues_list.append([(\n",
      "            name, \n",
      "            lat, \n",
      "            lng, \n",
      "            v['venue']['name'], \n",
      "            v['venue']['location']['lat'], \n",
      "            v['venue']['location']['lng'],  \n",
      "            v['venue']['categories'][0]['name']) for v in results])\n",
      "\n",
      "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
      "    nearby_venues.columns = ['Neighborhood', \n",
      "              'Neighborhood Latitude', \n",
      "              'Neighborhood Longitude', \n",
      "              'Venue', \n",
      "              'Venue Latitude', \n",
      "              'Venue Longitude', \n",
      "              'Venue Category']\n",
      "\n",
      "    return(nearby_venues)\n",
      "144/33:\n",
      "toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'][:1],\n",
      "                                   latitudes=df_postal_codes['Latitude'][:1],\n",
      "                                   longitudes=df_postal_codes['Longitude'][:1]\n",
      "                                  )\n",
      "144/34:\n",
      "def get_nearby_venues(names, latitudes, longitudes, radius=500):\n",
      "    venues_list = []\n",
      "    \n",
      "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
      "        print(name)\n",
      "\n",
      "        # create the API request URL\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            radius, \n",
      "            LIMIT)\n",
      "\n",
      "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
      "        #print(results)\n",
      "        \n",
      "        venues_list.append([(\n",
      "            name, \n",
      "            lat, \n",
      "            lng, \n",
      "            v['venue']['name'], \n",
      "            v['venue']['location']['lat'], \n",
      "            v['venue']['location']['lng'],  \n",
      "            v['venue']['categories'][0]['name']) for v in results])\n",
      "\n",
      "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
      "    nearby_venues.columns = ['Neighborhood', \n",
      "              'Neighborhood Latitude', \n",
      "              'Neighborhood Longitude', \n",
      "              'Venue', \n",
      "              'Venue Latitude', \n",
      "              'Venue Longitude', \n",
      "              'Venue Category']\n",
      "\n",
      "    return(nearby_venues)\n",
      "144/35:\n",
      "toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'],\n",
      "                                   latitudes=df_postal_codes['Latitude'],\n",
      "                                   longitudes=df_postal_codes['Longitude']\n",
      "                                  )\n",
      "144/36: toronto_venues.shape\n",
      "144/37: toronto_venues.head()\n",
      "144/38: toronto_venues.head(20)\n",
      "144/39: df_postal_codes.concat(toronto_venues, axis=1)\n",
      "144/40: df_postal_codes.append(toronto_venues, axis=1)\n",
      "144/41: df_postal_codes.join(toronto_venues)\n",
      "144/42: df_postal_codes.join(toronto_venues, how='left')\n",
      "144/43: pd.merge([df_postal_codes, toronto_venues])\n",
      "144/44: df_postal_codes.merge(toronto_venues)\n",
      "144/45: toronto_venues.to_csv('toronto_venues.csv')\n",
      "144/46: toronto_venues.groupby('Neighborhood').count().sort_values(by='Venue')\n",
      "144/47:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 100\n",
      "144/48:\n",
      "def get_nearby_venues(names, latitudes, longitudes, radius=500):\n",
      "    venues_list = []\n",
      "    \n",
      "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
      "        print(name)\n",
      "\n",
      "        # create the API request URL\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            radius, \n",
      "            LIMIT)\n",
      "\n",
      "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
      "        #print(results)\n",
      "        \n",
      "        venues_list.append([(\n",
      "            name, \n",
      "            lat, \n",
      "            lng, \n",
      "            v['venue']['name'], \n",
      "            v['venue']['location']['lat'], \n",
      "            v['venue']['location']['lng'],  \n",
      "            v['venue']['categories'][0]['name']) for v in results])\n",
      "\n",
      "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
      "    nearby_venues.columns = ['Neighborhood', \n",
      "              'Neighborhood Latitude', \n",
      "              'Neighborhood Longitude', \n",
      "              'Venue', \n",
      "              'Venue Latitude', \n",
      "              'Venue Longitude', \n",
      "              'Venue Category']\n",
      "\n",
      "    return(nearby_venues)\n",
      "144/49:\n",
      "toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'],\n",
      "                                   latitudes=df_postal_codes['Latitude'],\n",
      "                                   longitudes=df_postal_codes['Longitude']\n",
      "                                  )\n",
      "144/50: toronto_venues.shape\n",
      "144/51: toronto_venues.head(20)\n",
      "144/52: toronto_venues.to_csv('toronto_venues.csv')\n",
      "144/53: toronto_venues.groupby('Neighborhood').count().sort_values(by='Venue')\n",
      "144/54: toronto_venues.count_values()\n",
      "144/55: toronto_venues.Neighborhood.unique()\n",
      "144/56: len(toronto_venues.Neighborhood.unique())\n",
      "144/57: df_postal_codes.loc[df.Neighborhood.isin[toronto_venues.Neighborhood.unique()]]\n",
      "144/58: df_postal_codes.loc[df_postl_codes.Neighborhood.isin[toronto_venues.Neighborhood.unique()]]\n",
      "144/59: df_postal_codes.loc[df_postal_codes.Neighborhood.isin[toronto_venues.Neighborhood.unique()]]\n",
      "144/60: df_postal_codes.loc[df_postal_codes.Neighborhood.isin(toronto_venues.Neighborhood.unique())]\n",
      "144/61: df_postal_codes.loc[~df_postal_codes.Neighborhood.isin(toronto_venues.Neighborhood.unique())]\n",
      "144/62: postal_codes_coordinates[95]\n",
      "144/63: postal_codes_coordinates[95].geojson\n",
      "144/64: toronto_venues.loc['Scarborough']\n",
      "144/65: toronto_venues.head()\n",
      "144/66: df_postal_codes.loc[df_postal_codes.Neighborhood=='Scarborough']\n",
      "144/67:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "    CLIENT_ID, \n",
      "    CLIENT_SECRET, \n",
      "    VERSION, \n",
      "    '43.834768',\n",
      "    '79.204101' \n",
      "    radius, \n",
      "    LIMIT)\n",
      "\n",
      "results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
      "144/68:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "    CLIENT_ID, \n",
      "    CLIENT_SECRET, \n",
      "    VERSION, \n",
      "    '43.834768',\n",
      "    '79.204101', \n",
      "    radius, \n",
      "    LIMIT)\n",
      "\n",
      "results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
      "144/69:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "    CLIENT_ID, \n",
      "    CLIENT_SECRET, \n",
      "    VERSION, \n",
      "    '43.834768',\n",
      "    '79.204101', \n",
      "    500, \n",
      "    LIMIT)\n",
      "\n",
      "results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
      "144/70: results\n",
      "144/71:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "    CLIENT_ID, \n",
      "    CLIENT_SECRET, \n",
      "    VERSION, \n",
      "    '43.834768',\n",
      "    '79.204101', \n",
      "    500, \n",
      "    LIMIT)\n",
      "\n",
      "results = requests.get(url).json()#[\"response\"]['groups'][0]['items']\n",
      "144/72: results\n",
      "144/73: toronto_venues.groupby('Venue Category').count().sort_values(by='Venue', ascending=False)\n",
      "144/74:\n",
      "# one hot encoding\n",
      "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
      "\n",
      "# add neighborhood column back to dataframe\n",
      "toronto_onehot['Neighborhood'] = manhattan_venues['Neighborhood'] \n",
      "\n",
      "# move neighborhood column to the first column\n",
      "fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])\n",
      "toronto_onehot = toronto_onehot[fixed_columns]\n",
      "\n",
      "toronto_onehot.head()\n",
      "144/75:\n",
      "# one hot encoding\n",
      "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
      "\n",
      "# add neighborhood column back to dataframe\n",
      "toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] \n",
      "\n",
      "# move neighborhood column to the first column\n",
      "fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])\n",
      "toronto_onehot = toronto_onehot[fixed_columns]\n",
      "\n",
      "toronto_onehot.head()\n",
      "144/76: toronto_onehot.size\n",
      "144/77: toronto_onehot.shape\n",
      "144/78:\n",
      "# one hot encoding\n",
      "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
      "\n",
      "# add neighborhood column back to dataframe\n",
      "toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] \n",
      "\n",
      "# move neighborhood column to the first column\n",
      "fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])\n",
      "toronto_onehot = toronto_onehot[fixed_columns]\n",
      "\n",
      "toronto_onehot.head()\n",
      "toronto_onehot.Neighborhood\n",
      "144/79:\n",
      "# one hot encoding\n",
      "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
      "\n",
      "# add neighborhood column back to dataframe\n",
      "toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] \n",
      "\n",
      "# move neighborhood column to the first column\n",
      "fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])\n",
      "toronto_onehot = toronto_onehot[fixed_columns]\n",
      "\n",
      "toronto_onehot.head()\n",
      "144/80:\n",
      "# one hot encoding\n",
      "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
      "\n",
      "# add neighborhood column back to dataframe\n",
      "toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] \n",
      "\n",
      "# move neighborhood column to the first column\n",
      "fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])\n",
      "toronto_onehot = toronto_onehot[fixed_columns]\n",
      "\n",
      "toronto_onehot.columns\n",
      "144/81:\n",
      "# one hot encoding\n",
      "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
      "\n",
      "# add neighborhood column back to dataframe\n",
      "toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] \n",
      "\n",
      "# move neighborhood column to the first column\n",
      "fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])\n",
      "toronto_onehot = toronto_onehot[fixed_columns]\n",
      "\n",
      "toronto_onehot.head()\n",
      "144/82:\n",
      "# one hot encoding\n",
      "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
      "toronto_onehot.head()\n",
      "144/83:\n",
      "# add neighborhood column back to dataframe\n",
      "toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] \n",
      "toronto_onehot.head()\n",
      "144/84:\n",
      "# add neighborhood column back to dataframe\n",
      "toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] \n",
      "toronto_onehot.Neighborhood\n",
      "144/85:\n",
      "# move neighborhood column to the first column\n",
      "fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])\n",
      "fixed_columns\n",
      "144/86:\n",
      "# add neighborhood column back to dataframe\n",
      "toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] \n",
      "toronto_onehot.columns[-1]\n",
      "144/87:\n",
      "# add neighborhood column back to dataframe\n",
      "toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] \n",
      "toronto_onehot.head()\n",
      "144/88:\n",
      "# one hot encoding\n",
      "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
      "toronto_onehot.head()\n",
      "144/89:\n",
      "# add neighborhood column back to dataframe\n",
      "toronto_onehot= pd.DataFrame(toronto_venues['Neighborhood']).join(toronto_onehot)\n",
      "toronto_onehot.head()\n",
      "144/90:\n",
      "# add neighborhood column back to dataframe\n",
      "toronto_onehot= pd.DataFrame(toronto_venues['Neighborhood']).merge(toronto_onehot)\n",
      "toronto_onehot.head()\n",
      "144/91:\n",
      "# add neighborhood column back to dataframe\n",
      "toronto_neighborhood_df = pd.DataFrame(toronto_venues['Neighborhood'])\n",
      "toronto_onehot= pd.concat([toronto_neighborhood_df,toronto_onehot], axis=1)\n",
      "toronto_onehot.head()\n",
      "144/92:\n",
      "manhattan_grouped = manhattan_onehot.groupby('Neighborhood').mean().reset_index()\n",
      "manhattan_grouped\n",
      "144/93:\n",
      "toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()\n",
      "toronto_grouped\n",
      "144/94:\n",
      "# one hot encoding\n",
      "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
      "toronto_onehot.head()\n",
      "144/95:\n",
      "# concatenate Neighborhood and onehot\n",
      "toronto_neighborhood_df = pd.DataFrame(toronto_venues['Neighborhood'])\n",
      "toronto_onehot= pd.concat([toronto_neighborhood_df,toronto_onehot], axis=1)\n",
      "toronto_onehot.head()\n",
      "144/96: toronto_onehot.shape\n",
      "144/97:\n",
      "toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()\n",
      "toronto_grouped\n",
      "144/98: toronto_onehot['Neighborhood']\n",
      "144/99:\n",
      "# one hot encoding\n",
      "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
      "toronto_onehot.columns\n",
      "144/100: toronto_venues.head()\n",
      "144/101:\n",
      "# one hot encoding\n",
      "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
      "toronto_onehot['Neighborhood']\n",
      "144/102: toronto_venues[['Venue Category']]\n",
      "144/103: 'Neighborhood'.isin(toronto_venues[['Venue Category']])\n",
      "144/104: toronto_venues[['Venue Category']]\n",
      "144/105: toronto_venues[['Venue Category']].loc[toronto_venues[['Venue Category']]=='Neighborhood']\n",
      "144/106: toronto_venues[['Venue Category']]\n",
      "144/107: type(toronto_venues[['Venue Category']])\n",
      "144/108: toronto_venues[['Venue Category']]=='Neighborhood'\n",
      "144/109: toronto_venues[['Venue Category']].loc[toronto_venues[['Venue Category']]=='Neighborhood']\n",
      "144/110: toronto_venues[['Venue Category']]=='Neighborhood'\n",
      "144/111: toronto_venues[['Venue Category']]=='Neighborhood'.any()\n",
      "144/112: (toronto_venues[['Venue Category']]=='Neighborhood').any()\n",
      "144/113: [False, False].any()\n",
      "144/114: any([False, False])\n",
      "144/115: print(toronto_venues[['Venue Category']])\n",
      "144/116: print(toronto_venues[['Venue Category']].to_string())\n",
      "144/117: print(toronto_venues[['Venue Category']].sort_values().to_string())\n",
      "144/118: print(toronto_venues[['Venue Category']].sort_values(by='Venue Category').to_string())\n",
      "144/119: toronto_venues.iloc[[619, 967, 512]]\n",
      "144/120: toronto_venues = toronto_venues.loc[~toronto_venues['Venue Category']=='Neighborhood']\n",
      "144/121: toronto_venues = toronto_venues.loc[~(toronto_venues['Venue Category']=='Neighborhood')]\n",
      "144/122: print(toronto_venues[['Venue Category']].sort_values(by='Venue Category').to_string())\n",
      "144/123:\n",
      "# one hot encoding\n",
      "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
      "toronto_onehot.head()\n",
      "144/124:\n",
      "# concatenate Neighborhood and onehot\n",
      "toronto_neighborhood_df = pd.DataFrame(toronto_venues['Neighborhood'])\n",
      "toronto_onehot= pd.concat([toronto_neighborhood_df,toronto_onehot], axis=1)\n",
      "toronto_onehot.head()\n",
      "144/125:\n",
      "toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()\n",
      "toronto_grouped\n",
      "144/126:\n",
      "toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()\n",
      "toronto_grouped.head()\n",
      "144/127: toronto_grouped.shape()\n",
      "144/128: toronto_grouped.shape\n",
      "144/129:\n",
      "num_top_venues = 5\n",
      "\n",
      "for hood in toronto_grouped['Neighborhood']:\n",
      "    print(\"----\"+hood+\"----\")\n",
      "    temp = toronto_grouped[toronto_grouped['Neighborhood'] == hood].T.reset_index()\n",
      "    temp.columns = ['venue','freq']\n",
      "    temp = temp.iloc[1:]\n",
      "    temp['freq'] = temp['freq'].astype(float)\n",
      "    temp = temp.round({'freq': 2})\n",
      "    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n",
      "    print('\\n')\n",
      "144/130:\n",
      "def return_most_common_venues(row, num_top_venues):\n",
      "    row_categories = row.iloc[1:]\n",
      "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
      "    \n",
      "    return row_categories_sorted.index.values[0:num_top_venues]\n",
      "144/131:\n",
      "num_top_venues = 10\n",
      "\n",
      "indicators = ['st', 'nd', 'rd']\n",
      "\n",
      "# create columns according to number of top venues\n",
      "columns = ['Neighborhood']\n",
      "for ind in np.arange(num_top_venues):\n",
      "    try:\n",
      "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
      "    except:\n",
      "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
      "\n",
      "# create a new dataframe\n",
      "neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
      "neighborhoods_venues_sorted['Neighborhood'] = toronto_grouped['Neighborhood']\n",
      "\n",
      "for ind in np.arange(toronto_grouped.shape[0]):\n",
      "    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(toronto_grouped.iloc[ind, :], num_top_venues)\n",
      "\n",
      "neighborhoods_venues_sorted.head()\n",
      "144/132:\n",
      "# set number of clusters\n",
      "kclusters = 5\n",
      "\n",
      "toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)\n",
      "\n",
      "# run k-means clustering\n",
      "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)\n",
      "\n",
      "# check cluster labels generated for each row in the dataframe\n",
      "kmeans.labels_[0:10]\n",
      "144/133:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import geocoder\n",
      "import folium\n",
      "import os\n",
      "import requests\n",
      "from sklearn.cluster import KMeans\n",
      "144/134:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import geocoder\n",
      "import folium\n",
      "import os\n",
      "import requests\n",
      "from sklearn.cluster import KMeans\n",
      "import matplotlib.cm as cm\n",
      "import matplotlib.colors as colors\n",
      "144/135:\n",
      "# set number of clusters\n",
      "kclusters = 5\n",
      "\n",
      "toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)\n",
      "\n",
      "# run k-means clustering\n",
      "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)\n",
      "\n",
      "# check cluster labels generated for each row in the dataframe\n",
      "kmeans.labels_[0:10]\n",
      "144/136:\n",
      "# add clustering labels\n",
      "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
      "\n",
      "toronto_merged = toronto_data\n",
      "\n",
      "# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood\n",
      "toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n",
      "\n",
      "toronto_merged.head() # check the last columns!\n",
      "144/137: df_postal_codes.drop('index')\n",
      "144/138: df_postal_codes.drop(column = 'index')\n",
      "144/139: df_postal_codes.drop('index', axis=1)\n",
      "144/140: df_postal_codes.loc[df_postal_codes['Postal Code'].isin(['M5G', 'M2H','M4M','M9L', 'M1B'])]\n",
      "144/141: df_postal_codes.drop('index', axis=1, inplace=True)\n",
      "144/142: df_postal_codes.loc[df_postal_codes['Postal Code'].isin(['M5G', 'M2H','M4M','M9L', 'M1B'])]\n",
      "144/143:\n",
      "# add clustering labels\n",
      "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
      "\n",
      "toronto_merged = df_postal_codes\n",
      "\n",
      "# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood\n",
      "toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n",
      "\n",
      "toronto_merged.head() # check the last columns!\n",
      "144/144:\n",
      "num_top_venues = 10\n",
      "\n",
      "indicators = ['st', 'nd', 'rd']\n",
      "\n",
      "# create columns according to number of top venues\n",
      "columns = ['Neighborhood']\n",
      "for ind in np.arange(num_top_venues):\n",
      "    try:\n",
      "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
      "    except:\n",
      "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
      "\n",
      "# create a new dataframe\n",
      "neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
      "neighborhoods_venues_sorted['Neighborhood'] = toronto_grouped['Neighborhood']\n",
      "\n",
      "for ind in np.arange(toronto_grouped.shape[0]):\n",
      "    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(toronto_grouped.iloc[ind, :], num_top_venues)\n",
      "\n",
      "neighborhoods_venues_sorted.head()\n",
      "144/145:\n",
      "# set number of clusters\n",
      "kclusters = 5\n",
      "\n",
      "toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)\n",
      "\n",
      "# run k-means clustering\n",
      "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)\n",
      "\n",
      "# check cluster labels generated for each row in the dataframe\n",
      "kmeans.labels_[0:10]\n",
      "144/146:\n",
      "# add clustering labels\n",
      "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
      "\n",
      "toronto_merged = df_postal_codes\n",
      "\n",
      "# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood\n",
      "toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n",
      "\n",
      "toronto_merged.head() # check the last columns!\n",
      "144/147:\n",
      "# create map\n",
      "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
      "\n",
      "# set color scheme for the clusters\n",
      "x = np.arange(kclusters)\n",
      "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
      "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
      "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
      "\n",
      "# add markers to the map\n",
      "markers_colors = []\n",
      "for lat, lon, poi, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'], toronto_merged['Neighborhood'], toronto_merged['Cluster Labels']):\n",
      "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
      "    folium.CircleMarker(\n",
      "        [lat, lon],\n",
      "        radius=5,\n",
      "        popup=label,\n",
      "        color=rainbow[cluster-1],\n",
      "        fill=True,\n",
      "        fill_color=rainbow[cluster-1],\n",
      "        fill_opacity=0.7).add_to(map_clusters)\n",
      "       \n",
      "map_clusters\n",
      "144/148:\n",
      "# create map\n",
      "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
      "\n",
      "# set color scheme for the clusters\n",
      "x = np.arange(kclusters)\n",
      "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
      "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
      "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
      "\n",
      "# add markers to the map\n",
      "markers_colors = []\n",
      "for lat, lon, poi, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'], toronto_merged['Neighborhood'], toronto_merged['Cluster Labels']):\n",
      "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
      "    folium.CircleMarker(\n",
      "        [lat, lon],\n",
      "        radius=5,\n",
      "        popup=label,\n",
      "        color=rainbow[int(cluster)-1],\n",
      "        fill=True,\n",
      "        fill_color=rainbow[int(cluster)-1],\n",
      "        fill_opacity=0.7).add_to(map_clusters)\n",
      "       \n",
      "map_clusters\n",
      "144/149: toronto_merged.isnull().sum()\n",
      "144/150: toronto_merged.sort_values(by='Cluster Labels')\n",
      "144/151: toronto_merged[toronto_merged['Cluster Labels'].isnull()]\n",
      "144/152: neighborhoods_venues_sorted.head()\n",
      "144/153: neighborhoods_venues_sorted[neighborhood_venus_sorted.Neighborhood=='Scarborough']\n",
      "144/154: neighborhoods_venues_sorted[neighborhood_venus_sorted.Neighborhood=='Scarborough']\n",
      "144/155: neighborhoods_venues_sorted[neighborhoods_venus_sorted.Neighborhood=='Scarborough']\n",
      "144/156: neighborhoods_venues_sorted[neighborhoods_venues_sorted.Neighborhood=='Scarborough']\n",
      "144/157: neighborhoods_venues_sorted[neighborhoods_venues_sorted.Borough=='Scarborough']\n",
      "144/158: neighborhood_venues_sorted.columns\n",
      "144/159: neighborhood_venues_sorted.columns()\n",
      "144/160: neighborhoods_venues_sorted.columns()\n",
      "144/161: neighborhoods_venues_sorted.columns\n",
      "144/162: neighborhoods_venues_sorted[neighborhoods_venues_sorted.Neighborhood=='Upper Rouge']\n",
      "144/163: toronto_neighborhood_df[toronot_neighborhood_df.Neighborhood=='Scarborough']\n",
      "144/164: toronto_neighborhood_df[toronto_neighborhood_df.Neighborhood=='Scarborough']\n",
      "144/165: toronto_neighborhood_df[toronto_neighborhood_df.Neighborhood=='Parkwoods']\n",
      "144/166: toronto_csv = pd.read_csv('toronto_venues.csv')\n",
      "144/167: toronto_csv.head()\n",
      "144/168: toronto_csv[toronto_csv.Neighborhood=='Scarborough']\n",
      "144/169: toronto_csv[toronto_csv.Borough=='Scarborough']\n",
      "144/170: toronto_csv[toronto_csv.Neighborhood=='Upper Rouge']\n",
      "144/171: neighborhoods_venues_sorted.loc[neighborhoods_venues_sorted.Neighborhood=='Upper Rouge']\n",
      "144/172: toronto_merged.loc[toronto_merged.Neighborhood=='Upper Rouge']\n",
      "144/173: neighborhoods_venues_sorted.loc[neighborhoods_venues_sorted.Neighborhood=='Upper Rouge']\n",
      "144/174: toronto_grouped.loc[toronto_grouped.Neighborhood=='Upper Rouge']\n",
      "144/175: toronto_neighborhood_df[toronto_neighborhood_df.Neighborhood=='Upper Rouge']\n",
      "144/176: toronto_venues.loc[toronto_venues.Neighborhood=='Upper Rouge']\n",
      "144/177: toronto_csv.loc[toronto_csv.Neighborhood=='Upper Rouge']\n",
      "144/178: toronto_csv = pd.read_csv('Toronto_postal_codes.csv')\n",
      "144/179: toronto_csv.loc[toronto_csv.Neighborhood=='Upper Rouge']\n",
      "144/180:\n",
      "        # create the API request URL\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            '43.834768', \n",
      "            '-79.204101', \n",
      "            radius, \n",
      "            LIMIT)\n",
      "\n",
      "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
      "144/181:\n",
      "        # create the API request URL\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            '43.834768', \n",
      "            '-79.204101', \n",
      "            500, \n",
      "            LIMIT)\n",
      "\n",
      "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
      "144/182: results\n",
      "144/183:\n",
      "        # create the API request URL\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            '43.834768', \n",
      "            '-79.204101', \n",
      "            500, \n",
      "            LIMIT)\n",
      "\n",
      "        results = requests.get(url).json()\n",
      "144/184: results\n",
      "144/185: toronto_merged.sort_values(by='Cluster Labels').head()\n",
      "144/186: toronto_merged[toronto_merged['Cluster Labels'].isnull()]\n",
      "144/187:\n",
      "        # create the API request URL\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            '43.834768', \n",
      "            '-79.204101', \n",
      "            500, \n",
      "            LIMIT)\n",
      "\n",
      "        print(requests.get(url).json())\n",
      "144/188:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "    CLIENT_ID, \n",
      "    CLIENT_SECRET, \n",
      "    VERSION, \n",
      "    '43.834768', \n",
      "    '-79.204101', \n",
      "    500, \n",
      "    LIMIT)\n",
      "\n",
      "print(requests.get(url).json())\n",
      "144/189: toronto_merged[~toronto_merged.loc['Cluster Labels'].isnull()]\n",
      "144/190: toronto_merged.loc[~toronto_merged['Cluster Labels'].isnull()]\n",
      "144/191: toronto merged = toronto_merged.loc[~toronto_merged['Cluster Labels'].isnull()]\n",
      "144/192: toronto_merged = toronto_merged.loc[~toronto_merged['Cluster Labels'].isnull()]\n",
      "144/193:\n",
      "# create map\n",
      "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
      "\n",
      "# set color scheme for the clusters\n",
      "x = np.arange(kclusters)\n",
      "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
      "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
      "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
      "\n",
      "# add markers to the map\n",
      "markers_colors = []\n",
      "for lat, lon, poi, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'], toronto_merged['Neighborhood'], toronto_merged['Cluster Labels']):\n",
      "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
      "    folium.CircleMarker(\n",
      "        [lat, lon],\n",
      "        radius=5,\n",
      "        popup=label,\n",
      "        color=rainbow[int(cluster)-1],\n",
      "        fill=True,\n",
      "        fill_color=rainbow[int(cluster)-1],\n",
      "        fill_opacity=0.7).add_to(map_clusters)\n",
      "       \n",
      "map_clusters\n",
      "144/194:\n",
      "c1 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 0, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c1\n",
      "144/195:\n",
      "c0 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 0, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c0.head()\n",
      "144/196: c0.iloc[:,1:].stack().value_counts()[:10]\n",
      "144/197:\n",
      "c1 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 1, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c1.head()\n",
      "144/198: c0.iloc[:,1:].stack().value_counts()[:5]\n",
      "144/199: c1.iloc[:,1:].stack().value_counts()[:5]\n",
      "144/200:\n",
      "c2 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 2, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c2.head()\n",
      "144/201: toronto_merged.iloc[[6,39]]\n",
      "144/202: toronto_merged.iloc[[6,39]]\n",
      "144/203:\n",
      "c1 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 3, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c1.head()\n",
      "144/204:\n",
      "c3 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 3, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c3.head()\n",
      "144/205: c3.iloc[:,1:].stack().value_counts()[:5]\n",
      "144/206:\n",
      "c4 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 4, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c4.head()\n",
      "144/207:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import geocoder\n",
      "import folium\n",
      "import os\n",
      "import requests\n",
      "from sklearn.cluster import KMeans\n",
      "import matplotlib.cm as cm\n",
      "import matplotlib.colors as colors\n",
      "144/208: toronto_venues.shape\n",
      "144/209: toronto_venues.head()\n",
      "144/210: toronto_venues.to_csv('toronto_venues.csv')\n",
      "144/211: (toronto_venues[['Venue Category']]=='Neighborhood').any()\n",
      "144/212: toronto_venues = toronto_venues.loc[~(toronto_venues['Venue Category']=='Neighborhood')]\n",
      "144/213:\n",
      "def return_most_common_venues(row, num_top_venues):\n",
      "    row_categories = row.iloc[1:]\n",
      "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
      "    \n",
      "    return row_categories_sorted.index.values[0:num_top_venues]\n",
      "144/214: toronto_merged.isnull().count()\n",
      "144/215: toronto_merged.isnull().sum()\n",
      "144/216: toronto_venues.groupby('Neighborhood').count().sort_values(by='Venue')\n",
      "144/217: len(toronto_venues.Neighborhood.unique())\n",
      "144/218: toronto_venues.groupby('Venue Category').count().sort_values(by='Venue', ascending=False)\n",
      "144/219: (toronto_venues[['Venue Category']]=='Neighborhood').any()\n",
      "144/220: toronto_venues = toronto_venues.loc[~(toronto_venues['Venue Category']=='Neighborhood')]\n",
      "144/221: toronto_venue.read_csv('toronto_venues.csv')\n",
      "144/222: toronto_venues = pd.read_csv('toronto_venues.csv')\n",
      "144/223: toronto_venues.groupby('Neighborhood').count().sort_values(by='Venue')\n",
      "144/224: len(toronto_venues.Neighborhood.unique())\n",
      "144/225: toronto_venues.groupby('Venue Category').count().sort_values(by='Venue', ascending=False)\n",
      "144/226: (toronto_venues[['Venue Category']]=='Neighborhood').any()\n",
      "144/227: toronto_venues = toronto_venues.loc[~(toronto_venues['Venue Category']=='Neighborhood')]\n",
      "144/228:\n",
      "toronto_venues = get_nearby_venues(names=df_postal_codes['Neighborhood'],\n",
      "                                   latitudes=df_postal_codes['Latitude'],\n",
      "                                   longitudes=df_postal_codes['Longitude']\n",
      "                                  )\n",
      "144/229: toronto_venues.shape\n",
      "144/230: toronto_venues.head()\n",
      "144/231: toronto_venues.to_csv('toronto_venues.csv')\n",
      "144/232: #toronto_venues = pd.read_csv('toronto_venues.csv')\n",
      "144/233: toronto_venues.groupby('Neighborhood').count().sort_values(by='Venue')\n",
      "144/234: len(toronto_venues.Neighborhood.unique())\n",
      "144/235: toronto_venues.groupby('Venue Category').count().sort_values(by='Venue', ascending=False)\n",
      "144/236: (toronto_venues[['Venue Category']]=='Neighborhood').any()\n",
      "144/237: toronto_venues = toronto_venues.loc[~(toronto_venues['Venue Category']=='Neighborhood')]\n",
      "144/238:\n",
      "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
      "toronto_onehot.head()\n",
      "144/239:\n",
      "toronto_neighborhood_df = pd.DataFrame(toronto_venues['Neighborhood'])\n",
      "toronto_onehot= pd.concat([toronto_neighborhood_df,toronto_onehot], axis=1)\n",
      "toronto_onehot.head()\n",
      "144/240: toronto_onehot.shape\n",
      "144/241:\n",
      "toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()\n",
      "toronto_grouped.head()\n",
      "144/242: toronto_grouped.shape\n",
      "144/243:\n",
      "num_top_venues = 5\n",
      "\n",
      "for hood in toronto_grouped['Neighborhood']:\n",
      "    print(\"----\"+hood+\"----\")\n",
      "    temp = toronto_grouped[toronto_grouped['Neighborhood'] == hood].T.reset_index()\n",
      "    temp.columns = ['venue','freq']\n",
      "    temp = temp.iloc[1:]\n",
      "    temp['freq'] = temp['freq'].astype(float)\n",
      "    temp = temp.round({'freq': 2})\n",
      "    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n",
      "    print('\\n')\n",
      "144/244:\n",
      "def return_most_common_venues(row, num_top_venues):\n",
      "    row_categories = row.iloc[1:]\n",
      "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
      "    \n",
      "    return row_categories_sorted.index.values[0:num_top_venues]\n",
      "144/245:\n",
      "num_top_venues = 10\n",
      "\n",
      "indicators = ['st', 'nd', 'rd']\n",
      "\n",
      "# create columns according to number of top venues\n",
      "columns = ['Neighborhood']\n",
      "for ind in np.arange(num_top_venues):\n",
      "    try:\n",
      "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
      "    except:\n",
      "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
      "\n",
      "# create a new dataframe\n",
      "neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
      "neighborhoods_venues_sorted['Neighborhood'] = toronto_grouped['Neighborhood']\n",
      "\n",
      "for ind in np.arange(toronto_grouped.shape[0]):\n",
      "    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(toronto_grouped.iloc[ind, :], num_top_venues)\n",
      "\n",
      "neighborhoods_venues_sorted.head()\n",
      "144/246:\n",
      "kclusters = 5\n",
      "\n",
      "toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)\n",
      "\n",
      "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)\n",
      "\n",
      "kmeans.labels_[0:10]\n",
      "144/247:\n",
      "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
      "\n",
      "toronto_merged = df_postal_codes\n",
      "\n",
      "toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n",
      "\n",
      "toronto_merged.head()\n",
      "144/248: toronto_merged.isnull().sum()\n",
      "144/249: toronto_merged.loc[toronot_merged.isnull()]\n",
      "144/250: toronto_merged.loc[toronot_merged['Clister labels'].isnull()]\n",
      "144/251: toronto_merged.loc[toronot_merged['Claster labels'].isnull()]\n",
      "144/252: toronto_merged.loc[toronto_merged['Claster labels'].isnull()]\n",
      "144/253: toronto_merged.loc[toronto_merged['Claster Labels'].isnull()]\n",
      "144/254: toronto_merged.loc[toronto_merged['Cluster Labels'].isnull()]\n",
      "144/255:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "    CLIENT_ID, \n",
      "    CLIENT_SECRET, \n",
      "    VERSION, \n",
      "    '43.834768', \n",
      "    '-79.204101', \n",
      "    500, \n",
      "    LIMIT)\n",
      "\n",
      "print(requests.get(url).json())\n",
      "144/256: toronto_merged = toronto_merged.loc[~toronto_merged['Cluster Labels'].isnull()]\n",
      "144/257:\n",
      "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
      "\n",
      "x = np.arange(kclusters)\n",
      "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
      "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
      "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
      "\n",
      "markers_colors = []\n",
      "for lat, lon, poi, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'], toronto_merged['Neighborhood'], toronto_merged['Cluster Labels']):\n",
      "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
      "    folium.CircleMarker(\n",
      "        [lat, lon],\n",
      "        radius=5,\n",
      "        popup=label,\n",
      "        color=rainbow[int(cluster)-1],\n",
      "        fill=True,\n",
      "        fill_color=rainbow[int(cluster)-1],\n",
      "        fill_opacity=0.7).add_to(map_clusters)\n",
      "       \n",
      "map_clusters\n",
      "144/258:\n",
      "c0 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 0, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c0.head()\n",
      "144/259: c0.iloc[:,1:].stack().value_counts()[:5]\n",
      "144/260: c0.stack().value_counts()[:5]\n",
      "144/261: c0.stack().value_counts()[:10]\n",
      "144/262:\n",
      "c0 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 0, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c0\n",
      "144/263:\n",
      "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
      "\n",
      "toronto_merged = df_postal_codes.iloc[:,[1:]]\n",
      "\n",
      "toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n",
      "\n",
      "toronto_merged.head()\n",
      "144/264:\n",
      "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
      "\n",
      "toronto_merged = df_postal_codes.iloc[:,1:]\n",
      "\n",
      "toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n",
      "\n",
      "toronto_merged.head()\n",
      "144/265:\n",
      "num_top_venues = 10\n",
      "\n",
      "indicators = ['st', 'nd', 'rd']\n",
      "\n",
      "# create columns according to number of top venues\n",
      "columns = ['Neighborhood']\n",
      "for ind in np.arange(num_top_venues):\n",
      "    try:\n",
      "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
      "    except:\n",
      "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
      "\n",
      "# create a new dataframe\n",
      "neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
      "neighborhoods_venues_sorted['Neighborhood'] = toronto_grouped['Neighborhood']\n",
      "\n",
      "for ind in np.arange(toronto_grouped.shape[0]):\n",
      "    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(toronto_grouped.iloc[ind, :], num_top_venues)\n",
      "\n",
      "neighborhoods_venues_sorted.head()\n",
      "144/266:\n",
      "kclusters = 5\n",
      "\n",
      "toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)\n",
      "\n",
      "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)\n",
      "\n",
      "kmeans.labels_[0:10]\n",
      "144/267:\n",
      "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
      "\n",
      "toronto_merged = df_postal_codes.iloc[:,1:]\n",
      "\n",
      "toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n",
      "\n",
      "toronto_merged.head()\n",
      "144/268: toronto_merged.isnull().sum()\n",
      "144/269: toronto_merged.loc[toronto_merged['Cluster Labels'].isnull()]\n",
      "144/270:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "    CLIENT_ID, \n",
      "    CLIENT_SECRET, \n",
      "    VERSION, \n",
      "    '43.834768', \n",
      "    '-79.204101', \n",
      "    500, \n",
      "    LIMIT)\n",
      "\n",
      "print(requests.get(url).json())\n",
      "144/271: toronto_merged = toronto_merged.loc[~toronto_merged['Cluster Labels'].isnull()]\n",
      "144/272:\n",
      "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
      "\n",
      "x = np.arange(kclusters)\n",
      "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
      "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
      "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
      "\n",
      "markers_colors = []\n",
      "for lat, lon, poi, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'], toronto_merged['Neighborhood'], toronto_merged['Cluster Labels']):\n",
      "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
      "    folium.CircleMarker(\n",
      "        [lat, lon],\n",
      "        radius=5,\n",
      "        popup=label,\n",
      "        color=rainbow[int(cluster)-1],\n",
      "        fill=True,\n",
      "        fill_color=rainbow[int(cluster)-1],\n",
      "        fill_opacity=0.7).add_to(map_clusters)\n",
      "       \n",
      "map_clusters\n",
      "144/273:\n",
      "c0 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 0, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c0\n",
      "144/274: df_postal_codes.iloc[26,39]\n",
      "144/275: df_postal_codes.iloc[[26,39]]\n",
      "144/276: c0.stack().value_counts()[:5]\n",
      "144/277:\n",
      "c1 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 1, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c1.head()\n",
      "144/278: toronto_merged.iloc[[6,39]]\n",
      "144/279: c1.iloc[:,1:].stack().value_counts()[:5]\n",
      "144/280: c1.shape\n",
      "144/281:\n",
      "c1 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 1, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c1.head(10)\n",
      "144/282: c1['1st Most Common Venue'].values_count()\n",
      "144/283: c1.values_count()\n",
      "144/284: c1.value_counts()\n",
      "144/285: c1['1st Most Common Venue'].value_counts()\n",
      "144/286: c1['1st Most Common Venue'].value_counts()[5]\n",
      "144/287: c1['1st Most Common Venue'].value_counts()[:5]\n",
      "144/288: c1['1st Most Common Venue'].value_counts()[:5]\n",
      "144/289:\n",
      "c1 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 1, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c1.head()\n",
      "144/290: c1.shape\n",
      "144/291: c1.iloc[:,1:].stack().value_counts()[:5]\n",
      "144/292: c1['1st Most Common Venue'].value_counts()[:5]\n",
      "144/293:\n",
      "c2 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 2, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c2.head()\n",
      "144/294:\n",
      "c3 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 3, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c3.head()\n",
      "144/295: c3.iloc[:,1:].stack().value_counts()[:5]\n",
      "144/296: c3.shape()\n",
      "144/297: c3.shape\n",
      "144/298:\n",
      "c4 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 4, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c4.head()\n",
      "144/299: c4.shape\n",
      "144/300:\n",
      "c4 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 4, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c4.head()\n",
      "144/301: c4.shape\n",
      "144/302: c4.iloc[:,1:].stack().value_counts()[:5]\n",
      "144/303: c4['1st Most Common Venue'].value_counts()[:5]\n",
      "144/304:\n",
      "c4 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 4, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c4.head()\n",
      "144/305: c4.shape\n",
      "144/306: c4.iloc[:,1:].stack().value_counts()[:5]\n",
      "144/307: c4.iloc[:,1:].stack().value_counts()[:10]\n",
      "144/308:\n",
      "c4 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 4, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c4.head(10)\n",
      "144/309:\n",
      "c4 = toronto_merged.loc[toronto_merged['Cluster Labels'] == 4, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
      "c4.head()\n",
      "145/1:\n",
      "import pandas as pd\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import folium\n",
      "import requests\n",
      "145/2: poznan_coordinates = [52.2428.8, 16.56011]\n",
      "145/3: poznan_coordinates = [52.24288, 16.56011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/4: poznan_old_market = folium.Map(location=poznan_coordinates)\n",
      "145/5: poznan_old_market\n",
      "145/6:\n",
      "poznan_coordinates = [52.4069200, 16.9299300]\n",
      "# zerokość: 52.4069200° \n",
      "# Długość: 16.9299300°\n",
      "145/7: poznan_old_market = folium.Map(location=poznan_coordinates)\n",
      "145/8: poznan_old_market\n",
      "145/9: poznan_old_market = folium.Map(location=poznan_coordinates, zoom=6)\n",
      "145/10: poznan_old_market\n",
      "145/11: poznan_old_market = folium.Map(location=poznan_coordinates, zoom_start=6)\n",
      "145/12: poznan_old_market\n",
      "145/13: poznan_old_market = folium.Map(location=poznan_coordinates, zoom_start=12)\n",
      "145/14: poznan_old_market\n",
      "145/15: poznan_old_market = folium.Map(location=poznan_coordinates, zoom_start=13)\n",
      "145/16: poznan_old_market\n",
      "145/17: poznan_old_market = folium.Map(location=poznan_coordinates, zoom_start=16)\n",
      "145/18: poznan_old_market\n",
      "145/19: warsaw_geofile = 'geodata/warsaw_districts/dzielnice_Warszawy.shp'\n",
      "145/20: w = open(warsaw_geofile)\n",
      "145/21:\n",
      "for x in range(10):\n",
      "    w.read_line()\n",
      "145/22: w.read()\n",
      "145/23: w.read_line()\n",
      "145/24: w.read_lines()\n",
      "145/25: warsaw_geofile = 'geodata/warsaw_districts/warsaw_districts.geojson'\n",
      "145/26: w = open(warsaw_geofile)\n",
      "145/27: w.read_line()\n",
      "145/28: w.read_lines()\n",
      "145/29: help(w)\n",
      "145/30: w.readline()\n",
      "145/31: w.readline()\n",
      "145/32: w = open(warsaw_geofile)\n",
      "145/33:\n",
      "for line in w:\n",
      "    print(line)\n",
      "145/34:\n",
      "with open(warsaw_geofile) as w:\n",
      "    warsaw_geojson = w.read()\n",
      "145/35: warsaw_geojson\n",
      "145/36: type(warsaw_geojson)\n",
      "145/37:\n",
      "import pandas as pd\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import folium\n",
      "import json\n",
      "import requests\n",
      "145/38:\n",
      "with open(warsaw_geofile) as w:\n",
      "    file_content = w.read()\n",
      "    warsaw_geojson = json.load(file_content)\n",
      "145/39:\n",
      "with open(warsaw_geofile) as w:\n",
      "    #file_content = w.read()\n",
      "    warsaw_geojson = json.load(w)\n",
      "145/40: type(warsaw_geojson)\n",
      "145/41: warsaw_geojson.keys()\n",
      "145/42: warsaw_geojson['features']\n",
      "145/43: len(warsaw_geojson['features'])\n",
      "145/44:\n",
      "for x in warsaw_geojson['features']:\n",
      "    print(x['properties'])\n",
      "145/45:\n",
      "url = 'https://raw.githubusercontent.com/python-visualization/folium/master/examples/data'\n",
      "state_geo = f'{url}/us-states.json'\n",
      "state_unemployment = f'{url}/US_Unemployment_Oct2012.csv'\n",
      "state_data = pd.read_csv(state_unemployment)\n",
      "\n",
      "m = folium.Map(location=[48, -102], zoom_start=3)\n",
      "\n",
      "folium.Choropleth(\n",
      "    geo_data=state_geo,\n",
      "    name='choropleth',\n",
      "    data=state_data,\n",
      "    columns=['State', 'Unemployment'],\n",
      "    key_on='feature.id',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ").add_to(m)\n",
      "\n",
      "folium.LayerControl().add_to(m)\n",
      "145/46:\n",
      "url = 'https://raw.githubusercontent.com/python-visualization/folium/master/examples/data'\n",
      "state_geo = f'{url}/us-states.json'\n",
      "state_unemployment = f'{url}/US_Unemployment_Oct2012.csv'\n",
      "state_data = pd.read_csv(state_unemployment)\n",
      "\n",
      "m = folium.Map(location=[48, -102], zoom_start=3)\n",
      "\n",
      "# folium.Choropleth(\n",
      "#     geo_data=state_geo,\n",
      "#     name='choropleth',\n",
      "#     data=state_data,\n",
      "#     columns=['State', 'Unemployment'],\n",
      "#     key_on='feature.id',\n",
      "#     fill_color='YlGn',\n",
      "#     fill_opacity=0.7,\n",
      "#     line_opacity=0.2,\n",
      "#     legend_name='Unemployment Rate (%)'\n",
      "# ).add_to(m)\n",
      "\n",
      "# folium.LayerControl().add_to(m)\n",
      "145/47: state_data\n",
      "145/48: state_data.head()\n",
      "145/49: state_unemployment\n",
      "145/50: state_geo\n",
      "145/51: z = requests.get(state_geo)\n",
      "145/52: z.response()\n",
      "145/53: z\n",
      "145/54: help(z)\n",
      "145/55: z.text\n",
      "145/56: z.json()\n",
      "145/57:\n",
      "for x in warsaw_geojson['features']:\n",
      "    print(x['properties']['nazwa_dzie'])\n",
      "145/58:\n",
      "districts_names = []\n",
      "for x in warsaw_geojson['features']:\n",
      "    district_names.append(x['properties']['nazwa_dzie'])\n",
      "145/59:\n",
      "districts_names = []\n",
      "for x in warsaw_geojson['features']:\n",
      "    districts_names.append(x['properties']['nazwa_dzie'])\n",
      "145/60: test_density = list(range(len(districts_names)))\n",
      "145/61: df = pd.DataFrame([district_names, test_denisty])\n",
      "145/62: df = pd.DataFrame([districts_names, test_denisty])\n",
      "145/63: df = pd.DataFrame([districts_names, test_density])\n",
      "145/64: df.head()\n",
      "145/65: df = pd.DataFrame([districts_names, test_density]).T()\n",
      "145/66: df = pd.DataFrame([districts_names, test_density]).T\n",
      "145/67: df.head()\n",
      "145/68:\n",
      "df = pd.DataFrame([districts_names, test_density]).T\n",
      "df.columns = ['District_name', 'Density']\n",
      "145/69: df.head()\n",
      "145/70:\n",
      "folium.Choropleth(\n",
      "    geo_data=state_geo,\n",
      "    name='choropleth',\n",
      "    data=state_data,\n",
      "    columns=['State', 'Unemployment'],\n",
      "    key_on='feature.id',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ").add_to(m)\n",
      "\n",
      "folium.LayerControl().add_to(m)\n",
      "145/71:\n",
      "folium.choropleth(\n",
      "    geo_data=state_geo,\n",
      "    name='choropleth',\n",
      "    data=state_data,\n",
      "    columns=['State', 'Unemployment'],\n",
      "    key_on='feature.id',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ").add_to(m)\n",
      "\n",
      "folium.LayerControl().add_to(m)\n",
      "145/72:\n",
      "m.choropleth(\n",
      "    geo_data=state_geo,\n",
      "    name='choropleth',\n",
      "    data=state_data,\n",
      "    columns=['State', 'Unemployment'],\n",
      "    key_on='feature.id',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ")\n",
      "\n",
      "folium.LayerControl().add_to(m)\n",
      "145/73: m\n",
      "145/74: warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "145/75:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=15)\n",
      "warsaw_map\n",
      "145/76:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=12)\n",
      "warsaw_map\n",
      "145/77:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=10)\n",
      "warsaw_map\n",
      "145/78:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map\n",
      "145/79:\n",
      "for line in w[200]:\n",
      "    print(line)\n",
      "145/80:\n",
      "for line in w:\n",
      "    print(line)\n",
      "145/81: warsaw_geojson\n",
      "145/82:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'density'],\n",
      "    key_on='feature.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ")\n",
      "145/83:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ")\n",
      "145/84: warsaw_map\n",
      "145/85:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ")\n",
      "145/86: warsaw_map\n",
      "145/87:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ")\n",
      "145/88:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='features.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ")\n",
      "145/89: warsaw_map\n",
      "145/90:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='Feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ")\n",
      "145/91: warsaw_map\n",
      "145/92:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ")\n",
      "145/93: warsaw_map\n",
      "145/94:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ")\n",
      "145/95: warsaw_map\n",
      "145/96:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "#warsaw_map\n",
      "145/97:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ")\n",
      "145/98: warsaw_map\n",
      "145/99:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "#warsaw_map\n",
      "145/100:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='Feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ")\n",
      "145/101: warsaw_map\n",
      "145/102:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ")\n",
      "145/103: warsaw_map\n",
      "145/104:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "#warsaw_map\n",
      "145/105:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Unemployment Rate (%)'\n",
      ")\n",
      "145/106: warsaw_map\n",
      "145/107:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "145/108: warsaw_map\n",
      "145/109:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "#warsaw_map\n",
      "145/110:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "145/111: warsaw_map\n",
      "145/112: df.head()\n",
      "145/113:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "145/114: warsaw_map\n",
      "145/115:\n",
      "df = pd.DataFrame([districts_names, test_density]).T\n",
      "df.columns = ['District_name', 'Density']\n",
      "145/116: df.head()\n",
      "145/117:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "#warsaw_map\n",
      "145/118:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "145/119: warsaw_map\n",
      "145/120:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "#warsaw_map\n",
      "145/121:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='Feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "145/122: warsaw_map\n",
      "145/123:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "#warsaw_map\n",
      "145/124:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='Feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "145/125: warsaw_map\n",
      "145/126:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "#warsaw_map\n",
      "145/127:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='features.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "145/128: warsaw_map\n",
      "145/129:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='Features.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "145/130: warsaw_map\n",
      "145/131:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "#warsaw_map\n",
      "145/132:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "145/133: warsaw_map\n",
      "145/134: df.sort_values(by='District_name', inplace=True)\n",
      "145/135:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "#warsaw_map\n",
      "145/136:\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "145/137: warsaw_map\n",
      "145/138: warsaw_geojson\n",
      "145/139:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/140:\n",
      "with open(warsaw_geofile) as w:\n",
      "    warsaw_geojson = json.load(w)\n",
      "145/141:\n",
      "districts_names = []\n",
      "for x in warsaw_geojson['features']:\n",
      "    districts_names.append(x['properties']['nazwa_dzie'])\n",
      "145/142: test_density = list(range(len(districts_names)))\n",
      "145/143:\n",
      "df = pd.DataFrame([districts_names, test_density]).T\n",
      "df.columns = ['District_name', 'Density']\n",
      "145/144: df.head()\n",
      "145/145: df.sort_values(by='District_name', inplace=True)\n",
      "145/146:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/147: warsaw_geojson\n",
      "145/148: df.iloc[0]\n",
      "145/149: df.iloc[1]\n",
      "145/150: df.iloc[16]\n",
      "145/151: df.iloc[18]\n",
      "145/152: df.iloc[17]\n",
      "145/153:\n",
      "df = pd.DataFrame([districts_names, test_density]).T\n",
      "df.columns = ['Nazwa_dzie', 'Density']\n",
      "145/154:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['District_name', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/155:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['Nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/156:\n",
      "df = pd.DataFrame([districts_names, test_density]).T\n",
      "df.columns = ['nazwa_dzie', 'Density']\n",
      "145/157:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/158: warsaw_geofile = 'geodata/warsaw_districts/warsaw_districts.geojson'\n",
      "145/159:\n",
      "with open(warsaw_geofile) as w:\n",
      "    warsaw_geojson = json.load(w)\n",
      "145/160:\n",
      "districts_names = []\n",
      "for x in warsaw_geojson['features']:\n",
      "    districts_names.append(x['properties']['nazwa_dzie'])\n",
      "145/161: test_density = list(range(len(districts_names)))\n",
      "145/162:\n",
      "df = pd.DataFrame([districts_names, test_density]).T\n",
      "df.columns = ['nazwa_dzie', 'Density']\n",
      "145/163:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/164:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "folium.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ").add_to(warsaw_map)\n",
      "warsaw_map\n",
      "145/165:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "folium.Choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ").add_to(warsaw_map)\n",
      "warsaw_map\n",
      "145/166: warsaw_geofile = 'geodata/warsaw_districts/warsaw_districts.geojson'\n",
      "145/167:\n",
      "with open(warsaw_geofile) as w:\n",
      "    warsaw_geojson = json.load(w)\n",
      "145/168:\n",
      "districts_names = []\n",
      "for x in warsaw_geojson['features']:\n",
      "    districts_names.append(x['properties']['nazwa_dzie'])\n",
      "145/169: test_density = list(range(len(districts_names)))\n",
      "145/170:\n",
      "df = pd.DataFrame([districts_names, test_density]).T\n",
      "df.columns = ['nazwa_dzie', 'Density']\n",
      "145/171:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "folium.Choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ").add_to(warsaw_map)\n",
      "warsaw_map\n",
      "145/172:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.Choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/173:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/174: districts_names\n",
      "145/175:\n",
      "for x in warsaw_geojson['features']:\n",
      "    print(x['properties']['nazwa_dzie'])\n",
      "145/176:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=5)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/177: warsaw_geojson\n",
      "145/178:\n",
      "folium.GeoJson(\n",
      "    warsaw_geojson,\n",
      "    name='geojson'\n",
      ").add_to(warsaw_map)\n",
      "warsaw_map\n",
      "145/179:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=12)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/180:\n",
      "folium.GeoJson(\n",
      "    warsaw_geojson,\n",
      "    name='geojson'\n",
      ").add_to(warsaw_map)\n",
      "warsaw_map\n",
      "145/181:\n",
      "warsaw_geofile = 'geodata/warsaw_districts/warsaw_districts.geojson'\n",
      "cracow_geofile = 'geodata/cracow_districts/cracow_districts'\n",
      "145/182:\n",
      "with open(cracow_geofile) as c:\n",
      "    cracow_geojson = json.load(c)\n",
      "145/183:\n",
      "warsaw_geofile = 'geodata/Warsaw_districts/warsaw_districts.geojson'\n",
      "cracow_geofile = 'geodata/Cracow_districts/cracow_districts.geojson'\n",
      "145/184:\n",
      "with open(warsaw_geofile) as w:\n",
      "    warsaw_geojson = json.load(w)\n",
      "145/185:\n",
      "with open(cracow_geofile) as c:\n",
      "    cracow_geojson = json.load(c)\n",
      "145/186:\n",
      "districts_names = []\n",
      "for x in warsaw_geojson['features']:\n",
      "    districts_names.append(x['properties']['nazwa_dzie'])\n",
      "145/187: test_density = list(range(len(districts_names)))\n",
      "145/188:\n",
      "df = pd.DataFrame([districts_names, test_density]).T\n",
      "df.columns = ['nazwa_dzie', 'Density']\n",
      "145/189: districts_names\n",
      "145/190:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=12)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/191:\n",
      "folium.GeoJson(\n",
      "    warsaw_geojson,\n",
      "    name='geojson'\n",
      ").add_to(warsaw_map)\n",
      "warsaw_map\n",
      "145/192:\n",
      "cracow_coordinates = [50.06143, 19.93658]\n",
      "cracow_map = folium.Map(location=cracow_coordinates, zoom_start=12)\n",
      "\n",
      "folium.GeoJson(\n",
      "    cracow_geojson,\n",
      "    name='geojson'\n",
      ").add_to(cracow_map)\n",
      "cracow_map\n",
      "145/193:\n",
      "with open(cracow_geofile) as c:\n",
      "    cracow_geojson = json.load(c)\n",
      "145/194:\n",
      "with open(cracow_geofile) as c:\n",
      "    cracow_geojson = json.load(c)\n",
      "145/195:\n",
      "cracow_coordinates = [50.06143, 19.93658]\n",
      "cracow_map = folium.Map(location=cracow_coordinates, zoom_start=12)\n",
      "\n",
      "folium.GeoJson(\n",
      "    cracow_geojson,\n",
      "    name='geojson'\n",
      ").add_to(cracow_map)\n",
      "cracow_map\n",
      "145/196:\n",
      "cracow_coordinates = [50.06143, 19.93658]\n",
      "cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)\n",
      "\n",
      "folium.GeoJson(\n",
      "    cracow_geojson,\n",
      "    name='geojson'\n",
      ").add_to(cracow_map)\n",
      "cracow_map\n",
      "145/197:\n",
      "warsaw_geofile = 'geodata/Warsaw_districts/warsaw_districts.geojson'\n",
      "cracow_geofile = 'geodata/Cracow_districts/cracow_districts.geojson'\n",
      "145/198:\n",
      "with open(warsaw_geofile) as w:\n",
      "    warsaw_geojson = json.load(w)\n",
      "145/199:\n",
      "with open(cracow_geofile) as c:\n",
      "    cracow_geojson = json.load(c)\n",
      "145/200:\n",
      "districts_names = []\n",
      "for x in warsaw_geojson['features']:\n",
      "    districts_names.append(x['properties']['nazwa_dzie'])\n",
      "145/201: test_density = list(range(len(districts_names)))\n",
      "145/202:\n",
      "df = pd.DataFrame([districts_names, test_density]).T\n",
      "df.columns = ['nazwa_dzie', 'Density']\n",
      "145/203: districts_names\n",
      "145/204:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=12)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/205:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=10)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/206:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.2,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/207: poznan_old_market = folium.Map(location=poznan_coordinates, zoom_start=16)\n",
      "145/208: poznan_old_market\n",
      "145/209:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/210:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 200\n",
      "145/211:\n",
      "import pandas as pd\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import folium\n",
      "import json\n",
      "import requests\n",
      "import os\n",
      "145/212:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 200\n",
      "145/213: CLIENT_ID\n",
      "145/214:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 200\n",
      "RADIUS=1000\n",
      "145/215:\n",
      "folium.Marker(warsaw_location).add_to(warsaw_map)\n",
      "warsaw_map\n",
      "145/216:\n",
      "folium.Marker(warsaw_coordinates).add_to(warsaw_map)\n",
      "warsaw_map\n",
      "145/217: lat, lng = warsaw_coordinates\n",
      "145/218: lat\n",
      "145/219:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            radius, \n",
      "            LIMIT)\n",
      "145/220:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "145/221: result = requests.get(url).response\n",
      "145/222: result = requests.get(url).json()\n",
      "145/223: result\n",
      "145/224: result.keys()\n",
      "145/225: result['meta']\n",
      "145/226: result['response']\n",
      "145/227: result['response'].keys()\n",
      "145/228: result['response']\n",
      "145/229: result['response'].keys()\n",
      "145/230: result['response']['groups']\n",
      "145/231: type(result['response']['groups'])\n",
      "145/232:\n",
      "#type(result['response']['groups'])\n",
      "len(result['response']['groups'])\n",
      "145/233: result['response']['groups'][0]\n",
      "145/234:\n",
      "#type(result['response']['groups'])\n",
      "#len(result['response']['groups'])\n",
      "result['response']['groups'][0].keys()\n",
      "145/235: result['response']['groups'][0]['items']\n",
      "145/236:\n",
      "type(result['response']['groups'][0]['items'])\n",
      "#len(result['response']['groups'])\n",
      "#result['response']['groups'][0].keys()\n",
      "145/237:\n",
      "#type(result['response']['groups'][0]['items'])\n",
      "len(result['response']['groups'][0]['items'])\n",
      "#result['response']['groups'][0].keys()\n",
      "145/238:\n",
      "#type(result['response']['groups'][0]['items'])\n",
      "#len(result['response']['groups'][0]['items'])\n",
      "result['response']['groups'][0].keys()\n",
      "145/239:\n",
      "#type(result['response']['groups'][0]['items'])\n",
      "#len(result['response']['groups'][0]['items'])\n",
      "result['response']['groups'][0]['items'][0].keys()\n",
      "145/240: result['response']['groups'][0]['items'][0]\n",
      "145/241: result['response']['groups'][0]['items'][0]['name']\n",
      "145/242: result['response']['groups'][0]['items'][0]['venue']\n",
      "145/243: result['response']['groups'][0]['items'][0]['venue']['name']\n",
      "145/244: result['response']['groups'][0]['items'][0]['venue']['categories'][0]['name']\n",
      "145/245:\n",
      "for item in result['response']['groups'][0]['items']:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "145/246:\n",
      "#type(result['response']['groups'][0]['items'])\n",
      "len(result['response']['groups'][0]['items'])\n",
      "#result['response']['groups'][0]['items'][0].keys()\n",
      "145/247: len(result['response']['groups'])\n",
      "145/248: len(result['response']['groups']['items'])\n",
      "145/249: len(result['response']['groups'][0]['items'])\n",
      "145/250: result\n",
      "145/251: r = requests.get(url)\n",
      "145/252: r\n",
      "145/253: help(r)\n",
      "145/254: r.text\n",
      "145/255:\n",
      "for item in result['response']['groups'][0]['items']:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['lat'], item['venue']['lng']]).add_to(warsaw_map)\n",
      "145/256:\n",
      "for item in result['response']['groups'][0]['items']:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "145/257: warsaw_map\n",
      "145/258:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 500\n",
      "RADIUS=1000\n",
      "145/259:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "145/260: result = requests.get(url).json()\n",
      "145/261: len(result['response']['groups'][0]['items'])\n",
      "145/262: result['response']\n",
      "145/263:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 500\n",
      "RADIUS=1500\n",
      "145/264:\n",
      "folium.Marker(warsaw_coordinates).add_to(warsaw_map)\n",
      "warsaw_map\n",
      "145/265: lat, lng = warsaw_coordinates\n",
      "145/266:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "145/267: result = requests.get(url).json()\n",
      "145/268: #r = requests.get(url)\n",
      "145/269: len(result['response']['groups'][0]['items'])\n",
      "145/270: result['response']\n",
      "145/271: result['response'].keys()\n",
      "145/272:\n",
      "#type(result['response']['groups'][0]['items'])\n",
      "len(result['response']['groups'][0]['items'])\n",
      "#result['response']['groups'][0]['items'][0].keys()\n",
      "145/273: result['response']['groups'][0]['items'][0]['name']\n",
      "145/274: result['response']['groups'][0]['items'][0]['venue']['name']\n",
      "145/275: result['response']['groups'][0]['items'][0]['venue']['categories'][0]['name']\n",
      "145/276:\n",
      "for item in result['response']['groups'][0]['items']:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "145/277: warsaw_map\n",
      "145/278: result\n",
      "145/279: help(r)\n",
      "145/280: r.next\n",
      "145/281:\n",
      "x = r.next\n",
      "print(x)\n",
      "145/282: r.text\n",
      "145/283: r.headers\n",
      "145/284:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 50\n",
      "RADIUS=1500\n",
      "145/285:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "145/286: result = requests.get(url).json()\n",
      "145/287: #r = requests.get(url)\n",
      "145/288: r.headers\n",
      "145/289: len(result['response']['groups'][0]['items'])\n",
      "145/290: result\n",
      "145/291:\n",
      "OFFSET=50\n",
      "url_2 = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      ")\n",
      "145/292: result_2 = requests.get(url).json()\n",
      "145/293:\n",
      "for item in result_2['response']['groups'][0]['items']:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "145/294: warsaw_map\n",
      "145/295: warsaw_coordinates\n",
      "145/296: warsaw_districts\n",
      "145/297: warsaw_geojson\n",
      "145/298: len(warsaw_geojson['features'])\n",
      "145/299: warsaw_geojson['features'][0]['geometry']['coordinates']\n",
      "145/300: type(warsaw_geojson['features'][0]['geometry']['coordinates'])\n",
      "145/301: warsaw_geojson['features'][0]['geometry']['coordinates'][0]\n",
      "145/302: warsaw_geojson['features'][0]['geometry']['coordinates'][0][0]\n",
      "145/303: len(warsaw_geojson['features'][0]['geometry']['coordinates'][0][0])\n",
      "145/304: t = pd.DataFrame(warsaw_geojson['features'][0]['geometry']['coordinates'][0][0]))\n",
      "145/305: t = pd.DataFrame(warsaw_geojson['features'][0]['geometry']['coordinates'][0][0])\n",
      "145/306: t\n",
      "145/307: t.columns = ['latitude', 'longitude']\n",
      "145/308: t.avg(axis=1)\n",
      "145/309: t.latitude.average()\n",
      "145/310: t.latitude.avg()\n",
      "145/311: help(t.latitude)\n",
      "145/312: t.latitude.mean()\n",
      "145/313: zoliborz_center = [t.latitude.mean(), t.longitude.mean()]\n",
      "145/314:\n",
      "folium.Marker(zoliborz_center).add_to(warsaw_map)\n",
      "warsaw_map\n",
      "145/315: zoliborz_center\n",
      "145/316: t.head()\n",
      "145/317: t.columns = ['longitude', 'latitude']\n",
      "145/318: zoliborz_center = [t.latitude.mean(), t.longitude.mean()]\n",
      "145/319:\n",
      "folium.Marker(zoliborz_center).add_to(warsaw_map)\n",
      "warsaw_map\n",
      "145/320: zoliborz_center\n",
      "145/321:\n",
      "def get_district_center(city_geojson):\n",
      "    district_geometry = pd.DataFrame(\n",
      "        city_geojson['features'][0]['geometry']['coordinates'][0][0],\n",
      "        columns=['longitude', 'latitude']\n",
      "    )\n",
      "    district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]\n",
      "    \n",
      "    return district_center\n",
      "145/322:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]\n",
      "        district_centers[city_geojson['features']['properties']['nazwa_dzie']]=district_center\n",
      "    \n",
      "    district_centers = pd.DataFrame(district_centers, columns=['Distric_name', 'District_center'])\n",
      "    \n",
      "    return district_centers\n",
      "145/323: print( get_district_centers(warsaw_geojson))\n",
      "145/324:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "    \n",
      "    district_centers = pd.DataFrame(district_centers, columns=['Distric_name', 'District_center'])\n",
      "    \n",
      "    return district_centers\n",
      "145/325: print( get_district_centers(warsaw_geojson))\n",
      "145/326:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]\n",
      "        print(district_center)\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "    \n",
      "    district_centers = pd.DataFrame(district_centers, columns=['Distric_name', 'District_center'])\n",
      "    \n",
      "    return district_centers\n",
      "145/327: print( get_district_centers(warsaw_geojson))\n",
      "145/328:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]\n",
      "        #print(district_center)\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "        print(district_centers)\n",
      "    \n",
      "    district_centers = pd.DataFrame(district_centers, columns=['Distric_name', 'District_center'])\n",
      "    \n",
      "    return district_centers\n",
      "145/329: print( get_district_centers(warsaw_geojson))\n",
      "145/330:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]\n",
      "        #print(district_center)\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "        #print(district_centers)\n",
      "    \n",
      "    district_centers = pd.DataFrame(district_centers, columns=['Distric_name', 'District_center'])\n",
      "    print(district_centers)\n",
      "    \n",
      "    return district_centers\n",
      "145/331: print( get_district_centers(warsaw_geojson))\n",
      "145/332:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]\n",
      "        #print(district_center)\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "        #print(district_centers)\n",
      "    \n",
      "    print(district_centers)\n",
      "    district_centers = pd.DataFrame(district_centers, columns=['Distric_name', 'District_center'])\n",
      "    #print(district_centers)\n",
      "    \n",
      "    return district_centers\n",
      "145/333: print( get_district_centers(warsaw_geojson))\n",
      "145/334:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 50\n",
      "RADIUS=15000\n",
      "145/335:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "145/336: result = requests.get(url).json()\n",
      "145/337: #r = requests.get(url)\n",
      "145/338:\n",
      "# number of items depends on LIMIT parameter from the URL\n",
      "len(result['response']['groups'][0]['items'])\n",
      "145/339: result\n",
      "145/340:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 100\n",
      "RADIUS=15000\n",
      "145/341:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "145/342: result = requests.get(url).json()\n",
      "145/343: #r = requests.get(url)\n",
      "145/344:\n",
      "# number of items depends on LIMIT parameter from the URL\n",
      "len(result['response']['groups'][0]['items'])\n",
      "145/345: result\n",
      "145/346:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20180605'\n",
      "LIMIT = 100\n",
      "RADIUS=150\n",
      "145/347:\n",
      "folium.Marker(warsaw_coordinates).add_to(warsaw_map)\n",
      "warsaw_map\n",
      "145/348:\n",
      "#folium.Marker(warsaw_coordinates).add_to(warsaw_map)\n",
      "#warsaw_map\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/349: lat, lng = warsaw_coordinates\n",
      "145/350:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "145/351: result = requests.get(url).json()\n",
      "145/352: #r = requests.get(url)\n",
      "145/353:\n",
      "# number of items depends on LIMIT parameter from the URL\n",
      "len(result['response']['groups'][0]['items'])\n",
      "145/354: result\n",
      "145/355:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=150\n",
      "145/356:\n",
      "#folium.Marker(warsaw_coordinates).add_to(warsaw_map)\n",
      "#warsaw_map\n",
      "145/357: lat, lng = warsaw_coordinates\n",
      "145/358:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=15000\n",
      "145/359:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=15000\n",
      "145/360:\n",
      "#folium.Marker(warsaw_coordinates).add_to(warsaw_map)\n",
      "#warsaw_map\n",
      "145/361: lat, lng = warsaw_coordinates\n",
      "145/362:\n",
      "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "145/363: result = requests.get(url).json()\n",
      "145/364: #r = requests.get(url)\n",
      "145/365:\n",
      "# number of items depends on LIMIT parameter from the URL\n",
      "len(result['response']['groups'][0]['items'])\n",
      "145/366: result\n",
      "145/367: result['response'].keys()\n",
      "145/368:\n",
      "#type(result['response']['groups'][0]['items'])\n",
      "len(result['response']['groups'][0]['items'])\n",
      "#result['response']['groups'][0]['items'][0].keys()\n",
      "145/369: result['response']['groups'][0]['items'][0]['venue']['name']\n",
      "145/370: result['response']['groups'][0]['items'][0]['venue']['categories'][0]['name']\n",
      "145/371:\n",
      "OFFSET=100\n",
      "url_2 = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      ")\n",
      "145/372: result_2 = requests.get(url).json()\n",
      "145/373:\n",
      "for item in result['response']['groups'][0]['items']:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "145/374:\n",
      "for item in result_2['response']['groups'][0]['items']:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "145/375: warsaw_map\n",
      "145/376: a,b = [1,2]\n",
      "145/377: a\n",
      "145/378: result['response']['totalResults']\n",
      "145/379: 238//100\n",
      "145/380:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=15000\n",
      "OFFSET=LIMIT\n",
      "145/381: 38//100\n",
      "145/382: 238//100\n",
      "145/383: ['a'].extend(['b','c'])\n",
      "145/384: a = ['a'].extend(['b','c'])\n",
      "145/385:\n",
      "a = ['a'].extend(['b','c'])\n",
      "a\n",
      "145/386: a\n",
      "145/387: print(a)\n",
      "145/388: ['a'].extend(['b','c'])\n",
      "145/389:\n",
      "a = ['a']\n",
      "a.extend(['b','c'])\n",
      "145/390: print(a)\n",
      "145/391:\n",
      "a = ['a']\n",
      "a.append(['b','c'])\n",
      "145/392: print(a)\n",
      "145/393: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "145/394: zoliborz_venues = get_venues(warsaw_district_centers['Żoliborz'])\n",
      "145/395:\n",
      "def get_venues(distric_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = request.get(url)\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    \n",
      "    requests_to_perform = total_results/100\n",
      "    \n",
      "    for request in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = request.get(url)\n",
      "        venues.expand(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "145/396: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "145/397: zoliborz_venues = get_venues(warsaw_district_centers['Żoliborz'])\n",
      "145/398: warsaw_district_centers\n",
      "145/399: z = {'Żoliborz': [52.2688536216614, 20.985135391429377], 'Praga-Południe': [52.235168690871, 21.071138918572952], 'Mokotów': [52.18866196405862, 21.052814696946914], 'Wola': [52.22969762335711, 20.94634273758449], 'Wilanów': [52.15030830502675, 21.091139416474352], 'Wesoła': [52.225748918635986, 21.226622601358553], 'Wawer': [52.19528077282254, 21.172119282184973], 'Włochy': [52.18437620836855, 20.942760488079408], 'Ursynów': [52.1364151251951, 21.048950071266518], 'Śródmieście': [52.228730887415864, 21.036411009522975], 'Praga-Północ': [52.26643873898277, 21.02908592658115], 'Ursus': [52.192484448920005, 20.884160623792084], 'Targówek': [52.282296973303794, 21.063078989886197], 'Rembertów': [52.257272334893905, 21.141358877459673], 'Ochota': [52.21177552923926, 20.968494312669034], 'Bielany': [52.291264937036225, 20.935172056234144], 'Białołęka': [52.328042537247185, 21.019200173516968], 'Bemowo': [52.24189442018256, 20.90545046459841]}\n",
      "145/400: type(z)\n",
      "145/401: c = pd.DataFrame(z)\n",
      "145/402: c\n",
      "145/403: c = pd.DataFrame(z).T\n",
      "145/404: c\n",
      "145/405:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]\n",
      "        #print(district_center)\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "        #print(district_centers)\n",
      "    \n",
      "    print(district_centers)\n",
      "    district_centers = pd.DataFrame.from_dict(district_centers,\n",
      "                                              orient='index',\n",
      "                                              columns=['Distric_name', 'District_center'])\n",
      "    #print(district_centers)\n",
      "    \n",
      "    return district_centers\n",
      "145/406: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "145/407: zoliborz_venues = get_venues(warsaw_district_centers['Żoliborz'])\n",
      "145/408: warsaw_district_centers\n",
      "145/409: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])\n",
      "145/410:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = request.get(url)\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    \n",
      "    requests_to_perform = total_results/100\n",
      "    \n",
      "    for request in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = request.get(url)\n",
      "        venues.expand(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "145/411: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])\n",
      "145/412:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url)\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    \n",
      "    requests_to_perform = total_results/100\n",
      "    \n",
      "    for request in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = request.get(url)\n",
      "        venues.expand(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "145/413: warsaw_district_centers\n",
      "145/414: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "145/415: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])\n",
      "145/416:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    \n",
      "    requests_to_perform = total_results/100\n",
      "    \n",
      "    for request in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = request.get(url).json()\n",
      "        venues.expand(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "145/417: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "145/418: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])\n",
      "145/419: 238/100\n",
      "145/420: 238//100\n",
      "145/421:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    \n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for request in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = request.get(url).json()\n",
      "        venues.expand(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "145/422: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "145/423: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])\n",
      "145/424:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    \n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.expand(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "145/425: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "145/426: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])\n",
      "145/427: [1].expand([2])\n",
      "145/428: [1].expend([2])\n",
      "145/429: [1].extand([2])\n",
      "145/430: [1].extend([2])\n",
      "145/431:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    \n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "145/432: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "145/433: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])\n",
      "145/434: zoliborz_venues\n",
      "145/435:\n",
      "for item in zoliborz_venues:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "145/436: warsaw_map\n",
      "145/437:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/438:\n",
      "for item in zoliborz_venues:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "145/439: warsaw_map\n",
      "145/440: len(zoliborz_venues)\n",
      "145/441:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=3000\n",
      "OFFSET=LIMIT\n",
      "145/442:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    \n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "145/443: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "145/444: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])\n",
      "145/445: len(zoliborz_venues)\n",
      "145/446:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/447:\n",
      "for item in zoliborz_venues:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "145/448: warsaw_map\n",
      "145/449:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=10000\n",
      "OFFSET=LIMIT\n",
      "145/450:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/451: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])\n",
      "145/452: warsaw_map\n",
      "145/453: len(zoliborz_venues)\n",
      "145/454:\n",
      "for item in zoliborz_venues:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "145/455: warsaw_map\n",
      "145/456:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('Total results: ', total_results)\n",
      "    \n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "145/457: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])\n",
      "145/458:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/459:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=15000\n",
      "OFFSET=LIMIT\n",
      "145/460:\n",
      "#folium.Marker(warsaw_coordinates).add_to(warsaw_map)\n",
      "#warsaw_map\n",
      "145/461: result = requests.get(url).json()\n",
      "145/462: #r = requests.get(url)\n",
      "145/463: zoliborz_venues = get_venues(warsaw_district_centers.loc['Żoliborz'])\n",
      "145/464: zoliborz_venues = get_venues(warsaw_district_centers.loc['Wola'])\n",
      "145/465: zoliborz_venues = get_venues(warsaw_district_centers.loc['Włochy'])\n",
      "145/466: zoliborz_venues = get_venues(warsaw_district_centers.loc['Bielany'])\n",
      "145/467: warsaw_map\n",
      "145/468:\n",
      "for item in zoliborz_venues:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "145/469: warsaw_map\n",
      "145/470:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "145/471:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=2000\n",
      "OFFSET=LIMIT\n",
      "145/472: zoliborz_venues = get_venues(warsaw_district_centers.loc['Bielany'])\n",
      "145/473: warsaw_map\n",
      "145/474:\n",
      "for item in zoliborz_venues:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "145/475: warsaw_map\n",
      "145/476:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=3000\n",
      "OFFSET=LIMIT\n",
      "145/477: zoliborz_venues = get_venues(warsaw_district_centers.loc['Bielany'])\n",
      "145/478:\n",
      "for item in zoliborz_venues:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "145/479: warsaw_map\n",
      "145/480: zoliborz_venues = get_venues(warsaw_district_centers.loc['Wawer'])\n",
      "145/481:\n",
      "for item in zoliborz_venues:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "145/482: warsaw_map\n",
      "145/483: zoliborz_venues = get_venues(warsaw_district_centers.loc['Śródmieście'])\n",
      "145/484:\n",
      "for item in zoliborz_venues:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "145/485: warsaw_map\n",
      "145/486: zoliborz_venues = get_venues(warsaw_district_centers.loc['Mokotów'])\n",
      "145/487:\n",
      "for item in zoliborz_venues:\n",
      "    print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "    folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "145/488: warsaw_map\n",
      "147/1:\n",
      "import pandas as pd\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import folium\n",
      "import json\n",
      "import requests\n",
      "import os\n",
      "147/2:\n",
      "# geojson files exported from .shp with QGIS (EPSG:4326 WGS 84)\n",
      "warsaw_geofile = 'geodata/Warsaw_districts/warsaw_districts.geojson'\n",
      "cracow_geofile = 'geodata/Cracow_districts/cracow_districts.geojson'\n",
      "147/3:\n",
      "with open(warsaw_geofile) as w:\n",
      "    warsaw_geojson = json.load(w)\n",
      "147/4:\n",
      "with open(cracow_geofile) as c:\n",
      "    cracow_geojson = json.load(c)\n",
      "147/5:\n",
      "districts_names = []\n",
      "for x in warsaw_geojson['features']:\n",
      "    districts_names.append(x['properties']['nazwa_dzie'])\n",
      "147/6: test_density = list(range(len(districts_names)))\n",
      "147/7:\n",
      "df = pd.DataFrame([districts_names, test_density]).T\n",
      "df.columns = ['nazwa_dzie', 'Density']\n",
      "147/8:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "147/9:\n",
      "cracow_coordinates = [50.06143, 19.93658]\n",
      "cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)\n",
      "\n",
      "folium.GeoJson(\n",
      "    cracow_geojson,\n",
      "    name='geojson'\n",
      ").add_to(cracow_map)\n",
      "cracow_map\n",
      "147/10:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=3000\n",
      "OFFSET=LIMIT\n",
      "147/11:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "    \n",
      "    district_centers = pd.DataFrame.from_dict(district_centers,\n",
      "                                              orient='index',\n",
      "                                              columns=['Distric_name', 'District_center'])\n",
      "    \n",
      "    return district_centers\n",
      "147/12:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('Total results: ', total_results)\n",
      "    \n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "147/13: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "147/14: warsaw_distric_centers\n",
      "147/15: warsaw_district_centers\n",
      "147/16: warsaw_district_centers.index\n",
      "147/17:\n",
      "warsaw_districts_venues = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    warsaw_district_venues[district] = get_venues(warsaw_district_centers.loc[district])\n",
      "147/18:\n",
      "warsaw_districts_venues = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district])\n",
      "147/19: warsaw_districts_venues\n",
      "147/20: warsaw_map\n",
      "147/21:\n",
      "for key in warsaw_districts_venues.keys():    \n",
      "    for item in warsaw_districts_venues[key]:\n",
      "        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "147/22: warsaw_map\n",
      "147/23:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "147/24: warsaw_map\n",
      "147/25:\n",
      "for center in warsaw_districts_centers:\n",
      "    #folium.Marker()\n",
      "    print(center)\n",
      "147/26:\n",
      "for center in warsaw_district_centers:\n",
      "    #folium.Marker()\n",
      "    print(center)\n",
      "147/27: warsaw_district_centers.head()\n",
      "147/28:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    #print('Total results: ', total_results)\n",
      "    \n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "147/29: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "147/30: warsaw_district_centers.head()\n",
      "147/31:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "    \n",
      "    print(district_centers)\n",
      "    district_centers = pd.DataFrame.from_dict(district_centers,\n",
      "                                              orient='index',\n",
      "                                              columns=['Distric_name', 'District_center'])\n",
      "    \n",
      "    return district_centers\n",
      "147/32: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "147/33:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "    \n",
      "    print(district_centers)\n",
      "    district_centers = pd.DataFrame.from_dict(district_centers,\n",
      "                                              orient='index',\n",
      "                                              columns=['Distric_name', 'District_center'])\n",
      "    \n",
      "    return district_centers\n",
      "147/34: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "147/35:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "    \n",
      "    print(district_centers)\n",
      "    district_centers = pd.DataFrame.from_dict(district_centers,\n",
      "                                              orient='index',\n",
      "                                              columns=['District_center'])\n",
      "    \n",
      "    return district_centers\n",
      "147/36: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "147/37: warsaw_district_centers\n",
      "147/38: warsaw_district_centers.head()\n",
      "147/39:\n",
      "for center in warsaw_district_centers.index:\n",
      "    folium.Marker(warsaw_district_centers.loc[center]).add_to(warsaw_map)\n",
      "    #print(center)\n",
      "147/40: warsaw_map\n",
      "147/41:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "147/42: warsaw_map\n",
      "147/43:\n",
      "for center in warsaw_district_centers.index:\n",
      "    folium.Marker(warsaw_district_centers.loc[center]).add_to(warsaw_map)\n",
      "    #print(center)\n",
      "147/44: warsaw_map\n",
      "147/45: warsaw_district_centers.loc['Wola']\n",
      "147/46: folium.Marker(warsaw_district_centers.loc['Wola']).add_to(warsaw_map)\n",
      "147/47: warsaw_map\n",
      "147/48: warsaw_map\n",
      "147/49:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "147/50: warsaw_map\n",
      "147/51: folium.Marker(warsaw_district_centers.loc['Wola']).add_to(warsaw_map)\n",
      "147/52: warsaw_map\n",
      "147/53: folium.Marker(warsaw_district_centers.loc['Wola']).add_to(warsaw_map)\n",
      "147/54: warsaw_map\n",
      "147/55: folium.Marker([warsaw_district_centers.loc['Wola'][1], warsaw_district_centers.loc['Wola'][0]]).add_to(warsaw_map)\n",
      "147/56: [warsaw_district_centers.loc['Wola'][1], warsaw_district_centers.loc['Wola'][0]]\n",
      "147/57: warsaw_district_centers.loc['Wola'][1], warsaw_district_centers.loc['Wola'][0]\n",
      "147/58: warsaw_district_centers.loc['Wola'][1]\n",
      "147/59: warsaw_district_centers.loc['Wola']\n",
      "147/60: type(warsaw_district_centers.loc['Wola'])\n",
      "147/61: type(warsaw_district_centers.loc['Wola'][0])\n",
      "147/62: type(warsaw_district_centers.loc['Wola'][0][0])\n",
      "147/63: folium.Marker([warsaw_district_centers.loc['Wola'][0][1], warsaw_district_centers.loc['Wola'][0][0]]).add_to(warsaw_map)\n",
      "147/64: warsaw_map\n",
      "147/65:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "147/66: warsaw_map\n",
      "147/67: folium.Marker([warsaw_district_centers.loc['Wola'][0][1], warsaw_district_centers.loc['Wola'][0][0]]).add_to(warsaw_map)\n",
      "147/68: warsaw_map\n",
      "147/69:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "147/70: folium.Marker([warsaw_district_centers.loc['Wola'][0][0], warsaw_district_centers.loc['Wola'][0][1]]).add_to(warsaw_map)\n",
      "147/71: warsaw_map\n",
      "147/72: warsaw_district_centers.loc['Wola']\n",
      "147/73: list(warsaw_district_centers.loc['Wola'])\n",
      "147/74: warsaw_district_centers.loc['Wola']['Distric_center']\n",
      "147/75: warsaw_district_centers.loc['Wola']['District_center']\n",
      "147/76: warsaw_district_centers.loc['Wola','District_center']\n",
      "147/77:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "147/78:\n",
      "for center in warsaw_district_centers.index:\n",
      "    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)\n",
      "    #print(center)\n",
      "147/79: warsaw_map\n",
      "147/80:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('Total results: ', total_results)\n",
      "    \n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "147/81:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=4000\n",
      "OFFSET=LIMIT\n",
      "147/82:\n",
      "warsaw_districts_venues = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district])\n",
      "147/83:\n",
      "warsaw_districts_venues = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "147/84:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=2000\n",
      "OFFSET=LIMIT\n",
      "147/85:\n",
      "warsaw_districts_venues = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "147/86: len([warsaw_district_venues[key] for key in warsaw_district_venues.keys()])\n",
      "147/87: len([warsaw_districts_venues[key] for key in warsaw_district_venues.keys()])\n",
      "147/88: len([warsaw_districts_venues[key] for key in warsaw_districts_venues.keys()])\n",
      "147/89: warsaw_district_venues['Wola']\n",
      "147/90: warsaw_districts_venues['Wola']\n",
      "147/91: sum(len([warsaw_districts_venues[key]) for key in warsaw_districts_venues.keys()])\n",
      "147/92:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=6000\n",
      "OFFSET=LIMIT\n",
      "147/93:\n",
      "warsaw_districts_venues = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "147/94:\n",
      "for key in warsaw_districts_venues.keys():    \n",
      "    for item in warsaw_districts_venues[key]:\n",
      "        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "147/95: warsaw_map\n",
      "148/1:\n",
      "import pandas as pd\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import folium\n",
      "import json\n",
      "import requests\n",
      "import os\n",
      "from shapely.geometry import Point, Polygon\n",
      "148/2: p1 = warsaw_district_centers.loc['Wola', 'Distric_Center']\n",
      "148/3:\n",
      "import pandas as pd\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import folium\n",
      "import json\n",
      "import requests\n",
      "import os\n",
      "from shapely.geometry import Point, Polygon\n",
      "148/4:\n",
      "# geojson files exported from .shp with QGIS (EPSG:4326 WGS 84)\n",
      "warsaw_geofile = 'geodata/Warsaw_districts/warsaw_districts.geojson'\n",
      "cracow_geofile = 'geodata/Cracow_districts/cracow_districts.geojson'\n",
      "148/5:\n",
      "with open(warsaw_geofile) as w:\n",
      "    warsaw_geojson = json.load(w)\n",
      "148/6:\n",
      "with open(cracow_geofile) as c:\n",
      "    cracow_geojson = json.load(c)\n",
      "148/7:\n",
      "districts_names = []\n",
      "for x in warsaw_geojson['features']:\n",
      "    districts_names.append(x['properties']['nazwa_dzie'])\n",
      "148/8: test_density = list(range(len(districts_names)))\n",
      "148/9:\n",
      "df = pd.DataFrame([districts_names, test_density]).T\n",
      "df.columns = ['nazwa_dzie', 'Density']\n",
      "148/10:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "148/11:\n",
      "cracow_coordinates = [50.06143, 19.93658]\n",
      "cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)\n",
      "\n",
      "folium.GeoJson(\n",
      "    cracow_geojson,\n",
      "    name='geojson'\n",
      ").add_to(cracow_map)\n",
      "cracow_map\n",
      "148/12:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=6000\n",
      "OFFSET=LIMIT\n",
      "148/13:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=2000\n",
      "OFFSET=LIMIT\n",
      "148/14:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "    \n",
      "    print(district_centers)\n",
      "    district_centers = pd.DataFrame.from_dict(district_centers,\n",
      "                                              orient='index',\n",
      "                                              columns=['District_center'])\n",
      "    \n",
      "    return district_centers\n",
      "148/15:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('Total results: ', total_results)\n",
      "    \n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "148/16: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "148/17:\n",
      "warsaw_districts_venues = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "148/18: def check_if_inside_district(venue, district_shape):\n",
      "148/19: warsaw_district_centers.head()\n",
      "148/20:\n",
      "for center in warsaw_district_centers.index:\n",
      "    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)\n",
      "    #print(center)\n",
      "148/21:\n",
      "for key in warsaw_districts_venues.keys():    \n",
      "    for item in warsaw_districts_venues[key]:\n",
      "        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "148/22: warsaw_map\n",
      "148/23: warsaw_geojson\n",
      "148/24: p1 = warsaw_district_centers.loc['Wola', 'Distric_Center']\n",
      "148/25: p1 = warsaw_district_centers.loc['Wola', 'Distric_center']\n",
      "148/26: p1 = warsaw_district_centers.loc['Wola', 'District_center']\n",
      "148/27: p1\n",
      "148/28: warsaw_geojson['features'][0]\n",
      "148/29: warsaw_geojson['features'][0]['geometry']\n",
      "148/30: p1 = [warsaw_district_centers.loc['Wola', 'District_center'][1], warsaw_district_centers.loc['Wola', 'District_center'][0]]\n",
      "148/31: p1\n",
      "148/32: warsaw_geojson['features'][0]['geometry']['coordinates'][0]\n",
      "148/33: warsaw_geojson['features'][0]['geometry']['coordinates'][0][0]\n",
      "148/34: coords = warsaw_geojson['features'][0]['geometry']['coordinates'][0][0]\n",
      "148/35: poly = Polygon(coords)\n",
      "148/36: poly\n",
      "148/37: p1 = [warsaw_district_centers.loc['Żoliborz', 'District_center'][1], warsaw_district_centers.loc['Żoliborz', 'District_center'][0]]\n",
      "148/38: p1\n",
      "148/39: p1.within(poly)\n",
      "148/40: p1 = Point(p1)\n",
      "148/41: p1.within(poly)\n",
      "148/42: p2 = [warsaw_district_centers.loc['Wola', 'District_center'][1], warsaw_district_centers.loc['Wola', 'District_center'][0]]\n",
      "148/43: p2 = Point(p2)\n",
      "148/44: p2.within(poly)\n",
      "148/45:\n",
      "def check_if_inside_district(venue_coords, district_shape):\n",
      "    p = Point(venue_coords)\n",
      "    poly = Polygon(district_shape)\n",
      "    return p.within(poly)\n",
      "148/46: warsaw_district_venues\n",
      "148/47: warsaw_districts_venues\n",
      "148/48: warsaw_districts_venues['Żoliborz']\n",
      "148/49: x = pd.DataFrame(warsaw_districts_venues)\n",
      "148/50: x = pd.DataFrame.from_dict(warsaw_districts_venues)\n",
      "148/51: warsaw_districts_venues['Żoliborz'][0]\n",
      "148/52: warsaw_districts_venues['Żoliborz'][1]\n",
      "148/53: x = pd.DataFrame.from_dict(warsaw_districts_venues['Żoliborz'])\n",
      "148/54: x\n",
      "148/55: x = pd.DataFrame.from_dict(warsaw_districts_venues['Żoliborz']['venue'])\n",
      "148/56: x\n",
      "148/57: x = pd.DataFrame.from_dict(warsaw_districts_venues['Żoliborz'][0]['venue'])\n",
      "148/58: x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][0]['venue'])\n",
      "148/59: x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue'])\n",
      "148/60: warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "148/61: warsaw_districts_venues['Żoliborz'][1]#['venue']\n",
      "148/62:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "zol_venues=[]\n",
      "for item in warsaw_districts_venues['Żoliborz']:\n",
      "    x={}\n",
      "    x['name']=item['venue']['name']\n",
      "    x['lat'] = item['venue']['location']['lat']\n",
      "    x['lon'] = item['venue']['location']['lon']\n",
      "    zol_venues.append(x)\n",
      "148/63:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "zol_venues=[]\n",
      "for item in warsaw_districts_venues['Żoliborz']:\n",
      "    x={}\n",
      "    x['name']=item['venue']['name']\n",
      "    x['lat'] = item['venue']['location']['lat']\n",
      "    x['lon'] = item['venue']['location']['lng']\n",
      "    zol_venues.append(x)\n",
      "148/64: zol_venues\n",
      "148/65: zol_df = pd.DataFrame(zol_venues)\n",
      "148/66: zol_df\n",
      "148/67: zol_venues_2 = dict('Żoliborz': zol_venues)\n",
      "148/68: zol_venues_2 = dict(Żoliborz: zol_venues)\n",
      "148/69: zol_venues_2 = dict(Żoliborz=zol_venues)\n",
      "148/70: zol_df = pd.DataFrame(zol_venues_2)\n",
      "148/71: zol_df\n",
      "148/72: zol_df = pd.DataFrame.from_dict(zol_venues_2, orient='index')\n",
      "148/73: zol_df\n",
      "148/74:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "zol_venues=[]\n",
      "for item in warsaw_districts_venues['Żoliborz']:\n",
      "    x={}\n",
      "    x['district']='Żoliborz'\n",
      "    x['name']=item['venue']['name']\n",
      "    x['lat'] = item['venue']['location']['lat']\n",
      "    x['lon'] = item['venue']['location']['lng']\n",
      "    zol_venues.append(x)\n",
      "148/75: zol_venues\n",
      "148/76: zol_venues_2 = dict(Żoliborz=zol_venues)\n",
      "148/77: zol_df = pd.DataFrame(zol_venues_2)\n",
      "148/78: zol_df\n",
      "148/79:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "zol_venues=[]\n",
      "for item in warsaw_districts_venues['Żoliborz']:\n",
      "    x={}\n",
      "    x['district']='Żoliborz'\n",
      "    x['name']=item['venue']['name']\n",
      "    x['lat'] = item['venue']['location']['lat']\n",
      "    x['lon'] = item['venue']['location']['lng']\n",
      "    zol_venues.append(x)\n",
      "148/80: zol_venues\n",
      "148/81: zol_df = pd.DataFrame(zol_venues)\n",
      "148/82: zol_df\n",
      "148/83:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "def get_district_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']='district'\n",
      "            x['Name']=item['venue']['name']\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "    all_city_venues.append(district_df)\n",
      "148/84: h = get_districts_venues(warsaw_districts_venues)\n",
      "148/85:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']='district'\n",
      "            x['Name']=item['venue']['name']\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "    all_city_venues.append(district_df)\n",
      "148/86: h = get_districts_venues(warsaw_districts_venues)\n",
      "148/87: h.head()\n",
      "148/88:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']='district'\n",
      "            x['Name']=item['venue']['name']\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "    all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "148/89: h = get_districts_venues(warsaw_districts_venues)\n",
      "148/90: h.head()\n",
      "148/91:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        print(district)\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']='district'\n",
      "            x['Name']=item['venue']['name']\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "    all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "148/92: h = get_districts_venues(warsaw_districts_venues)\n",
      "148/93:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        print(district)\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']='district'\n",
      "            x['Name']=item['venue']['name']\n",
      "            print(x['Name'])\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "    all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "148/94: h = get_districts_venues(warsaw_districts_venues)\n",
      "148/95:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        print(district)\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']='district'\n",
      "            x['Name']=item['venue']['name']\n",
      "            #print(x['Name'])\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "        print(district_df.head())\n",
      "    all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "148/96: h = get_districts_venues(warsaw_districts_venues)\n",
      "148/97:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        print(district)\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            #print(x['Name'])\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "        print(district_df.head())\n",
      "    all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "148/98: h = get_districts_venues(warsaw_districts_venues)\n",
      "148/99:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        print(district)\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            #print(x['Name'])\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "       # print(district_df.head())\n",
      "    all_city_venues.append(district_df)\n",
      "    print(all_city_venues.head())\n",
      "    return all_city_venues\n",
      "148/100: h = get_districts_venues(warsaw_districts_venues)\n",
      "148/101:\n",
      "h = pd.DataFrame()\n",
      "c = pd.DataFrame({'a':1, 'b':2})\n",
      "148/102:\n",
      "h = pd.DataFrame()\n",
      "c = pd.DataFrame({'a'=1, 'b'=2})\n",
      "148/103:\n",
      "h = pd.DataFrame()\n",
      "c = pd.DataFrame([{'a':1, 'b':2}])\n",
      "148/104: c\n",
      "148/105: h\n",
      "148/106: h.append(c)\n",
      "148/107:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        print(district)\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            #print(x['Name'])\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "       # print(district_df.head())\n",
      "        all_city_venues.append(district_df)\n",
      "        print(all_city_venues.head())\n",
      "    return all_city_venues\n",
      "148/108: h = get_districts_venues(warsaw_districts_venues)\n",
      "148/109: h\n",
      "148/110:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        print(district)\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            #print(x['Name'])\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "        print(district_df.head())\n",
      "        all_city_venues.append(district_df)\n",
      "        print(all_city_venues.head())\n",
      "    return all_city_venues\n",
      "148/111: h = get_districts_venues(warsaw_districts_venues)\n",
      "148/112:\n",
      "h = pd.DataFrame()\n",
      "c = pd.DataFrame([{'a':1, 'b':2}])\n",
      "148/113: h.append(c)\n",
      "148/114: h\n",
      "148/115: h.append(c, inplace=True)\n",
      "148/116: h.append(c)\n",
      "148/117: h\n",
      "148/118: h = h.append(c)\n",
      "148/119: h\n",
      "148/120:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        print(district)\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            #print(x['Name'])\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "        print(district_df.head())\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "        print(all_city_venues.head())\n",
      "    return all_city_venues\n",
      "148/121: h = get_districts_venues(warsaw_districts_venues)\n",
      "148/122: h\n",
      "148/123: warsaw_geojson\n",
      "148/124: cracow_geojson\n",
      "150/1:\n",
      "import pandas as pd\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import folium\n",
      "import json\n",
      "import requests\n",
      "import os\n",
      "from shapely.geometry import Point, Polygon\n",
      "150/2:\n",
      "# geojson files exported from .shp with QGIS (EPSG:4326 WGS 84)\n",
      "warsaw_geofile = 'geodata/Warsaw_districts/warsaw_districts.geojson'\n",
      "cracow_geofile = 'geodata/Cracow_districts/cracow_districts.geojson'\n",
      "150/3:\n",
      "with open(warsaw_geofile) as w:\n",
      "    warsaw_geojson = json.load(w)\n",
      "150/4:\n",
      "with open(cracow_geofile) as c:\n",
      "    cracow_geojson = json.load(c)\n",
      "150/5:\n",
      "districts_names = []\n",
      "for x in warsaw_geojson['features']:\n",
      "    districts_names.append(x['properties']['nazwa_dzie'])\n",
      "150/6:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "150/7: test_density = list(range(len(districts_names)))\n",
      "150/8:\n",
      "df = pd.DataFrame([districts_names, test_density]).T\n",
      "df.columns = ['nazwa_dzie', 'Density']\n",
      "150/9:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "150/10:\n",
      "# cracow_coordinates = [50.06143, 19.93658]\n",
      "# cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)\n",
      "\n",
      "# folium.GeoJson(\n",
      "#     cracow_geojson,\n",
      "#     name='geojson'\n",
      "# ).add_to(cracow_map)\n",
      "# cracow_map\n",
      "150/11:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=2000\n",
      "OFFSET=LIMIT\n",
      "150/12:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "    \n",
      "    print(district_centers)\n",
      "    district_centers = pd.DataFrame.from_dict(district_centers,\n",
      "                                              orient='index',\n",
      "                                              columns=['District_center'])\n",
      "    \n",
      "    return district_centers\n",
      "150/13:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('Total results: ', total_results)\n",
      "    \n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/14: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "150/15:\n",
      "warsaw_districts_venues = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/16:\n",
      "def check_if_inside_district(venue_coords, district_shape):\n",
      "    p = Point(venue_coords)\n",
      "    poly = Polygon(district_shape)\n",
      "    return p.within(poly)\n",
      "150/17: warsaw_district_centers.head()\n",
      "150/18:\n",
      "for center in warsaw_district_centers.index:\n",
      "    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)\n",
      "    #print(center)\n",
      "150/19:\n",
      "for key in warsaw_districts_venues.keys():    \n",
      "    for item in warsaw_districts_venues[key]:\n",
      "        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "150/20: warsaw_map\n",
      "150/21: coords = warsaw_geojson['features'][0]['geometry']['coordinates'][0][0]\n",
      "150/22: warsaw_geojson\n",
      "150/23:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        print(district)\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            #print(x['Name'])\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "        print(district_df.head())\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "        print(all_city_venues.head())\n",
      "    return all_city_venues\n",
      "150/24: h = get_districts_venues(warsaw_districts_venues)\n",
      "150/25:\n",
      "#x = pd.DataFrame(warsaw_districts_venues['Żoliborz'][1]['venue']\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        print(district)\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            #print(x['Name'])\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "       # print(district_df.head())\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "        #print(all_city_venues.head())\n",
      "    return all_city_venues\n",
      "150/26: h = get_districts_venues(warsaw_districts_venues)\n",
      "150/27: h\n",
      "150/28:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=1500\n",
      "OFFSET=LIMIT\n",
      "150/29:\n",
      "warsaw_districts_venues = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/30:\n",
      "for key in warsaw_districts_venues.keys():    \n",
      "    for item in warsaw_districts_venues[key]:\n",
      "        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "150/31: warsaw_map\n",
      "150/32:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        print(district)\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            #print(x['Name'])\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "       # print(district_df.head())\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "        #print(all_city_venues.head())\n",
      "    return all_city_venues\n",
      "150/33: h = get_districts_venues(warsaw_districts_venues)\n",
      "150/34: h\n",
      "150/35: h.size()\n",
      "150/36: h.size\n",
      "150/37: h.shape\n",
      "150/38: h\n",
      "150/39: h.Name.unique()\n",
      "150/40: len(h.Name.unique())\n",
      "150/41:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "150/42:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=2000\n",
      "OFFSET=LIMIT\n",
      "150/43:\n",
      "warsaw_districts_venues = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/44:\n",
      "def check_if_inside_district(venue_coords, district_shape):\n",
      "    p = Point(venue_coords)\n",
      "    poly = Polygon(district_shape)\n",
      "    return p.within(poly)\n",
      "150/45: warsaw_district_centers.head()\n",
      "150/46:\n",
      "for center in warsaw_district_centers.index:\n",
      "    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)\n",
      "    #print(center)\n",
      "150/47:\n",
      "for key in warsaw_districts_venues.keys():    \n",
      "    for item in warsaw_districts_venues[key]:\n",
      "        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "150/48: warsaw_map\n",
      "150/49: h.loc['Wawer']\n",
      "150/50: h = get_districts_venues(warsaw_districts_venues)\n",
      "150/51: h.shape\n",
      "150/52: len(h.Name.unique())\n",
      "150/53: h.loc['Wawer']\n",
      "150/54: h\n",
      "150/55: h.loc[h.District=='Wawer']\n",
      "150/56: h.loc[h.District=='Wawer'].Name.unique()\n",
      "150/57:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=5000\n",
      "OFFSET=LIMIT\n",
      "150/58:\n",
      "warsaw_districts_venues = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/59:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "150/60: warsaw_map\n",
      "150/61:\n",
      "for key in warsaw_districts_venues.keys():    \n",
      "    for item in warsaw_districts_venues[key]:\n",
      "        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "150/62: warsaw_map\n",
      "150/63: h = get_districts_venues(warsaw_districts_venues)\n",
      "150/64: len(h.Name.unique())\n",
      "150/65: h\n",
      "150/66: len(h.Name.unique())\n",
      "150/67: h.loc[h.District=='Wawer'].Name.unique()\n",
      "150/68:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "    \n",
      "    district_centers = pd.DataFrame.from_dict(district_centers,\n",
      "                                              orient='index',\n",
      "                                              columns=['District_center'])\n",
      "    \n",
      "    return district_centers\n",
      "150/69: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "150/70: warsaw_district_venues\n",
      "150/71: warsaw_districts_venues\n",
      "150/72: warsaw_districts_venues['Żoliborz']\n",
      "150/73: warsaw_districts_venues['Żoliborz'][0]\n",
      "150/74:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        print(district)\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            x['VenueId'] = item['venue']['id']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/75: h = get_districts_venues(warsaw_districts_venues)\n",
      "150/76: h\n",
      "150/77: len(h.VenueId.unique())\n",
      "150/78: h.shape\n",
      "150/79: h.drop_duplicates(subset='VenueId',inplace=True)\n",
      "150/80: h.shape\n",
      "150/81: warsaw_geojson\n",
      "150/82:\n",
      "def create_districts_polygons(geojson):\n",
      "    polygons = {}\n",
      "    for district in geojson['features']:\n",
      "        polygons[district['nazwa_dzie']]=distric['geometry'][coordinates][0][0]\n",
      "    return polygons\n",
      "150/83: create_districts_polygons(warsaw_geojson)\n",
      "150/84:\n",
      "def create_districts_polygons(geojson):\n",
      "    polygons = {}\n",
      "    for district in geojson['features']:\n",
      "        polygons[district['nazwa_dzie']]=district['geometry'][coordinates][0][0]\n",
      "    return polygons\n",
      "150/85: create_districts_polygons(warsaw_geojson)\n",
      "150/86:\n",
      "def create_districts_polygons(geojson):\n",
      "    polygons = {}\n",
      "    for district in geojson['features']:\n",
      "        polygons[district['nazwa_dzie']]=district['geometry']['coordinates'][0][0]\n",
      "    return polygons\n",
      "150/87: create_districts_polygons(warsaw_geojson)\n",
      "150/88:\n",
      "def create_districts_polygons(geojson):\n",
      "    polygons = {}\n",
      "    for district in geojson['features']:\n",
      "        polygons[district['properties']['nazwa_dzie']]=district['geometry']['coordinates'][0][0]\n",
      "    return polygons\n",
      "150/89: create_districts_polygons(warsaw_geojson)\n",
      "150/90: test_density = 1 #list(range(len(districts_names)))\n",
      "150/91:\n",
      "df = pd.DataFrame([districts_names, test_density]).T\n",
      "df.columns = ['nazwa_dzie', 'Density']\n",
      "150/92: test_density = list(range(len(districts_names)))\n",
      "150/93:\n",
      "df = pd.DataFrame([districts_names, test_density]).T\n",
      "df.columns = ['nazwa_dzie', 'Density']\n",
      "150/94:\n",
      "warsaw_districts_names = []\n",
      "for x in warsaw_geojson['features']:\n",
      "    districts_names.append(x['properties']['nazwa_dzie'])\n",
      "150/95: warsaw_district_centers\n",
      "150/96: warsaw_district_centers.head()\n",
      "150/97:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "150/98:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "warsaw_map\n",
      "150/99:\n",
      "for center in warsaw_district_centers.index:\n",
      "    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)\n",
      "    warsaw_map\n",
      "150/100:\n",
      "for center in warsaw_district_centers.index:\n",
      "    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)\n",
      "warsaw_map\n",
      "150/101:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('\\tTotal results: ', total_results, '\\n')\n",
      "    \n",
      "    #checking if there is more results -if true, next request with offset is send\n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/102:\n",
      "warsaw_districts_venues = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/103:\n",
      "warsaw_districts_venues = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/104:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('\\tTotal results: ', total_results, '\\n')\n",
      "    \n",
      "    #checking if there is more results -if true, next request with offset is send\n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/105:\n",
      "warsaw_districts_venues = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/106:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "    \n",
      "    district_centers = pd.DataFrame.from_dict(district_centers,\n",
      "                                              orient='index',\n",
      "                                              columns=['District_center'])\n",
      "    \n",
      "    return district_centers\n",
      "150/107:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        print(district)\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            x['VenueId'] = item['venue']['id']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/108: warsaw_districts_venues = get_district_venues(warsaw_districts_venues)\n",
      "150/109: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues)\n",
      "150/110:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            x['VenueId'] = item['venue']['id']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/111: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues)\n",
      "150/112:\n",
      "warsaw_districts_venues_foursquare = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/113:\n",
      "warsaw_districts_venues_foursquare = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/114:\n",
      "warsaw_districts_venues_foursquare = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/115: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "150/116: warsaw_district_centers.head()\n",
      "150/117:\n",
      "for center in warsaw_district_centers.index:\n",
      "    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)\n",
      "warsaw_map\n",
      "150/118:\n",
      "warsaw_districts_venues_foursquare = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/119:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "    \n",
      "    district_centers = pd.DataFrame.from_dict(district_centers,\n",
      "                                              orient='index',\n",
      "                                              columns=['District_center'])\n",
      "    \n",
      "    return district_centers\n",
      "150/120:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=5000\n",
      "OFFSET=LIMIT\n",
      "150/121:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('\\tTotal results: ', total_results, '\\n')\n",
      "    \n",
      "    #checking if there is more results -if true, next request with offset is send\n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/122: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "150/123: warsaw_district_centers.head()\n",
      "150/124:\n",
      "for center in warsaw_district_centers.index:\n",
      "    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)\n",
      "warsaw_map\n",
      "150/125:\n",
      "warsaw_districts_venues_foursquare = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/126:\n",
      "warsaw_districts_venues_foursquare = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues_foursquare[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/127:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            x['VenueId'] = item['venue']['id']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/128: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues)\n",
      "150/129: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/130: warsaw_district_venues.head()\n",
      "150/131: warsaw_districts_venues.head()\n",
      "150/132: warsaw_districts_venues.shape\n",
      "150/133: warsaw_districts_venues = warsaw_districts_venues.drop_duplicates(subset='VenueId')\n",
      "150/134: warsaw_districts_venues.shape\n",
      "150/135:\n",
      "def create_districts_polygons(geojson):\n",
      "    polygons = {}\n",
      "    for district in geojson['features']:\n",
      "        polygons[district['properties']['nazwa_dzie']]=district['geometry']['coordinates'][0][0]\n",
      "    return polygons\n",
      "150/136: warsaw_districts_venues.head()\n",
      "150/137:\n",
      "def create_districts_polygons(geojson):\n",
      "    polygons = {}\n",
      "    for district in geojson['features']:\n",
      "        polygons[district['properties']['nazwa_dzie']]=district['geometry']['coordinates'][0][0]\n",
      "    return polygons\n",
      "150/138: warsaw_districts_polygons = create_districts_polygons(warsaw_geojson)\n",
      "150/139: warsaw_districts_polygons.head()\n",
      "150/140: warsaw_districts_polygons\n",
      "150/141:\n",
      "def check_if_inside_district(venue_coords, district_shape):\n",
      "    p = Point(venue_coords)\n",
      "    poly = Polygon(district_shape)\n",
      "    return p.within(poly)\n",
      "150/142:\n",
      "warsaw_districts_venues['inside'] = check_if_inside_district(\n",
      "                                        [warsaw_districts_venues['Lat'], warsaw_districrs_venues['Lon']],\n",
      "                                        warsaw_districts_polygons[warsaw_districts_venues['District']])\n",
      "150/143:\n",
      "warsaw_districts_venues['inside'] = check_if_inside_district(\n",
      "                                        [warsaw_districts_venues['Lat'], warsaw_districts_venues['Lon']],\n",
      "                                        warsaw_districts_polygons[warsaw_districts_venues['District']])\n",
      "150/144: warsaw_districts_venues.index\n",
      "150/145: warsaw_districts_venues.index[90]\n",
      "150/146: warsaw_districts_venues.index[390]\n",
      "150/147: warsaw_districts_venues\n",
      "150/148: warsaw_districts_venues.head(200)\n",
      "150/149: warsaw_districts_venues.reset_index(inplace=True)\n",
      "150/150: warsaw_districts_venues.index[390]\n",
      "150/151: warsaw_districts_venues.loc[0]\n",
      "150/152: warsaw_districts_venues.loc[0, 'District']\n",
      "150/153:\n",
      "for i in warsaw_districts_venues.index:\n",
      "    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [warsaw_districts_venues[i,'Lat'], warsaw_districts_venues[i,'Lon']],\n",
      "                                        warsaw_districts_polygons[warsaw_districts_venues[i,'District']])\n",
      "150/154: warsaw_districts_venues.loc[0, 'Lat']\n",
      "150/155:\n",
      "for i in warsaw_districts_venues.index:\n",
      "    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [warsaw_districts_venues.loc[i,'Lat'], warsaw_districts_venues.loc[i,'Lon']],\n",
      "                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])\n",
      "150/156: warsaw_districts_venues.head()\n",
      "150/157: warsaw_districts_venues.head(60)\n",
      "150/158:\n",
      "for i in warsaw_districts_venues.index:\n",
      "    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],\n",
      "                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])\n",
      "150/159: warsaw_districts_venues.head(60)\n",
      "150/160: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]\n",
      "150/161:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=1000\n",
      "OFFSET=LIMIT\n",
      "150/162:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('\\tTotal results: ', total_results, '\\n')\n",
      "    \n",
      "    #checking if there is more results -if true, next request with offset is send\n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/163: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "150/164: warsaw_district_centers.head()\n",
      "150/165:\n",
      "for center in warsaw_district_centers.index:\n",
      "    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)\n",
      "warsaw_map\n",
      "150/166:\n",
      "warsaw_districts_venues_foursquare = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues_foursquare[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/167:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            x['VenueId'] = item['venue']['id']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/168: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/169: warsaw_districts_venues.shape\n",
      "150/170: #warsaw_districts_venues = warsaw_districts_venues.drop_duplicates(subset='VenueId')\n",
      "150/171: #warsaw_districts_venues.shape\n",
      "150/172: #warsaw_districts_venues.head()\n",
      "150/173:\n",
      "def create_districts_polygons(geojson):\n",
      "    polygons = {}\n",
      "    for district in geojson['features']:\n",
      "        polygons[district['properties']['nazwa_dzie']]=district['geometry']['coordinates'][0][0]\n",
      "    return polygons\n",
      "150/174: warsaw_districts_polygons = create_districts_polygons(warsaw_geojson)\n",
      "150/175: warsaw_districts_polygons\n",
      "150/176:\n",
      "def check_if_inside_district(venue_coords, district_shape):\n",
      "    p = Point(venue_coords)\n",
      "    poly = Polygon(district_shape)\n",
      "    return p.within(poly)\n",
      "150/177:\n",
      "for i in warsaw_districts_venues.index:\n",
      "    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],\n",
      "                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])\n",
      "150/178: warsaw_districts_venues.reset_index(inplace=True)\n",
      "150/179:\n",
      "for i in warsaw_districts_venues.index:\n",
      "    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],\n",
      "                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])\n",
      "150/180: warsaw_districts_venues.reset_index(inplace=True)\n",
      "150/181: warsaw_districts_venues.index[390]\n",
      "150/182: warsaw_districts_venues.loc[0, 'Lat']\n",
      "150/183: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]\n",
      "150/184:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=1500\n",
      "OFFSET=LIMIT\n",
      "150/185:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('\\tTotal results: ', total_results, '\\n')\n",
      "    \n",
      "    #checking if there is more results -if true, next request with offset is send\n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/186: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "150/187: warsaw_district_centers.head()\n",
      "150/188:\n",
      "for center in warsaw_district_centers.index:\n",
      "    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)\n",
      "warsaw_map\n",
      "150/189:\n",
      "warsaw_districts_venues_foursquare = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues_foursquare[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/190:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            x['VenueId'] = item['venue']['id']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/191: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/192: warsaw_districts_venues.shape\n",
      "150/193: #warsaw_districts_venues = warsaw_districts_venues.drop_duplicates(subset='VenueId')\n",
      "150/194: #warsaw_districts_venues.shape\n",
      "150/195: #warsaw_districts_venues.head()\n",
      "150/196:\n",
      "def create_districts_polygons(geojson):\n",
      "    polygons = {}\n",
      "    for district in geojson['features']:\n",
      "        polygons[district['properties']['nazwa_dzie']]=district['geometry']['coordinates'][0][0]\n",
      "    return polygons\n",
      "150/197: warsaw_districts_polygons = create_districts_polygons(warsaw_geojson)\n",
      "150/198: warsaw_districts_polygons\n",
      "150/199:\n",
      "def check_if_inside_district(venue_coords, district_shape):\n",
      "    p = Point(venue_coords)\n",
      "    poly = Polygon(district_shape)\n",
      "    return p.within(poly)\n",
      "150/200: warsaw_districts_venues.reset_index(inplace=True)\n",
      "150/201:\n",
      "for i in warsaw_districts_venues.index:\n",
      "    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],\n",
      "                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])\n",
      "150/202: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]\n",
      "150/203:\n",
      "for key in warsaw_districts_venues.keys():    \n",
      "    for item in warsaw_districts_venues[key]:\n",
      "        #print(item['venue']['name'], ' ', item['venue']['categories'][0]['name'])\n",
      "        folium.Marker([item['venue']['location']['lat'], item['venue']['location']['lng']]).add_to(warsaw_map)\n",
      "150/204:\n",
      "for i in warsaw_districts_venues.index:    \n",
      "    folium.Marker(warsaw_districts_venues.loc[i,'Lat'], warsaw_districts_venues.loc[i,'Lon']).add_to(warsaw_map)\n",
      "150/205: warsaw_districts_venues.loc[0,'Lat']\n",
      "150/206: warsaw_districts_venues.loc[0,'Lon']\n",
      "150/207:\n",
      "for i in warsaw_districts_venues.index:    \n",
      "    folium.Marker([warsaw_districts_venues.loc[i,'Lat'], warsaw_districts_venues.loc[i,'Lon']]).add_to(warsaw_map)\n",
      "150/208: warsaw_map\n",
      "150/209:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "150/210:\n",
      "# cracow_coordinates = [50.06143, 19.93658]\n",
      "# cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)\n",
      "\n",
      "# folium.GeoJson(\n",
      "#     cracow_geojson,\n",
      "#     name='geojson'\n",
      "# ).add_to(cracow_map)\n",
      "# cracow_map\n",
      "150/211:\n",
      "def get_district_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]\n",
      "        district_centers[district['properties']['nazwa_dzie']]=district_center\n",
      "    \n",
      "    district_centers = pd.DataFrame.from_dict(district_centers,\n",
      "                                              orient='index',\n",
      "                                              columns=['District_center'])\n",
      "    \n",
      "    return district_centers\n",
      "150/212:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=1500\n",
      "OFFSET=LIMIT\n",
      "150/213:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('\\tTotal results: ', total_results, '\\n')\n",
      "    \n",
      "    #checking if there is more results -if true, next request with offset is send\n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/214:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=2500\n",
      "OFFSET=LIMIT\n",
      "150/215:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=2500\n",
      "OFFSET=LIMIT\n",
      "150/216:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('\\tTotal results: ', total_results, '\\n')\n",
      "    \n",
      "    #checking if there is more results -if true, next request with offset is send\n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/217: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "150/218: warsaw_district_centers.head()\n",
      "150/219:\n",
      "for center in warsaw_district_centers.index:\n",
      "    folium.Marker(warsaw_district_centers.loc[center, 'District_center']).add_to(warsaw_map)\n",
      "warsaw_map\n",
      "150/220:\n",
      "warsaw_districts_venues_foursquare = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues_foursquare[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/221:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            x['VenueId'] = item['venue']['id']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/222: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/223: warsaw_districts_venues.shape\n",
      "150/224: warsaw_districts_venues.reset_index(inplace=True)\n",
      "150/225:\n",
      "for i in warsaw_districts_venues.index:\n",
      "    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],\n",
      "                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])\n",
      "150/226: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]\n",
      "150/227:\n",
      "for i in warsaw_districts_venues.index:    \n",
      "    folium.Marker([warsaw_districts_venues.loc[i,'Lat'], warsaw_districts_venues.loc[i,'Lon']]).add_to(warsaw_map)\n",
      "150/228: warsaw_map\n",
      "150/229:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "150/230:\n",
      "# cracow_coordinates = [50.06143, 19.93658]\n",
      "# cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)\n",
      "\n",
      "# folium.GeoJson(\n",
      "#     cracow_geojson,\n",
      "#     name='geojson'\n",
      "# ).add_to(cracow_map)\n",
      "# cracow_map\n",
      "150/231:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=3500\n",
      "OFFSET=LIMIT\n",
      "150/232:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('\\tTotal results: ', total_results, '\\n')\n",
      "    \n",
      "    #checking if there is more results -if true, next request with offset is send\n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/233:\n",
      "warsaw_districts_venues_foursquare = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues_foursquare[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/234:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            x['VenueId'] = item['venue']['id']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/235: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/236: warsaw_districts_venues.shape\n",
      "150/237: warsaw_districts_venues.reset_index(inplace=True)\n",
      "150/238:\n",
      "for i in warsaw_districts_venues.index:\n",
      "    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],\n",
      "                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])\n",
      "150/239: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]\n",
      "150/240:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "150/241:\n",
      "# cracow_coordinates = [50.06143, 19.93658]\n",
      "# cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)\n",
      "\n",
      "# folium.GeoJson(\n",
      "#     cracow_geojson,\n",
      "#     name='geojson'\n",
      "# ).add_to(cracow_map)\n",
      "# cracow_map\n",
      "150/242:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=3000\n",
      "OFFSET=LIMIT\n",
      "150/243:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('\\tTotal results: ', total_results, '\\n')\n",
      "    \n",
      "    #checking if there is more results -if true, next request with offset is send\n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/244:\n",
      "warsaw_districts_venues_foursquare = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues_foursquare[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/245: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/246: warsaw_districts_venues.shape\n",
      "150/247: warsaw_districts_venues.reset_index(inplace=True)\n",
      "150/248:\n",
      "for i in warsaw_districts_venues.index:\n",
      "    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],\n",
      "                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])\n",
      "150/249: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]\n",
      "150/250:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=2500\n",
      "OFFSET=LIMIT\n",
      "150/251:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('\\tTotal results: ', total_results, '\\n')\n",
      "    \n",
      "    #checking if there is more results -if true, next request with offset is send\n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/252: warsaw_district_centers = get_district_centers(warsaw_geojson)\n",
      "150/253: warsaw_district_centers.head()\n",
      "150/254:\n",
      "warsaw_districts_venues_foursquare = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues_foursquare[district] = get_venues(warsaw_district_centers.loc[district, 'District_center'])\n",
      "150/255:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            x['VenueId'] = item['venue']['id']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/256: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/257: warsaw_districts_venues.shape\n",
      "150/258: warsaw_districts_venues.reset_index(inplace=True)\n",
      "150/259:\n",
      "for i in warsaw_districts_venues.index:\n",
      "    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],\n",
      "                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])\n",
      "150/260: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]\n",
      "150/261:\n",
      "for i in warsaw_districts_venues.index:    \n",
      "    folium.Marker([warsaw_districts_venues.loc[i,'Lat'], warsaw_districts_venues.loc[i,'Lon']]).add_to(warsaw_map)\n",
      "150/262: warsaw_map\n",
      "150/263: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer']\n",
      "150/264: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer'].unique()\n",
      "150/265: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer'].unique\n",
      "150/266: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer']\n",
      "150/267: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer'].VenueId.unique()\n",
      "150/268: len(warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer'].VenueId.unique())\n",
      "150/269: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer']\n",
      "150/270: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wawer'].head()\n",
      "150/271: warsaw_districts_centers\n",
      "150/272: warsaw_district_centers\n",
      "150/273: x = get_venues(warsaw_district_centers.loc['Wawer', 'District_center'])\n",
      "150/274: x = get_venues(warsaw_district_centers.loc['Bielany', 'District_center'])\n",
      "150/275:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=2700\n",
      "OFFSET=LIMIT\n",
      "150/276:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('\\tTotal results: ', total_results, '\\n')\n",
      "    \n",
      "    #checking if there is more results -if true, next request with offset is send\n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/277: x = get_venues(warsaw_district_centers.loc['Bielany', 'District_center'])\n",
      "150/278: x = get_venues(warsaw_district_centers.loc['Wawer', 'District_center'])\n",
      "150/279: warsaw_districts_venues.groupby('Category').value_counts()\n",
      "150/280: x = get_venues(warsaw_district_centers.loc['Wawer', 'District_center'])\n",
      "150/281: warsaw_districts_venues.groupby('Category')['VenueId'].value_counts()\n",
      "150/282: warsaw_districts_venues.groupby('Category')['VenueId'].value_counts(sort=True)\n",
      "150/283: warsaw_districts_venues.groupby('Category').value_counts(sort=True)\n",
      "150/284: warsaw_districts_venues.groupby('Category').count()\n",
      "150/285: warsaw_districts_venues.groupby('Category').count().sort_values(by='VenueId')\n",
      "150/286: warsaw_districts_venues.groupby('Category').count().sort_values(by='VenueId', ascending=False)\n",
      "150/287: warsaw_districts_venues.groupby('Category').count().sort_values(by='VenueId', ascending=False).head(10)\n",
      "150/288:\n",
      "warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "warsaw_map.choropleth(\n",
      "    geo_data=warsaw_geojson,\n",
      "    name='choropleth',\n",
      "    data=df,\n",
      "    columns=['nazwa_dzie', 'Density'],\n",
      "    key_on='feature.properties.nazwa_dzie',\n",
      "    fill_color='YlGn',\n",
      "    fill_opacity=0.7,\n",
      "    line_opacity=0.9,\n",
      "    legend_name='Density'\n",
      ")\n",
      "#warsaw_map\n",
      "150/289: warsaw_map\n",
      "150/290: x\n",
      "150/291: x = get_district_venues(x)\n",
      "150/292: x = get_districts_venues(x)\n",
      "150/293: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wesoła'].head()\n",
      "150/294: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wesoła'].head(10)\n",
      "150/295: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wesoła'].head(20)\n",
      "150/296: x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/297:\n",
      "def extract_data(district_venues_foursquare):\n",
      "    district_venues=[]\n",
      "    for item in district_venues_foursquare:\n",
      "        x={}\n",
      "        #x['District']=district\n",
      "        x['Name']=item['venue']['name']\n",
      "        x['Category']=item['venue']['categories'][0]['name']\n",
      "        x['Lat'] = item['venue']['location']['lat']\n",
      "        x['Lon'] = item['venue']['location']['lng']\n",
      "        x['VenueId'] = item['venue']['id']\n",
      "        district_venues.append(x)\n",
      "    district_df = pd.DataFrame(district_venues)\n",
      "    return district_df\n",
      "150/298: z = extract_data(x)\n",
      "150/299: z\n",
      "150/300: x = get_venues(warsaw_district_centers.loc['Wesoła', 'District_center'])\n",
      "150/301: z = extract_data(x)\n",
      "150/302: z\n",
      "150/303:\n",
      "def pin_venues(venues_df,city_map):\n",
      "    for i in venues_df.index:\n",
      "        folium.Marker([venues_df.loc[i,'Lat'], venues_df.loc[i,'Lon']]).add_to(city_map)\n",
      "150/304: pin_venues(z, warsaw_map)\n",
      "150/305: warsaw_map\n",
      "150/306: x = get_venues(warsaw_district_centers.loc['Białołęka', 'District_center'])\n",
      "150/307: z = extract_data(x)\n",
      "150/308: z.head()\n",
      "150/309: pin_venues(z, warsaw_map)\n",
      "150/310: warsaw_map\n",
      "150/311:\n",
      "def create_warsaw_map():    \n",
      "    warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "    warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "    warsaw_map.choropleth(\n",
      "        geo_data=warsaw_geojson,\n",
      "        name='choropleth',\n",
      "        data=df,\n",
      "        columns=['nazwa_dzie', 'Density'],\n",
      "        key_on='feature.properties.nazwa_dzie',\n",
      "        fill_color='YlGn',\n",
      "        fill_opacity=0.7,\n",
      "        line_opacity=0.9,\n",
      "        legend_name='Density'\n",
      "    )\n",
      "#warsaw_map\n",
      "150/312:\n",
      "def create_warsaw_map():    \n",
      "    warsaw_coordinates = [52.2297700, 21.0117800]\n",
      "    warsaw_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "    warsaw_map.choropleth(\n",
      "        geo_data=warsaw_geojson,\n",
      "        name='choropleth',\n",
      "        data=df,\n",
      "        columns=['nazwa_dzie', 'Density'],\n",
      "        key_on='feature.properties.nazwa_dzie',\n",
      "        fill_color='YlGn',\n",
      "        fill_opacity=0.7,\n",
      "        line_opacity=0.9,\n",
      "        legend_name='Density'\n",
      "    )\n",
      "    return warsaw_map\n",
      "150/313: warsaw_map = create_warsaw_map\n",
      "150/314: x = get_venues(warsaw_district_centers.loc['Wawer', 'District_center'])\n",
      "150/315: warsaw_map = create_warsaw_map\n",
      "150/316: z = extract_data(x)\n",
      "150/317: z.head()\n",
      "150/318: pin_venues(z, warsaw_map)\n",
      "150/319: warsaw_map = create_warsaw_map()\n",
      "150/320: z = extract_data(x)\n",
      "150/321: z.head()\n",
      "150/322:\n",
      "def pin_venues(venues_df,city_map):\n",
      "    for i in venues_df.index:\n",
      "        folium.Marker([venues_df.loc[i,'Lat'], venues_df.loc[i,'Lon']]).add_to(city_map)\n",
      "150/323: pin_venues(z, warsaw_map)\n",
      "150/324: warsaw_map\n",
      "150/325:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=3500\n",
      "OFFSET=LIMIT\n",
      "150/326:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('\\tTotal results: ', total_results, '\\n')\n",
      "    \n",
      "    #checking if there is more results -if true, next request with offset is send\n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/327: warsaw_districts_venues.groupby('Category').count().sort_values(by='VenueId', ascending=False).head(5)\n",
      "150/328: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wesoła'].head(5)\n",
      "150/329: warsaw_map = create_warsaw_map()\n",
      "150/330: z = extract_data(x)\n",
      "150/331: z.head()\n",
      "150/332:\n",
      "def pin_venues(venues_df,city_map):\n",
      "    for i in venues_df.index:\n",
      "        folium.Marker([venues_df.loc[i,'Lat'], venues_df.loc[i,'Lon']]).add_to(city_map)\n",
      "150/333: pin_venues(z, warsaw_map)\n",
      "150/334: warsaw_map\n",
      "150/335:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=4000\n",
      "OFFSET=LIMIT\n",
      "150/336:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('\\tTotal results: ', total_results, '\\n')\n",
      "    \n",
      "    #checking if there is more results -if true, next request with offset is send\n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/337: x = get_venues(warsaw_district_centers.loc['Wawer', 'District_center'])\n",
      "150/338: z = extract_data(x)\n",
      "150/339: z.head()\n",
      "150/340:\n",
      "def pin_venues(venues_df,city_map):\n",
      "    for i in venues_df.index:\n",
      "        folium.Marker([venues_df.loc[i,'Lat'], venues_df.loc[i,'Lon']]).add_to(city_map)\n",
      "150/341: pin_venues(z, warsaw_map)\n",
      "150/342: warsaw_map\n",
      "150/343:\n",
      "CLIENT_ID = os.environ.get('FOURSQUAREID')\n",
      "CLIENT_SECRET = os.environ.get('FOURSQUARESECRET')\n",
      "VERSION = '20200605'\n",
      "LIMIT = 100\n",
      "RADIUS=4500\n",
      "OFFSET=LIMIT\n",
      "150/344:\n",
      "def get_venues(district_center):\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('\\tTotal results: ', total_results, '\\n')\n",
      "    \n",
      "    #checking if there is more results -if true, next request with offset is send\n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/345: x = get_venues(warsaw_district_centers.loc['Wawer', 'District_center'])\n",
      "150/346: warsaw_map = create_warsaw_map()\n",
      "150/347:\n",
      "def extract_data(district_venues_foursquare):\n",
      "    district_venues=[]\n",
      "    for item in district_venues_foursquare:\n",
      "        x={}\n",
      "        #x['District']=district\n",
      "        x['Name']=item['venue']['name']\n",
      "        x['Category']=item['venue']['categories'][0]['name']\n",
      "        x['Lat'] = item['venue']['location']['lat']\n",
      "        x['Lon'] = item['venue']['location']['lng']\n",
      "        x['VenueId'] = item['venue']['id']\n",
      "        district_venues.append(x)\n",
      "    district_df = pd.DataFrame(district_venues)\n",
      "    return district_df\n",
      "150/348: z = extract_data(x)\n",
      "150/349: z.head()\n",
      "150/350:\n",
      "def pin_venues(venues_df,city_map):\n",
      "    for i in venues_df.index:\n",
      "        folium.Marker([venues_df.loc[i,'Lat'], venues_df.loc[i,'Lon']]).add_to(city_map)\n",
      "150/351: pin_venues(z, warsaw_map)\n",
      "150/352: warsaw_map\n",
      "150/353: x = get_venues(warsaw_district_centers.loc['Białołęka', 'District_center'])\n",
      "150/354: z = extract_data(x)\n",
      "150/355: pin_venues(z, warsaw_map)\n",
      "150/356: warsaw_map\n",
      "150/357: warsaw_districts_polygons\n",
      "150/358: warsaw_districts_centers\n",
      "150/359: warsaw_district_centers\n",
      "150/360:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_districts_centers(district_name)\n",
      "    center_point = Point(center_coords)\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons(district_name):\n",
      "        polygon_point = Point(point_coords)\n",
      "        polygon_point.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    return max(distances)\n",
      "150/361: max([0,1])\n",
      "150/362: c = calculate_radius('Wawer')\n",
      "150/363:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers(district_name)\n",
      "    center_point = Point(center_coords)\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons(district_name):\n",
      "        polygon_point = Point(point_coords)\n",
      "        polygon_point.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    return max(distances)\n",
      "150/364: max([0,1])\n",
      "150/365: c = calculate_radius('Wawer')\n",
      "150/366:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc(district_name)\n",
      "    center_point = Point(center_coords)\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons(district_name):\n",
      "        polygon_point = Point(point_coords)\n",
      "        polygon_point.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    return max(distances)\n",
      "150/367: max([0,1])\n",
      "150/368: c = calculate_radius('Wawer')\n",
      "150/369: warsaw_district_centers\n",
      "150/370: warsaw_district_centers.loc['Wawer']\n",
      "150/371:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name]\n",
      "    center_point = Point(center_coords)\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons(district_name):\n",
      "        polygon_point = Point(point_coords)\n",
      "        polygon_point.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    return max(distances)\n",
      "150/372: c = calculate_radius('Wawer')\n",
      "150/373:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name]\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons(district_name):\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_point.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    return max(distances)\n",
      "150/374: c = calculate_radius('Wawer')\n",
      "150/375:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name]\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons(district_name):\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_point.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    return max(distances)\n",
      "150/376: c = calculate_radius('Wawer')\n",
      "150/377:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name]\n",
      "    print(center_coords)\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons(district_name):\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_point.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    return max(distances)\n",
      "150/378: c = calculate_radius('Wawer')\n",
      "150/379: warsaw_district_centers.loc['Wawer'][1]\n",
      "150/380: warsaw_district_centers.loc['Wawer']\n",
      "150/381: warsaw_district_centers#.loc['Wawer']\n",
      "150/382: warsaw_district_centers.loc['Wawer']\n",
      "150/383: warsaw_district_centers.loc['Wawer'][0]\n",
      "150/384: list(warsaw_district_centers.loc['Wawer'])\n",
      "150/385: list(warsaw_district_centers.loc['Wawer', 'Distric_center'])\n",
      "150/386: list(warsaw_district_centers.loc['Wawer', 'District_center'])\n",
      "150/387:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name, 'District_center']\n",
      "    print(center_coords)\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons(district_name):\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_point.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    return max(distances)\n",
      "150/388: c = calculate_radius('Wawer')\n",
      "150/389:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name, 'District_center']\n",
      "    print(center_coords)\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons [district_name]:\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_point.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    return max(distances)\n",
      "150/390: c = calculate_radius('Wawer')\n",
      "150/391:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name, 'District_center']\n",
      "    print(center_coords)\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons [district_name]:\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_points.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    return max(distances)\n",
      "150/392: c = calculate_radius('Wawer')\n",
      "150/393: c\n",
      "150/394: k = 0.6157*111\n",
      "150/395: c*k\n",
      "150/396:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name, 'District_center']\n",
      "    print(center_coords)\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons [district_name]:\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_points.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    max_distance = max(distances)    \n",
      "    return max(distances)\n",
      "150/397:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name, 'District_center']\n",
      "    print(center_coords)\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons [district_name]:\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_points.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    max_distance = max(distances)\n",
      "    factor = 0.6157*111/1000\n",
      "    max_distance_meters = max_distance*factor\n",
      "    \n",
      "    return max_distance_meters\n",
      "150/398: c = calculate_radius('Wawer')\n",
      "150/399:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name, 'District_center']\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons [district_name]:\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_points.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    max_distance = max(distances)\n",
      "    factor = 0.6157*111/1000\n",
      "    max_distance_meters = max_distance*factor\n",
      "    \n",
      "    return max_distance_meters\n",
      "150/400: c = calculate_radius('Wawer')\n",
      "150/401: c\n",
      "150/402:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name, 'District_center']\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons [district_name]:\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_points.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    max_distance = max(distances)\n",
      "    factor = 0.6157*111\n",
      "    max_distance_meters = max_distance*factor\n",
      "    \n",
      "    return max_distance_meters\n",
      "150/403: c = calculate_radius('Wawer')\n",
      "150/404: c\n",
      "150/405:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name, 'District_center']\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons [district_name]:\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_points.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    max_distance = max(distances)\n",
      "    factor = 0.6157*111*1000\n",
      "    max_distance_meters = max_distance*factor\n",
      "    \n",
      "    return max_distance_meters\n",
      "150/406: c = calculate_radius('Wawer')\n",
      "150/407: c\n",
      "150/408: c = calculate_radius('Wesoła')\n",
      "150/409: c\n",
      "150/410: DISTANCE\n",
      "150/411: RADIUS\n",
      "150/412:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name, 'District_center']\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons [district_name]:\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_points.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    max_distance = max(distances)\n",
      "    factor = 0.6157*111,3*1000\n",
      "    max_distance_meters = max_distance*factor\n",
      "    \n",
      "    return max_distance_meters\n",
      "150/413: c = calculate_radius('Wesoła')\n",
      "150/414:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name, 'District_center']\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons [district_name]:\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_points.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    max_distance = max(distances)\n",
      "    factor = 0.6157*111.3*1000\n",
      "    max_distance_meters = max_distance*factor\n",
      "    \n",
      "    return max_distance_meters\n",
      "150/415: c = calculate_radius('Wesoła')\n",
      "150/416: c\n",
      "150/417: c = calculate_radius('Żoliborz')\n",
      "150/418: c\n",
      "150/419: c = calculate_radius('Wawer')\n",
      "150/420: c\n",
      "150/421: warsaw_district_centers\n",
      "150/422:\n",
      "warsaw_districts_radiuses = {}\n",
      "for district in warsaw_districts_centers.index:\n",
      "    warsaw_districts_radiuses[district] = calculate_radius(district)\n",
      "150/423:\n",
      "warsaw_districts_radiuses = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    warsaw_districts_radiuses[district] = calculate_radius(district)\n",
      "150/424: warsaw_districts_radiuses\n",
      "150/425:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name, 'District_center']\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons [district_name]:\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_points.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    max_distance = max(distances)\n",
      "    factor = 0.6157*111.3*1000\n",
      "    max_distance_meters = int(max_distance*factor)\n",
      "    \n",
      "    return max_distance_meters\n",
      "150/426: c = calculate_radius('Wawer')\n",
      "150/427:\n",
      "warsaw_districts_radiuses = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    warsaw_districts_radiuses[district] = calculate_radius(district)\n",
      "150/428: warsaw_districts_radiuses\n",
      "150/429:\n",
      "def calculate_radius(district_name):\n",
      "    center_coords = warsaw_district_centers.loc[district_name, 'District_center']\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in warsaw_districts_polygons [district_name]:\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_points.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    max_distance = max(distances)\n",
      "    factor = 0.6157*111.3*1000\n",
      "    max_distance_meters = int(max_distance*factor)\n",
      "    \n",
      "    return max_distance_meters\n",
      "150/430:\n",
      "warsaw_districts_radiuses = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    warsaw_districts_radiuses[district] = calculate_radius(district)\n",
      "150/431:\n",
      "def get_venues(district_center, radius):\n",
      "    RADIUS = radius\n",
      "    lat, lng = district_center\n",
      "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT)\n",
      "    result = requests.get(url).json()\n",
      "    \n",
      "    venues = result['response']['groups'][0]['items']\n",
      "    \n",
      "    total_results = result['response']['totalResults']\n",
      "    print('\\tTotal results: ', total_results, '\\n')\n",
      "    \n",
      "    #checking if there is more results -if true, next request with offset is send\n",
      "    requests_to_perform = total_results//100\n",
      "    \n",
      "    for _ in range(requests_to_perform):\n",
      "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&offset={}'.format(\n",
      "            CLIENT_ID, \n",
      "            CLIENT_SECRET, \n",
      "            VERSION, \n",
      "            lat, \n",
      "            lng, \n",
      "            RADIUS, \n",
      "            LIMIT,\n",
      "            OFFSET\n",
      "        )\n",
      "        result = requests.get(url).json()\n",
      "        venues.extend(result['response']['groups'][0]['items'])\n",
      "        \n",
      "    return venues\n",
      "150/432:\n",
      "warsaw_districts_venues_foursquare = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues_foursquare[district] = get_venues(\n",
      "        warsaw_district_centers.loc[district, 'District_center'],\n",
      "        warsaw_districts_radiuses[district])\n",
      "150/433:\n",
      "warsaw_districts_venues_foursquare = {}\n",
      "for district in warsaw_district_centers.index:\n",
      "    print(district)\n",
      "    warsaw_districts_venues_foursquare[district] = get_venues(\n",
      "        warsaw_district_centers.loc[district, 'District_center'],\n",
      "        warsaw_districts_radiuses[district])\n",
      "150/434:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_venues=[]\n",
      "        for item in city_districts_venues[district]:\n",
      "            x={}\n",
      "            x['District']=district\n",
      "            x['Name']=item['venue']['name']\n",
      "            x['Category']=item['venue']['categories'][0]['name']\n",
      "            x['Lat'] = item['venue']['location']['lat']\n",
      "            x['Lon'] = item['venue']['location']['lng']\n",
      "            x['VenueId'] = item['venue']['id']\n",
      "            district_venues.append(x)\n",
      "        district_df = pd.DataFrame(district_venues)\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/435: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/436: warsaw_districts_venues.shape\n",
      "150/437:\n",
      "def create_districts_polygons(geojson):\n",
      "    polygons = {}\n",
      "    for district in geojson['features']:\n",
      "        polygons[district['properties']['nazwa_dzie']]=district['geometry']['coordinates'][0][0]\n",
      "    return polygons\n",
      "150/438: warsaw_districts_polygons = create_districts_polygons(warsaw_geojson)\n",
      "150/439:\n",
      "def check_if_inside_district(venue_coords, district_shape):\n",
      "    p = Point(venue_coords)\n",
      "    poly = Polygon(district_shape)\n",
      "    return p.within(poly)\n",
      "150/440: warsaw_districts_venues.reset_index(inplace=True)\n",
      "150/441:\n",
      "for i in warsaw_districts_venues.index:\n",
      "    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],\n",
      "                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])\n",
      "150/442: warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]\n",
      "150/443: warsaw_map = create_warsaw_map()\n",
      "150/444:\n",
      "for i in warsaw_districts_venues.index:    \n",
      "    folium.Marker([warsaw_districts_venues.loc[i,'Lat'], warsaw_districts_venues.loc[i,'Lon']]).add_to(warsaw_map)\n",
      "150/445: warsaw_map\n",
      "150/446: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wesoła'].head(20)\n",
      "150/447: warsaw_districts_venues = warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]\n",
      "150/448: warsaw_map = create_warsaw_map()\n",
      "150/449:\n",
      "for i in warsaw_districts_venues.index:    \n",
      "    folium.Marker([warsaw_districts_venues.loc[i,'Lat'], warsaw_districts_venues.loc[i,'Lon']]).add_to(warsaw_map)\n",
      "150/450: warsaw_map\n",
      "150/451:\n",
      "def create_districts_polygons(geojson):\n",
      "    polygons = {}\n",
      "    for district in geojson['features']:\n",
      "        polygons[district['properties']['nazwa_dzie']]=district['geometry']['coordinates'][0][0]\n",
      "    return polygons\n",
      "150/452: warsaw_districts_polygons = create_districts_polygons(warsaw_geojson)\n",
      "150/453: warsaw_districts_venues.loc[warsaw_districts_venues['District']=='Wesoła'].head()\n",
      "150/454: warsaw_top5 = warsaw_districts_venues.groupby('Category').count().sort_values(by='VenueId', ascending=False).head()\n",
      "150/455: warsaw_top5\n",
      "150/456:\n",
      "def extract_data(district_venues_foursquare):\n",
      "    district_venues=[]\n",
      "    for item in district_venues_foursquare:\n",
      "        x={}\n",
      "        x['Name']=item['venue']['name']\n",
      "        x['Category']=item['venue']['categories'][0]['name']\n",
      "        x['Lat'] = item['venue']['location']['lat']\n",
      "        x['Lon'] = item['venue']['location']['lng']\n",
      "        x['VenueId'] = item['venue']['id']\n",
      "        district_venues.append(x)\n",
      "    district_df = pd.DataFrame(district_venues)\n",
      "    return district_df\n",
      "150/457:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_df = extract_data(district)\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/458: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/459:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_df = extract_data[city_district_venues]\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/460:\n",
      "# def get_districts_venues(city_districts_venues):\n",
      "#     all_city_venues= pd.DataFrame()\n",
      "#     for district in city_districts_venues.keys():\n",
      "#         district_venues=[]\n",
      "#         for item in city_districts_venues[district]:\n",
      "#             x={}\n",
      "#             x['District']=district\n",
      "#             x['Name']=item['venue']['name']\n",
      "#             x['Category']=item['venue']['categories'][0]['name']\n",
      "#             x['Lat'] = item['venue']['location']['lat']\n",
      "#             x['Lon'] = item['venue']['location']['lng']\n",
      "#             x['VenueId'] = item['venue']['id']\n",
      "#             district_venues.append(x)\n",
      "#         district_df = pd.DataFrame(district_venues)\n",
      "#         all_city_venues = all_city_venues.append(district_df)\n",
      "#     return all_city_venues\n",
      "150/461: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/462:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_df = extract_data[city_districts_venues]\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/463:\n",
      "# def get_districts_venues(city_districts_venues):\n",
      "#     all_city_venues= pd.DataFrame()\n",
      "#     for district in city_districts_venues.keys():\n",
      "#         district_venues=[]\n",
      "#         for item in city_districts_venues[district]:\n",
      "#             x={}\n",
      "#             x['District']=district\n",
      "#             x['Name']=item['venue']['name']\n",
      "#             x['Category']=item['venue']['categories'][0]['name']\n",
      "#             x['Lat'] = item['venue']['location']['lat']\n",
      "#             x['Lon'] = item['venue']['location']['lng']\n",
      "#             x['VenueId'] = item['venue']['id']\n",
      "#             district_venues.append(x)\n",
      "#         district_df = pd.DataFrame(district_venues)\n",
      "#         all_city_venues = all_city_venues.append(district_df)\n",
      "#     return all_city_venues\n",
      "150/464: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/465:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_df = extract_data(city_districts_venues[district])\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/466:\n",
      "# def get_districts_venues(city_districts_venues):\n",
      "#     all_city_venues= pd.DataFrame()\n",
      "#     for district in city_districts_venues.keys():\n",
      "#         district_venues=[]\n",
      "#         for item in city_districts_venues[district]:\n",
      "#             x={}\n",
      "#             x['District']=district\n",
      "#             x['Name']=item['venue']['name']\n",
      "#             x['Category']=item['venue']['categories'][0]['name']\n",
      "#             x['Lat'] = item['venue']['location']['lat']\n",
      "#             x['Lon'] = item['venue']['location']['lng']\n",
      "#             x['VenueId'] = item['venue']['id']\n",
      "#             district_venues.append(x)\n",
      "#         district_df = pd.DataFrame(district_venues)\n",
      "#         all_city_venues = all_city_venues.append(district_df)\n",
      "#     return all_city_venues\n",
      "150/467: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/468: warsaw_districts_venues.shape\n",
      "150/469: warsaw_map\n",
      "150/470: cracow_geojson\n",
      "150/471: cracow_geojson['features']\n",
      "150/472:\n",
      "cracow_districts_names = []\n",
      "for x in warsaw_geojson['features']:\n",
      "    districts_names.append(x['properties']['nazwa'])\n",
      "150/473:\n",
      "cracow_districts_names = []\n",
      "for x in cracow_geojson['features']:\n",
      "    districts_names.append(x['properties']['nazwa'])\n",
      "150/474: cracow_districts_names\n",
      "150/475: cracow_geojson['features'][0]\n",
      "150/476:\n",
      "cracow_districts_names = []\n",
      "for x in cracow_geojson['features']:\n",
      "    districts_names.append(x['properties']['nazwa'])\n",
      "150/477: cracow_geojson['features'][0]\n",
      "150/478: cracow_districts_names\n",
      "150/479:\n",
      "cracow_districts_names = []\n",
      "for x in cracow_geojson['features']:\n",
      "    cracow_districts_names.append(x['properties']['nazwa'])\n",
      "150/480: cracow_geojson['features'][0]\n",
      "150/481: cracow_districts_names\n",
      "150/482: cracow_test_density = list(range(len(districts_names)))\n",
      "150/483:\n",
      "df = pd.DataFrame([cracow_districts_names, test_density]).T\n",
      "df.columns = ['nazwa', 'Density']\n",
      "150/484:\n",
      "def create_cracow_map():    \n",
      "    cracow_coordinates = [50.06143, 19.93658]\n",
      "    cracow_map = folium.Map(location=warsaw_coordinates, zoom_start=11)\n",
      "    cracow_map.choropleth(\n",
      "        geo_data=cracow_geojson,\n",
      "        name='choropleth',\n",
      "        data=df,\n",
      "        columns=['nazwa', 'Density'],\n",
      "        key_on='feature.properties.nazwa',\n",
      "        fill_color='YlGn',\n",
      "        fill_opacity=0.7,\n",
      "        line_opacity=0.9,\n",
      "        legend_name='Density'\n",
      "    )\n",
      "    return cracow_map\n",
      "150/485:\n",
      "cracow_map = create_cracow_map\n",
      "cracow_map\n",
      "150/486:\n",
      "cracow_map = create_cracow_map()\n",
      "cracow_map\n",
      "150/487:\n",
      "def create_cracow_map():    \n",
      "    cracow_coordinates = [50.06143, 19.93658]\n",
      "    cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)\n",
      "    cracow_map.choropleth(\n",
      "        geo_data=cracow_geojson,\n",
      "        name='choropleth',\n",
      "        data=df,\n",
      "        columns=['nazwa', 'Density'],\n",
      "        key_on='feature.properties.nazwa',\n",
      "        fill_color='YlGn',\n",
      "        fill_opacity=0.7,\n",
      "        line_opacity=0.9,\n",
      "        legend_name='Density'\n",
      "    )\n",
      "    return cracow_map\n",
      "150/488:\n",
      "cracow_map = create_cracow_map()\n",
      "cracow_map\n",
      "150/489:\n",
      "def get_cracow_centers(city_geojson):\n",
      "    district_centers = {}\n",
      "    for district in city_geojson['features']:\n",
      "        district_geometry = pd.DataFrame(\n",
      "            district['geometry']['coordinates'][0][0],\n",
      "            columns=['longitude', 'latitude']\n",
      "        )\n",
      "        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]\n",
      "        district_centers[district['properties']['nazwa']]=district_center\n",
      "    \n",
      "    district_centers = pd.DataFrame.from_dict(district_centers,\n",
      "                                              orient='index',\n",
      "                                              columns=['District_center'])\n",
      "    \n",
      "    return district_centers\n",
      "150/490: cracow_districts_centers = get_cracow_centers(cracow_geojson)\n",
      "150/491: cracow_districts_centers\n",
      "150/492: cracow_districts_centers.head()\n",
      "150/493:\n",
      "for center in cracow_districts_centers.index:\n",
      "    folium.Marker(cracow_districts_centers.loc[center, 'District_center']).add_to(cracow_map)\n",
      "cracow_map\n",
      "150/494:\n",
      "def create_cracow_polygons(geojson):\n",
      "    polygons = {}\n",
      "    for district in geojson['features']:\n",
      "        polygons[district['properties']['nazwa']]=district['geometry']['coordinates'][0][0]\n",
      "    return polygons\n",
      "150/495: cracow_districts_polygons = cracow_districts_polygons(warsaw_geojson)\n",
      "150/496: cracow_districts_polygons = create_cracow_polygons(cracow_geojson)\n",
      "150/497:\n",
      "def calculate_cracow_radius(district_name):\n",
      "    center_coords = cracow_district_centers.loc[district_name, 'District_center']\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in cracow_districts_polygons [district_name]:\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_points.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    max_distance = max(distances)\n",
      "    factor = 0.6157*111.3*1000\n",
      "    max_distance_meters = int(max_distance*factor)\n",
      "    \n",
      "    return max_distance_meters\n",
      "150/498:\n",
      "cracow_districts_radiuses = {}\n",
      "for district in cracow_district_centers.index:\n",
      "    cracow_districts_radiuses[district] = calculate_radius(district)\n",
      "150/499:\n",
      "cracow_districts_radiuses = {}\n",
      "for district in cracow_districts_centers.index:\n",
      "    cracow_districts_radiuses[district] = calculate_radius(district)\n",
      "150/500:\n",
      "cracow_districts_radiuses = {}\n",
      "for district in cracow_districts_centers.index:\n",
      "    cracow_districts_radiuses[district] = calculate_radius(district)\n",
      "150/501:\n",
      "cracow_districts_radiuses = {}\n",
      "for district in cracow_districts_centers.index:\n",
      "    cracow_districts_radiuses[district] = calculate_cracow_radius(district)\n",
      "150/502:\n",
      "def calculate_cracow_radius(district_name):\n",
      "    center_coords = cracow_districts_centers.loc[district_name, 'District_center']\n",
      "    center_point = Point(center_coords[1], center_coords[0])\n",
      "    \n",
      "    polygon_points = []\n",
      "    for point_coords in cracow_districts_polygons [district_name]:\n",
      "        polygon_point = Point(point_coords[0], point_coords[1])\n",
      "        polygon_points.append(polygon_point)\n",
      "    \n",
      "    distances = []\n",
      "    for point in polygon_points:\n",
      "        distance_to_center = center_point.distance(point)\n",
      "        distances.append(distance_to_center)\n",
      "        \n",
      "    max_distance = max(distances)\n",
      "    factor = 0.6157*111.3*1000\n",
      "    max_distance_meters = int(max_distance*factor)\n",
      "    \n",
      "    return max_distance_meters\n",
      "150/503:\n",
      "cracow_districts_radiuses = {}\n",
      "for district in cracow_districts_centers.index:\n",
      "    cracow_districts_radiuses[district] = calculate_cracow_radius(district)\n",
      "150/504:\n",
      "cracow_districts_venues_foursquare = {}\n",
      "for district in cracow_district_centers.index:\n",
      "    print(district)\n",
      "    cracow_districts_venues_foursquare[district] = get_venues(\n",
      "        cracow_district_centers.loc[district, 'District_center'],\n",
      "        cracow_districts_radiuses[district])\n",
      "150/505:\n",
      "cracow_districts_venues_foursquare = {}\n",
      "for district in cracow_districts_centers.index:\n",
      "    print(district)\n",
      "    cracow_districts_venues_foursquare[district] = get_venues(\n",
      "        cracow_districts_centers.loc[district, 'District_center'],\n",
      "        cracow_districts_radiuses[district])\n",
      "150/506:\n",
      "for i in cracow_districts_venues.index:    \n",
      "    folium.Marker([cracow_districts_venues.loc[i,'Lat'], cracow_districts_venues.loc[i,'Lon']]).add_to(cracow_map)\n",
      "150/507: cracow_districts_venues = get_districts_venues(cracow_districts_venues_foursquare)\n",
      "150/508:\n",
      "for i in cracow_districts_venues.index:    \n",
      "    folium.Marker([cracow_districts_venues.loc[i,'Lat'], cracow_districts_venues.loc[i,'Lon']]).add_to(cracow_map)\n",
      "150/509: cracow_districts_venues\n",
      "150/510: cracow_districts_venues.loc[cracow_districts_venues.Lat is False]\n",
      "150/511: cracow_districts_venues.loc[cracow_districts_venues.Lon is False]\n",
      "150/512: cracow_districts_venues.loc[cracow_districts_venues.isna().any(axis=1)]\n",
      "150/513: cracow_districts_venues[cracow_districts_venues.isna().any(axis=1)]\n",
      "150/514: cracow_districts_venues[cracow_districts_venues.isna().any()]\n",
      "150/515: cracow_districts_venues.isna().any()\n",
      "150/516: cracow_districts_venues = get_districts_venues(cracow_districts_venues_foursquare)\n",
      "150/517: cracow_districts_venues.isna().any()\n",
      "150/518:\n",
      "for i in cracow_districts_venues.index:    \n",
      "    folium.Marker([cracow_districts_venues.loc[i,'Lat'], cracow_districts_venues.loc[i,'Lon']]).add_to(cracow_map)\n",
      "150/519: cracow_districts_venues#.isna().any()\n",
      "150/520: [cracow_districts_venues.loc[0,'Lat'], cracow_districts_venues.loc[0,'Lon']]\n",
      "150/521: [cracow_districts_venues.loc[0,'Lat']#, cracow_districts_venues.loc[0,'Lon']]\n",
      "150/522: cracow_districts_venues.loc[0,'Lat'], cracow_districts_venues.loc[0,'Lon']\n",
      "150/523: cracow_districts_venues.loc[1,'Lat'], cracow_districts_venues.loc[0,'Lon']\n",
      "150/524: cracow_districts_venues.loc[1,'Lat']#, cracow_districts_venues.loc[0,'Lon']\n",
      "150/525: cracow_districts_venues.iloc[1,'Lat']#, cracow_districts_venues.loc[0,'Lon']\n",
      "150/526: cracow_districts_venues[1,'Lat']#, cracow_districts_venues.loc[0,'Lon']\n",
      "150/527: cracow_districts_venues.loc[1,'Lat']#, cracow_districts_venues.loc[0,'Lon']\n",
      "150/528: cracow_districts_venues.loc[[1,'Lat']]#, cracow_districts_venues.loc[0,'Lon']\n",
      "150/529: cracow_districts_venues.loc[1,'Lat']#, cracow_districts_venues.loc[0,'Lon']\n",
      "150/530: cracow_districts_venues.at[1,'Lat']#, cracow_districts_venues.loc[0,'Lon']\n",
      "150/531: cracow_districts_venues.at[0,'Lat']#, cracow_districts_venues.loc[0,'Lon']\n",
      "150/532: cracow_districts_venues.at[0,'Lon']#, cracow_districts_venues.loc[0,'Lon']\n",
      "150/533: cracow_districts_venues.loc[0]['Lon']#, cracow_districts_venues.loc[0,'Lon']\n",
      "150/534: cracow_districts_venues.iloc[0]['Lon']#, cracow_districts_venues.loc[0,'Lon']\n",
      "150/535:\n",
      "for i in cracow_districts_venues.index:    \n",
      "    folium.Marker([cracow_districts_venues.iloc[i]['Lat'], cracow_districts_venues.iloc[i]['Lon']]).add_to(cracow_map)\n",
      "150/536: cracow_map\n",
      "150/537: cracow_districts_venues.head()\n",
      "150/538: warsaw_districts_venues.reset_index(inplace=True)\n",
      "150/539:\n",
      "for i in cracow_districts_venues.index:    \n",
      "    folium.Marker([cracow_districts_venues.iloc[i]['Lat'], cracow_districts_venues.iloc[i]['Lon']]).add_to(cracow_map)\n",
      "150/540: cracow_map\n",
      "150/541: cracow_districts_venues.head(5-)\n",
      "150/542: cracow_districts_venues.head(50)\n",
      "150/543: cracow_districts_venues.tail(50)\n",
      "150/544: cracow_districts_venues.reset_index(inplace=True)\n",
      "150/545: cracow_districts_venues.tail(50)\n",
      "150/546:\n",
      "for i in cracow_districts_venues.index:    \n",
      "    folium.Marker([cracow_districts_venues.iloc[i]['Lat'], cracow_districts_venues.iloc[i]['Lon']]).add_to(cracow_map)\n",
      "150/547: cracow_map\n",
      "150/548:\n",
      "for i in cracow_districts_venues.index:\n",
      "    cracow_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [cracow_districts_venues.loc[i,'Lon'], cracow_districts_venues.loc[i,'Lat']],\n",
      "                                        cracow_districts_polygons[cracow_districts_venues.loc[i,'District']])\n",
      "150/549: cracow_district_venues\n",
      "150/550: cracow_districts_venues\n",
      "150/551:\n",
      "for i in cracow_districts_venues.index:\n",
      "    cracow_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [cracow_districts_venues.iloc[i]['Lon'], cracow_districts_venues.loc[i]['Lat']],\n",
      "                                        cracow_districts_polygons[cracow_districts_venues.iloc[i]['District']])\n",
      "150/552: warsaw_districts_venues.head()\n",
      "150/553:\n",
      "def check_if_inside_district(venue_coords, district_shape):\n",
      "    p = Point(venue_coords)\n",
      "    poly = Polygon(district_shape)\n",
      "    return p.within(poly)\n",
      "150/554: warsaw_districts_venues.reset_index(inplace=True)\n",
      "150/555:\n",
      "for i in warsaw_districts_venues.index:\n",
      "    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],\n",
      "                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])\n",
      "150/556: warsaw_districts_venues_foursquare\n",
      "150/557:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_df = extract_data(city_districts_venues[district])\n",
      "        all_city_venues = all_city_venues.append({district: district_df})\n",
      "    return all_city_venues\n",
      "150/558: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/559:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_df = pd.DataFrame({district:extract_data(city_districts_venues[district])})\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/560: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/561:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_df = extract_data(city_districts_venues[district])\n",
      "        district_df['District'] = district\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/562: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/563: warsaw_districts_venues.head()\n",
      "150/564:\n",
      "def extract_data(district,district_venues_foursquare):\n",
      "    district_venues=[]\n",
      "    for item in district_venues_foursquare:\n",
      "        x={}\n",
      "        x['District']=district\n",
      "        x['Name']=item['venue']['name']\n",
      "        x['Category']=item['venue']['categories'][0]['name']\n",
      "        x['Lat'] = item['venue']['location']['lat']\n",
      "        x['Lon'] = item['venue']['location']['lng']\n",
      "        x['VenueId'] = item['venue']['id']\n",
      "        district_venues.append(x)\n",
      "    district_df = pd.DataFrame(district_venues)\n",
      "    return district_df\n",
      "150/565:\n",
      "def get_districts_venues(city_districts_venues):\n",
      "    all_city_venues= pd.DataFrame()\n",
      "    for district in city_districts_venues.keys():\n",
      "        district_df = extract_data(district, city_districts_venues[district])\n",
      "        district_df['District'] = district\n",
      "        all_city_venues = all_city_venues.append(district_df)\n",
      "    return all_city_venues\n",
      "150/566: warsaw_districts_venues = get_districts_venues(warsaw_districts_venues_foursquare)\n",
      "150/567: warsaw_districts_venues.head()\n",
      "150/568:\n",
      "def check_if_inside_district(venue_coords, district_shape):\n",
      "    p = Point(venue_coords)\n",
      "    poly = Polygon(district_shape)\n",
      "    return p.within(poly)\n",
      "150/569: warsaw_districts_venues.reset_index(inplace=True)\n",
      "150/570:\n",
      "for i in warsaw_districts_venues.index:\n",
      "    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],\n",
      "                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])\n",
      "150/571: cracow_districts_venues = get_districts_venues(cracow_districts_venues_foursquare)\n",
      "150/572: cracow_districts_venues.reset_index(inplace=True)\n",
      "150/573:\n",
      "for i in cracow_districts_venues.index:\n",
      "    cracow_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
      "                                        [cracow_districts_venues.iloc[i]['Lon'], cracow_districts_venues.loc[i]['Lat']],\n",
      "                                        cracow_districts_polygons[cracow_districts_venues.iloc[i]['District']])\n",
      "150/574:\n",
      "for i in cracow_districts_venues.index:    \n",
      "    folium.Marker([cracow_districts_venues.iloc[i]['Lat'], cracow_districts_venues.iloc[i]['Lon']]).add_to(cracow_map)\n",
      "150/575: cracow_districts_venues\n",
      "150/576: cracow_districts_venues = cracow_districts_venues.loc[cracow_districts_venues['Inside']==True]\n",
      "150/577: cracow_top5 = cracow_districts_venues.groupby('Category').count().sort_values(by='VenueId', ascending=False).head()\n",
      "150/578: cracow_top5\n",
      "150/579: cracow_districts_venues\n",
      "150/580:\n",
      "for i in cracow_districts_venues.index:    \n",
      "    folium.Marker([cracow_districts_venues.loc[i,'Lat'], cracow_districts_venues.loc[i,'Lon']]).add_to(cracow_map)\n",
      "150/581: cracow_map\n",
      "150/582:\n",
      "cracow_map = create_cracow_map()\n",
      "cracow_map\n",
      "150/583:\n",
      "for i in cracow_districts_venues.index:    \n",
      "    folium.Marker([cracow_districts_venues.loc[i,'Lat'], cracow_districts_venues.loc[i,'Lon']]).add_to(cracow_map)\n",
      "150/584: cracow_map\n",
      "150/585: top_categories = warsaw_top5.add(cracow_top5, fill_value=0)\n",
      "150/586: top_categories\n",
      "150/587: warsaw_top5\n",
      "150/588: top_5_categories = top_categories.head()\n",
      "150/589: top_5_categories = top_categories.head().index\n",
      "150/590: top_5_categories\n",
      "150/591: warsaw_districts_wiki = 'https://pl.wikipedia.org/wiki/Podział_administracyjny_Warszawy'#Dzielnice_Warszawy\n",
      "150/592: warsaw_districts_wiki = 'https://pl.wikipedia.org/wiki/Podział_administracyjny_Warszawy'\n",
      "150/593: cracow_districts_wiki = 'https://pl.wikipedia.org/wiki/Podział_administracyjny_Krakowa'\n",
      "150/594: warsaw_districts_df = pd.read_html(warsaw_districts_wiki)\n",
      "150/595: warsaw_districts_df = pd.read_html(warsaw_districts_wiki)\n",
      "150/596: warsaw_districts_df = pd.read_html(warsaw_districts_wiki)\n",
      "153/1: %history -g\n",
      "   1: %history -g -f the_battle_of_neighborhoods.ipynb\n",
      "   2: %history -g -f the_battle_of_neighborhoods_xxx.ipynb\n",
      "   3: %history -g\n",
      "   4: %history\n",
      "   5: %history -g -f history_file.json\n",
      "   6: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pin_venues(venues_df,city_map):\n",
    "    for i in venues_df.index:\n",
    "        folium.Marker([venues_df.loc[i,'Lat'], venues_df.loc[i,'Lon']]).add_to(city_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_districts_venues.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in warsaw_districts_venues.index:\n",
    "    warsaw_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
    "                                        [warsaw_districts_venues.loc[i,'Lon'], warsaw_districts_venues.loc[i,'Lat']],\n",
    "                                        warsaw_districts_polygons[warsaw_districts_venues.loc[i,'District']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_districts_venues = warsaw_districts_venues.loc[warsaw_districts_venues['Inside']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_top5 = warsaw_districts_venues.groupby('Category').count().sort_values(by='VenueId', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>District</th>\n",
       "      <th>Name</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "      <th>VenueId</th>\n",
       "      <th>Inside</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Café</th>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Park</th>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italian Restaurant</th>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coffee Shop</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Supermarket</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    index  District  Name  Lat  Lon  VenueId  Inside\n",
       "Category                                                            \n",
       "Café                   66        66    66   66   66       66      66\n",
       "Park                   56        56    56   56   56       56      56\n",
       "Italian Restaurant     49        49    49   49   49       49      49\n",
       "Coffee Shop            38        38    38   38   38       38      38\n",
       "Supermarket            36        36    36   36   36       36      36"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warsaw_top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cracow_districts_names = []\n",
    "for x in cracow_geojson['features']:\n",
    "    cracow_districts_names.append(x['properties']['nazwa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cracow_test_density = list(range(len(cracow_districts_names)))\n",
    "df = pd.DataFrame([cracow_districts_names, test_density]).T\n",
    "df.columns = ['nazwa', 'Density']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cracow_map():    \n",
    "    cracow_coordinates = [50.06143, 19.93658]\n",
    "    cracow_map = folium.Map(location=cracow_coordinates, zoom_start=11)\n",
    "    cracow_map.choropleth(\n",
    "        geo_data=cracow_geojson,\n",
    "        name='choropleth',\n",
    "        data=df,\n",
    "        columns=['nazwa', 'Density'],\n",
    "        key_on='feature.properties.nazwa',\n",
    "        fill_color='YlGn',\n",
    "        fill_opacity=0.7,\n",
    "        line_opacity=0.9,\n",
    "        legend_name='Density'\n",
    "    )\n",
    "    return cracow_map\n",
    "\n",
    "cracow_map = create_cracow_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cracow_polygons(geojson):\n",
    "    polygons = {}\n",
    "    for district in geojson['features']:\n",
    "        polygons[district['properties']['nazwa']]=district['geometry']['coordinates'][0][0]\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cracow_centers(city_geojson):\n",
    "    district_centers = {}\n",
    "    for district in city_geojson['features']:\n",
    "        district_geometry = pd.DataFrame(\n",
    "            district['geometry']['coordinates'][0][0],\n",
    "            columns=['longitude', 'latitude']\n",
    "        )\n",
    "        district_center = [[district_geometry['latitude'].mean(), district_geometry['longitude'].mean()]]\n",
    "        district_centers[district['properties']['nazwa']]=district_center\n",
    "    \n",
    "    district_centers = pd.DataFrame.from_dict(district_centers,\n",
    "                                              orient='index',\n",
    "                                              columns=['District_center'])\n",
    "    \n",
    "    return district_centers\n",
    "cracow_districts_centers = get_cracow_centers(cracow_geojson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cracow_districts_polygons = create_cracow_polygons(cracow_geojson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cracow_radius(district_name):\n",
    "    center_coords = cracow_districts_centers.loc[district_name, 'District_center']\n",
    "    center_point = Point(center_coords[1], center_coords[0])\n",
    "    \n",
    "    polygon_points = []\n",
    "    for point_coords in cracow_districts_polygons [district_name]:\n",
    "        polygon_point = Point(point_coords[0], point_coords[1])\n",
    "        polygon_points.append(polygon_point)\n",
    "    \n",
    "    distances = []\n",
    "    for point in polygon_points:\n",
    "        distance_to_center = center_point.distance(point)\n",
    "        distances.append(distance_to_center)\n",
    "        \n",
    "    max_distance = max(distances)\n",
    "    factor = 0.6157*111.3*1000\n",
    "    max_distance_meters = int(max_distance*factor)\n",
    "    \n",
    "    return max_distance_meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cracow_districts_radiuses = {}\n",
    "for district in cracow_districts_centers.index:\n",
    "    cracow_districts_radiuses[district] = calculate_cracow_radius(district)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stare Miasto\n",
      "\tTotal results:  215 \n",
      "\n",
      "Grzegórzki\n",
      "\tTotal results:  155 \n",
      "\n",
      "Prądnik Czerwony\n",
      "\tTotal results:  63 \n",
      "\n",
      "Prądnik Biały\n",
      "\tTotal results:  118 \n",
      "\n",
      "Krowodrza\n",
      "\tTotal results:  204 \n",
      "\n",
      "Bronowice\n",
      "\tTotal results:  91 \n",
      "\n",
      "Zwierzyniec\n",
      "\tTotal results:  133 \n",
      "\n",
      "Dębniki\n",
      "\tTotal results:  51 \n",
      "\n",
      "Łagiewniki-Borek Fałęcki\n",
      "\tTotal results:  37 \n",
      "\n",
      "Swoszowice\n",
      "\tTotal results:  50 \n",
      "\n",
      "Podgórze Duchackie\n",
      "\tTotal results:  56 \n",
      "\n",
      "Bieżanów-Prokocim\n",
      "\tTotal results:  28 \n",
      "\n",
      "Podgórze\n",
      "\tTotal results:  209 \n",
      "\n",
      "Czyżyny\n",
      "\tTotal results:  84 \n",
      "\n",
      "Mistrzejowice\n",
      "\tTotal results:  28 \n",
      "\n",
      "Bieńczyce\n",
      "\tTotal results:  41 \n",
      "\n",
      "Wzgórza Krzesławickie\n",
      "\tTotal results:  17 \n",
      "\n",
      "Nowa Huta\n",
      "\tTotal results:  31 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cracow_districts_venues_foursquare = {}\n",
    "for district in cracow_districts_centers.index:\n",
    "    print(district)\n",
    "    cracow_districts_venues_foursquare[district] = get_venues(\n",
    "        cracow_districts_centers.loc[district, 'District_center'],\n",
    "        cracow_districts_radiuses[district])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cracow_districts_venues = get_districts_venues(cracow_districts_venues_foursquare)\n",
    "cracow_districts_venues.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cracow_districts_venues.index:\n",
    "    cracow_districts_venues.loc[i, 'Inside'] = check_if_inside_district(\n",
    "                                        [cracow_districts_venues.iloc[i]['Lon'], cracow_districts_venues.loc[i]['Lat']],\n",
    "                                        cracow_districts_polygons[cracow_districts_venues.iloc[i]['District']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cracow_districts_venues = cracow_districts_venues.loc[cracow_districts_venues['Inside']==True]\n",
    "cracow_top5 = cracow_districts_venues.groupby('Category').count().sort_values(by='VenueId', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cracow_map = create_cracow_map()\n",
    "for i in cracow_districts_venues.index:    \n",
    "    folium.Marker([cracow_districts_venues.loc[i,'Lat'], cracow_districts_venues.loc[i,'Lon']]).add_to(cracow_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_categories = warsaw_top5.add(cracow_top5, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Café', 'Coffee Shop', 'Hotel', 'Italian Restaurant', 'Park'], dtype='object', name='Category')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_categories = top_categories.head().index\n",
    "top_5_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_districts_wiki = 'https://pl.wikipedia.org/wiki/Podział_administracyjny_Warszawy'\n",
    "cracow_districts_wiki = 'https://pl.wikipedia.org/wiki/Podział_administracyjny_Krakowa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "BeautifulSoup4 (bs4) not found, please install it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-1ca74f9538f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwarsaw_districts_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarsaw_districts_wiki\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/coursera/lib/python3.7/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0mkeep_default_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_default_na\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m         \u001b[0mdisplayed_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplayed_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/envs/coursera/lib/python3.7/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[0mretained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflav\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflavor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled_match\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayed_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/coursera/lib/python3.7/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_parser_dispatch\u001b[0;34m(flavor)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"html5lib not found, please install it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_HAS_BS4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BeautifulSoup4 (bs4) not found, please install it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m         \u001b[0;31m# Although we call this above, we want to raise here right before use.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mbs4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bs4\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa:F841\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: BeautifulSoup4 (bs4) not found, please install it"
     ]
    }
   ],
   "source": [
    "warsaw_districts_df = pd.read_html(warsaw_districts_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coursera] *",
   "language": "python",
   "name": "conda-env-coursera-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
